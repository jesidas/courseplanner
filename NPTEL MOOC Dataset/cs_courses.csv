xZk8qwG0AM8,Arithmetic operations  Let's start learning the syntax of Python  First of all  you have arithmetic operations  and I'll quickly go through a sample of those  I'll show you how they work  13 + 3 is 16  13- 3 gives you 10  13 x 3 is 39  2 raised to the 3rd power is 8  13 divided by 3 is 4  13 divided by 3 in this way  this is an integer line  and divide it to and find the integer and throw away the fraction  it's just 4  And 13% 3 Is the remainder upon dividing 13 by 3 
FJNAokhimfg,Our first functions  Now let's look at a simple function in Python  The kernel that we have  percent  pound percent sent  and pound percent sent  and they mark out a cell  These lines show you the boundaries of the cell  Here we have a function defined within a cell  and we can execute that by typing Ctrl  Enter  On a Macintosh it would be command return  Now this function can be run by typing in the console the name of the function  Hello  world! A couple of things to note about this  One is it's got a single statement  Print hello world  And it does that  And this statement is indented four spaces  one  two  three  four  We always indent four spaces we don't use tabs that can sometimes cause confusion on the part of python  I've also got a string here enclosed in triple quotes  This string can be used to document the program  and if you notice up here in [INAUDIBLE] inspector  you can get the definitions of various functions and we can get the definition of our function by clicking on hello  Yeah  I'm here type in control I  Then our function shows up here in this triple queried string is a documentation so that you can put instructions on how to run your program for example in that or your function  Now let's write another function  Here's the area of a circle of a given radius  Again we've got pound percent cent to mark out the cell  We can execute this [INAUDIBLE] anywhere in the cell and type in control enter or command return on a Macintosh  That puts  that loads it into the console window over here  And now we can run it by typing  Area of a circle  the name of the function  And I'll give it a value of five for the radius  It prints out the area of a circle of radius 5 is 78 5  Now let's look at does that first of all  we have the formula for the area of a circle  Area is equal to pi  3 14 times the radius  squared  Just a simple formula for the area of a circle  Now  we print the phrase  the area of a circle of radius  all about the radius is all about the area that we computed  Now you try writing a function  here is an exercise to do  Write a function def area triangle BH where B is the base and H is the height of the triangle  to compute the area of a triangle the formula is area equals one half base times the height  output should look like the following  The area of a triangle of base 3 and height 5 is 7 and a half  You can test your function by executing the following code  Here's a triangle of base 3 and height 5  here's one of base 2 and height 20  Here is my run of this  The area of a triangle of base 3 and height 5 is 7 5  The area of a triangle of base 2 and height 20 is 20  Now you try it  when you get through  press continue and I'll show you my solution  Okay  here is my solution  Definition area of triangle  bh is area equal one-half base times the height  just that same formula  Print the area of a triangle of base  b  and height h  is area 
gbOc1Hw5Ifc,Creating strings and using them in print statements  You can form a string in Python by using quotes  either single quotes or double quotes  it doesn't matter  Here are a couple of examples  This is using double quotes to form the string  And this just uses single quotes to form a string  Now  there is one condition here  This string has a single quote in it  So to use single quotes outside to form the string would cause some confusion  Here's one that has double quotes in it  and to use double quotes outside to form a string would cause confusion  But if you've got a single quote  you can use double quotes outside to form the string  If you've got double quotes in it  we can use single quotes outside to form a string  As you can see  both of those worked  Now  what if you've got a single quote and a double quote in the string? Here's an example on how you can do that  If you have both  here's  My cat's name is Butters is what I want to write  All right  so I use double quotes outside  but I've gotta tell Python that this double quote and this double quote are not the double quotes that mark the string  So I want it to print an actual quote  so I put a backslash in front of that and a backslash in front of this one  And that tells it that this next character is actually to be printed as is  So that's called escaping that character  So the escape symbol is a backslash  Now let's write another Python function  I want to compute  I want to convert Fahrenheit degrees to Celsius degrees  So this is the function that's going to do that  It's called fahrenheit_to_celsius  I want you to run it once or twice so that you get an idea of what it does  F-A-R-H  So I converted 212 degrees Fahrenheit to 100 degrees Celsius  That should be a familiar [COUGH] conversion  Now let's take a look at the function and see how that does that  First of all  we have a description in triple quotes  And so if I click on this  I'll see that our function is documented up here  Next  note that I have two lines that begin with pound sign  Anything beyond a pound sign on a given line is not executed by Python  it's just simply ignored  So you can add comments to your code  Next we have the formula  The new temperature is 5 times the Fahrenheit temperature minus 32 divided 9  it's five-ninths  so whatever number of degrees above 32 we have  Then we'll print the Fahrenheit temperature  and the temperature  is equivalent to  and the newly computed temperature  and then end equal  I'll explain that in a minute  degrees Celsius  Now  what has happened here is that my print statement is going to go too long  it's going to be too wide to print out on a piece of paper if I had to  So I've run up against the edge  you might say  of the paper  And I'll need to have two print statements  But the problem of two print statements is that they would be on different lines  So to eliminate creating a new line character and causing it to have two lines  I'm going to say end=''  So instead of the new line character  I'm just going to print a blank  Well  I'm actually going to print nothing  because they're jammed together  So what that effectively does is it keeps it all on one line  And here you can see that it all kept on one line  Let's run it one more time on a familiar temperature  32 degrees Fahrenheit is a commonly known Celsius temperature  that's 0 degrees Celsius  Now it's your turn to write such a function  Here's an exercise  write a function Celsius to Fahrenheit that will convert Celsius temperatures back to Fahrenheit degrees  Now  the formula for that is nine-fifths times the temperature plus 32 degrees  Now  I'm giving you my example run  and here are three test values  100 degrees Celsius  0 degrees Celsius  50 degrees Celsius  And here's what my program did with those  100 degrees Celsius  it wrote the Celsius temperature at 100 is equivalent to 212 degrees Fahrenheit  So you can write your function and then use these statements to test your function  and you can see if you get the same thing that I get  Now  press the continuation button when you've got yours running  and I'll show you mine  Okay  here's my solution  The Celsius to Fahrenheit temperature is  and I've got a documentation line here  then do the computation  the new temperature is nine-fifths times the temperature plus 32 degrees  And then I've got a print line that prints the Celsius temperature  and I give it as equivalent to  and I give the new computed value  And then I use end='' so that I don't have a new line  And then I can continue this statement on the next line 
xs7M3wmoGUc,"The ""input"" statement and combining strings  Now let's write a function that takes input from the keyboard and also combines strings  Here's an example  This function is supposed to get my name and then write it out  All right  so I write name  My name is Bill  that was first name  My last names Boyd and it will write out your name is Bill Boyd  So  let's look and see how that worked  First of all you know we still have the documentation and I can see that at the top  I type in Ctrl+I  okay  I'm using an input statement to get from the keyboard  Now I have to have a prompt  so that the person will know what I want to be entered  So  the input statement has a prompt as a string  enter your first name  you see it here  And then I typed in Bill and it stored that in this variable called fname  F for first name  And then I ask for another input  and I ask for last name this time is started into a variable called lname for last name  Now I make the full name by taking the first name concatenate or adding the string  A little blank to separate the first name from the last name  And then I'll add on the last name  So  I've got the full name put together here and then I'm going to print it together  print it out  Print \""your name is\"" and we'll print the full name  So  what's new here is with the input statement and with concatenation  or adding of strings  Now  you try it  I want you to take the same function and add a couple of features to it  In addition to getting your name  putting it together into a string and printing it  I want you to get the city that you live in and the state or nation that you live in  And write that out in the fashion that I'm showing you here  Here is my run  Enter your first name  Bill  Enter your last name  Boyd  Enter the city you live in  Middletown  Enter the state you live in  my case Connecticut  okay? Your name is Bill Boyd  This is from the previous program example  And now  you live in Middletown  and I've put that Middletown down  Connecticut  And I made a longer string that has Middletown  Connecticut in it  but they're separated by a comma  So  when you go and try and write this your self and then when you get through  press the continuation button and I'll show you my example  Okay  here is my solution  I have documentation  if you like to see that  Input first and last name  this documentation is useful and that gives you instructions to some work to do  Input first and last name  Combine to one string and print  Also input the city and state and print  Now  this is exactly what we had before that's not new but here's an input statement that gets the city  Here's an input statement that gets the state and then I combine the city and state into one string  And that string has got comma space in the middle of it to separate city from state  Now I can print your name is and print you live in  city  state  And you don't want to put this input statement after you've printed out the name  Because then over here it wouldn't have collected all the data and then written out the result  It would have connected part of the data or written out part of the result  collected some more data and written out the rest of the result  That just wouldn't be coot "
-2hpoIjNSb4,"Using the ""if"" statement  Now let's take up the if statement  Here's an example function that uses the if statement  And here's how it works  I'm going to set x = 5  y = 0 and z = 0  And then I'm going to test it  I'm going to test if x is greater than 0  current x is positive  If y is greater than 0  I'll print y is positive else  I'll print y is not positive  Let's see how that works  So x is 5  So when we get down to this statement  we're really comparing 5 to greater than 0  which is true  So  we'll print that x is positive  And there you see it's done  okay? If y is greater than 0  print y as positive else  it'll print y as not positive  But y is 0  So  y greater than 0 is not true so  it will not print this  It will print y is not positive instead and you see that is the case  And then  you've got an else if statement  Let's take z  We'll test to see if z is greater than 0  Well  that's not true  it is 0  So we won't print z is positive  Then we'll check if z is greater than 0  which it's not  else if  That's what elif means  else if z is less than 0  we'll print z is negative  Well  it's not negative either so that won't happen  And else will take care of anything else  It'll print z must be 0  and that's what it prints  Okay  if we change the values of these  things would work differently  Okay? So you got if  notice the if statement  There's a condition that you test to see whether it's true and then it ends on a colon  And then anything that's indented  four spaces underneath that  get's executed if this is true  In the case of this  here's another if statement  notice it end to colon  This stuff that's indented  if this is true  will get executed  Else  this gets executed  And this got executed  notice else has a colon  If z is greater than 0  print z is positive  else if z less than 0  print z is negative  notice the colon  else  print z is must be 0  Now I want to talk about equal signs for just a minute  Equal is an assignment  okay? If I want to set x equal to 5  I set it this way  If I want to test to see if x is equal to 5  I use double equals  If I want to check to see if x is not equal to 5  I put exclamation point equals  So this is assignment  This test for equality  this test test for not equals  Here are some examples  We give those x  y  and z values  And then  we'll try them out on these statements  Okay? This statement says print x = y  okay? So we test to see if x = y  And that will either be true or false  X is equal to y  is true so it prints true  This has a value of true and it prints that  Okay? Print x is not equal to y  so we'll test for not equal  Well  they are equal so this is false  X is not equals to y is false and that's what it prints  If x is equal to z  we'll text to see if x is equals to z  Well  x is 5 and z is 6 so that's just not true that x is equal to z  And x is equal to z since it's not true  it's false  okay? The last one is we'll check to see if x is not equal to z  And x is not equal to z  x is 5 and z is 6  So x not equal to z is true  and that's what it prints  Here's an example of a function that makes use of the double equals for comparison  as well as the equals for assignment  and does a computation  Here  let me run it first and then we'll talk about it  This function  it completes the area of a square or a circle  First of all  I've got to tell it whether we're talking about a circle or square  okay? If I tell it that we want the area of a circle  then I give it the radius of the circle  5  to say and it computes the area of the circle  If we say that we're computing the area of a square and I give it a 5  then it computes a different number  Let's see how it does that  Well  the definition of the function here  and I use a variable type  which will be either circle or square and I give either the radius if it's a circle or the length of a side if it's a square  Now we have our usual documentation here  Tells you how to run it  Now we compare type  To circle  Notice we used the double equal  If type == \""circle\""  then we'll do the following  We'll do these two lines  They're both indented the same under that if statement  Else  if type == \""square\""  then we'll do these two statements and they're both indented four spaces underneath the else if  Else will print  I don't know that one  Let's do an example of that  area of a parallelogram  4  okay? So  and by the way  this unusual naming here is because type  I wanted to use type here and type is a keyword in Python  It's a word like print  that has a special meaning  Or elif  that has a special meaning  or if that has a special meaning  or else  that has a special meaning  Type names  the type of the variable  like I said if an integer  or a real number or a string  okay? Now  so  I put an underscore right after that to make it not just the word type  Just a little technical thing  Now  lets check this  if type == \""circle\""  Notice we have the colon at the end of the equal  the if  We have double equals here for this is to check to see if they are equal that doesn't do an assignment  But here is an equal using assignment  We're going to set area equal to pi and x squared  That's the formula for the area of a circle  And then we'll print the area  elif type happens to equal to square  And again  we use the double equals for comparison  And we'll compute the area of a square if that is true  And that's the side spread  And we'll print that area  else we'll just print we don't know  We could expand this by repeating elifs  and add additional types of figures  Now  I want you to write a function that makes use of the if statement  I want you to compute the absolute value of a number  Here is my run of the program on these test data  Absolute value of 5 is 5  The absolute value of -5 is 5  The absolute value of 0 is 0  So there are three runs of my program  I think you should write the program and make the print out look like mine  the absolute value of  Write that string  and then give the number  And then is  and then compute the absolute value of that number and print that  Remember  that if the number is less than 0  you just multiply by minus 1 to make it greater than 0  When you finish writing your program  press  The continue button to continue  Here's my solution  Definition  absolute value  number  colon  If the number is greater than or equal to 0  then the absolute value of the number is the number  Else  the absolute value of the number is the negative of the number  Or minus one times the number  And then  once you've computed the absolute value  then you could print the absolute value of num is abs_num "
c3OLRBt8TI4,Converting strings to numbers  Using the remainder operator  Here's another example of a function converting Fahrenheit temperature to Celsius temperature  Let's try it out  First  it asks me to enter a Fahrenheit temperature and I'll enter 212 degrees  And it converted that to 100 degrees Celsius  so it seems to work fine  So this would seem to be a good program  Now let's take a look at the writing of the program  and then we'll talk more about it  First of all  I have the documentation and I say that it's bad  It does seem to work  Let's go on and look at it and then we'll see why that I say it's bad  The temperature is being input from the keyboard  So it says enter a Fahrenheit temperature  There it is  and we enter 212 degrees  We converted that  now this is input as a string  anything typed in is a string  So we take that string and we convert it to an integer  Can't do arithmetic on it if we don't convert it from a string to an integer  to a number  anyway  Okay  then we compute a new temperature using the formula for converting Fahrenheit temperatures to Celsius temperatures  So now we've got the temperature and the new temperature  Now we'll print the Fahrenheit temperature  temp  is equivalent to  and we're going to keep from going to a new line and then we're going to print a new temperature  the computed Celsius temperature  degrees Celsius  and so it worked  Okay  now why  if it works  do I say that that's bad? Well  let's run it again and I'm going to show you what can go wrong  I'm going to press up arrow  that repeats the previous command  Okay  and this time  instead of typing 212 degrees  I'm just going to simply press Enter  Well  this is an example of a Python error  It says here  invalid literal for int with base 10 and it gives the thing that I entered  I just pressed Return  I didn't give it a real number  And so when it tried to convert it  a blank to an integer  it couldn't  And so the program crashed right there  You can see that the statement that it crashed on is temp = int(temp_str)  So this is not a very robust program  If somebody tried to run the program  they might get this error message and they would be puzzled as to what's going on  Let's try to improve the program and here's fahrenheit_to_celsius2  Now what we want to do  is we want to check to see if they entered something  So after entering the Fahrenheit temperature  I'm going to say  if temp_string and then I'm going to do all of those things  Now temp_string is considered to be true if it's anything other than just empty  okay? This is programming technicality kind of things  Let's run it and see if it corrects the problem  Now I'm going to press up arrow  Two or three times until I get back to this one  I'm going to change it from fahrenheit_to_celsius1 to fahrenheit_to_celsius2  save me some typing  Okay  enter a Fahrenheit temperature  212 works  okay  and we'll run it again  And this time  I'm going to just press Enter  Now it didn't crash  So we made an improvement on this thing  We could have put an else statement in here and say you have to enter a temperature  But this one has problems too  Let's try 212 0  We know it works on 212  212 0  The problem is that 212 0 is not an integer  It's considered a real number because it's got the 0 0 there  Or in programmer talk  it's a float  They usually say float rather than an integer  Here's an improvement on this thing that's going to correct that  Here is fahrenheit_to_celsius3  We're going to start off the same way  We're going to input the temperature string  and we're going to check  if the temperature string is not empty  then we're going to do the rest  okay? But the rest considered is we're going to check to see if the temperature string has a digit  Now we haven't seen this before  but strings have various methods  These are things that strings can do for themselves  And they're separated from the name of the string by a period  So isdigit is like a function for that string  it's called a method  okay? If it's entirely digits  it's okay  But if it has a period in it  then it's not all digits  So if everything is a digit is true  then we're going to say  we're going to apply the integer  convert the string to integer  Ad then we're going to compute the new temperature and we're going to print out the result  Else we'll print out that you must enter a number  say goodbye  Let's see how that one works  Again  we'll press up arrow till we get to this and we'll convert it to 3 and we'll enter the temperature  works fine  Try it again  up arrow  Press blank  works fine  We tested to see if we actually entered a string  and since we didn't  we didn't go through the code that's inside of this if statement  Now let's try it again  And this time  we'll enter 212 0  We got inside here  and we check to see if it's a digit and it wasn't  everything wasn't a digit  the period's there  Else prints  you must enter a number  bye  Here's a function that uses integer division  It converts inches to feet  Let's run it  77 inches is 6 feet  5 inches  Now let's look and see how that's done  First of all  we have to have the 6 feet  so we use integer division  divide by 12 and we look and see how many times 12 will go into the number of inches  In this case we use 77  And it went 6 times  Now 6 12s are 72  so there are inches left over  When we have these extra inches  we compute them by taking the total inches given minus 12 times the number of feet we computed  12 times 6 was 72  so when we subtracted 72 from 77 we got 5 inches  So we printed inches is 77 inches is  6 feet and the extra inches  5 inches  So we used integer divide right here  But we got the remainder by multiplying by 12 and subtracting it from the total inches  What I want you to do is to rewrite this function so that it uses the remainder function percent to compute the inches that are left over  Remember how that works  If we take 19 and divide by 5  we get a remainder of 4  Now I want you to modify this function  call it inches_to_feet2  and replace the middle line with one that computes the extra inches by using the remainder function  the percent operator  Press the Continue button when you are finished writing your function  Here's my solution  The only difference is right here in this line  You take inches % 12  that gives us the remainder on dividing by 12  that's the extra inches 
CoBIpAZQYiA,"Introduction to loops - the ""while"" loop  Often  we need to execute a section of code with many variations over and over and over again  Such a construction is called a loop  Here is an example of a program with a particular kind of loop called a while loop  The while loop executes over and over again while a particular condition is true  Here's the example  In this case  we write the numbers 2  4  6  8  Who do we appreciate? COURSERA  Now the loop produces the 2 and the 4 and the 6 and the 8  Let's see how that's done  First of all  we have a counter  a little variable called ct  stands for count  While ct <= 8  we're going to print the ct  And then we're not going to go start a new line  we're going to skip a space  And then we're going to add 2 to the ct  and then we double back  and we try to test whether or not this thing is still less than or equal to 8  So the first time through  when we get to the while ct <= 8  ct = 2 so it's definitely less than or equal to 8  So we then print the 2  and we see it over here  And then we print  instead of a carriage return line feed or a new line  we print a space  Then we add 2 to ct  then we double back  Now ct is 4  4 is still less than or equal to 8  so we'll repeat the two lines of the loop again  And after that repetition  ct is now at 6  6 is still less than or equal to 8  So we go through print the 6 and a space  And then we add 2 to it  and now go back  And it's still less than or equal to 8  it's actually equal to 8  And we go through the loop one more time  And then we add 2 to it  and now it's 10  So we go back and do the test  while 10 <= 8  and it's not  so then we don't repeat the loop anymore  We go down to the next line after the loop  which is this print statement and a print without anything in it like that  just starts a new line  And then we print Who do we appreciate?  and there it is  And then we print COURSERA  That's the while loop  Now  it is your turn to write a function with a loop in it  This next exercise is to write a function count_down that pretends to be a rocket launch countdown  It'll start at 10  It'll type 10  9  8  7  6  5  4  3  2  1  and then BLASTOFF  Here's my run of that program  Yours should look the same way  Before you get started on this  I want to point out the three parts of the loop that you need to be conscious of  First of all  a variable is initialized before you enter the loop  Second is  variable is compared to some value of units at the loop  and while this comparison turns out to be true  you continue to do the loop  And third  inside the loop  this variable needs to be changed in some manner  otherwise  the loop will loop forever  Now you write your function  and when you get through  press Continue and I'll show you my solution  Here is my solution  def count_down()  Rocket count down routine  I initialized the variable ct at 10  I check to see that it's greater than 0  if it is  I'm going to continue through the loop  And what I do inside the loop  I'm going to print that ct and then I'm going to reduce the ct by 1 because we're going from 10  to 9  to 8  to 7  and so on down the line  When we get through  we're going to print a line  and then we'll write BLASTOFF  Now every time I write the number I print  instead of having a new line immediately  I print end = That'll give me a space between each of the numbers and it won't give me a new line  I'll get that new line just before I do the BLASTOFF "
dwR1R-L9HEU,"The ""for"" loop  tracking down errors  Now letâ€™s take a look at another kind of loop  the for loop  Here is the cheer program rewritten using a for loop  Let me run it  It prints two  four  six  eight  who do we appreciate? Coursera  Now let's take a look at this loop  Let's start count at two  we'll go up two  but not including none  and we'll go in steps of two  So it'll be two  and then the next time through  it'll be two more  four  then the next tim  e through it'll be two more  six  and the next time through it'll be two more  eight  There won't be a next time  because the next time it would be 10  and that would exceed nine  Every time through the loop we're going to print the count  and we're going to print a space  It's just like the other program  otherwise  We print  use the print to get a fresh line  print  who do we appreciate  and then we print Coursera  Let me emphasize that this one is the first one that it does  This one  when count takes on the value nine  it will not do that one  It will not go through the loop when count is equal to nine  It's up to but not including nine and the step size will be two  So  this will go two  and then four  then six  then eight  Now I want you to try and write a function and using the for loop  I want you to write the countdown program using that  Now remember that this step can be negative  Let me show you my run of this program  Countdown one writes 10  9  8  7  6  5  4  3  2  1  And then Blast Off  this is a rocket launch countdown  Now  I want to emphasize that the range  we have a starting position  we have a stopping position  it won't do that one  And this is the step size  The step sign could be positive or negative  Okay  so take a moment to write this program  and then when you get it written  press the continue button and I'll show you mine  All right  here is my version of doing the countdown  Countdown1  for count in a range of ten  so we will start at ten going down to zero but we won't do the zeroth one  And since we want to go down we want to use a negative step size  we want to go down one at a time from ten to nine  to eight until negative one  We're going to print the count  We're going to print a space  instead of a new line  each time through so that we end up ten  nine  eight  seven  six  five  four  three  two  one  Since we won't use zero  there isn't a zero  And then we print a new line  and then we print  Blast off  Now let's speak a minute about finding errors in programs  Here's an example of a function written in Python that has errors in it  And I want you to think about how you would go about finding errors  Of course  you can look through this thing very carefully  and probably determine what the errors are  but let's do it by trial and error  Let's run the function and see what python tells us  Okay  control enter  Here's the function and it says here we have an indention here  unexpected indent  and it points to this print  So if we look over here  you see this line is indented four space  one  two  three  four  but this one is indented five spaces  So this one needs to be moved over and that's what it's telling this  the indent is an unexpected amount  So  back it up one  then we'll try again  Now we didn't get an error  So  let's try running it and see if it runs properly  What is my favorite toy  So  I'm going to type in football  And we get an error  Name error  The name Ma is not defined  Now we were talking about my tor why is it say my  Well its interpreting this as being a subtraction  It wants to take my minus toy  Now let's look back at our program  We defined a variable called underscore toy  and we wrote my hyphen toy  So this needs to be corrected to be the right name  and now let's run this again and see what happens  Again  of course  it works as far as the loading of the program  So now the program works  We found two errors in it  I'm going to put this back like we had it  Space  and notice that the editor is telling us right here that there's an error  So that the editor can detect some of the errors before they even run it  Python itself helps you to track down the errors "
ZfB5I0wmu1s,Introduction to lists  Python has lists  Let's see how they work  The empty list  Is just empty bracket  A list with one element  let's say a letter a in it  would look like this  A list with two letters would look like this  A list can have numbers  Like this  A list can have strings  Like that  And it can have all of these things  Hello  3 14  A number  Let's define a list of letters  Let me just execute this line  and it gives us this list named lis  And it's equal to the letters a through f  If I type lis  you'll see that I get that list back  Let's talk about how the items of a list are accessed  This is item zero  this is item one  this is item two  and so on  And notice that Python starts with item zero  not item one  So zero  one  two  three  four  five  That's the way these are accessed  Let's take a look at how to pull an element out  If you want item 0  you type lis[0]  If you want item 1  you type lis[1]  If you want to know how many items are in the list  you can use the len function  the length function  And it'll tell you that there's six  there's three  four  five  six  Now  what if we wanted to get the last element of the list? Well you might be tempted to ask for the following  lis[6]  But let's see if that would work  This is item zero  one  two  three  four  five so the last item of this list is item five  not item six  So this won't work  but I can say item five and get it  That worked just fine  There is a simpler way and that is that I can type -1  That says count from the end of the list so this is first one at the end of the list  And the next to the last one will be lis[-2]  And so on  let me repeat the list  Now I might want to get items two through four back into it this way  Now what that will tell me is  items two and three but it will not print four  Zero  one  two so item two is a c  so it'll start there and it'll do three but it will not do four  which is the e  Let's see that  zero  one  two  There's the two  Three  there's the third one  Four is not there  I want to get everything from item three on to the end  I can do that  And not put an ending spot  I want to get everything up to  but not including three  I can do that  Zero  one  two  here's three  It's d  it's not there  If I want to add onto the end of the list  I can use a method of lis called append and I can  in this case  add g on in this manner  You see g is now added on to the end of the list  We can ask whether something is in the list  So I'm going to ask if d in lis  I can make that assertion  Python will tell me whether that's true or false  It is true  If I make the assertion that x is in the list  it'll tell me that it's false  So I can tell whether something is in the list by doing an assertion and see whether I get true or false  Now let's look at a function that makes use of the list  First of all I'll go over here and click on  in that cell  and execute it  Who_is_there is the name of the function  And I'll give it a simple list with just a lion in it  And it prints  there's a lion  There's no horse in the list and the list has one item  Let's take a look at the code If bear's in the list  it'll do something  But there's no bear in the list  There's only lions in the list  If lion is in the list  then it'll print there's a lion  And it does that  If daisy or iris is in the list  it'll print there are flowers  but they're not there  If they both are there  it'll print something  If donkey's in the list  it'll print there's a donkey  but that's not in the list  And now let's look at this horse not in list  The assertion horse not in list is true  only a lion's in the list  so this is true  If this is true  then it'll print there's no horse in the list  and it does  The last line prints  the list has a length of list items  and there's one item  Let's execute it  and let's say iris  Put that in the list  Now again the lion's in the list  so it prints there is a lion  But now  it'll see this line  if daisy in list or iris in list then  there are flowers  Well  iris is in the list so it'll print flowers  If it had seen daisy it would of print flowers too  Either or  There's still no horse in the list so it prints that  and now there are two items  Let's put daisy in the list  Now the lion is still there  So it's still checks the other line and prints there is a lion  If daisy is in the list  and it is  it'll print there are flowers  It would've also printed it because iris is also in the list  Since daisy is in the list and iris is in the list  there are at least two flowers and it prints that  There's still no horse in the list  and now the length of the list has three items  If we put horse in the list  Then there is no horse disappears because horse not in the list is no longer true  horse is in fact in the list  So this if statement is not true  so we'll not write there is no horse in the list  You should make up some lists and pass them to this function and make sure you understand how all those ifs work  Let me point out that we didn't have to type the list directly into the argument  Let me find a few lists here  I'll show what I mean  We've got a list called alion  we got another list ld  has a lion and a daisy in it  and lbf  that list has a lion  a bear  and an iris  So we could have passed these into the function who_is_there(lbf) and it would say  there is a bear  there is a lion  and there are flowers  because iris is there  There's no horse in the list  and the list has three items 
LJpwQ2da8ss,Lists continued  Now let's look at a function that makes use of lists  First of all  I'm going to define two lists  Here's a list that has one A in it  and here's a list that has four A's in it  I want to write a function that's just going to count the number of A's in the list  And this is it  Count A  To run it  type count_a of and then we'll pass it a list  Now  And it says there one letter A is in the list  we'll say count A less one  we'll get four letter As in the list  These are correct  Let's see how that works  First of all  we start off by setting a counter equal to zero  We're going to count one  two  three  four  five  however many letters  A  and we have to start with zero count  When we go through the loop a number of times  and then when we get out of the loop  we're going to make use of that count and print our count letters A in the list  Now let's see how the loop works  This says for every letter in a list  so it's going to look at the first letter in a list  and then later it's going to look to the next letter in a list and the next letter in a list  If that letter is an A  we use the double equals for a comparison  we're going to add one to count  Then we're going to double back and a get it another letter  compared it to A and then add one to count if it is a letter A  If it's not a letter A we won't do anything  That's the way the function works  Now let's look at the various aspects of writing this loop  First of all  there was the initialization of this list counter  CT  Next we went to the loop a number of times  and in the loop we changed that counter  And then when we got out we made use of the counter here  So those three parts  Initialize the counter  change the counter every time through the loop  and then print the result out  This is a design pattern that you will see over and over and over again in loops  Now I want you to write a function that makes use of a loop and makes use of a list  This exercise is to write a function that computes the average of the numbers in the list  That means that you're going to have to write a loop that steps through each element in the list  you're going to have to add up these elements  you're going to have to count those elements  and then you're going to have to divide that sum by the count  Here are a couple of test lists that you can use to test your function  This list should have an average of 41 5  this list should have an average of minus 9 215  Here's my run  this is a run on n list  I listed the elements  I computed the average  and I wrote the count  Here is my run on our r list  and I wrote each element out  Here's the average and here's the count  Now why did I write each element out? Well  often when you go through a list  you miss the first element or the last element  And if you write the list out  you can be sure that you got them all  For example  n list starts with two and ends with one  Mine starts with two and ends with one  so I know I got the whole list  Our list starts with 3 14 and ends with four  Here's 3 14 and here's four  so I know I got the whole list  And your function should be average numlist and use numlist as your list of variables throughout  Don't use end list because it'll only work on this list  Don't use our r list because it'll only work on this r list  You've got to use a variable that will work on both of them  Now you write this function and press Continue when you're finished and I'll show you my solution  Here's my solution  and there are others by the way  Define average numlist colon now I start off with the sum equals zero  and I write an underscore here because sum is a keyword and I want to use sum but I don't want to use the keyword  For i in the range from zero to the length of the list  This is one way to step through the list  Go through the proper number of times and add to that  I'm going to take sum  sum plus the number  And that will give me a new sum  And then I'm going to print the number that I just used  I got all of these  And then the average is going to be the sum divided by the length of the list  Now I'm going to print average is and I'll print the average  Its count is n under the note  That's one way to do this 
yE8oCPmKX0I,Stepping through lists using loops  Now let's do another example of a function using a loop to step through a list  Notice that we have a list called newEngland that has the states of newEngland  Maine  New Hampshire  Vermont and so on  And we have a function for_state  and it will step through this last slis  and for each state in slis  it'll print the state  Now let's see this run  Note that instead of using slis  we're giving it the actual list  newEngland  and there they are  In this case  we stepped through the items on the list one at a time  and printed them  Now  what you need to write is a simple function that will print a list  Here's some lists to test it out on  Here's my run of my solution  press return when you have finished your solution and I will show you mine  Here is the function print list  for each item in the list  just simply print the item  This will step through the list one item at a time  and then that will print the item  That's all there is to it  notice that it works on any kind of list 
TVYgu5tXlX4,Introduction to datatypes  Now let's talk about Python data types  Here are a number of examples  I'll just execute that so you can see them  First of all  x up there is an integer  y is called a float  that's a real number  it's got a fractional part  And z is a string  a string of characters  z1 is also a string  it's a string of digits  z2 is a string of digits as well  Here is a list  it's a list of vowels  Here  a list of numbers  Actually  it's a list of strings each of which is a number string  Here is another list  and this is a list of strings  Twas brillig and slithy toves  Did gyre and gimble in the wabe  so it has two strings in it  Here is a boolean that's true  here's a boolean that's false  Now generally speaking  you've gotta be a little bit careful about these  For example  if you're trying to add z1+2  it can't convert it  That's because z1 is a string  I can actually do that if I change z1 to an integer  then I can do it that way  Also  if you take  let's write z1 is that  z2 is that  Now z1 + z2  what's going to happen there? Well  those are not integers and they're not floating point numbers  so they won't add as numbers  They won't give me 70  They'll give me a string  which consists of z1 followed by z2  Now I want to add those two as numbers  I can do it this way  Often you can convert from one type to another  So if I want to take y  convert it to a string  I just apply the string function  I can do the same thing with x  but I can't do that with z  because that's a walrus  I can't convert the walrus and the carpenter to a string 
VIQ5jto-nUA,Converting datatypes  Now let's look at a function where we need to convert from one data type to another  This function will multiply two numbers  Let's watch it work  going to enter a number  7  and another number  3  And we get the product of 21  Let's see how that's done  First we have an input statement to get one number in  but it comes in as a string  We then have another input statement to enter the other number  and it also will come in as a string  Then we take the string  numstr1 and convert it to a float  num1  Then we do the same thing with num2  convert it to a float  And then we can take the product num1 * num2  and print that out  Now let me emphasize that if we had tried to multiply these two  it wouldn't work  Here is an example of that  Let me comment this line out  I'm going to go up here to edit and put comment  And then I'm going to un-comment this line  go back to edit  un-comment that  Now let's run this again and see what goes wrong  That's up arrow  go back to the multiply  Now we're going to enter seven again  times three  boom  We get a type error  Can't multiply sequence by non integers of type strain  Now let me restore these 
xKJjyn8DaAw,Working with lists of sublists  writing a small report  Let's talk a moment about the range function and converting it to a list  Here are an example  First of all  I've got a range function  It starts at 2  it stops at 20  and it goes up at steps of 3  In this version I've converted it to a list and printed it  Let's take a look at that  2  that's the starting one  goes up by 3 to 5  3 more to 8  3 to 11  3 to 14  3 to 17  and 3 to 20  but it doesn't do 20  That works as we would expect  It takes the range and converts it to a list and prints it  If you don't convert it to a list  however  you just get the phrase  range 2 through 20 in steps of 3 back again  In Python 2  the range function actually produced a list  and they had to introduce a different function called x range that would work like ours  The reason had to do with using the range function with a large range  It would produce a list which were just huge and use up lots of memory Now let's take a look at integers  We have a big integer right here  We could mark off every three digits with commas  like this  but you gotta be careful about that because Python interprets those as three different numbers as you can see when you print them out  So  that's not a good idea  Here's another caution that Python has keywords  and this is the list of them right here  You don't want to use those as a variable  Let's see what happens if you use except which is one of the keywords as a variable  It gives you a syntax error  Now what I have done when I felt like I needed to use one of the keywords as a variable is I just put an underscore after it and that makes it different  Let's recall a function that we wrote previously  Here's another copy  which I'll execute  We defined a list New England  consisting of the states of New England  Maine  New Hampshire  Vermont  and then we wrote a function that stepped through the list printing each state  Now what I want to do is do a little bit more sophisticated version of that  Here is New England redefined and this time  redefined each state to have a population  That meant that each state really is represented by a sublist  Massachusetts and its population is that sublist  Connecticut and its population is this sublist  And so on  So each item of the list New England  Is a sublist consisting of the name of the state and its population  Now before we write our functions  let's make sure that we understand how to access the various parts of this list  This is an exercise that you can do along with me  What is the first item of New England? Well the first item of New England is the one with index zero  and that is Massachusetts and its population  It's a sublist  it's not just Massachusetts  What is the second item? Well  that's the item with index one and that's Connecticut  Now what is the name of the state in the second element that is the element of index one? Well  we know that this will give us the name and then give us the sublist that consists of Connecticut to us and its population  If we want the name of the state  we need the 0th element of that list  and that's Connecticut  If we want the population of Connecticut  we go to the item of index one  Let me give you a more complete solution here that you can study  This is for port one  It will take the New England list and print both its population and state name for each item  There's our little report  We put population at the top of that column  state at the top of that column  We give the population of Massachusetts and the state name of Massachusetts  Now let's take a look at the function and see how that works  The function is for port one and we passed state data in  In this case we passed the New England state data  It will print  first of all  the population and the state  and we see that here  Now  for each state item in state data  state data is the big list that we passed into the function  we will print state item of one  State item of one is going to be item one of each one of these sublists  And that's the population  Then we'll print a blank  And then we'll print state item of zero  The state item of zero is the 0th item of each of these lists which is the name of the state  That's how it works  It's very simple and straightforward  Now notice that since we used state data here  we didn't use New England  this can work on any big list that has this general shape  So for example  I can define mid-Atlantic states to be this  New York and its population  New Jersey  its population  Pennsylvania  its population  Let me write that out so that it is more obvious  We can use this function on the mid Atlantic states  report one mid Atlantic  And it prints the proper report for the mid Atlantic state because it was not written specifically for New England  It will work on other regional lists in the proper manner  It is instructive to write this function in a slightly different way  so here it is again  Report two  Now what's different about this? Well  we still print the headings in the same way  but this time we just use i and we ranged from zero to the length of the left  Instead of stepping through each item in the list  we do it this way  And i of course is just an integer  0  1  2  3  4 up to the length of the list  And so we'll print the state data of i  Bracket one for the population and bracket zero for the state name 
m_Eu74e9S_A,"Lists continued  Now let's look at a slightly more sophisticated program using the same list  Where the data is in the sub-lists  This function computes the total of population of the states of New England  It writes out the total population of this list of states is 14 million and something  There's six states in this list of states  so it does two things  It adds up the population and it counts the states  Let's look and see how this function works  First of all  we're going to have to have a loop  And here's our loop  And we're going to start off the loop by initializing sum  We're going to add up and sum the populations of the states  We're also going to use num_states for the length of the state data  Then we step through the loop  We will build the sum and then we will print our results making use of the sum we computed in the loop  Let's take a look at the loop  The loop goes from zero through the number of states  We're going to pull out a particular state  the ith state  we'll call it one_state  and then we'll get the population of that  Remember  the population of the state is the sublist item of index one  and then we'll add that population into the sum  So we've got sum initialized  we pull out the population  and then we add the population to the sum to produce a larger sum  Then we'll double back and we'll get the next item i of the list  i will go from zero to one to two up to the number of states  And we will have built the sum and printed it  Also number of states tells us how many states there are  And then we're printing that as well  Let's write this function one more time to illustrate a few more things about programming  As you can see  it gives the same answer as before  Let's look at the code just a moment  We're going to say population is equal to one and the sum is equal to zero  which we did before  We initialized this variable that will accumulate the sum of the populations n  The number of states is the link to the state data  same thing  Now for the state in the range of zero to number of states  Instead of using the variable i  which doesn't mean anything really  we'll use state  And then  we add to the sum the state_data  state_data is our big list of the state  this is the particular sublist that we're working with on this passage through the loop  And the population  the population is always in item one of the sublist  And zero being the name of the state  The bottom two lines are exactly the same  What's different here is mainly using variable names that might be more meaningful  like state instead of i  And population instead of one  Now I want you to write a function that'll compute the average of a list  Your output should look something like this  Here are a couple of lists to test on  Here's my run  Now my version of this function on these two test lists  Average of numlis which is this first list is 53 1  The average of numlis2 is 8 28  Notice that I wrote out these lists so I can be sure I got them entirely  Numlis starts with 65 and ends with 42  65 here  42 here  Numlis2 begins at four and ends with 19  Four and 19  It is not necessary to write them now  But it is useful to make sure that you included the whole list  Now you write this function  average of numlis  gives you a starter  You need to replace this pass with actual code  When you've finished press continue and I'll show you my solution  Here is my solution  Average(nlis)  see I have the same starting function  Commented out the pass so that I can write my code  I'm going to sum up  The numbers in the list  I'm going to count the number of elements in the list  And to compute the average  I'm going to take the sum and divide by that number  That's the way you compute averages  Now here is my loop  For i in the range of zero to num  num is the length of list  I'm going to take sum and add to the ith element of the list  I'm going to print that ith element so that I can actually see that I got them all  And I'm going to put end = \"" \"" so that these numbers are separated by spaces  and I don't use a whole lot of lines to do this  To make sure that I go to the next line before printing out the average  I have a print statement with nothing in it  And that'll finish off the line that we're working on  by printing a new line  Here's another version of that function that I think illustrates some pretty good points  First let's execute it  As you can see  it gives the same solutions  Let's take a look at this  First of all  we should initialize our sum to zero  On and we will divide that sum by the length of the list  That's the same as the number of items in the list  So that gives us the average  Now instead of using i to step through a range as we did earlier  we'll use num and step through the list itself  Then we will add to sum num  and then print the nums with a space after it so that we're sure we got them all  This way we're stepping through the list rather than stepping through a range of numbers that's equal to the number of items that are in the list "
oFOPRPufisk,Introduction to libraries  The random library  Python has libraries that add functionality to the basic interpreter of Python  The point here is that if you put every capability into every program  even your smallest programs would be very small  By having libraries  these are portions of Python that are set aside that are not incorporated into every program  You can write smaller programs  and only call in these libraries when you need their capabilities  Now  let's take a look at libraries  and how you use them a little bit  Here is the key for getting a library in  You import and you give the name library  That's saying that import random will import a random library  Here's an example of importing it  Okay  just executing that statement import in the random capability  In here  we can execute something of it  Print random random number  Random is a method of this random library  and it generates a random number between 0 and 1  If we repeat this  we'll get a different random number between 0 and 1  We can repeat it several times  and it'll keep giving us different numbers  because it's generating them randomly  Here is another method in the random library  This will generate a random integer between 3 and 8  3 might be given and 8 might be given  you'll be in that range  So let's do this again  I'm going to print another random number  say it's 4  Here's another one  it's 3  see we've got the lower end  3 again  they're random so I never know what's going to come out  Notice that we did manage to get 3 and 8 in there and these are generating only integers 3 4 5 6 7 or 8  randomly  Now let's make use of another random capability and write a little program that will generate a sentence  Now this is not a very sophisticated sentence so don't expect too much of it  but here's what we are going to do  I'm going to write a run function sentence  And it says  a bicycle chews gingerly  A car shoots gingerly  A bicycle shoots sweetly  It generates grammatically correct sentences  not very sophisticated ones  It doesn't have a very large vocabulary  Let's look and see how this works  We have a list of verbs and just put these in  cook  goes  cooks  shoots  faints  chews  screams  It has a list of nouns  bear  lion  mother  so on  Has a list of adverbs  And it has a list of articles  And it would generate a random English sentence  So what we do in the function is  first of all we create a random article  Now  one of the methods of random is choice  That means it will randomly choose one of the items out of that list  We randomly choose an article  we randomly choose one of the nouns  we randomly choose one of the verbs  we randomly choose one of the adverbs  then we put together our sentence  The sentence is always going to be an article  followed by space  followed by a noun  followed by space  followed by a verb  followed by space  followed by an adverb  with a period on the end  Just to make it look more like a sentence we're going to use a method of stings called Capitalize that will capitalize the first letter  So that it actually looks like a sentence  And then we'll print the sentence  And that's what it does  In this case  it generated a random article  this  then a random named bicycle then a random verb shoots  and then a random adverb sweetly  We put that together to make a sentence with spaces and a period  And then we capitalize the first letter and then we printed it out  That's a simple little function making use of the library  random  Now you're going to adapt this program to write a simple four-line poem  Here's the function repeated again so that you can modify it to write a simple poem  Everything inside this simple poem is exactly what's in previous sentence  but you need to write a loops so that it goes through and generates four sentences  and then we will call that point  Now in order to do that you are going to have to enclose this in a loop  and that means you need indent all these lines  Now  I want to call your attention to the edit function  which has an indent capability  If I highlight this  for example  I'll indent everything  There it moved it all over  if I unindent I can move it all back  So you write a function that writes a simple poem by adapting our sentence function or adapting this copy of it right here  Let me show you my example so that you'll know where you're going  Here is my solution  there is one run of a simple poem  here's another run  here's a different poem each time  Now you go write yours and when you've finished it  press Continue and I'll show you our solution  Here is my solution  I simply moved everything over and put a loop around it  For i in the range 4  that's 0 through 4  Just does all the whole thing 
fH4zUvUVXn8,"More on Loops and Append  Now  let's look at a design pattern for loops  Here's a function add up  Three  five  seven  and to end  it says  type zero  What that did was  it added up the number three and five and seven and got 15  This is not sophisticated program  it just look at loops and how they're designed  Okay  We started out by setting sum equals zero  Then we went through the loop  In this case  we went through the loop while true is true and it's always true so we just go forever  And then  we input a number  Add zero to quit  Okay  That number got converted when it got input to an integer  This converts that string to an integer  Now  you test that number  If it's equal to zero  we use a double equal to make comparison  We'll break  That means we'll break out of the loop  If we don't break out of the loop  then we add number to sum and replace sum of that value  When we get through with the loop  we just print sum  So  the three parts where we initialized this variable  we updated this variable and then we made use this variable  In this case  we started with zero  Now  let's take a look at the similar thing for lists  Here is a list called baseball  Now  what I've done here is  I started out with an empty list  then I appended ball to it  then I appended bat to it  then I appended mitt to it  then I appended baseball  You see ball  bat and mitt are all appended  This is the list equivalent of adding numbers to a song  Now  let's write a program to build a list of numbers  We'll call this store_up  Well  and remember three  five  seven  we'll enter zero quit  So  now we've added three to the list and five to the list and seven to the list  We appended them  Now  let's compare these two  Whereas we have sum equals zero up here  we've got num_list as empty list down here  While true  we're going to loop forever  Okay  We're going to input a number  and this is exactly the same  If this next number is zero  we're going to break  That's exactly what we did before  Before we added the number to the sum  now  we're going to append the number to the list  And then when we get out of the list  we're going to print the number list  So that we have an initialization as before  we loop through the loop  we do something with the initial last list  and then  that will change it  And then we will make use of it when we get out of the loop  Now  I want you to write a function that will build the list  Here's the problem  You want to pretend that you're a waitress at a diner  That's what the function is going to be  You're going to be a waitress in a diner taking an order  So you'll come up to the table and say  \""Hello  I'll be your waitress  What will you have?\"" Then as a customer  you'll say something like eggs  and she'll put that into the list  Bacon  she'll write that on her list  Toast  she'll write that on her list  And jelly  she'll write that on her list  You can order anything you want to she would just put it on the list  Then when you say  \""that's all\""  she'll stop taking the order and print out the list of items that you ordered  That's the function that I want you to write and it involves creating a list  looping through and adding things to the list that will be numbered this time it will be foods  And then  indicating when you want the loop to end by entering that's all  and then printing out the list  Here's my run on that program just so you can see how it works  Diner waitress  That's what will happen when you run the program  Hello  I'll be your waitress  What will you have? It'll ask that question  Then it will loop through an answer menu item and you'll type in eggs  then it'll loop through  ask for another menu item  and you say bacon  Then it'll ask for another menu item and you give another one  And then it'll ask for another one and then you give them another one  And then  if you ever enter  that's all  that particular string  then it will quit taking the order and print out the list of things that you've ordered  Now  you try to write this function  write diner waitress  and when you get done  press continue and I'll show you my solution  Here's my solution  Let me run it once for you  I'll show you how it does work  Okay  I'll type bacon  eggs  toast  that's all  And it'll print out what I ordered  Here's the code definition  diner waitress  parenthesis colon  Print  Hello  I'll be your waitress  What will you have? Now  we'll start off a list with an empty list that'll be our order  We'll loop through forever  while true is true  we'll loop through  Here is the details of the loop  When we get through the loop  we'll print the phrase  you've ordered  and then we'll print order  An order of course is this list that started out as an empty list  but which we've been adding to  Let's see what we do every time through the loop  First of all  we'll issue an input statement  which will ask for a menu item  Over here  menu item  We'll store that in food  Now  I'm going to convert food to lowercase  so I can compare it to that's all  so that I can type it upper or lower case or however I feel like  It'll always be converted to lower case and I can compare to this  Notice that  we used a method of strings called lower  We convert it to lowercase  We use the double equals for comparison and we have the phrase that's all  If those are equal then we'll break out of the loop  and they will go print you've ordered and print your order  If the food ordered is not  the phrase  \""that's all\""  then  we'll go to the else and we'll append  use the append method of list to append the food onto that list  Now  I want to point out that I knew the lower case existed  In your case  you wouldn't know that and so you probably would just say food  if food is equal to that's all  That mean you would have to type in that's all in lower case always  This is a little bit more flexible  I just thought I'd throw that in  so that you'd be aware of "
FO5XLAMh9E0,Using tuples and data dictionaries  So far we've talked about one data collection type  namely the list  Now we want to take up two other data collection types  the tuple and the dictionary  First  the tuple  The tuple comes in various sizes  here's a two tuple  here's a three tuple  here's a tuple over the letters  here's a tuple of words  Let's define a tuple and then show how to access various parts of it  First of all  to print the whole tuple  we just type that and the name of the variable  We can print it just as easily  We can access the various parts  Here's the first element of the tuple  item 0  Here would be item 1  second element of the tuple  Here's the last element of the tuple  And of course  the next to the last element of the tuple  We can do a range elements of the tuple  Notice this is 2 through 4  Item 0  1  2  i is the first one  3 is the next one  but we don't do item 4  that's typical Python methodology  We can do everything up to item 4  We can do everything beyond item 2  That pretty much covers how to access the various parts of a tuple  Now tuple is immutable  that means it can't be changed  Now  let's look at the third collection data type  the data dictionary  The simplest data dictionary  of course  is the empty one  Notice that we use braces to indicate a data dictionary  Let's do a more complex one that will show what they're really all about  You have a key  in this case  Johnny and a value  5 years old  and another key Sally  with a value 7 years old  And it keeps going like that through the data dictionary  If you type d  you'll see the data dictionary repeated  And notice  that Eva is third up here when we defined it  but it's first in here  This is characteristic of a data dictionary  You have no idea what order it's going to print out  It's not an ordered data type  And if you ask for the zero-th item it just gives you an error  The way you access something is by key  So you typed in the key Sally and you get out the value 7 years old  Do another key  Eva  you get 10 years old  You can list the items in this manner  You can list the keys in this manner  You can list the values in this manner  Let's write some loops showing how you can access these various parts of the data dictionary to do useful stuff  All right  here is an example  For the key and value in d items  key  value  Print the key and print the value  Notice that this is a tuple  key and value  Here's another example  For item in d items  we'll print the whole item each time  And notice  the item is printed as a tuple  Peggy  7 years old  Let's do another one  For every item in d items  we can have a look at item of 0  that's the key  our item of 1  that's the value  Peggy  the value  Peggy  7 years old  We can also step through the keys and we can print the key  And since we have the key  we can also print the value if we'd like to  Peggy  7 years old  In this case  instead of stepping through the whole thing  we're stepping through d keys and we get the same result  If you want to see only the values  you can step through the values  for every value in d values  print that value out  Now  if you want to add something to a data dictionary  that's fairly simple to do  If d of this key has to equal something  let's make him 5 years old  And now  if I type d you can see that Ted has been added in the data dictionary  Now let's look at some of the things that you can't do  D[sally] won't work because Sally is spelled with a capital S in the data dictionary  D [Jim] won't work  I'll show you that  Why won't Jim work? Well  Jim's just not there  And you can't look something up by value either  d[5 years old] doesn't really get you anywhere  Let's define another data dictionary and you try to do some things with it  Here is ascars  that's a data dictionary  That means affordable sports cars  And this says that Ford's sports car is a Mustang  Mazda's is a Miata and so on  So  the first thing is if you type ascars  it'll display the whole thing  The second thing I ask you to do is retrieve the Nissan sports car  So we type ascars['Nissan']  and you can do this with me and you can retrieve the 370Z  And you can retrieve the Chevy sports car if you like  it's a Camaro  Now the next one I want to do is to change the MINI Cooper  Now if I misspell MINI Cooper in doing this  I'll wind up adding  A new Key-Value pair  So I want to try and recover this first  Now  I got the Roadster  so that is spelled as correctly  I've spelled it correctly  So  I'll set that equal to Coupe  Now  if I type  ascars  I will see that the MINI Cooper is now changed to a Coupe  Now the next thing I'd like to ask you to do is write a small two-line loop to display all the values  and only the values  The Challenger  These just show the values  and this is the way to do that  Now for every value in ascar values  print that value  Now  the next thing I want you to do is write a two line loop to display all the keys and only the keys  This is what you should get when you display only the keys  Here is my solution right here  For key in ascar keys  print key  To summarize  lists can be appended onto and their items can be addressed by number  Tuples cannot be changed  they're immutable  Their items can be addressed by numbers  just as in lists  Dictionaries can be appended to  like in lists  but you cannot address their items by number because they don't have it in here in at order  Their values can be achieved by using the keys  Now  here's an exercise to help clarify all of these things for you  First of all  you can execute the cell  Now you have a name list  you have a tuple  mytuple  and you have an age dictionary  We have a dictionary  age dictionary  Now answer this question  can you retrieve namelist[1]? Can you retrieve mytuple[1]  can you retrieve agedict[1]? You answered those questions  and you can test it out by copying one of these  pasting it into place  and then executing it  So you were able to retrieve item one out of namelist  So you go on down through these  and you test them out on your own 
GlobK-eWDSo,"Reading and writing files  Before we go on  I want to talk about a few operating system commands and how you can execute those from within IPython console  First of all  I'm going to get a fresh IPython console  Now if you want to see what directory you're in  on a Linux machine or a Mac  you can type PWD  and I'll show you  That won't work on a PC  but it will work in an IPython console window on a PC  or on a Mac  If you want to see what files are there  you can type LS on a Mac  or a Linux machine to see them  That won't work on a PC  but it will work in an IPython console window  On a PC if you want to see what your working directory is you have to type \""cd Now if you type an exclamation point in front of a command  IPython will pass that on down to the underlying operating system so this works  It won't work within the IPython window  But it will work if I pass it down to the PC operating system  And in a PC if you want to see what files are there you type DIR in iPython console when I have to type an exclamation point in front of pass it all down  Anyway  there are a couple of really basic things that you should know about iPython and about the operating systems  Now  we want to move on to reading and writing files  Now  before starting that  I want to get a fresh console window  I click that on a way  open an act IPython console here  Now  let's talk about reading and writing text files  Here's an example  I have a function  I have a textile up here  humptydumpty text and I'm going to print it  There it is  now let's see how it did that  First of all  we passed it the file name  and we use the file name to open a file and we assign the handle to that file infile  We'll refer to this infile throughout the rest of the program  where the line an infile  we'll print that line and then we'll close the file  The reason we have and equal blank is that the text files have a new line at the end of every line  print has a new line at the end of every line  If we had two new lines it would be double space so we get rid of the one with print  Now let's do another example  We will copy one file to another so you can see how reading and writing differ  Right  I'm going to copy humptydumpty txt to xhumpty txt  Okay  and you didn't see anything happened here because all that had happened right up here  There is humpty  there he is  it's been copied  Now lets see how we did that  We had two files  an in-file name and an out-file name  Here they are  Okay  We open the in-file name and we all sign the handle in-file  we'll open the out-file name and we'll assign a handle out file  This time we are going to write to it so we'll use this option W  For every line in that in file  we are going to write that line  and use an out file dot write  that's a method of out file  Then we are going to close both files  It's just simple as that  Now I want you to write a function that we'll take data from the screen and write it into a file  Let me show you mine  I've given you a starter file write_to_file(filename  myname  myage  major)  And this one almost would write the name out  So we got to open the file first  You're going to have to close the file eventually  okay  These lines tell you what you need to do where  This is mine  this is where I ran it  and this is where I displayed it  My name is George  my age is 21  I major in physics  You should do something like that  Here's my solution  I'll open that file name for write  and I'll sign out-file as the handle  Out-file write  my name is  then I'll stick in my name and then a character turn  Out-file write my name is  I'm going to convert my age  which is a number to a string so that I can take this string and tack on this string and then tack on this string  Then I'm going to pick out found right and but my major I am majoring in but the major  So and the only thing that you may have overlooked as you're sticking in that that conversion and number trust string  And then of course you close the file "
Bwo2oD8cu68,"Running Python Programs (Windows)  This video shows how to run Python programs on Windows 10 PC without using Spyder  The remaining videos use Spyder to do the same thing  using a feature Spyder  which may be phased out by the time you get to this point  That feature  I'll discuss at the end of this video  This is the program that we're going to run  print(\""Hello World!\"")  just a simple program  doesn't have to be complicated to illustrate running programs  Here it is in my directory  Now  to open a command prompt  I'm going to get in here to Cortana  search field  then I'll type CMD  then I'll choose command prompt  and there it opens  Then I will type  cd space documents/pythoncoursera/lesson3  That's where that pattern is on my machine  Yours may be at a different place  Now type  python hello  and run it  That's all there's to  Let me give you a couple of other things that you might need  and might not know  If you need to know whether the program is saying you can type dir  Now  in this case  there's so many files there  I can't find it  So  I type dir and hello  and I use this wildcard asterisk to show me everything that begins with \""hello\""  and ends in py  And there are two things out there  So  I can see that the file is there  Also in getting there  you may need to move upper directory  Say  if we are from lesson three to the directory above it  it may need to move down a directory  and you can do that  Now  let me show you another way to do this  First  I'm going to shrink this to kind of get it over either way  then I'm opening up this directory  This is Windows Explorer  and it shows \""hello  right there!\""  in this directory  Now  let's go run hello again  Click the end of the Cortana search field  and type CMD  run the command prompt  And I'm going to move this parallel to that  so that they're both visible easily  Up here  notice that this tells you where I am  but it's not in a path for me  But if I click here in the white  it changes and highlights  and I can type  Control+C and copy it  Then I go over here  and type cd space  Control+V to paste it there  That keeps you from having to do all of this typing and avoid errors and typos or somethings like that  So  now we're at the right directory  We can type python hello py  and it runs again  So  you just click here  paste here  Now  let's look at a third way of doing this  Back here in Spyder  we've got the directory up there  \""Hello\"" is here  I'm going to use that to get there  Click down here  CMD  bring this up  put it up here parallel  Now  I'm going to click over here  and highlight that  press Control+C to copy it over the Window  cd space  Control+V  put on the python hello py  and then run it  Now  earlier in the course  we wrote our programs here  and then we ran them in the IPython console down here  What we're going to do when we have whole words  we're going to write them here  and run them over here  So  we've got a different pane that's outside of Spyder running the program  but it works no different  Let me type the change to this program  Print(\""Goodbye\"")  save that change over here  paste that there  bring it in  run it  do you see a change? Now  let's look ahead at what's going to be coming up  I'm going to be writing programs like this  In my case  I have a copy of Spyder  then in the tools menu  there is an open command prompt  watch down here  When I click on open command prompt  I got a command prompt just like this one  Colors are reversed  It's just like this one  I can type python hello py  and run this program "
764m9ESzLvw,Installing Environments on a Mac  So far we've been writing and running functions  Now we want to start writing and running whole programs  And on a Macintosh and on Linux machines  Python 2 comes installed  Your machine may attempt to run our programs in Python 2  which doesn't work  or it may run them in Python 3  We're going to determine that right now  Go up to spotlight  type in terminal  press enter  run it  Type Python  press return  On my machine it returns Python 3 point something  so mine is going to run Python 3 automatically  yours may too  If it does  skip to the next video  you don't need to create an environment for Python 3  If yours displays Python 2 point something  then you need to create an environment  and I'm going to show you how to do that very quick  Okay  let me exit from Python  First  let me show you how to determine what version of Python you're using  Okay  if I bring Spyder up  mine says 3 6 2  you should do that and write down the version  If you installed an environment for that version  I think it will go a little bit faster than if you install it for a different version  although it'll work perfectly fine  Once you've done that  Go back to this  Now I will show you how to determine if you have an environments already on your machine  You use conda info --envs  And you can see that I have four environments out there  Yours will probably have just one  and that'll be root  Now we're going to create other environments  And this is the command that you use to do that  conda create -n for name  And then I'm going to create one called python362  because I'm going to create an environment for running Python 3 6 2  which is a version that I had when I looked at Spyder  You've written it down  you can- Do whatever version your machine has  Now this name is can be anything  you can name it whatever you want  And you put for Python version  whatever version you got  and Spyder  Once you've entered that  press return and it'll start fetching what it needs to from the web to install  When it gets through  it'll ask you whether you want to proceed and you say yes  And now it's installing that additional software and creating an environment  This may take a few minutes  Yours may display additional lines as it does  depending on how much work it has to do  Now it tells you to activate that environment  You type source activate and then the name for your environment  So I'm going to activate mine  source activate- python362  The name for yours will likely be different from mine  Now I can test that in a couple of ways  One  is I can run Python  and it should display Python 3 something really  it's not critical on what it does just as long as it displays the Python 3 version  Also  I can say conda info --envs  and I can see the environments out there  This asterisk shows which one is activated  Since we use source activate to activate Python362  it's the one that's activated  Now  let's repeat the process that you'll need to use in order to activate this environment  First of all  I'm going to close terminal  and we just going to quit it altogether  I'm going to we go up here  I'm going to type in terminal  Press enter  and I bring it up  Now  I'm going to type source activate python362  and now I'm ready to go  So now go onto the next video and we'll show you how to run Python programs  Remember  you need to bring up a terminal and type source activate and the name of your Python 3 environment  and then you're ready to go 
6CCwWCstDGc,Running Python Programs (Mac)  The purpose of this video is to show you how to run Python programs  Here's a very simple Python program  In fact  it is so simple we can run it in our IPython console like we used to  As you can see it prints hello  World and that's all  Most Python programs you cannot run this way  You cannot run them in IPython console  So let me show you how to do that  First of all  I'm going to shrink Spyder to half window  I'm going to go to Spotlight  and I'm going to type in terminal  And that brings up a terminal window  The next thing that I have to do is to go to the current working directory  I highlight it  type Cmd+C  Then I go back over here to the terminal  And I type cd space Cmd+V  to paste it into the command  And then I press Return and I'm there  You can check that you're there by typing pwd  print working directory  and you can see it  You can make sure that hello  the program that we're going to run  is in there  By typing  ls hello* py  that'll print all Python programs that begin in hello  For those of you that had to create an environment you need to change that environment right now  So you type source activate and the name of your Python three environment  Mine is named python36  yours is probably named something different  For those of you that didn't have to create an environment for Python three you don't do this step  Now we type python hello py and run the program  And there you see it ran hello  World  Previously we had typed our work into the Editor and we would type changes to our program  And then we'd run them down here in the IPython console  We're going to change that slightly  We'll still type our work into the Editor  but when we run it we're going to run it over in the terminal  Let me show you how to do that  I'm going to make a change to this program  Print Goodbye  I save the change  that's critical  Then I go over here and I can press up arrow to call back that last command and run it  And you can see that goodbye is now being printed  Now for the rest of this course we're going to write an occasional Python program  And we'll have to run it this way  I have an older version of Spyder that provides me with a terminal right down in this area  I'm going to use that for the rest of the course  You may or may not be able to do that  I'm using an older version to record the rest of the video so that I have that convenience 
MPoMnU2aGhk,Writing scripts in Python  So far in this course  we've been writing functions and running them  Now  we want to start writing whole programs or scripts  as Python calls them  And we need to be able to run those from a command window or a terminal window  And on a PC  you can do that by typing cmd exe  And you will get a Command Prompt like this  And now  you have to move to the directory where your programs are  And you will say change directory  documents  pythoncoursera  this is where I keep mine  And then  I can look at  see there by typing dir  And I can see where I am because of the Prompt  Or I can type this  On the Macintosh  you would use the search window  in the top right and typing terminal  And run terminal  The terminal window on Mac is just like this Command window on a PC  except that it uses Linux Command  You'll type ls  to see what's there  And not dir  And you would type pwd  to see what directory you're in and not cd  Now  we're not really going to do things this way  I just want you to know that it can be done  And as long as you have only the Python that we've installed on your machine  That this works just fine  However  Macintosh's some other machines come with Python already installed  This  in the case of a Mac  this is Python 2 7  which is not compatible with 3 4 that we've been writing  without making some minor modifications in the code  So we're going to not use this  We're going to took that away because Spider provides us with the equivalent  If you go to Tools menu  you can click Open command prompt  And there  you have this same command prompt  And I can type dir to see the files in the directory  Type cd and see what directory that I'm in  On the Macintosh  of course  those would be ls and pwd  Now  the advantage of this is that it is set up for Python 3 4  Whether you're on a Mac or PC  And we don't have to deal with those differences  Now  what we want to do is  Run a program  Now  one way to do that is that we can create this program  But first  I'm going to run it  I've got it already out there  And to run that program  we type python then the name of the program  Our first one's going to be print_file  Now  since we cannot pass in arguments we have to pass it command line arguments and we want to print humptydumpty  And there it is  So notice  we type python first  then the program we want to run  And then  more and more arguments afterwards  This would be equivalent to the arguments in a function  Now  let's create that from this  We're going to copy everything between the pound  percent signs marked at the cell  Cmd+C  I'm going to go up here to File menu  I'm going to create a New file  I'm going to paste this in to that new file  Now  this new file got it started but wait  we've already got everything  I think there something  so  I'm just going to delete  Now  it doesn't have a name  So  I'm going to File>Save As and notice that I'm in my default directory  And I'm going to save it as print_file which is what I called it [SOUND]  This already exists  now we've got that file ready to go  And we can run it as we did just a moment ago  Now  let's see how we did that  First of all  we had to use the system library  system library is going to enable us to read this  for example  The first thing we do is use argument of value of one to get the file name  This program is starting at zero  then here's argument one  If there were more  then it'd be argument two  three and four and so on  Now  from that file name  we'll open it  and we'll get a file name in file  This is just like our print_file earlier all the way through  We're relining in file  we'll print that line  We won't put a carriage return on it  And then  we'll close the file  This is the difference  To get our arguments  we do that  Let me run it one more time  So satisfying to see these things run  Now  let's do an exercise  And we're going to do this together  And this is going to be another conversion  You want to take a New file  Paste this in  I don't need this stuff  And we have instructions on how to convert it right here  To convert this function to a stand alone program's Script  That takes two file names from the command line copies  one to another  Let's run this thing just so y'all know what it does before I save it  Python  you remember to run the program  this one's going to be called copy_file  So we give it the name of the program  Then we're going to copy humptydumpty  our old standby  And we're going to call it  tie it to humpty txt so I can find it easily  It would be at the end  And it doesn't say anything here  But hopefully  it did it right up here  And there's humpty dumpty all copied  Now  what I want to do is write this program from this function  So I'm going to say Save As and I'm going to call it copy_file  It's going to complain because it's already there  But I've got the instruction here on how to do that  Delete this def line  And notice  the def line gives us the infile and outfile name and we're going to loss that when delete this  The next thing we do is we're going to use the Edit menu to Unindent all lines The next thing we're going to do is import the system library  Now  along the way you might want to save this thing occasionally so that you don't lose any changes  Now  we lost an infile name and an outfile name  Since it's no longer a function with arguments  So we gotta get the infile name from somewhere and that's where the sys is useful  sys argv(1) is the first thing after the program name  sys argv of 0 is the program  sys argv1 is this  this is sys argv of 2  Outfilename = sys argv(2)  So now  we've gotten these two arguments from the command line  And the rest of the program is the same  We're going to open the infilename  We're going to open the outfilename for write  We're going to step through the infile  line at a time  and write it out to the outfile  Then we're going to close them both  Let's save this  function  And now  just to see that it actually works  I'm going to run it again  That way  let me Delete this  So you will also have [SOUND] cheating  And there it is  and it should be a copy  Now  let's look at another Script run from the command line  You can copy this into a file or you can use mine  clwc py  To run this from the command line  I need a console  What this has done is taken our file  humptydumpty and counted how many letters are in the file  how many words are in the file  and how many lines are in the file  Now  let's look and see how it does this  First of all  we import our system library  We get the file name from sys argv(1)  Remember  this is sysrv(0)  this is sysrv(1)  We're going to count the lines and characters  So we set up a variable initialized to zero to do that  For every line in the text file we're going to add one to the line count  For every word  And the lines split up into words  We're going to add one to the word count  And then  we're going to add the length of a line which is the number of characters in the line to the character count  And we close the file and we print out the result  Let's look back at this line split for a second  Let me grab that line  go back over into Python itself  And I'll define line equal to that line  There it is  line split() fills a list of the words in that line  So we're saying for every word in this list of words  add one to that word count  That's the program  Now  let's look at another program that reads from a file and also uses a dictionary  This will illustrate the value of using dictionaries  First  let's put it into play and write  What this does is it reads through the file Humpty Dumpty  Picks out each word  As you can see  there's a long list of them  And counts how many times that word is used in the text file  Now  let's see how it goes about doing that  First of all  as you expect  We're going to open the text file  we get the name as an argument  we open it and we have a handle called  text_file  Ultimately  we'll close that  Meanwhile  we'll use one of our standard loop paradigms  We'll set up an empty dictionary  word dic  And then  we will add to that inside the loop  So we step through the loop for every line in the text_file  We'll take each word in that line after we change it to lower case  So all the words can be compared even if they are capitalized in one place or another  And then  we'll split the line into words  So for every word in this line  converted to lower case split into words  That's a list  We will strip the word of punctuation  If the word is not in the word dictionary  we'll put it in there with a count of zero  Where it's in there or not  whether we just inserted it or whether it was already in there  We'll add one to the words count  So the first time it's there  it'll be in there with a count of one  The next time  its count will be increased by one  So that's our loop  We'll go through every line  we'll go through every word in that line  we'll strip out punctuation like commas and semicolons and so on  And then  we will insert that word into the dictionary if it's not already there and add one to its count  Then when we close the file  we'll have a dictionary  And the key will be the word and the value will be the number of times that word appears in the file  We'll print out what we're going to do  And then  we will sort the dictionary and convert it to a list  Actually  that will sort the keys and convert that to a list  For every word in that list  we're going to print the value of that word in the dictionary  That's the count  the number of times it appears  And we'll print the word  And there you see that printout right there  The exercise that I want you to do now is to convert this to a standalone program or grab my standalone program and run it from the command line  Let me show you running it from a command line  This is the program  and notice that the indentions have been removed  We'll open a Command Prompt  Python count_words py and it'll run it on HumptyDumpty txt  I have another file out there that you can run it on too  ADream txt which is a longer file  it has more words in it  Let me bring that up  This is A Dream Within a Dream  by Edgar Allan Poe  It's a poem 
gv1SKCt0vnc,Reading and writing CSV files  Now let's look at a particular kind of a file called a CSV file  CSV stands for comma separated value file  Let me give you a couple of examples  And this is a column basically for name and a column for phone number  separated by comma  In this case  there's a column for the day of the week and then a column for the meat on the menu and a column for each separate food  Let's go out to the file explorer and look at these  There's menu And there's a column for each of the foods  CSV files can be read by lots of different applications and they're often used To transfer data from one application to another  Now let's see how to read these files  I'll show you a program run first  Notice that this first row  George and his phone number  were read in to a list  Jackson and his phone number were read in to another list  each row of a CSV file is read into a list  Let's see how it looks  Here's our program  We import this library CSV so we can read and write with it  We open the file in the usual way  we close it in the usual way and what's different is this loop  Instead of saying for every row in f  we say for every row in csv reader(f)  So that we have the CSV library read everything in and separates it into the columns  in effect and then we print the row  No  CSV reader will return a role as a list  so when we print that row  it will print as a list  Next  let's look and another application  In this case  what we're doing is we're reading each row  and adding it into a bigger list  Now  remember  each row is a list  So  here's what one row  Here's another row and here's another row but they're all in this bigger list  Let's see how we do that  First of all  we're going to import to CSD library  We're going to open the file in the usual way  We're going to close it in the usual way  Now we've got one of our standard loops  We'll start off by initializing an empty list  which we'll call data  Then for every row that the csv reader returns from the file  we're going to append that row onto data  So we'll just loop through until we read all of the file in and stick each row into data as a sublist of the bigger list of data  Then when we get through  we'll just print the data list  Now I want you to try an exercise  I want you to convert read_csv_file to read_csv_file2  Actually  I've got that copied down as a starter file here  so that you print each row without the bracket  CSV file puts a bracket around everything because it reads in the rows as lists and we don't always want them to be displayed as lists  So I want you to modify this so that it prints without the brackets basically  You're going to open the file in the usual way  You're going to close the file in the usual way  You're going to read it in  in the usual way but now you gotta write code  So instead of it printing the whole row as a list  I want you to print each item in the list  Now of course  that means that you have to know how many items are in the row so that you can refer to them as row of whatever  row of whatever  row of whatever  Let me give you my solution  And then you write yours  so here's my solution  Let me talk about it for just a second  There's a column for the author's name  and then a comma  and then there's a comment  A column for the book name then a comma and then a column that tells you what kind of book it is  And notice that the authors name has a comma in it but the comma's used to separate the columns  So this author has to have his whole name enclosed in quotes  The CSV reader takes care of that for you  So you notice all the authors have a comma and so all the authors in enclosing quote  CSV reader automatically does that so that there's no confusion  When CSV reader reads something in it's going to read it into a list and so you have to know how to access each part of the list in order to print it out  How do you do that? Here is my solution  Read CSV file to a file name  of course I have to import CSV library  I open the file and I close the file and for every row in the CSV reader  I've got to print the data in that row  Well  I print the author is row of Item 0  The name of the book is row of Item 1 and the type of book it is is row of Item 2  Here's another example of writing to the CSV file  In this case we have a list of rows in the and we want to write them out to the CSV file  Let's run it first and then let's look at how the code works  Okay  so we wrote this list out to holidays csv and then I've retyped it and we see that it is right there  Let's run it in Excel and see what Excel shows  So these are some US holidays  This day  Martin Luther King Day  that's a Federal holiday  This day is Groundhog Day  that's an observance  And here is the program [INAUDIBLE] and of course we're going to import the library CSV  We're ultimately going to open a file  and we're going to open it for write and we're going to set the new line equal to blank  That's because we don't want to double space  Every time we write out there  it'll do a complete line  If we don't have new line equal to blank  there will be a new line there and it'll skip two lines  One for the right and one for the new line at the end of the right  Item in the list l  we're going to use CSV writer on the file handle f to write the row consisting of item  Okay  so here is the list where we're writing  The first row will be date  name  and notes  The next row will have the date  Martin Luther King Day  federal holiday  and so on down the road  Now  let's write a modification of this  This modification will be to take the data from the keyboard and write it into file  Let me run mine first  and then we'll talk about it  Okay  here is a run of my program  Friend's name  Elaine Bends  Number (212) 431-1414  Jerry Seinfeld 212 555-1345  Cosmo Kramer  212-18 4 1  2  3  4  Now the way this works is if you press Return and not enter a name  it stops  Now we can see what the value looks like see if it has those three names and numbers in it  I'm here to do somewhat the same thing  I've got an example run here  That you can model everything after and here's a starter function  What you've got to do is open the CSV file and ultimately close it and there's a loop  says while true  that's always true  So this loop will loop forever  I'll show you how we end it  later  What we do is  we'll ask for the name  Enter friend's name  press return to end  Okay  so we enter friend's name  If the name is blank  we'll break out of the loop and that's what happened at the end  I pressed return here  There wasn't any name  and so it just ended the program  And then if the name wasn't blank it'll go into this next statement and it will print the name  So up here I entered Elaine Benes and her phone number  and it printed out Elaine Benes's phone number  Now  this particular version doesn't Input the phone number printed out  That's what you're going to add so the first thing you should do is to make sure this runs for you and then you add lines to build that row  You got the name you got the phone number to build a row you got to put the name and phone number into a list and then you need to write that list using CSD Writer  That's all you have to do  it's fairly short  Now  let me show you what My solution was I imported the CSV file  I used CSV file as a handle and opened the CSV file name that was given to me  I [INAUDIBLE] it from right I opened with a new line equal to blank so it wouldn't double space  Now while true  that was already there  next thing already is there  if the next name is blank  you break  Now  what we have to do  is get the next phone  and then we print the next name and the phone  Then  we've got to build the row  so  we'll have line = []  we'll append to that line the next name  we'll append to that line the next phone  And then we'll use csv writer on this file handle to write the row  So what you would do is have to add this row  and this row  and all of this writing material  Now let's do another example  And what we're going to do is we're going to read in CSV file and updated  The particular one is fairly straightforward  What we're going to do  let me run it first  Now wait  let's see what that looks like  It has a date and a weight  I want to do with this is to compute the average weight over that time period and write that as the last line of the new file  So we're updating CSV We've got an old file name and a new file name  We're going to import the CSV library  We're going to open the old file name as the input file  We're going to open the new name as the output file and we're going to open it for right  We're going to set new line equal to blank  Here's what we have to do  We have to add up all the weights and we have divide by the number of weights  So  we've got to have two counters before we go into our loop  W have to have a counter set to zero to count the number of items and we have to set the total weight equal to zero  We'll add into those weights  all of that gets done In this loop  And then when we get out of the loop  we've got the total weight added up  and we divide it by the count  and that gives us the average  And so we have a new row that says Average in the first column  and this number that is the average in the second column  Then  when you CSV write it to write it to the output file  we'll write just one row  Namely  the row we just constructed  Then we'll close both the files  Now  let's look at the loop here  remember  count is 0  Total weight is 0  So for every row in CSV reader of  So if we look at every row and we let the CSV reader bring the row in so that it is broken up into the various columns  okay  The zero position is going to be the date  Notice the first row says date and weight  we can't average those in  so  we have to check the row  If it starts with the date If the item zero is date  we're just going to skip it  okay? So if the row is not date  then we're going to add one to the count  we've got a new good row  we're going to add to the total weight  The weight in the second column  which is item one  But we've got to convert it to a real number so that we can add it in because these are strings and then we will write the row out to the new file unchanged  So whether it says data or not  we're going to write the row out to the new file  If it doesn't say date  we're going to add one to the count and we're going to add the weight to our total weight  So when we get to the end  we've got the weight totaled up  we've got the count counted  we can divide the cannon to the total weight to get the average  And then we write the row out  that's all there is to it 
FACJs5M1ypQ,Long strings  random library  building and sorting lists  Let's look at a couple of ways of making long strings that won't fit on one line in the editor  In both of these cases you have a string on each line  that can be two or more lines  In this first method you tie them together by putting a plus sign at the end of a line and a back slash to connect it to the next line  In the second method you put a parentheses at the beginning and an end parentheses at the end to tie them together  In both cases you wind up with one long string  Sometimes you need a list of numbers  We're going to import the Python library random and use it to generate a list of random numbers  Call the function makeRandom and it generates ten random numbers  Just take a look at that function  First of all  we're following our usual basic loop pattern  We're going to create a list here that's empty  We're going to step through a loop ten times  We're going to generate a random number between 1 and 100  a random integer  and we're going to append it to that list  And then afterwards  we're going to return this list  Now  normally we've been printing these out but returning this has the effect of assigning a value to makeRandom that can be used by other functions and programs  As you can see  it works virtually the same as printing it out from the iPython window  Now what I want to do is show how we make use of this  So we're going to have a function called call_make_random  It will call this function and it will  this is the call right here  it will make the random number which is returned and assigned to this variable  And then we're going to print that variable  Call_make_random  and it prints out the list of random numbers is this  So you see  our function returned a value and that value was a set  set equal to this variable and then we printed this variable out  Sometimes we don't want a different set of random numbers each time  We want to generate the same numbers over and over again  This is particularly useful when you're debugging a program  You don't want to be surprised by a different set of numbers and have your program work differently  We can do this by setting a random seed to some particular number  and then it will generate the same random numbers every time  Here's a function called make_same_random that will set the seed to 17  We run it again  We get the same numbers  Now  we have a function named call_make_random that will call the random function and get a set of integers  print them  and then it'll call the function again and get another set of integers and print that  And you'll see that they're the same ones every time  I want you to write a function that will generate random real numbers  You'll use random random which returns a random number between zero and one instead of rand_int which returns an integer  Here is what mine looks like  make_random_real runs and it generates a list of ten random real numbers  Now let me show you my solution  We'll create an empty list  then we go through a loop ten times  we generate a random number between zero and one and we append it to the list  and then we return that value  Now I want you to modify your solution to use a random seed  Use 17 as your random seed  In my solution I did this and I ran it twice  and I get the same numbers  Here is that solution  The only real difference is having this random seed there  Now  let's look at sorting lists  Here's a number list that we can work with  And what we've done is we've printed the number list  there it is  and we sorted the number list and then we printed it again  As you can see  the number list is permantly changed and is now in order  Let me restore this number list  In the past  we had this function sorted  It was a sorted list but it's not permanent  It printed it out in order all right  but nonetheless  it's still in the original order  Now let's create a list of letters  Make out the list produces a list of letters  Let's look at how that works  We import the random function  Here are the complete list of English alphabet letters  We set a random seed so that it always comes out to be the same  Then we have a basic loop  We set alpha list equal to a blank list  and then we step through this loop ten times and we choose a random letter out of the alphabet and then append it onto that alpha list  And then we return that alpha list  Now I want to have a list that's equal to that alpha list  and so I'm going to execute this  This says run alpha list and return the value and set it equal to alpha list  You can see I just did that  This is the same thing  I just wanted to show you how to generate a random list of any arbitrary size of letters  Now let's sort them  This prints the alpha list  Then this sorts the alpha list  And then this prints the alpha list again  And you can see that it got sorted  Now here's an alpha list that has a mixture of upper and lower case letters  If we try the same thing with it  look what happens  Here is the list unchanged  then it sorted and then it printed out sorted  But all of the capital letters precede all the lowercase letters  Here's a capital D for example which precedes a lowercase b  That's not what we typically want to happen  but we can change the sort function very slightly and we can tell it to use as its key the string changed to lowercase  Let's start the list all over again  and now  let's sort that  Here's the list as we originally had it  And here's the sorted list right here  And notice that the sorted list has a  b  d  d  e  It's in alphabetical order regardless of capitalization  Let's create a list of strings  Sort that  Worked out just fine but everything's lowercase  Let's have the same string as a mixture of upper and lower case  Now when we sort it  all the words with capital letters precede the ones with lowercase letters  Let's restore the list  Now we can use key equals string lower which changes all the characters to lowercase before it sorts  And this is what we get  This is more like what we expect  Here's the original and here's the sorted one  The sorted one is in alphabetical order regardless of the case of the letter 
H00_PVX2bRw,Descriptive statistics  Now we're going to do a little descriptive statistics  First  let's look at Python documentation for a moment  I'm going to go to this website  Notice that it brought up 3 5 1  When you do this  it might bring up a different version because it's going to bring up the latest version  Now  I'm using 3 4 and its table saying  I'm going to click on that  That 3 5 is in development  At this point and time  Now look up statistics and that search produced a whole batch of things  Statistics as a library module is right here  And it's in Lib/statistics py  And we're going to look at several functions  Mean  median  mode  standard deviation  and variance  Let me click through on one of those  So here's the mean  you can give it a list and it'll compute the mean of the list  You can give it a list of reals and it'll compute the mean of that  You go back to Spider you can get to the same place I type in Python documentation  click on index  statistics  and you see the same page  Mean  median  mode  standard deviation  and variance  So let's define some lists so that we can do the statistics on them  I'm also going to import the statistics library so that we can use it  Now I'm going to compute the mean of this N list  There it is  That's adding up the values in N list and dividing it by the number values which is seven  We can do the same thing for the real list  We're going to add up all these numbers and divide by seven to get that  Now lets look at This in list again  And I'm going to print out a sorted version of it  Now the median  is supposed to be the middle value  From the sorted version  we can find the middle value is five  Smaller three larger  Now I have another list  endless even  This has an odd number  so there is a middle value  nlist even has a little bit more complication than that  There are two middle values  three smaller  three larger than the two  What the median function does is it averages those two middle values and we terms that as the median  With our list we have the same kind of situation there have 3 14 as three 3 larger and 3 smaller  So  that's the median  And that's what Python finds  Now I have another r list and it has two values in the middle  three smaller  three larger and what does Python do with that? It takes the average of those two in the middle and returns that as the median  same way  Now let's define another list  mlist  now I want to print mlist and a sorted version of it  If you'll notice three occurs more times than any other  And that makes it the mode  The mode is the value that occurs most often  Now we have standard deviation  Then we have variance  You can look those up in the statistics book if you're not familiar with it  The standard deviation is a square root of the variance and they're both measures of how spread out the data is about the mean  Now  here are three more functions  Maximum  minimum  and sum  Defining another in-list here  the maximum of this is 106  Found that using max  Then  we'll find the minimum of this list  which is minus 4  And the sum of the list is sum of n list  There it is  Now  let's write some functions that make use of these descriptive statistics  First of all  I'm going to generate a list  stat_list  Stat_list has 100 random reals between 5000 and 6000  How do we get that? Well  random random produces a random number between zero and one  And we have to manage to convert that to random numbers from five to six thousand  One of the things that we can do is multiply it by thousand  then we have a random number between zero and a thousand Clinical add 5000 to that  we have a random number between five and six thousand  So this is how we wide up with a number between five and six thousand that's pseudo random  And we going to append that to the start list  We are going to that 100 times  So if I type stat list  see there are a lot of numbers  That list is a list that I'm just defining  this small list of integers  Now  I can compute the stats on these using my stats  Now what this does is it imports the statistics then it prints the mean and uses statistics mean to do it  Computes the median  computes the mode  computes the standard deviation and completes the variance  I wanted to try that out  Add list  Look what's happened here  It crashed  It did manage to compute the mean and median and print those out before it crashed  But on the mode  It ran into trouble  And ultimately after it crashed it gives you an error message  No unique mode found 100 equal to common values  Well if you've got real numbers between five and six thousand  with only 100 of them the chances of two of them coming out to be the same are slim  So  What has happened here is that that error occurred in this module  statistics py  the library in fact line 434  And that's where we go the error message  But that was  we wound up in that place by calling statistics dot node and we wind up calling statistics dot node because we ran our function  So there's a trace down through the series of calls that made this error happen  The main problem is from our programmers point of view is statistics mode failed  Now  what I want to do is correct that  I'm not going to keep it from having the error  What I'm going to do is keep program from crashing  And this is the way you do it in Python  You have a tri-accept pair of statements  We try and then we are trying to do the same statement that failed  If it fails  if it's a statistic error  we are going to store that error in e and we are going to print mode error and e the mode was where had the promise saying mode error  If this works just fine the exception doesn't get executed  In either case we go on to the next statement  We don't crash  Lets actually keep that and see that that happens  See? No crash  but we do get that same error message at the end  Now let's try another function that involves using a try and accept statement  This function  as you input a number then its going to print what type that is  Actually inputs are always string so it's always going to print string  Okay  we're going to try to convert that number  we're going to try to convert that number to float and print it  If that fails we gotta grab that exception  Stored in e and print the exception was e  But we won't crash  Here is test_try()  Enter a number  Now  what it's supposed to do is convert a number like 5 to 5 0  You can see  that it actually printed the type  was a string  And it converted to five  We try something else  3 14  the same thing happens  It's a string  converts it to 3 14  which It was to begin with except that it was a string  Wonder where it failed  Let's suppose we enter some letters? It couldn't convert that  What if we just typed and put an extra period in the number? Can't convert that  But notice that it never crashes  It tries  and then it grabs the exception and prints the exception out  Could not convert string to float  Now  here's an exercise  I want you to run this code on your machine  it creates a list of temperatures  These are farenheit temperatures between 20 and 95  And we built that by using a random seat so it comes out to be the same for everybody  We start with an empty list  we go through this loop 20 times  we generate a random integer in this range  and append it onto that list and there we are  And here is your starter function if you want  When you finish it  it should print the list  the mean  the median  the standard deviation and the variance  Here is my solution  There  What it does is it prints the temperature list  It prints the statistics mean  the statistics median  the statistics standard deviation  and the statistics variance  I stayed away from mode on this version  Now  I want you to try and soup that up to add mode  There won't be a mode on this  So you should get an error  Here's a run of my solution  You add the necessary code to your solution  to get this  Here is my solution  No unique mode found for equal  notice the program did not crash  What we did was we added a track steps statement to the end  We try to print the mode  Just like we did for all the others we use mode as setup variance for example  And if there is an exception we store it the exception in e and then print mode error e  The mode error was no unique mode four equal to
5c41KiZXh-M,Formatting print statements  Now let's look at Formatting Print Statements  Here's some variables that we'll work with  and here is a string we want to print  It'll have name  colon  and then we intend to put the name Teddy Roosevelt into that slot  And his age and his weight here  Here is an example  In the first one  we put his shortened version of his name Teddy in age and weight  So there's Teddy Roosevelt  age  weight  In the second one  we used a longer version of his name  and you see that it caused age and weight to move over  One of the things that we might typically want is to have these aligned a little bit better  Here's another formatting string  And in the 0 slot  we're allowing 26 spaces  in the 1 slot 4 spaces  and in the 2 slot 10 spaces  This is minimum number of spaces allowed  Also  this right arrow indicates right alignment within that space  Let's see what that looks like  So Teddy Roosevelt is right aligned within a space of 26 characters and the long version and the short version are both lined up one above another in a nice fashion  The ages are aligned up and the weights are aligned up  Now the weight is too many decimal places  So I'm going to change that a little bit  In this case we're going to say that the weight is going to have at least five spaces  two of which are to the right of the decimal  This is a floating point number  So here it is  At least five spaces two to the right of the decimal  It looks nicer  Now one of the things that we can do is use this string down here  this variable name instead of the actual string  It gives you the same result  And this is more compact  I want you to try some of this and get familiar with what is actually done  Here's a short string  What I've got here is a string that begins start bar bar and ends with bar bar end  So that you can see that this string got fitted into here and it is left justified  If we printed the number into that space the string is exactly the same  But we're sticking in the number  The number is right-aligned instead of left-aligned like the string  Here's an exercise I'd like you to do to become familiar with it  I want you to take a string S and a format statement like I just used  And try various things with it  Use 0 to just print the string with no formatting  If I press up arrow  I'll bring this back up  And I can put the 0 there  press Enter  That's what it does  Now you can up arrow it  and use  25  See what it does  so why don't you go down through this list and try all of these options  Here's what I get when I do it  Here are my solutions  Start zero end  Format s allow 25 spaces  here's 25 spaces right aligned  here's 25 spaces left aligned and here's 25 spaces centered
uZlSDHHyzoQ,Starting the database application  Now let's write a small menu-driven database application to make use of what we've learned  First  I want to run it to give you the idea of where we're going  I'll open the command prompt  python phones py  This is the menu  I type show  it will show all the records we have there  So we have three names and three phone numbers  We can create a new name With a new phone number  I'm going to show you that it's there  We can delete a name  We're going to delete number three  show you that it's gone  We can edit a name  I'm going to edit number three  That's the lame [INAUDIBLE] and I'll press return and leave that name unchanged  But I'm going to enter a new phone number  I'm going to show you that it's there  And then I can quit  what happens when we quit is that it saves the changes that you've made  That's the way we left it  All of this is stored in a CSV file  My files  All of this is stored in a CSV file  Name and phone number  Now let's go over and look and see how we do this  First of all  we're going to write a simplified version of this program  We go to app python consult and run that simplified version  It doesn't really load the file it just announces that it's loading the file  If you quit  it announces that it's saving the file and doesn't actually change the file  Run it again  If I show  it just announces it's showing  If I type new  it just announces it's adding  If I type delete  it announces it's deleting  And if I type edit  it announces that it's editing  And that's all that it does  The reason is we can't write all of these at once  What we're going to do is we're going to write this main portion that displays the menu  and here is the first version of our program  Deleting the phone is just a function that announces that it's deleting  Edit  the same way  Each one of these actions is just a stub that announces what it does  I've got a function for displaying the menu  We've got a function for going through what we'll call the main loop and the way you start the program is we have a statement like this  There are two underscores before and after name  two underscores before and after main  If the name is main  then we're going to start at the main_loop  and the main loop is a function  First of all it loads the phone list  the last thing it does is saves the phone list and next it runs a loop  While true  do everything in this loop  Well  true  is always true  so this loop will run forever  I'll show you how we get out of that later  First thing we do is we get their menu choice  Menu choice will print the menu and retrieve what your choice is  If your choice is none  that is you didn't make a choice  you'll just double back and do the loop again  Continue means continue at the beginning of the loop  If your choice is q  then we'll print exiting and break out of the loop  Which means we'll do this next statement which will save the phone last and then we quit  If your choice is new  we'll create a file  If the choice is delete  we'll delete a file  If the choice is show phones  we'll show '  The choice is edit  we'll edit the file  If there's anything else  we'll just announce that as an invalid choice  Now let's see how we get  if we print the menu and make the choice  We call this function  And  first of all  it says print  Choose one of the following options  Then it prints all of the options  So we see that here  And then we use an input statement to get that choice  Again  we see there's right along in there  Now we changed the choice that the person makes to lowercase so we can compare it to these letters in this table  If it is in this table  then we'll return that choice changed to lowercase  So  that down here in our main loop all of the choices will be lower case when they get there  So  I don't have to compare the q and capital q  If the choice is not in that  they will print the choices of question mark  That is an invalid option return none  Let's try that and see what happens  Choose one of the following options  I'll choose t  that's an invalid option  I'll choose 3  that's an invalid option  But if I choose capital Q  that works just fine  Even though capital Q is not on this list  capital Q gets change to lowercase before we look  That is the first iteration of this program  We're going to grow this program bigger  Let's take a look at it  We just have a bunch of stubs  And we only have one main loop  that function is written  and a second function that gathers the menu choices  And we start this by having this special statement at the end  It tells Python where to start executing the program  it says starts executing at the main loop  if this is the main program  The first stub that we are going to flash in  right before function form  is show phones  the reason for that is and if we did delete  we would know whether something was deleted  If we did new we would know whether something new was done  with show phones we can see whether these other things are working when we write them  Let me run this one  just a moment and show you what it does  Nothing else really works  Show  Now the way we're doing this is  I've going to ask something to show  So I create a list of phones here  There are two phones Jerry Seinfeldâ€™s number and Cosmo Kramer his number  Each phone is a two element list consisting of name and number  The name position is zero the phone position is one and Iâ€™m going to use name position and phone position as zero and one to refer to this side so I donâ€™t get confused as to what I'm looking at  Then has to be a header for this name and phone number  And so  I create a header as though it were another phone  Now  here's the show phones function  First of all  I'm going to show phone the header  the first ones black  Iâ€™ll show you what this black is about later  Then I am going to step through the phones  For ever phone in phones  Iâ€™m going to show phone and Iâ€™m going to send this index  so there numbered 1 2 3 4 5  Start the index off at one  after I show a phone I'm going to bump the index up by one and I'm going to pass to show phone not only the phone itself but the index  And I'm not going to show the individual phone in this function  I'm going to have another function that will do that  And then at the end I'm going to skip a line so that there's a little space after the list of phones  Here's how we show an individual phone  We pass the phone which is remember a list with two things in it  the name and the phone number  And we send the index  We have a string  The index is going to occupy three spaces and it's going to be right to just to fact  The names is going to occupy 20 spaces and it's going to be a left justify  The phone number is going to occupy 16 spaces and it'll be right justified  We're going to use that output string and we're going to format it so that index goes into the zero element position  Phone name goes into the one position and phone number goes into the third position  And there it is  index  name  phone number  That's all there is to it  You've got show phones  which is going to send the header  it's going to step through the list of phones  and send show phone  each phone  And the show phones are going to be formatted and printed  Up here we did create these things and that's to get us by right now  Ultimately  we'll set the phones list to the empty list  Now let's replace some more of these stubs with real functions  Let's start by creating a new phone  First  let me show you what this version will run like  They are the ones that we've built in  and here's a new one  enter the data for the new phone  George Costanza  2128351419  Now we show  and there's George  Now we can delete something too  Well  which one do you want to delete? We'll delete George  Now he's gone  That's what's new in this version  First  let's start with how you create a phone  When you create a phone  it's going to print into the data for a new phone  There it is  Then you gotta use an input to get the new name  You're going to input to get a new phone number  Then you're going to build the sub-list  name and phone number  and then you're going to pend it onto the phones list and then you'll skip a line  That's all there is to it  Enter the name  enter the number  build a phone sub-list  pend it to the phones list  Remember the phones list  Here starts with Jerry Seinfeld and Cosmo Kramer  Now  let's look at delete  That's a little bit more complex  Now down in the main loop  when you choose delete  it's going to ask which item do you want to delete and reprint which one it is for the time being and then we'll try to delete that one  Here's the delete function  First of all  we check to see if there's a proper menu choice  At the time we did this deletion  there were only three people in there  If I asked it to 17  it couldn't do it  So we are going to check to see if that's a legitimate menu choice  Also if you typed in a letter or something like that  we want to fill to that out  So  we're going to write a routine that'll check to see if that is a proper menu choice  And if it's not  we're just going to return without deleting the phone  If it is a proper menu choice  I want to change to it an integer  And then I'm going to delete one of them  Now remember  we count these one  two  three  four  five  Those are the phone lines  But  Python counts zero  one  two  three  So  what we have to do is reduce that by one and then we delete that item from the phone's list  And then we'll print that we've deleted it  Now letâ€™s take a look at this proper_menu_choice business  We pass this which onto that proper_menu_choice option  Now remember which came from an input statement so itâ€™s a string  Now we going to have to convert it to a number  So first of all  we're going to need to check to see if thereâ€™s digits in it  If it has something in it which is not a digit then which isdigit will return false  So not is digit would be true  and we have a print  this which needs to be a number of a phone  and return false  If it returns false to delete phone  then not false would be true and it would return without deleting the phone  So the first thing that happens is we check to see if it's a number  And if it is a number  we'll convert it to an integer  Now  there's another problem  It could be a negative number  We could have three phones and it be the number 217  We check to see whether this negative  whether it's a bigger number than there are phones  If so  we'll print that it needs to be the number of a home  And then we'll return false so that delete_phone will not attempt to delete it  Now if it passes this test  that it's a number  it converts to a number and itâ€™s a number thatâ€™s within the proper range  then we are going to consider it an acceptable number to delete and will return true  So to summarize  delete_phones will be passed which phone to delete  It will check to see if thatâ€™s a number and if itâ€™s a number within the proper range  If it is it will convert it to an integer and it will delete that particular phone from the phone list 
_SxlpxqIs-s,Displaying the records  The first stub that we're going to flesh in  write the full function for  is show phones  The reason for that is  if we did delete  we wouldn't know whether something was deleted  If we did new  we wouldn't know whether something new was done  With show phones  we can see whether these other things are working when we write them  Let me run this one just a moment and show you what it does  Nothing else really works  Just show  Now  the way we're doing this is  I've gotta have something to show  So I create a list of phones here  It has two phones  Jerry Seinfeld and his number  Cosmo Kramer and his number  Each phone is a two-element list consisting of a name and number  The name position is 0  a phone position is 1  and I'm going to use name_pos and phone_pos  instead of 0 and 1  to refer to these  so I don't get confused as to what I'm looking at  There has to be a header for this name and phone number  and so I create a header as though it were another phone  Now here's the show phones function  First of all  I'm going to show phone  the header  and I'll pass around this blank  I'll show you what this blank is about later  Then I'm going to step through the phones  For every phone in phones  I'm going to show phone  and I'm going to send this index so they're numbered  1  2  3  4  5  I'll start the index off at one  after I show a phone  I'm going to bump the index up by 1  And I'm going to pass to show phone  not only the phone itself  but the index  And I'm not going to show the individual phone in this function  I'm going to have another function that will do that  And then at the end  I'm going to skip a line  so that there's a little space after the list of phones  Here's how we show an individual phone  We'll pass the phone  but just remember  a list with two things in it  the name and the phone number  And we send the index  We have a string  the index is going to occupy three spaces  and it's going to be right justified  The name is going to occupy 20 spaces  and it's going to be left justified  The phone number is going to occupy 16 spaces  and it'll be right justified  We're going to use that output string  and we're going to format it  so that index goes into the 0th element position  Phone name goes into the 1 position  and phone number goes into the 3rd position  And there it is  index  name  phone number  That's all there is to it  You've got show phones  which is going to send the header  It's going to step through the list of phones and send show phone each phone  And the show phones are going to be formatted and printed  Up here  we did create these things  and that's to get us by right now  Ultimately  we'll set the phones list to the empty list 
0IgOapAtauM,Adding and deleting records  Now let's replace some more of these stubs with real functions  Let's start by creating a new phone  First  let me show you what this version will run like  They're the ones that we built in  here's a new one  enter the data for the new phone  George Costanza  (212) 835-1419  Then we show  and there's George  Now we can delete something too  Well  which one do you want to delete? We'll delete George  And now he's gone  That's what's new in this version  First  let's start with how you create a phone  When you create a phone  it's going to print enter the data for a new phone  There it is  Then you're going to use an input to get the new name  You're going to get the input to get a new phone number  Then you're going to build the sub list  name  and phone number  And then you gotta append it onto the phones list  And then you skip a line  That's all there is to it  Enter the name  enter the number  Build a phone new sub list  Append it to the phones list  Here with the phone's list here  Starts with Jerry Seinfeld and Cosmo Kramer  Now  let's look at delete  that's a little bit more complex  Now down in the main loop  When you choose delete  it's going to ask which item do you want to delete  And we print  which one it is for the time being  and then we'll try to delete that one  Here's the delete function  First of all  we check to see if there's a proper menu choice  At the time we did this deletion  there were only three people in there  I asked for number 17  He couldn't do it  So we're going to check to see if that's a legitimate menu choice  Also  if you typed in a letter or something like that  If we want to filter that out  So we're going to write a routine that'll check to see if that is a proper menu choice and if it's not  we're just going to return without deleting the funnel  If it is a proper menu choice  I want to change it to an integer  And then I'm going to delete one of them  Now remember  we count these one  two  three  four  five  those are the phone lines  But Python counts zero  one  two  three  So what we have to do is reduce that by one and then we delete that item from the phone's list  And then we'll print that we deleted it  Now let's take a look at this proper menu choice business  We pass this switch onto that proper menu choice function  Now remember which came from an input statement  so it's a string  Now we're going to have to convert to a number  So first of all  we're going to need to check to see if it's digits in them  If it has something in it  which is not a digit then which is digit will return false? So not is digit would be true and we would print this which needs to be a number of a phone and return false  If it returns false to delete phone  then not false would be true and it would return without deleting the phone  So the first thing that happens is we check to see if it's a number  And if it is a number  we'll convert it to an integer  Now there's another problem  It could be a negative number  We could have three phones and it be the number 217  We check to see what this negative or whether it's a bigger number than our phones  If so  we'll print that it needs to be the number of the phone  And then we'll return false so that delete phone will not attempt to delete it  Now if it passes this test that it's a number  it converts to a number and it's a number that's within the proper range  then we're going to consider it an acceptable number to delete and we'll return true  So to summarize  delete phones will be passed which phone to delete  It will check to see if that's a number  and it's a number within the proper range  and if it is  it'll convert it to an integer and it'll delete that particular phone from the phone's list 
0xVl2_qvj1I,Editing records  Now  let's add the capability of editing a former entry  If we look down at the menu  if you choose E for edit  it'll ask you which one you want to edit  And then it'll call the edit phone routine  So  we need to look at edit phone routine  Let me run this thing so we know where we're going  Show  we have Kosmo Kramer and Jerry Seinfeld out there  So  if I choose Edit  it'll ask which one  I'll choose 2  So  I'll choose Cosmo Kramer  and I can press Enter and it'll leave the name unchanged  If I type a new name or phone number  however  It will replace the old version  That's what we're about to add  Here's edit_phone  Now  since we're choosing which one  we've got to make sure that the user made a proper menu choice  And we'll use the same proper menu choice function as we did with the delete function  When we get back from that  if it's not a proper menu choice  then it'll return without editing anybody  If it is a proper menu choice  we'll convert that to an editor  Then  we'll retrieve that particular phone  Now  remember  we're numbering these one  two  three  four  five  but Python calls them zero  one  two  three  four  So  we have to remove 1  Then we'll write  Enter the data for a new phone  Press <enter> to leave unchanged  What we're going to do first of all is print the name  Calling a name position  calling a 0 is the name  Then we'll ask  Enter the phone name to change or press return  We'll do an input and we'll get a new name  Now of course  if you press return  that new name is just an empty string  Otherwise  it's a new name  If the new name turns out to be an empty string  we're just going to set it equal to the old name  In either case  we've got a new name  Now  we do the same thing with the phone  We'll print the phone number  then we'll ask for a new phone  If it's blank  we'll just use the old phone as the new phone  Otherwise  it has a new phone  In both cases  we've got a new name and a new phone  calling that both of them could be the old one  We'll make a phone out of the new name and new phone number  And we'll put it back where we got it 
a-Xerqadt6I,Saving records to a CSV file  Now  let's replace the stub that says writing to CSV file with one that actually writes the CSV file  First we import the CSV library  Here is save the phone list  First we open the CSV file  myPhones csv for write  Then we write every item in the phones list to that CSV file  We use csv writer on the file handle f  and then we write a whole row  which consists of one item out of the phones list  And then we close  When do we use this? Let's go down to the main loop  save_phone_list is the last thing we do before we exit the program  Now let's run it  We have our usual list of phones  Jerry Seinfeld and Cosmo Kramer  and then we quit  And when we quit  we should execute save_phone_list  So  let's look at myPhones csv  And you see that Jerry Seinfeld and Cosmo Kramer are there 
b3OPl5wS4AQ,Loading the records from the CSV file  Now we need to write the function that reads in the CSV file  and we're done  We're going to import another library OS  This library deals with the operating system  you'll see how that plays a role in just a moment  Scroll down and look at load function  notice that it's relatively small  In fact  most of these functions that we've been writing are very small  That's one of the values of breaking the program up into little functions  you only have to write small manageable functions to put it all together to do a much bigger job  First  we're going to check to see if the CSV file is there  This method of the OS library  OS access will check and look for a file called MyPhones csv in the current Directory  and if it returns os F_OK then it's there  If it's there  then we can open it  We can read in the rows using csv reader and we can append each row onto our phones list  then we close it  That is  if it's there  if it's not there  we don't need to load it  One of the thing to point out here  let's go back up to the beginning  Since we're going to read the phones and we can eliminate our phone's list that had two entries in it already and we can make this phone's list an empty list  Let's see where it is it's actually called  In the main loop  the first thing we do is load the fund list and the last thing we do is save the fund list  so that we save it when we exit  we read it in fresh when we enter  So that it's always either saved on disc or loaded in the program  Now let's run our program  and see the whole thing work  Show  there's Jerry Seinfeld  and Cosmo Kramer that we saved when we tested our previous version of the program  You can add new log  [SOUND] And there is there  But  you can hit it  Let's say that item 2  Cosmo Kramer 2  We'll press return and keep the name  we will change the phone  There's the phone is changed  We can show that the phone is actually changed  Cosmo Kramer's phone now is 8789  We can delete 1  delete Cosmo Kramer  Sure that he's deleted now  and then we quit  Now  let's run the program again  And when we run the program  the same file should be right in and there's what we have  George Seinfeld and George Costanza  we don't have Cosmo Kramer anymore  That's our program 
mabRepmXLGA,Running our database application as a stand-alone program  Let's do one more thing  Let's run this as a standalone program from a command line  First of all  let me create this file  You want to go back up to the beginning  The final version  I'm going to copy it  Create a new file  I'm going to paste it  I'm going to save it  Now we see the new file  phones py  Now let's create command prompt  and we'll run python phones py  then show  There's our data  I add a new one  Elaine Denis phone number  (212) 123-4321  Show that  She's there  We will quit  Now let's run it again  So there's Elaine Denis  we're going to delete her  She's gone  quit  And that's our program  It will run from the command line  And we'll update this csv file that contains our data 
ag4OdO6q5uc,What is machine learning? In this video  we will try to define what it is and also try to give you a sense of when you want to use machine learning  Even among machine learning practitioners  there isn't a well accepted definition of what is and what isn't machine learning  But let me show you a couple of examples of the ways that people have tried to define it  Here's a definition of what is machine learning as due to Arthur Samuel  He defined machine learning as the field of study that gives computers the ability to learn without being explicitly learned  Samuel's claim to fame was that back in the 1950  he wrote a checkers playing program and the amazing thing about this checkers playing program was that Arthur Samuel himself wasn't a very good checkers player  But what he did was he had to programmed maybe tens of thousands of games against himself  and by watching what sorts of board positions tended to lead to wins and what sort of board positions tended to lead to losses  the checkers playing program learned over time what are good board positions and what are bad board positions  And eventually learn to play checkers better than the Arthur Samuel himself was able to  This was a remarkable result  Arthur Samuel himself turns out not to be a very good checkers player  But because a computer has the patience to play tens of thousands of games against itself  no human has the patience to play that many games  By doing this  a computer was able to get so much checkers playing experience that it eventually became a better checkers player than Arthur himself  This is a somewhat informal definition and an older one  Here's a slightly more recent definition by Tom Mitchell who's a friend of Carnegie Melon  So Tom defines machine learning by saying that a well-posed learning problem is defined as follows  He says  a computer program is said to learn from experience E with respect to some task T and some performance measure P  if its performance on T  as measured by P  improves with experience E  I actually think he came out with this definition just to make it rhyme  For the checkers playing examples  the experience E would be the experience of having the program play tens of thousands of games itself  The task T would be the task of playing checkers  and the performance measure P will be the probability that wins the next game of checkers against some new opponent  Throughout these videos  besides me trying to teach you stuff  I'll occasionally ask you a question to make sure you understand the content  Here's one  On top is a definition of machine learning by Tom Mitchell  Let's say your email program watches which emails you do or do not mark as spam  So in an email client like this  you might click the Spam button to report some email as spam but not other emails  And based on which emails you mark as spam  say your email program learns better how to filter spam email  What is the task T in this setting? In a few seconds  the video will pause and when it does so  you can use your mouse to select one of these four radio buttons to let me know which of these four you think is the right answer to this question  So hopefully you got that this is the right answer  classifying emails is the task T  In fact  this definition defines a task T performance measure P and some experience E  And so  watching you label emails as spam or not spam  this would be the experience E and and the fraction of emails correctly classified  that might be a performance measure P  And so on the task of systems performance  on the performance measure P will improve after the experience E  In this class  I hope to teach you about various different types of learning algorithms  There are several different types of learning algorithms  The main two types are what we call supervised learning and unsupervised learning  I'll define what these terms mean more in the next couple videos  It turns out that in supervised learning  the idea is we're going to teach the computer how to do something  Whereas in unsupervised learning  we're going to let it learn by itself  Don't worry if these two terms don't make sense yet  In the next two videos  I'm going to say exactly what these two types of learning are  You might also hear other ghost terms such as reinforcement learning and recommender systems  These are other types of machine learning algorithms that we'll talk about later  But the two most use types of learning algorithms are probably supervised learning and unsupervised learning  And I'll define them in the next two videos and we'll spend most of this class talking about these two types of learning algorithms  It turns out what are the other things to spend a lot of time on in this class is practical advice for applying learning algorithms  This is something that I feel pretty strongly about  And exactly something that I don't know if any other university teachers  Teaching about learning algorithms is like giving a set of tools  And equally important or more important than giving you the tools as they teach you how to apply these tools  I like to make an analogy to learning to become a carpenter  Imagine that someone is teaching you how to be a carpenter  and they say  here's a hammer  here's a screwdriver  here's a saw  good luck  Well  that's no good  You have all these tools but the more important thing is to learn how to use these tools properly  There's a huge difference between people that know how to use these machine learning algorithms  versus people that don't know how to use these tools well  Here  in Silicon Valley where I live  when I go visit different companies even at the top Silicon Valley companies  very often I see people trying to apply machine learning algorithms to some problem and sometimes they have been going at for six months  But sometimes when I look at what their doing  I say  I could have told them like  gee  I could have told you six months ago that you should be taking a learning algorithm and applying it in like the slightly modified way and your chance of success will have been much higher  So what we're going to do in this class is actually spend a lot of the time talking about how if you're actually trying to develop a machine learning system  how to make those best practices type decisions about the way in which you build your system  So that when you're finally learning algorithim  you're less likely to end up one of those people who end up persuing something after six months that someone else could have figured out just a waste of time for six months  So I'm actually going to spend a lot of time teaching you those sorts of best practices in machine learning and AI and how to get the stuff to work and how the best people do it in Silicon Valley and around the world  I hope to make you one of the best people in knowing how to design and build serious machine learning and AI systems  So that's machine learning  and these are the main topics I hope to teach  In the next video  I'm going to define what is supervised learning and after that what is unsupervised learning  And also time to talk about when you would use each of them 
0lBdSoKh5L4,"Supervised Learning  In this video  I'm going to define what is probably the most common type of Machine Learning problem  which is Supervised Learning  I'll define Supervised Learning more formally later  but it's probably best to explain or start with an example of what it is  and we'll do the formal definition later  Let's say you want to predict housing prices  A while back a student collected data sets from the City of Portland  Oregon  and let's say you plot the data set and it looks like this  Here on the horizontal axis  the size of different houses in square feet  and on the vertical axis  the price of different houses in thousands of dollars  So  given this data  let's say you have a friend who owns a house that is say 750 square feet  and they are hoping to sell the house  and they want to know how much they can get for the house  So  how can the learning algorithm help you? One thing a learning algorithm might be want to do is put a straight line through the data  also fit a straight line to the data  Based on that  it looks like maybe their house can be sold for maybe about $150 000  But maybe this isn't the only learning algorithm you can use  and there might be a better one  For example  instead of fitting a straight line to the data  we might decide that it's better to fit a quadratic function  or a second-order polynomial to this data  If you do that and make a prediction here  then it looks like  well  maybe they can sell the house for closer to $200 000  One of the things we'll talk about later is how to choose  and how to decide  do you want to fit a straight line to the data? Or do you want to fit a quadratic function to the data? There's no fair picking whichever one gives your friend the better house to sell  But each of these would be a fine example of a learning algorithm  So  this is an example of a Supervised Learning algorithm  The term Supervised Learning refers to the fact that we gave the algorithm a data set in which the  called  \""right answers\"" were given  That is we gave it a data set of houses in which for every example in this data set  we told it what is the right price  So  what was the actual price that that house sold for  and the task of the algorithm was to just produce more of these right answers such as for this new house that your friend may be trying to sell  To define a bit more terminology  this is also called a regression problem  By regression problem  I mean we're trying to predict a continuous valued output  Namely the price  So technically  I guess prices can be rounded off to the nearest cent  So  maybe prices are actually discrete value  But usually  we think of the price of a house as a real number  as a scalar value  as a continuous value number  and the term regression refers to the fact that we're trying to predict the sort of continuous values attribute  Here's another Supervised Learning examples  Some friends and I were actually working on this earlier  Let's say you want to look at medical records and try to predict of a breast cancer as malignant or benign  If someone discovers a breast tumor  a lump in their breast  a malignant tumor is a tumor that is harmful and dangerous  and a benign tumor is a tumor that is harmless  So obviously  people care a lot about this  Let's see collected data set  Suppose you are in your dataset  you have on your horizontal axis the size of the tumor  and on the vertical axis  I'm going to plot one or zero  yes or no  whether or not these are examples of tumors we've seen before are malignant  which is one  or zero or not malignant or benign  So  let's say your dataset looks like this  where we saw a tumor of this size that turned out to be benign  one of this size  one of this size  and so on  Sadly  we also saw a few malignant tumors cell  one of that size  one of that size  one of that size  so on  So in this example  I have five examples of benign tumors shown down here  and five examples of malignant tumors shown with a vertical axis value of one  Let's say a friend who tragically has a breast tumor  and let's say her breast tumor size is maybe somewhere around this value  the Machine Learning question is  can you estimate what is the probability  what's the chance that a tumor as malignant versus benign? To introduce a bit more terminology  this is an example of a classification problem  The term classification refers to the fact  that here  we're trying to predict a discrete value output zero or one  malignant or benign  It turns out that in classification problems  sometimes you can have more than two possible values for the output  As a concrete example  maybe there are three types of breast cancers  So  you may try to predict a discrete value output zero  one  two  or three  where zero may mean benign  benign tumor  so no cancer  and one may mean type one cancer  maybe three types of cancer  whatever type one means  and two mean a second type of cancer  and three may mean a third type of cancer  But this will also be a classification problem because this are the discrete value set of output corresponding to you're no cancer  or cancer type one  or cancer type two  or cancer types three  In classification problems  there is another way to plot this data  Let me show you what I mean  I'm going to use a slightly different set of symbols to plot this data  So  if tumor size is going to be the attribute that I'm going to use to predict malignancy or benignness  I can also draw my data like this  I'm going to use different symbols to denote my benign and malignant  or my negative and positive examples  So  instead of drawing crosses  I'm now going to draw O's for the benign tumors  like so  and I'm going to keep using X's to denote my malignant tumors  I hope this figure makes sense  All I did was I took my data set on top  and I just mapped it down to this real line like so  and started to use different symbols  circles and crosses to denote malignant versus benign examples  Now  in this example  we use only one feature or one attribute  namely the tumor size in order to predict whether a tumor is malignant or benign  In other machine learning problems  when we have more than one feature or more than one attribute  Here's an example  let's say that instead of just knowing the tumor size  we know both the age of the patients and the tumor size  In that case  maybe your data set would look like this  where I may have a set of patients with those ages  and that tumor size  and they look like this  and different set of patients that look a little different  whose tumors turn out to be malignant as denoted by the crosses  So  let's say you have a friend who tragically has a tumor  and maybe their tumor size and age falls around there  So  given a data set like this  what the learning algorithm might do is fit a straight line to the data to try to separate out the malignant tumors from the benign ones  and so the learning algorithm may decide to put a straight line like that to separate out the two causes of tumors  With this  hopefully we can decide that your friend's tumor is more likely  if it's over there that hopefully your learning algorithm will say that your friend's tumor falls on this benign side and is therefore more likely to be benign than malignant  In this example  we had two features namely  the age of the patient and the size of the tumor  In other Machine Learning problems  we will often have more features  My friends that worked on this problem actually used other features like these  which is clump thickness  clump thickness of the breast tumor  uniformity of cell size of the tumor  uniformity of cell shape the tumor  and so on  and other features as well  It turns out one of the most interesting learning algorithms that we'll see in this course  as the learning algorithm that can deal with not just two  or three  or five features  but an infinite number of features  On this slide  I've listed a total of five different features  Two on the axis and three more up here  But it turns out that for some learning problems what you really want is not to use like three or five features  but instead you want to use an infinite number of features  an infinite number of attributes  so that your learning algorithm has lots of attributes  or features  or cues with which to make those predictions  So  how do you deal with an infinite number of features? How do you even store an infinite number of things in the computer when your computer is going to run out of memory? It turns out that when we talk about an algorithm called the Support Vector Machine  there will be a neat mathematical trick that will allow a computer to deal with an infinite number of features  Imagine that I didn't just write down two features here and three features on the right  but imagine that I wrote down an infinitely long list  I just kept writing more and more features  like an infinitely long list of features  It turns out we will come up with an algorithm that can deal with that  So  just to recap  in this course  we'll talk about Supervised Learning  and the idea is that in Supervised Learning  in every example in our data set  we are told what is the correct answer that we would have quite liked the algorithms have predicted on that example  Such as the price of the house  or whether a tumor is malignant or benign  We also talked about the regression problem  and by regression that means that our goal is to predict a continuous valued output  We talked about the classification problem where the goal is to predict a discrete value output  Just a quick wrap up question  Suppose you're running a company and you want to develop learning algorithms to address each of two problems  In the first problem  you have a large inventory of identical items  So  imagine that you have thousands of copies of some identical items to sell  and you want to predict how many of these items you sell over the next three months  In the second problem  problem two  you have lots of users  and you want to write software to examine each individual of your customer's accounts  so each one of your customer's accounts  For each account  decide whether or not the account has been hacked or compromised  So  for each of these problems  should they be treated as a classification problem or as a regression problem? When the video pauses  please use your mouse to select whichever of these four options on the left you think is the correct answer  So hopefully  you got that  This is the answer  For problem one  I would treat this as a regression problem because if I have thousands of items  well  I would probably just treat this as a real value  as a continuous value  Therefore  the number of items I sell as a continuous value  For the second problem  I would treat that as a classification problem  because I might say set the value I want to predict with zero to denote the account has not been hacked  and set the value one to denote an account that has been hacked into  So  just like your breast cancers where zero is benign  one is malignant  So  I might set this be zero or one depending on whether it's been hacked  and have an algorithm try to predict each one of these two discrete values  Because there's a small number of discrete values  I would therefore treat it as a classification problem  So  that's it for Supervised Learning  In the next video  I'll talk about Unsupervised Learning  which is the other major category of learning algorithm "
pt-OOSSGezc,"Unsupervised Learning  In this video  we'll talk about the second major type of machine learning problem  called Unsupervised Learning  In the last video  we talked about Supervised Learning  Back then  recall data sets that look like this  where each example was labeled either as a positive or negative example  whether it was a benign or a malignant tumor  So for each example in Supervised Learning  we were told explicitly what is the so-called right answer  whether it's benign or malignant  In Unsupervised Learning  we're given data that looks different than data that looks like this that doesn't have any labels or that all has the same label or really no labels  So we're given the data set and we're not told what to do with it and we're not told what each data point is  Instead we're just told  here is a data set  Can you find some structure in the data? Given this data set  an Unsupervised Learning algorithm might decide that the data lives in two different clusters  And so there's one cluster and there's a different cluster  And yes  Supervised Learning algorithm may break these data into these two separate clusters  So this is called a clustering algorithm  And this turns out to be used in many places  One example where clustering is used is in Google News and if you have not seen this before  you can actually go to this URL news google com to take a look  What Google News does is everyday it goes and looks at tens of thousands or hundreds of thousands of new stories on the web and it groups them into cohesive news stories  For example  let's look here  The URLs here link to different news stories about the BP Oil Well story  So  let's click on one of these URL's and we'll click on one of these URL's  What I'll get to is a web page like this  Here's a Wall Street Journal article about  you know  the BP Oil Well Spill stories of \""BP Kills Macondo\""  which is a name of the spill and if you click on a different URL from that group then you might get the different story  Here's the CNN story about a game  the BP Oil Spill  and if you click on yet a third link  then you might get a different story  Here's the UK Guardian story about the BP Oil Spill  So what Google News has done is look for tens of thousands of news stories and automatically cluster them together  So  the news stories that are all about the same topic get displayed together  It turns out that clustering algorithms and Unsupervised Learning algorithms are used in many other problems as well  Here's one on understanding genomics  Here's an example of DNA microarray data  The idea is put a group of different individuals and for each of them  you measure how much they do or do not have a certain gene  Technically you measure how much certain genes are expressed  So these colors  red  green  gray and so on  they show the degree to which different individuals do or do not have a specific gene  And what you can do is then run a clustering algorithm to group individuals into different categories or into different types of people  So this is Unsupervised Learning because we're not telling the algorithm in advance that these are type 1 people  those are type 2 persons  those are type 3 persons and so on and instead what were saying is yeah here's a bunch of data  I don't know what's in this data  I don't know who's and what type  I don't even know what the different types of people are  but can you automatically find structure in the data from the you automatically cluster the individuals into these types that I don't know in advance? Because we're not giving the algorithm the right answer for the examples in my data set  this is Unsupervised Learning  Unsupervised Learning or clustering is used for a bunch of other applications  It's used to organize large computer clusters  I had some friends looking at large data centers  that is large computer clusters and trying to figure out which machines tend to work together and if you can put those machines together  you can make your data center work more efficiently  This second application is on social network analysis  So given knowledge about which friends you email the most or given your Facebook friends or your Google+ circles  can we automatically identify which are cohesive groups of friends  also which are groups of people that all know each other? Market segmentation  Many companies have huge databases of customer information  So  can you look at this customer data set and automatically discover market segments and automatically group your customers into different market segments so that you can automatically and more efficiently sell or market your different market segments together? Again  this is Unsupervised Learning because we have all this customer data  but we don't know in advance what are the market segments and for the customers in our data set  you know  we don't know in advance who is in market segment one  who is in market segment two  and so on  But we have to let the algorithm discover all this just from the data  Finally  it turns out that Unsupervised Learning is also used for surprisingly astronomical data analysis and these clustering algorithms gives surprisingly interesting useful theories of how galaxies are formed  All of these are examples of clustering  which is just one type of Unsupervised Learning  Let me tell you about another one  I'm gonna tell you about the cocktail party problem  So  you've been to cocktail parties before  right? Well  you can imagine there's a party  room full of people  all sitting around  all talking at the same time and there are all these overlapping voices because everyone is talking at the same time  and it is almost hard to hear the person in front of you  So maybe at a cocktail party with two people  two people talking at the same time  and it's a somewhat small cocktail party  And we're going to put two microphones in the room so there are microphones  and because these microphones are at two different distances from the speakers  each microphone records a different combination of these two speaker voices  Maybe speaker one is a little louder in microphone one and maybe speaker two is a little bit louder on microphone 2 because the 2 microphones are at different positions relative to the 2 speakers  but each microphone would cause an overlapping combination of both speakers' voices  So here's an actual recording of two speakers recorded by a researcher  Let me play for you the first  what the first microphone sounds like  One (uno)  two (dos)  three (tres)  four (cuatro)  five (cinco)  six (seis)  seven (siete)  eight (ocho)  nine (nueve)  ten (y diez)  All right  maybe not the most interesting cocktail party  there's two people counting from one to ten in two languages but you know  What you just heard was the first microphone recording  here's the second recording  Uno (one)  dos (two)  tres (three)  cuatro (four)  cinco (five)  seis (six)  siete (seven)  ocho (eight)  nueve (nine) y diez (ten)  So we can do  is take these two microphone recorders and give them to an Unsupervised Learning algorithm called the cocktail party algorithm  and tell the algorithm - find structure in this data for you  And what the algorithm will do is listen to these audio recordings and say  you know it sounds like the two audio recordings are being added together or that have being summed together to produce these recordings that we had  Moreover  what the cocktail party algorithm will do is separate out these two audio sources that were being added or being summed together to form other recordings and  in fact  here's the first output of the cocktail party algorithm  One  two  three  four  five  six  seven  eight  nine  ten  So  I separated out the English voice in one of the recordings  And here's the second of it  Uno  dos  tres  quatro  cinco  seis  siete  ocho  nueve y diez  Not too bad  to give you one more example  here's another recording of another similar situation  here's the first microphone   One  two  three  four  five  six  seven  eight  nine  ten  OK so the poor guy's gone home from the cocktail party and he 's now sitting in a room by himself talking to his radio  Here's the second microphone recording  One  two  three  four  five  six  seven  eight  nine  ten  When you give these two microphone recordings to the same algorithm  what it does  is again say  you know  it sounds like there are two audio sources  and moreover  the album says  here is the first of the audio sources I found  One  two  three  four  five  six  seven  eight  nine  ten  So that wasn't perfect  it got the voice  but it also got a little bit of the music in there  Then here's the second output to the algorithm  Not too bad  in that second output it managed to get rid of the voice entirely  And just  you know  cleaned up the music  got rid of the counting from one to ten  So you might look at an Unsupervised Learning algorithm like this and ask how complicated this is to implement this  right? It seems like in order to  you know  build this application  it seems like to do this audio processing you need to write a ton of code or maybe link into like a bunch of synthesizer Java libraries that process audio  seems like a really complicated program  to do this audio  separating out audio and so on  It turns out the algorithm  to do what you just heard  that can be done with one line of code - shown right here  It take researchers a long time to come up with this line of code  I'm not saying this is an easy problem  But it turns out that when you use the right programming environment  many learning algorithms can be really short programs  So this is also why in this class we're going to use the Octave programming environment  Octave  is free open source software  and using a tool like Octave or Matlab  many learning algorithms become just a few lines of code to implement  Later in this class  I'll just teach you a little bit about how to use Octave and you'll be implementing some of these algorithms in Octave  Or if you have Matlab you can use that too  It turns out the Silicon Valley  for a lot of machine learning algorithms  what we do is first prototype our software in Octave because software in Octave makes it incredibly fast to implement these learning algorithms  Here each of these functions like for example the SVD function that stands for singular value decomposition; but that turns out to be a linear algebra routine  that is just built into Octave  If you were trying to do this in C++ or Java  this would be many many lines of code linking complex C++ or Java libraries  So  you can implement this stuff as C++ or Java or Python  it's just much more complicated to do so in those languages  What I've seen after having taught machine learning for almost a decade now  is that  you learn much faster if you use Octave as your programming environment  and if you use Octave as your learning tool and as your prototyping tool  it'll let you learn and prototype learning algorithms much more quickly  And in fact what many people will do to in the large Silicon Valley companies is in fact  use an algorithm like Octave to first prototype the learning algorithm  and only after you've gotten it to work  then you migrate it to C++ or Java or whatever  It turns out that by doing things this way  you can often get your algorithm to work much faster than if you were starting out in C++  So  I know that as an instructor  I get to say \""trust me on this one\"" only a finite number of times  but for those of you who've never used these Octave type programming environments before  I am going to ask you to trust me on this one  and say that you  you will  I think your time  your development time is one of the most valuable resources  And having seen lots of people do this  I think you as a machine learning researcher  or machine learning developer will be much more productive if you learn to start in prototype  to start in Octave  in some other language  Finally  to wrap up this video  I have one quick review question for you  We talked about Unsupervised Learning  which is a learning setting where you give the algorithm a ton of data and just ask it to find structure in the data for us  Of the following four examples  which ones  which of these four do you think would will be an Unsupervised Learning algorithm as opposed to Supervised Learning problem  For each of the four check boxes on the left  check the ones for which you think Unsupervised Learning algorithm would be appropriate and then click the button on the lower right to check your answer  So when the video pauses  please answer the question on the slide  So  hopefully  you've remembered the spam folder problem  If you have labeled data  you know  with spam and non-spam e-mail  we'd treat this as a Supervised Learning problem  The news story example  that's exactly the Google News example that we saw in this video  we saw how you can use a clustering algorithm to cluster these articles together so that's Unsupervised Learning  The market segmentation example I talked a little bit earlier  you can do that as an Unsupervised Learning problem because I am just gonna get my algorithm data and ask it to discover market segments automatically  And the final example  diabetes  well  that's actually just like our breast cancer example from the last video  Only instead of  you know  good and bad cancer tumors or benign or malignant tumors we instead have diabetes or not and so we will use that as a supervised  we will solve that as a Supervised Learning problem just like we did for the breast tumor data  So  that's it for Unsupervised Learning and in the next video  we'll delve more into specific learning algorithms and start to talk about just how these algorithms work and how we can  how you can go about implementing them "
1-NW2fKIyLQ,"Model Representation  Our first learning algorithm will be linear regression  In this video  you'll see what the model looks like and more importantly you'll see what the overall process of supervised learning looks like  Let's use some motivating example of predicting housing prices  We're going to use a data set of housing prices from the city of Portland  Oregon  And here I'm gonna plot my data set of a number of houses that were different sizes that were sold for a range of different prices  Let's say that given this data set  you have a friend that's trying to sell a house and let's see if friend's house is size of 1250 square feet and you want to tell them how much they might be able to sell the house for  Well one thing you could do is fit a model  Maybe fit a straight line to this data  Looks something like that and based on that  maybe you could tell your friend that let's say maybe he can sell the house for around $220 000  So this is an example of a supervised learning algorithm  And it's supervised learning because we're given the  quotes  \""right answer\"" for each of our examples  Namely we're told what was the actual house  what was the actual price of each of the houses in our data set were sold for and moreover  this is an example of a regression problem where the term regression refers to the fact that we are predicting a real-valued output namely the price  And just to remind you the other most common type of supervised learning problem is called the classification problem where we predict discrete-valued outputs such as if we are looking at cancer tumors and trying to decide if a tumor is malignant or benign  So that's a zero-one valued discrete output  More formally  in supervised learning  we have a data set and this data set is called a training set  So for housing prices example  we have a training set of different housing prices and our job is to learn from this data how to predict prices of the houses  Let's define some notation that we're using throughout this course  We're going to define quite a lot of symbols  It's okay if you don't remember all the symbols right now but as the course progresses it will be useful [inaudible] convenient notation  So I'm gonna use lower case m throughout this course to denote the number of training examples  So in this data set  if I have  you know  let's say 47 rows in this table  Then I have 47 training examples and m equals 47  Let me use lowercase x to denote the input variables often also called the features  That would be the x is here  it would the input features  And I'm gonna use y to denote my output variables or the target variable which I'm going to predict and so that's the second column here  [inaudible] notation  I'm going to use (x  y) to denote a single training example  So  a single row in this table corresponds to a single training example and to refer to a specific training example  I'm going to use this notation x(i) comma gives me y(i) And  we're going to use this to refer to the ith training example  So this superscript i over here  this is not exponentiation right? This (x(i)  y(i))  the superscript i in parentheses that's just an index into my training set and refers to the ith row in this table  okay? So this is not x to the power of i  y to the power of i  Instead (x(i)  y(i)) just refers to the ith row of this table  So for example  x(1) refers to the input value for the first training example so that's 2104  That's this x in the first row  x(2) will be equal to 1416 right? That's the second x and y(1) will be equal to 460  The first  the y value for my first training example  that's what that (1) refers to  So as mentioned  occasionally I'll ask you a question to let you check your understanding and a few seconds in this video a multiple-choice question will pop up in the video  When it does  please use your mouse to select what you think is the right answer  What defined by the training set is  So here's how this supervised learning algorithm works  We saw that with the training set like our training set of housing prices and we feed that to our learning algorithm  Is the job of a learning algorithm to then output a function which by convention is usually denoted lowercase h and h stands for hypothesis And what the job of the hypothesis is  is  is a function that takes as input the size of a house like maybe the size of the new house your friend's trying to sell so it takes in the value of x and it tries to output the estimated value of y for the corresponding house  So h is a function that maps from x's to y's  People often ask me  you know  why is this function called hypothesis  Some of you may know the meaning of the term hypothesis  from the dictionary or from science or whatever  It turns out that in machine learning  this is a name that was used in the early days of machine learning and it kinda stuck  'Cause maybe not a great name for this sort of function  for mapping from sizes of houses to the predictions  that you know     I think the term hypothesis  maybe isn't the best possible name for this  but this is the standard terminology that people use in machine learning  So don't worry too much about why people call it that  When designing a learning algorithm  the next thing we need to decide is how do we represent this hypothesis h  For this and the next few videos  I'm going to choose our initial choice   for representing the hypothesis  will be the following  We're going to represent h as follows  And we will write this as h<u>theta(x) equals theta<u>0</u></u> plus theta<u>1 of x  And as a shorthand  sometimes instead of writing  you</u> know  h subscript theta of x  sometimes there's a shorthand  I'll just write as a h of x  But more often I'll write it as a subscript theta over there  And plotting this in the pictures  all this means is that  we are going to predict that y is a linear function of x  Right  so that's the data set and what this function is doing  is predicting that y is some straight line function of x  That's h of x equals theta 0 plus theta 1 x  okay? And why a linear function? Well  sometimes we'll want to fit more complicated  perhaps non-linear functions as well  But since this linear case is the simple building block  we will start with this example first of fitting linear functions  and we will build on this to eventually have more complex models  and more complex learning algorithms  Let me also give this particular model a name  This model is called linear regression or this  for example  is actually linear regression with one variable  with the variable being x  Predicting all the prices as functions of one variable X  And another name for this model is univariate linear regression  And univariate is just a fancy way of saying one variable  So  that's linear regression  In the next video we'll start to talk about just how we go about implementing this model "
BM24sU2Xm7w,"Cost function  In this video we'll define something called the cost function  this will let us figure out how to fit the best possible straight line to our data  In linear progression  we have a training set that I showed here remember on notation M was the number of training examples  so maybe m equals 47  And the form of our hypothesis  which we use to make predictions is this linear function  To introduce a little bit more terminology  these theta zero and theta one  they stabilize what I call the parameters of the model  And what we're going to do in this video is talk about how to go about choosing these two parameter values  theta 0 and theta 1  With different choices of the parameter's theta 0 and theta 1  we get different hypothesis  different hypothesis functions  I know some of you will probably be already familiar with what I am going to do on the slide  but just for review  here are a few examples  If theta 0 is 1 5 and theta 1 is 0  then the hypothesis function will look like this  Because your hypothesis function will be h of x equals 1 5 plus 0 times x which is this constant value function which is phat at 1 5  If theta0 = 0  theta1 = 0 5  then the hypothesis will look like this  and it should pass through this point 2 1 so that you now have h(x)  Or really h of theta(x)  but sometimes I'll just omit theta for brevity  So h(x) will be equal to just 0 5 times x  which looks like that  And finally  if theta zero equals one  and theta one equals 0 5  then we end up with a hypothesis that looks like this  Let's see  it should pass through the two-two point  Like so  and this is my new vector of x  or my new h subscript theta of x  Whatever way you remember  I said that this is h subscript theta of x  but that's a shorthand  sometimes I'll just write this as h of x  In linear regression  we have a training set  like maybe the one I've plotted here  What we want to do  is come up with values for the parameters theta zero and theta one so that the straight line we get out of this  corresponds to a straight line that somehow fits the data well  like maybe that line over there  So  how do we come up with values  theta zero  theta one  that corresponds to a good fit to the data? The idea is we get to choose our parameters theta 0  theta 1 so that h of x  meaning the value we predict on input x  that this is at least close to the values y for the examples in our training set  for our training examples  So in our training set  we've given a number of examples where we know X decides the wholes and we know the actual price is was sold for  So  let's try to choose values for the parameters so that  at least in the training set  given the X in the training set we make reason of the active predictions for the Y values  Let's formalize this  So linear regression  what we're going to do is  I'm going to want to solve a minimization problem  So I'll write minimize over theta0 theta1  And I want this to be small  right? I want the difference between h(x) and y to be small  And one thing I might do is try to minimize the square difference between the output of the hypothesis and the actual price of a house  Okay  So lets find some details  You remember that I was using the notation (x(i) y(i)) to represent the ith training example  So what I want really is to sum over my training set  something i = 1 to m  of the square difference between  this is the prediction of my hypothesis when it is input to size of house number i  Right? Minus the actual price that house number I was sold for  and I want to minimize the sum of my training set  sum from I equals one through M  of the difference of this squared error  the square difference between the predicted price of a house  and the price that it was actually sold for  And just remind you of notation  m here was the size of my training set right? So my m there is my number of training examples  Right that hash sign is the abbreviation for number of training examples  okay? And to make some of our  make the math a little bit easier  I'm going to actually look at we are 1 over m times that so let's try to minimize my average minimize one over 2m  Putting the 2 at the constant one half in front  it may just sound the math probably easier so minimizing one-half of something  right  should give you the same values of the process  theta 0 theta 1  as minimizing that function  And just to be sure  this equation is clear  right? This expression in here  h subscript theta(x)  this is our usual  right? That is equal to this plus theta one xi  And this notation  minimize over theta 0 theta 1  this means you'll find me the values of theta 0 and theta 1 that causes this expression to be minimized and this expression depends on theta 0 and theta 1  okay? So just a recap  We're closing this problem as  find me the values of theta zero and theta one so that the average  the 1 over the 2m  times the sum of square errors between my predictions on the training set minus the actual values of the houses on the training set is minimized  So this is going to be my overall objective function for linear regression  And just to rewrite this out a little bit more cleanly  what I'm going to do is  by convention we usually define a cost function  which is going to be exactly this  that formula I have up here  And what I want to do is minimize over theta0 and theta1  My function j(theta0  theta1)  Just write this out  This is my cost function  So  this cost function is also called the squared error function  When sometimes called the squared error cost function and it turns out that why do we take the squares of the erros  It turns out that these squared error cost function is a reasonable choice and works well for problems for most regression programs  There are other cost functions that will work pretty well  But the square cost function is probably the most commonly used one for regression problems  Later in this class we'll talk about alternative cost functions as well  but this choice that we just had should be a pretty reasonable thing to try for most linear regression problems  Okay  So that's the cost function  So far we've just seen a mathematical definition of this cost function  In case this function j of theta zero  theta one  In case this function seems a little bit abstract  and you still don't have a good sense of what it's doing  in the next video  in the next couple videos  I'm actually going to go a little bit deeper into what the cause function \""J\"" is doing and try to give you better intuition about what is computing and why we want to use it   "
5uic4Y2T9mU,Cost Function - Intuition I  In the previous video  we gave the mathematical definition of the cost function  In this video  let's look at some examples  to get back to intuition about what the cost function is doing  and why we want to use it  To recap  here's what we had last time  We want to fit a straight line to our data  so we had this formed as a hypothesis with these parameters theta zero and theta one  and with different choices of the parameters we end up with different straight line fits  So the data which are fit like so  and there's a cost function  and that was our optimization objective  [sound] So this video  in order to better visualize the cost function J  I'm going to work with a simplified hypothesis function  like that shown on the right  So I'm gonna use my simplified hypothesis  which is just theta one times X  We can  if you want  think of this as setting the parameter theta zero equal to 0  So I have only one parameter theta one and my cost function is similar to before except that now H of X that is now equal to just theta one times X  And I have only one parameter theta one and so my optimization objective is to minimize j of theta one  In pictures what this means is that if theta zero equals zero that corresponds to choosing only hypothesis functions that pass through the origin  that pass through the point (0  0)  Using this simplified definition of a hypothesizing cost function let's try to understand the cost function concept better  It turns out that two key functions we want to understand  The first is the hypothesis function  and the second is a cost function  So  notice that the hypothesis  right  H of X  For a face value of theta one  this is a function of X  So the hypothesis is a function of  what is the size of the house X  In contrast  the cost function  J  that's a function of the parameter  theta one  which controls the slope of the straight line  Let's plot these functions and try to understand them both better  Let's start with the hypothesis  On the left  let's say here's my training set with three points at (1  1)  (2  2)  and (3  3)  Let's pick a value theta one  so when theta one equals one  and if that's my choice for theta one  then my hypothesis is going to look like this straight line over here  And I'm gonna point out  when I'm plotting my hypothesis function  X-axis  my horizontal axis is labeled X  is labeled you know  size of the house over here  Now  of temporary  set theta one equals one  what I want to do is figure out what is j of theta one  when theta one equals one  So let's go ahead and compute what the cost function has for  You'll devalue one  Well  as usual  my cost function is defined as follows  right? Some from  some of 'em are training sets of this usual squared error term  And  this is therefore equal to  And this  Of theta one x I minus y I and if you simplify this turns out to be  That  Zero Squared to zero squared to zero squared which is of course  just equal to zero  Now  inside the cost function  It turns out each of these terms here is equal to zero  Because for the specific training set I have or my 3 training examples are (1  1)  (2  2)  (3 3)  If theta one is equal to one  Then h of x  H of x i  Is equal to y I exactly  let me write this better  Right? And so  h of x minus y  each of these terms is equal to zero  which is why I find that j of one is equal to zero  So  we now know that j of one Is equal to zero  Let's plot that  What I'm gonna do on the right is plot my cost function j  And notice  because my cost function is a function of my parameter theta one  when I plot my cost function  the horizontal axis is now labeled with theta one  So I have j of one zero zero so let's go ahead and plot that  End up with  An X over there  Now lets look at some other examples  Theta-1 can take on a range of different values  Right? So theta-1 can take on the negative values  zero  positive values  So what if theta-1 is equal to 0 5  What happens then? Let's go ahead and plot that  I'm now going to set theta-1 equals 0 5  and in that case my hypothesis now looks like this  As a line with slope equals to 0 5  and  lets compute J  of 0 5  So that is going to be one over 2M of  my usual cost function  It turns out that the cost function is going to be the sum of square values of the height of this line  Plus the sum of square of the height of that line  plus the sum of square of the height of that line  right? ?Cause just this vertical distance  that's the difference between  you know  Y  I  and the predicted value  H of XI  right? So the first example is going to be 0 5 minus one squared  Because my hypothesis predicted 0 5  Whereas  the actual value was one  For my second example  I get  one minus two squared  because my hypothesis predicted one  but the actual housing price was two  And then finally  plus  1 5 minus three squared  And so that's equal to one over two times three  Because  M when trading set size  right  have three training examples  In that  that's times simplifying for the parentheses it's 3 5  So that's 3 5 over six which is about 0 68  So now we know that j of 0 5 is about 0 68 [Should be 0 58] Lets go and plot that  Oh excuse me  math error  it's actually 0 58  So we plot that which is maybe about over there  Okay? Now  let's do one more  How about if theta one is equal to zero  what is J of zero equal to? It turns out that if theta one is equal to zero  then H of X is just equal to  you know  this flat line  right  that just goes horizontally like this  And so  measuring the errors  We have that J of zero is equal to one over two M  times one squared plus two squared plus three squared  which is  One six times fourteen which is about 2 3  So let's go ahead and plot as well  So it ends up with a value around 2 3 and of course we can keep on doing this for other values of theta one  It turns out that you can have you know negative values of theta one as well so if theta one is negative then h of x would be equal to say minus 0 5 times x then theta one is minus 0 5 and so that corresponds to a hypothesis with a slope of negative 0 5  And you can actually keep on computing these errors  This turns out to be  you know  for 0 5  it turns out to have really high error  It works out to be something  like  5 25  And so on  and the different values of theta one  you can compute these things  right? And it turns out that you  your computed range of values  you get something like that  And by computing the range of values  you can actually slowly create out  What does function J of Theta say and that's what J of Theta is  To recap  for each value of theta one  right? Each value of theta one corresponds to a different hypothesis  or to a different straight line fit on the left  And for each value of theta one  we could then derive a different value of j of theta one  And for example  you know  theta one=1  corresponded to this straight line straight through the data  Whereas theta one=0 5  And this point shown in magenta corresponded to maybe that line  and theta one=zero which is shown in blue that corresponds to this horizontal line  Right  so for each value of theta one we wound up with a different value of J of theta one and we could then use this to trace out this plot on the right  Now you remember  the optimization objective for our learning algorithm is we want to choose the value of theta one  That minimizes J of theta one  Right? This was our objective function for the linear regression  Well  looking at this curve  the value that minimizes j of theta one is  you know  theta one equals to one  And low and behold  that is indeed the best possible straight line fit through our data  by setting theta one equals one  And just  for this particular training set  we actually end up fitting it perfectly  And that's why minimizing j of theta one corresponds to finding a straight line that fits the data well  So  to wrap up  In this video  we looked up some plots  To understand the cost function  To do so  we simplify the algorithm  So that it only had one parameter theta one  And we set the parameter theta zero to be only zero  In the next video  We'll go back to the original problem formulation and look at some visualizations involving both theta zero and theta one  That is without setting theta zero to zero  And hopefully that will give you  an even better sense of what the cost function j is doing in the original linear regression formulation 
ySTr4TO3IiU,Cost Function - Intuition II  In this video  lets delve deeper and get even better intuition about what the cost function is doing  This video assumes that you're familiar with contour plots  If you are not familiar with contour plots or contour figures some of the illustrations in this video may or may not make sense to you but is okay and if you end up skipping this video or some of it does not quite make sense because you haven't seen contour plots before  That's okay and you will still understand the rest of this course without those parts of this  Here's our problem formulation as usual  with the hypothesis parameters  cost function  and our optimization objective  Unlike before  unlike the last video  I'm going to keep both of my parameters  theta zero  and theta one  as we generate our visualizations for the cost function  So  same as last time  we want to understand the hypothesis H and the cost function J  So  here's my training set of housing prices and let's make some hypothesis  You know  like that one  this is not a particularly good hypothesis  But  if I set theta zero=50 and theta one=0 06  then I end up with this hypothesis down here and that corresponds to that straight line  Now given these value of theta zero and theta one  we want to plot the corresponding  you know  cost function on the right  What we did last time was  right  when we only had theta one  In other words  drawing plots that look like this as a function of theta one  But now we have two parameters  theta zero  and theta one  and so the plot gets a little more complicated  It turns out that when we have only one parameter  that the parts we drew had this sort of bow shaped function  Now  when we have two parameters  it turns out the cost function also has a similar sort of bow shape  And  in fact  depending on your training set  you might get a cost function that maybe looks something like this  So  this is a 3-D surface plot  where the axes are labeled theta zero and theta one  So as you vary theta zero and theta one  the two parameters  you get different values of the cost function J (theta zero  theta one) and the height of this surface above a particular point of theta zero  theta one  Right  that's  that's the vertical axis  The height of the surface of the points indicates the value of J of theta zero  J of theta one  And you can see it sort of has this bow like shape  Let me show you the same plot in 3D  So here's the same figure in 3D  horizontal axis theta one and vertical axis J(theta zero  theta one)  and if I rotate this plot around  You kinda of a get a sense  I hope  of this bowl shaped surface as that's what the cost function J looks like  Now for the purpose of illustration in the rest of this video I'm not actually going to use these sort of 3D surfaces to show you the cost function J  instead I'm going to use contour plots  Or what I also call contour figures  I guess they mean the same thing  To show you these surfaces  So here's an example of a contour figure  shown on the right  where the axis are theta zero and theta one  And what each of these ovals  what each of these ellipsis shows is a set of points that takes on the same value for J(theta zero  theta one)  So concretely  for example this  you'll take that point and that point and that point  All three of these points that I just drew in magenta  they have the same value for J (theta zero  theta one)  Okay  Where  right  these  this is the theta zero  theta one axis but those three have the same Value for J (theta zero  theta one) and if you haven't seen contour plots much before think of  imagine if you will  A bow shaped function that's coming out of my screen  So that the minimum  so the bottom of the bow is this point right there  right? This middle  the middle of these concentric ellipses  And imagine a bow shape that sort of grows out of my screen like this  so that each of these ellipses  you know  has the same height above my screen  And the minimum with the bow  right  is right down there  And so the contour figures is a  is way to  is maybe a more convenient way to visualize my function J  [sound] So  let's look at some examples  Over here  I have a particular point  right? And so this is  with  you know  theta zero equals maybe about 800  and theta one equals maybe a -0 15   And so this point  right  this point in red corresponds to one set of pair values of theta zero  theta one and the corresponding  in fact  to that hypothesis  right  theta zero is about 800  that is  where it intersects the vertical axis is around 800  and this is slope of about -0 15  Now this line is really not such a good fit to the data  right  This hypothesis  h(x)  with these values of theta zero  theta one  it's really not such a good fit to the data  And so you find that  it's cost  Is a value that's out here that's you know pretty far from the minimum right it's pretty far this is a pretty high cost because this is just not that good a fit to the data  Let's look at some more examples  Now here's a different hypothesis that's you know still not a great fit for the data but may be slightly better so here right that's my point that those are my parameters theta zero theta one and so my theta zero value  Right? That's bout 360 and my value for theta one  Is equal to zero  So  you know  let's break it out  Let's take theta zero equals 360 theta one equals zero  And this pair of parameters corresponds to that hypothesis  corresponds to flat line  that is  h(x) equals 360 plus zero times x  So that's the hypothesis  And this hypothesis again has some cost  and that cost is  you know  plotted as the height of the J function at that point  Let's look at just a couple of examples  Here's one more  you know  at this value of theta zero  and at that value of theta one  we end up with this hypothesis  h(x) and again  not a great fit to the data  and is actually further away from the minimum  Last example  this is actually not quite at the minimum  but it's pretty close to the minimum  So this is not such a bad fit to the  to the data  where  for a particular value  of  theta zero  Which  one of them has value  as in for a particular value for theta one  We get a particular h(x)  And this is  this is not quite at the minimum  but it's pretty close  And so the sum of squares errors is sum of squares distances between my  training samples and my hypothesis  Really  that's a sum of square distances  right? Of all of these errors  This is pretty close to the minimum even though it's not quite the minimum  So with these figures I hope that gives you a better understanding of what values of the cost function J  how they are and how that corresponds to different hypothesis and so as how better hypotheses may corresponds to points that are closer to the minimum of this cost function J  Now of course what we really want is an efficient algorithm  right  a efficient piece of software for automatically finding The value of theta zero and theta one  that minimizes the cost function J  right? And what we  what we don't wanna do is to  you know  how to write software  to plot out this point  and then try to manually read off the numbers  that this is not a good way to do it  And  in fact  we'll see it later  that when we look at more complicated examples  we'll have high dimensional figures with more parameters  that  it turns out  we'll see in a few  we'll see later in this course  examples where this figure  you know  cannot really be plotted  and this becomes much harder to visualize  And so  what we want is to have software to find the value of theta zero  theta one that minimizes this function and in the next video we start to talk about an algorithm for automatically finding that value of theta zero and theta one that minimizes the cost function J 
hY6lydCMQB0,Gradient Descent  We previously defined the cost function J  In this video  I want to tell you about an algorithm called gradient descent for minimizing the cost function J  It turns out gradient descent is a more general algorithm  and is used not only in linear regression  It's actually used all over the place in machine learning  And later in the class  we'll use gradient descent to minimize other functions as well  not just the cost function J for the linear regression  So in this video  we'll talk about gradient descent for minimizing some arbitrary function J and then in later videos  we'll take this algorithm and apply it specifically to the cost function J that we have defined for linear regression  So here's the problem setup  Going to assume that we have some function J(theta 0  theta 1) maybe it's the cost function from linear regression  maybe it's some other function we wanna minimize  And we want to come up with an algorithm for minimizing that as a function of J(theta 0  theta 1)  Just as an aside it turns out that gradient descent actually applies to more general functions  So imagine  if you have a function that's a function of J  as theta 0  theta 1  theta 2  up to say some theta n  and you want to minimize theta 0  You minimize over theta 0 up to theta n of this J of theta 0 up to theta n  And it turns our gradient descent is an algorithm for solving this more general problem  But for the sake of brevity  for the sake of succinctness of notation  I'm just going to pretend I have only two parameters throughout the rest of this video  Here's the idea for gradient descent  What we're going to do is we're going to start off with some initial guesses for theta 0 and theta 1  Doesn't really matter what they are  but a common choice would be we set theta 0 to 0  and set theta 1 to 0  just initialize them to 0  What we're going to do in gradient descent is we'll keep changing theta 0 and theta 1 a little bit to try to reduce J(theta 0  theta 1)  until hopefully  we wind at a minimum  or maybe at a local minimum  So let's see in pictures what gradient descent does  Let's say you're trying to minimize this function  So notice the axes  this is theta 0  theta 1 on the horizontal axes and J is the vertical axis and so the height of the surface shows J and we want to minimize this function  So we're going to start off with theta 0  theta 1 at some point  So imagine picking some value for theta 0  theta 1  and that corresponds to starting at some point on the surface of this function  So whatever value of theta 0  theta 1 gives you some point here  I did initialize them to 0  0 but sometimes you initialize it to other values as well  Now  I want you to imagine that this figure shows a hole  Imagine this is like the landscape of some grassy park  with two hills like so  and I want us to imagine that you are physically standing at that point on the hill  on this little red hill in your park  In gradient descent  what we're going to do is we're going to spin 360 degrees around  just look all around us  and ask  if I were to take a little baby step in some direction  and I want to go downhill as quickly as possible  what direction do I take that little baby step in? If I wanna go down  so I wanna physically walk down this hill as rapidly as possible  Turns out  that if you're standing at that point on the hill  you look all around and you find that the best direction is to take a little step downhill is roughly that direction  Okay  and now you're at this new point on your hill  You're gonna  again  look all around and say what direction should I step in order to take a little baby step downhill? And if you do that and take another step  you take a step in that direction  And then you keep going  From this new point you look around  decide what direction would take you downhill most quickly  Take another step  another step  and so on until you converge to this local minimum down here  Gradient descent has an interesting property  This first time we ran gradient descent we were starting at this point over here  right? Started at that point over here  Now imagine we had initialized gradient descent just a couple steps to the right  Imagine we'd initialized gradient descent with that point on the upper right  If you were to repeat this process  so start from that point  look all around  take a little step in the direction of steepest descent  you would do that  Then look around  take another step  and so on  And if you started just a couple of steps to the right  gradient descent would've taken you to this second local optimum over on the right  So if you had started this first point  you would've wound up at this local optimum  but if you started just at a slightly different location  you would've wound up at a very different local optimum  And this is a property of gradient descent that we'll say a little bit more about later  So that's the intuition in pictures  Let's look at the math  This is the definition of the gradient descent algorithm  We're going to just repeatedly do this until convergence  we're going to update my parameter theta j by taking theta j and subtracting from it alpha times this term over here  okay? So let's see  there's lot of details in this equation so let me unpack some of it  First  this notation here   =  gonna use  = to denote assignment  so it's the assignment operator  So briefly  if I write a  = b  what this means is  it means in a computer  this means take the value in b and use it overwrite whatever value is a  So this means set a to be equal to the value of b  which is assignment  And I can also do a  = a + 1  This means take a and increase its value by one  Whereas in contrast  if I use the equal sign and I write a equals b  then this is a truth assertion  Okay? So if I write a equals b  then I'll asserting that the value of a equals to the value of b  right? So the left hand side  that's the computer operation  where we set the value of a to a new value  The right hand side  this is asserting  I'm just making a claim that the values of a and b are the same  and so whereas you can write a  = a + 1  that means increment a by 1  hopefully I won't ever write a = a + 1 because that's just wrong  a and a + 1 can never be equal to the same values  Okay? So this is first part of the definition  This alpha here is a number that is called the learning rate  And what alpha does is it basically controls how big a step we take downhill with creating descent  So if alpha is very large  then that corresponds to a very aggressive gradient descent procedure where we're trying take huge steps downhill and if alpha is very small  then we're taking little  little baby steps downhill  And I'll come back and say more about this later  about how to set alpha and so on  And finally  this term here  that's a derivative term  I don't wanna talk about it right now  but I will derive this derivative term and tell you exactly what this is later  okay? And some of you will be more familiar with calculus than others  but even if you aren't familiar with calculus  don't worry about it  I'll tell you what you need to know about this term here  Now  there's one more subtlety about gradient descent which is in gradient descent we're going to update  you know  theta 0 and theta 1  right? So this update takes place for j = 0 and j = 1  so you're gonna update theta 0 and update theta 1  And the subtlety of how you implement gradient descent is for this expression  for this update equation  you want to simultaneously update theta 0 and theta 1  What I mean by that is that in this equation  we're gonna update theta 0  = theta 0 minus something  and update theta 1  = theta 1 minus something  And the way to implement is you should compute the right hand side  right? Compute that thing for theta 0 and theta 1 and then simultaneously  at the same time  update theta 0 and theta 1  okay? So let me say what I mean by that  This is a correct implementation of gradient descent meaning simultaneous update  So I'm gonna set temp0 equals that  set temp1 equals that so basic compute the right-hand sides  and then having computed the right-hand sides and stored them into variables temp0 and temp1  I'm gonna update theta 0 and theta 1 simultaneously because that's the correct implementation  In contrast  here's an incorrect implementation that does not do a simultaneous update  So in this incorrect implementation  we compute temp0  and then we update theta 0  and then we compute temp1  and then we update temp1  And the difference between the right hand side and the left hand side implementations is that If you look down here  you look at this step  if by this time you've already updated theta 0  then you would be using the new value of theta 0 to compute this derivative term  And so this gives you a different value of temp1  than the left-hand side  right? Because you've now plugged in the new value of theta 0 into this equation  And so  this on the right-hand side is not a correct implementation of gradient descent  okay? So I don't wanna say why you need to do the simultaneous updates  It turns out that the way gradient descent is usually implemented  which I'll say more about later  it actually turns out to be more natural to implement the simultaneous updates  And when people talk about gradient descent  they always mean simultaneous update  If you implement the non simultaneous update  it turns out it will probably work anyway  But this algorithm wasn't right  It's not what people refer to as gradient descent  and this is some other algorithm with different properties  And for various reasons this can behave in slightly stranger ways  and so what you should do is really implement the simultaneous update of gradient descent  So  that's the outline of the gradient descent algorithm  In the next video  we're going to go into the details of the derivative term  which I wrote up but didn't really define  And if you've taken a calculus class before and if you're familiar with partial derivatives and derivatives  it turns out that's exactly what that derivative term is  but in case you aren't familiar with calculus  don't worry about it  The next video will give you all the intuitions and will tell you everything you need to know to compute that derivative term  even if you haven't seen calculus  or even if you haven't seen partial derivatives before  And with that  with the next video  hopefully we'll be able to give you all the intuitions you need to apply gradient descent 
FLg6PIIs6ko,Gradient Descent Intuition  In the previous video  we gave a mathematical definition of gradient descent  Let's delve deeper and in this video get better intuition about what the algorithm is doing and why the steps of the gradient descent algorithm might make sense  Here's a gradient descent algorithm that we saw last time and just to remind you this parameter  or this term alpha is called the learning rate  And it controls how big a step we take when updating my parameter theory j  And this second term here is the derivative term And what I wanna do in this video is give you that intuition about what each of these two terms is doing and why when put together  this entire update makes sense  In order to convey these intuitions  what I want to do is use a slightly simpler example  where we want to minimize the function of just one parameter  So say we have a cost function  j of just one parameter  theta one  like we did a few videos back  where theta one is a real number  So we can have one d plots  which are a little bit simpler to look at  Let's try to understand what gradient decent would do on this function  So let's say  here's my function  J of theta 1  And so that's mine  And where theta 1 is a real number  All right? Now  let's have in this slide its grade in descent with theta one at this location  So imagine that we start off at that point on my function  What grade in descent would do is it will update  Theta one gets updated as theta one minus alpha times d d theta one J of theta one  right? And as an aside  this derivative term  right  if you're wondering why I changed the notation from these partial derivative symbols  If you don't know what the difference is between these partial derivative symbols and the dd theta  don't worry about it  Technically in mathematics you call this a partial derivative and call this a derivative  depending on the number of parameters in the function J  But that's a mathematical technicality  And so for the purpose of this lecture  think of these partial symbols and d  d theta 1  as exactly the same thing  And don't worry about what the real difference is  I'm gonna try to use the mathematically precise notation  but for our purposes these two notations are really the same thing  And so let's see what this equation will do  So we're going to compute this derivative  not sure if you've seen derivatives in calculus before  but what the derivative at this point does  is basically saying  now let's take the tangent to that point  like that straight line  that red line  is just touching this function  and let's look at the slope of this red line  That's what the derivative is  it's saying what's the slope of the line that is just tangent to the function  Okay  the slope of a line is just this height divided by this horizontal thing  Now  this line has a positive slope  so it has a positive derivative  And so my update to theta is going to be theta 1  it gets updated as theta 1  minus alpha times some positive number  Okay  Alpha the the learning  is always a positive number  And  so we're going to take theta one is updated as theta one minus something  So I'm gonna end up moving theta one to the left  I'm gonna decrease theta one  and we can see this is the right thing to do cuz I actually wanna head in this direction  You know  to get me closer to the minimum over there  So  gradient descent so far says we're going the right thing  Let's look at another example  So let's take my same function J  let's try to draw from the same function  J of theta 1  And now  let's say I had to say initialize my parameter over there on the left  So theta 1 is here  I glare at that point on the surface  Now my derivative term DV theta one J of theta one when you value into that this point  we're gonna look at right the slope of that line  so this derivative term is a slope of this line  But this line is slanting down  so this line has negative slope  Right  Or alternatively  I say that this function has negative derivative  just means negative slope at that point  So this is less than equals to 0  so when I update theta  I'm gonna have theta  Just update this theta of minus alpha times a negative number  And so I have theta 1 minus a negative number which means I'm actually going to increase theta  because it's minus of a negative number  means I'm adding something to theta  And what that means is that I'm going to end up increasing theta until it's not here  and increase theta wish again seems like the thing I wanted to do to try to get me closer to the minimum  So this whole theory of intuition behind what a derivative is doing  let's take a look at the rate term alpha and see what that's doing  So here's my gradient descent update mural  that's this equation  And let's look at what could happen if alpha is either too small or if alpha is too large  So this first example  what happens if alpha is too small? So here's my function J  J of theta  Let's all start here  If alpha is too small  then what I'm gonna do is gonna multiply my update by some small number  so end up taking a baby step like that  Okay  so this one step  Then from this new point  I'm gonna have to take another step  But if alpha's too small  I take another little baby step  And so if my learning rate is too small I'm gonna end up taking these tiny tiny baby steps as you try to get to the minimum  And I'm gonna need a lot of steps to get to the minimum and so if alpha is too small gradient descent can be slow because it's gonna take these tiny tiny baby steps and so it's gonna need a lot of steps before it gets anywhere close to the global minimum  Now how about if our alpha is too large? So  here's my function Jf filter  turns out that alpha's too large  then gradient descent can overshoot the minimum and may even fail to convert or even divert  so here's what I mean  Let's say it's all our data there  it's actually close to minimum  So the derivative points to the right  but if alpha is too big  I want to take a huge step  Remember  take a huge step like that  So it ends up taking a huge step  and now my cost functions have strong roots  Cuz it starts off with this value  and now  my values are strong in verse  Now my derivative points to the left  it says I should decrease data  But if my learning is too big  I may take a huge step going from here all the way to out there  So we end up being over there  right? And if my is too big  we can take another huge step on the next elevation and kind of overshoot and overshoot and so on  until you already notice I'm actually getting further and further away from the minimum  So if alpha is to large  it can fail to converge or even diverge  Now  I have another question for you  So this a tricky one and when I was first learning this stuff it actually took me a long time to figure this out  What if your parameter theta 1 is already at a local minimum  what do you think one step of gradient descent will do? So let's suppose you initialize theta 1 at a local minimum  So  suppose this is your initial value of theta 1 over here and is already at a local optimum or the local minimum  It turns out the local optimum  your derivative will be equal to zero  So for that slope  that tangent point  so the slope of this line will be equal to zero and thus this derivative term is equal to zero  And so your gradient descent update  you have theta one cuz I updated this theta one minus alpha times zero  And so what this means is that if you're already at the local optimum it leaves theta 1 unchanged cause its updates as theta 1 equals theta 1  So if your parameters are already at a local minimum one step with gradient descent does absolutely nothing it doesn't your parameter which is what you want because it keeps your solution at the local optimum  This also explains why gradient descent can converse the local minimum even with the learning rate alpha fixed  Here's what I mean by that let's look in the example  So here's a cost function J of theta that maybe I want to minimize and let's say I initialize my algorithm  my gradient descent algorithm  out there at that magenta point  If I take one step in gradient descent  maybe it will take me to that point  because my derivative's pretty steep out there  Right? Now  I'm at this green point  and if I take another step in gradient descent  you notice that my derivative  meaning the slope  is less steep at the green point than compared to at the magenta point out there  Because as I approach the minimum  my derivative gets closer and closer to zero  as I approach the minimum  So after one step of descent  my new derivative is a little bit smaller  So I wanna take another step in the gradient descent  I will naturally take a somewhat smaller step from this green point right there from the magenta point  Now with a new point  a red point  and I'm even closer to global minimum so the derivative here will be even smaller than it was at the green point  So I'm gonna another step in the gradient descent  Now  my derivative term is even smaller and so the magnitude of the update to theta one is even smaller  so take a small step like so  And as gradient descent runs  you will automatically take smaller and smaller steps  Until eventually you're taking very small steps  you know  and you finally converge to the to the local minimum  So just to recap  in gradient descent as we approach a local minimum  gradient descent will automatically take smaller steps  And that's because as we approach the local minimum  by definition the local minimum is when the derivative is equal to zero  As we approach local minimum  this derivative term will automatically get smaller  and so gradient descent will automatically take smaller steps  This is what so no need to decrease alpha or the time  So that's the gradient descent algorithm and you can use it to try to minimize any cost function J  not the cost function J that we defined for linear regression  In the next video  we're going to take the function J and set that back to be exactly linear regression's cost function  the square cost function that we came up with earlier  And taking gradient descent and this great cause function and putting them together  That will give us our first learning algorithm  that'll give us a linear regression algorithm 
VIQCMDouxKo,Gradient Descent For Linear Regression  In previous videos  we talked about the gradient descent algorithm and we talked about the linear regression model and the squared error cost function  In this video we're gonna put together gradient descent with our cost function  and that will give us an algorithm for linear regression or putting a straight line to our data  So this was what we worked out in the previous videos  This gradient descent algorithm which you should be familiar and here's the linear regression model with our linear hypothesis and our squared error cost function  What we're going to do is apply gradient descent to minimize our squared error cost function  Now in order to apply gradient descent  in order to  you know  write this piece of code  the key term we need is this derivative term over here  So you need to figure out what is this partial derivative term and plugging in the definition of the cause function j  This turns out to be this  Sum from y equals 1 though m  Of this squared error cost function term  And all I did here was I just  you know plug in the definition of the cost function there  And simplifying a little bit more  this turns out to be equal to this  Sigma i equals one through m of theta zero plus theta one x i minus Yi squared  And all I did there was I took the definition for my hypothesis and plugged it in there  And turns out we need to figure out what is this partial derivative for two cases for J equals 0 and J equals 1  So we want to figure out what is this partial derivative for both the theta 0 case and the theta 1 case  And I'm just going to write out the answers  It turns out this first term is  simplifies to 1/M sum from over my training step of just that of X(i)- Y(i) and for this term partial derivative let's write the theta 1  it turns out I get this term  Minus Y(i) times X(i)  Okay and computing these partial derivatives  so we're going from this equation  Right going from this equation to either of the equations down there  Computing those partial derivative terms requires some multivariate calculus  If you know calculus  feel free to work through the derivations yourself and check that if you take the derivatives  you actually get the answers that I got  But if you're less familiar with calculus  don't worry about it and it's fine to just take these equations that were worked out and you won't need to know calculus or anything like that  in order to do the homework so let's implement gradient descent and get back to work  So armed with these definitions or armed with what we worked out to be the derivatives which is really just the slope of the cost function j we can now plug them back in to our gradient descent algorithm  So here's gradient descent for linear regression which is gonna repeat until convergence  theta 0 and theta 1 get updated as you know this thing minus alpha times the derivative term  So this term here  So here's our linear regression algorithm  This first term here  That term is of course just the partial derivative with respect to theta zero  that we worked out on a previous slide  And this second term here  that term is just a partial derivative in respect to theta 1  that we worked out on the previous line  And just as a quick reminder  you must  when implementing gradient descent  There's actually this detail that you should be implementing it so the update theta 0 and theta 1 simultaneously  So  Let's see how gradient descent works  One of the issues we saw with gradient descent is that it can be susceptible to local optima  So when I first explained gradient descent I showed you this picture of it going downhill on the surface  and we saw how depending on where you initialize it  you can end up at different local optima  You will either wind up here or here  But  it turns out that that the cost function for linear regression is always going to be a bow shaped function like this  The technical term for this is that this is called a convex function  And I'm not gonna give the formal definition for what is a convex function  C  O  N  V  E  X  But informally a convex function means a bowl shaped function and so this function doesn't have any local optima except for the one global optimum  And does gradient descent on this type of cost function which you get whenever you're using linear regression it will always converge to the global optimum  Because there are no other local optimum  global optimum  So now let's see this algorithm in action  As usual  here are plots of the hypothesis function and of my cost function j  And so let's say I've initialized my parameters at this value  Let's say  usually you initialize your parameters at zero  zero  Theta zero and theta equals zero  But for the demonstration  in this physical infrontation I've initialized you know  theta zero at 900 and theta one at about -0 1 okay  And so this corresponds to h(x)=-900-0 1x  [the intercept should be +900] is this line  out here on the cost function  Now  if we take one step in gradient descent  we end up going from this point out here  over to the down and left  to that second point over there  And you notice that my line changed a little bit  and as I take another step of gradient descent  my line on the left will change  Right? And I've also moved to a new point on my cost function  And as I take further steps of gradient descent  I'm going down in cost  So my parameters and such are following this trajectory  And if you look on the left  this corresponds with hypotheses  That seem to be getting to be better and better fits to the data until eventually I've now wound up at the global minimum and this global minimum corresponds to this hypothesis  which gets me a good fit to the data  And so that's gradient descent  and we've just run it and gotten a good fit to my data set of housing prices  And you can now use it to predict  you know  if your friend has a house size 1250 square feet  you can now read off the value and tell them that I don't know maybe they could get $250 000 for their house  Finally just to give this another name it turns out that the algorithm that we just went over is sometimes called batch gradient descent  And it turns out in machine learning I don't know I feel like us machine learning people were not always great at giving names to algorithms  But the term batch gradient descent refers to the fact that in every step of gradient descent  we're looking at all of the training examples  So in gradient descent  when computing the derivatives  we're computing the sums [INAUDIBLE]  So ever step of gradient descent we end up computing something like this that sums over our m training examples and so the term batch gradient descent refers to the fact that we're looking at the entire batch of training examples  And again  it's really not a great name  but this is what machine learning people call it  And it turns out that there are sometimes other versions of gradient descent that are not batch versions  but they are instead  Do not look at the entire training set but look at small subsets of the training sets at a time  And we'll talk about those versions later in this course as well  But for now using the algorithm we just learned about or using batch gradient descent you now know how to implement gradient descent for linear regression  So that's linear regression with gradient descent  If you've seen advanced linear algebra before  so some of you may have taken a class in advanced linear algebra  You might know that there exists a solution for numerically solving for the minimum of the cost function j without needing to use an iterative algorithm like gradient descent  Later in this course we'll talk about that method as well that just solves for the minimum of the cost function j without needing these multiple steps of gradient descent  That other method is called the normal equations method  But in case you've heard of that method it turns out that gradient descent will scale better to larger data sets than that normal equation method  And now that we know about gradient descent we'll be able to use it in lots of different contexts and we'll use it in lots of different machine learning problems as well  So congrats on learning about your first machine learning algorithm  We'll later have exercises in which we'll ask you to implement gradient descent and hopefully see these algorithms right for yourselves  But before that I first want to tell you in the next set of videos  The first one to tell you about a generalization of the gradient descent algorithm that will make it much more powerful  And I guess I'll tell you about that in the next video 
8s4b8mYCMAE,"Matrices and Vectors  Let's get started with our linear algebra review  In this video I want to tell you what are matrices and what are vectors  A matrix is a rectangular array of numbers written between square brackets  So  for example  here is a matrix on the right  a left square bracket  And then  write in a bunch of numbers  These could be features from a learning problem or it could be data from somewhere else  but the specific values don't matter  and then I'm going to close it with another right bracket on the right  And so that's one matrix  And  here's another example of the matrix  let's write 3  4  5 6  So matrix is just another way for saying  is a 2D or a two dimensional array  And the other piece of knowledge that we need is that the dimension of the matrix is going to be written as the number of row times the number of columns in the matrix  So  concretely  this example on the left  this has 1  2  3  4 rows and has 2 columns  and so this example on the left is a 4 by 2 matrix - number of rows by number of columns  So  four rows  two columns  This one on the right  this matrix has two rows  That's the first row  that's the second row  and it has three columns  That's the first column  that's the second column  that's the third column So  this second matrix we say it is a 2 by 3 matrix  So we say that the dimension of this matrix is 2 by 3  Sometimes you also see this written out  in the case of left  you will see this written out as R4 by 2 or concretely what people will sometimes say this matrix is an element of the set R 4 by 2  So  this thing here  this just means the set of all matrices that of dimension 4 by 2 and this thing on the right  sometimes this is written out as a matrix that is an R 2 by 3  So if you ever see  2 by 3  So if you ever see something like this are 4 by 2 or are 2 by 3  people are just referring to matrices of a specific dimension  Next  let's talk about how to refer to specific elements of the matrix  And by matrix elements  other than the matrix I just mean the entries  so the numbers inside the matrix  So  in the standard notation  if A is this matrix here  then A sub-strip IJ is going to refer to the i  j entry  meaning the entry in the matrix in the ith row and jth column  So for example a1-1 is going to refer to the entry in the 1st row and the 1st column  so that's the first row and the first column and so a1-1 is going to be equal to 1  4  0  2  Another example  8 1 2 is going to refer to the entry in the first row and the second column and so A 1 2 is going to be equal to one nine one  This come from a quick examples  Let's see  A  oh let's say A 3 2  is going to refer to the entry in the 3rd row  and second column  right  because that's 3 2 so that's equal to 1 4 3 7  And finally  8 4 1 is going to refer to this one right  fourth row  first column is equal to 1 4 7 and if  hopefully you won't  but if you were to write and say well this A 4 3  well  that refers to the fourth row  and the third column that  you know  this matrix has no third column so this is undefined  you know  or you can think of this as an error  There's no such element as 8 4 3  so  you know  you shouldn't be referring to 8 4 3  So  the matrix gets you a way of letting you quickly organize  index and access lots of data  In case I seem to be tossing up a lot of concepts  a lot of new notations very rapidly  you don't need to memorize all of this  but on the course website where we have posted the lecture notes  we also have all of these definitions written down  So you can always refer back  you know  either to these slides  possible coursework  so audible lecture notes if you forget well  A41 was that? Which row  which column was that? Don't worry about memorizing everything now  You can always refer back to the written materials on the course website  and use that as a reference  So that's what a matrix is  Next  let's talk about what is a vector  A vector turns out to be a special case of a matrix  A vector is a matrix that has only 1 column so you have an N x 1 matrix  then that's a remember  right? N is the number of rows  and 1 here is the number of columns  so  so matrix with just one column is what we call a vector  So here's an example of a vector  with I guess I have N equals four elements here  so we also call this thing  another term for this is a four dmensional vector  just means that this is a vector with four elements  with four numbers in it  And  just as earlier for matrices you saw this notation R3 by 2 to refer to 2 by 3 matrices  for this vector we are going to refer to this as a vector in the set R4  So this R4 means a set of four-dimensional vectors  Next let's talk about how to refer to the elements of the vector  We are going to use the notation yi to refer to the ith element of the vector y  So if y is this vector  y subscript i is the ith element  So y1 is the first element four sixty  y2 is equal to the second element  two thirty two -there's the first  There's the second  Y3 is equal to 315 and so on  and only y1 through y4 are defined consistency 4-dimensional vector  Also it turns out that there are actually 2 conventions for how to index into a vector and here they are  Sometimes  people will use one index and sometimes zero index factors  So this example on the left is a one in that specter where the element we write is y1  y2  y3  y4  And this example in the right is an example of a zero index factor where we start the indexing of the elements from zero  So the elements go from a zero up to y three  And this is a bit like the arrays of some primary languages where the arrays can either be indexed starting from one  The first element of an array is sometimes a Y1  this is sequence notation I guess  and sometimes it's zero index depending on what programming language you use  So it turns out that in most of math  the one index version is more common For a lot of machine learning applications  zero index vectors gives us a more convenient notation  So what you should usually do is  unless otherwised specified  you should assume we are using one index vectors  In fact  throughout the rest of these videos on linear algebra review  I will be using one index vectors  But just be aware that when we are talking about machine learning applications  sometimes I will explicitly say when we need to switch to  when we need to use the zero index vectors as well  Finally  by convention  usually when writing matrices and vectors  most people will use upper case to refer to matrices  So we're going to use capital letters like A  B  C  you know  X  to refer to matrices  and usually we'll use lowercase  like a  b  x  y  to refer to either numbers  or just raw numbers or scalars or to vectors  This isn't always true but this is the more common notation where we use lower case \""Y\"" for referring to vector and we usually use upper case to refer to a matrix  So  you now know what are matrices and vectors  Next  we'll talk about some of the things you can do with them"
AOnQvbPbP4g,Addition and Scalar Multiplication  In this video we'll talk about matrix addition and subtraction  as well as how to multiply a matrix by a number  also called Scalar Multiplication  Let's start an example  Given two matrices like these  let's say I want to add them together  How do I do that? And so  what does addition of matrices mean? It turns out that if you want to add two matrices  what you do is you just add up the elements of these matrices one at a time  So  my result of adding two matrices is going to be itself another matrix and the first element again just by taking one and four and multiplying them and adding them together  so I get five  The second element I get by taking two and two and adding them  so I get four  three plus three plus zero is three  and so on  I'm going to stop changing colors  I guess  And  on the right is open five  ten and two  And it turns out you can add only two matrices that are of the same dimensions  So this example is a three by two matrix  because this has 3 rows and 2 columns  so it's 3 by 2  This is also a 3 by 2 matrix  and the result of adding these two matrices is a 3 by 2 matrix again  So you can only add matrices of the same dimension  and the result will be another matrix that's of the same dimension as the ones you just added  Where as in contrast  if you were to take these two matrices  so this one is a 3 by 2 matrix  okay  3 rows  2 columns  This here is a 2 by 2 matrix  And because these two matrices are not of the same dimension  you know  this is an error  so you cannot add these two matrices and  you know  their sum is not well-defined  So that's matrix addition  Next  let's talk about multiplying matrices by a scalar number  And the scalar is just a  maybe a overly fancy term for  you know  a number or a real number  Alright  this means real number  So let's take the number 3 and multiply it by this matrix  And if you do that  the result is pretty much what you'll expect  You just take your elements of the matrix and multiply them by 3  one at a time  So  you know  one times three is three  What  two times three is six  3 times 3 is 9  and let's see  I'm going to stop changing colors again  Zero times 3 is zero  Three times 5 is 15  and 3 times 1 is three  And so this matrix is the result of multiplying that matrix on the left by 3  And you notice  again  this is a 3 by 2 matrix and the result is a matrix of the same dimension  This is a 3 by 2  both of these are 3 by 2 dimensional matrices  And by the way  you can write multiplication  you know  either way  So  I have three times this matrix  I could also have written this matrix and 0  2  5  3  1  right  I just copied this matrix over to the right  I can also take this matrix and multiply this by three  So whether it's you know  3 times the matrix or the matrix times three is the same thing and this thing here in the middle is the result  You can also take a matrix and divide it by a number  So  turns out taking this matrix and dividing it by four  this is actually the same as taking the number one quarter  and multiplying it by this matrix  4  0  6  3 and so  you can figure the answer  the result of this product is  one quarter times four is one  one quarter times zero is zero  One quarter times six is  what  three halves  about six over four is three halves  and one quarter times three is three quarters  And so that's the results of computing this matrix divided by four  Vectors give you the result  Finally  for a slightly more complicated example  you can also take these operations and combine them together  So in this calculation  I have three times a vector plus a vector minus another vector divided by three  So just make sure we know where these are  right  This multiplication  This is an example of scalar multiplication because I am taking three and multiplying it  And this is  you know  another scalar multiplication  Or more like scalar division  I guess  It really just means one zero times this  And so if we evaluate these two operations first  then what we get is this thing is equal to  let's see  so three times that vector is three  twelve  six  plus my vector in the middle which is a 005 minus one  zero  two-thirds  right? And again  just to make sure we understand what is going on here  this plus symbol  that is matrix addition  right? I really  since these are vectors  remember  vectors are special cases of matrices  right? This  you can also call this vector addition This minus sign here  this is again a matrix subtraction  but because this is an n by 1  really a three by one matrix  that this is actually a vector  so this is also vector  this column  We call this matrix a vector subtraction  as well  OK? And finally to wrap this up  This therefore gives me a vector  whose first element is going to be 3+0-1  so that's 3-1  which is 2  The second element is 12+0-0  which is 12  And the third element of this is  what  6+5-(2/3)  which is 11-(2/3)  so that's 10 and one-third and see  you close this square bracket  And so this gives me a 3 by 1 matrix  which is also just called a 3 dimensional vector  which is the outcome of this calculation over here  So that's how you add and subtract matrices and vectors and multiply them by scalars or by row numbers  So far I have only talked about how to multiply matrices and vectors by scalars  by row numbers  In the next video we will talk about a much more interesting step  of taking 2 matrices and multiplying 2 matrices together 
KTySRjpj2ss,"Matrix Vector Multiplication  In this video  I'd like to start talking about how to multiply together two matrices  We'll start with a special case of that  of matrix vector multiplication - multiplying a matrix together with a vector  Let's start with an example  Here is a matrix  and here is a vector  and let's say we want to multiply together this matrix with this vector  what's the result? Let me just work through this example and then we can step back and look at just what the steps were  It turns out the result of this multiplication process is going to be  itself  a vector  And I'm just going work with this first and later we'll come back and see just what I did here  To get the first element of this vector I am going to take these two numbers and multiply them with the first row of the matrix and add up the corresponding numbers  Take one multiplied by one  and take three and multiply it by five  and that's what  that's one plus fifteen so that gives me sixteen  I'm going to write sixteen here  then for the second row  second element  I am going to take the second row and multiply it by this vector  so I have four times one  plus zero times five  which is equal to four  so you'll have four there  And finally for the last one I have two one times one five  so two by one  plus one by 5  which is equal to a 7  and so I get a 7 over there  It turns out that the results of multiplying that's a 3x2 matrix by a 2x1 matrix is also just a two-dimensional vector  The result of this is going to be a 3x1 matrix  so that's why three by one 3x1 matrix  in other words a 3x1 matrix is just a three dimensional vector  So I realize that I did that pretty quickly  and you're probably not sure that you can repeat this process yourself  but let's look in more detail at what just happened and what this process of multiplying a matrix by a vector looks like  Here's the details of how to multiply a matrix by a vector  Let's say I have a matrix A and want to multiply it by a vector x  The result is going to be some vector y  So the matrix A is a m by n dimensional matrix  so m rows and n columns and we are going to multiply that by a n by 1 matrix  in other words an n dimensional vector  It turns out this \""n\"" here has to match this \""n\"" here  In other words  the number of columns in this matrix  so it's the number of n columns  The number of columns here has to match the number of rows here  It has to match the dimension of this vector  And the result of this product is going to be an n-dimensional vector y  Rows here  \""M\"" is going to be equal to the number of rows in this matrix \""A\""  So how do you actually compute this vector \""Y\""? Well it turns out to compute this vector \""Y\""  the process is to get \""Y\""\""I\""  multiply \""A's\"" \""I'th\"" row with the elements of the vector \""X\"" and add them up  So here's what I mean  In order to get the first element of \""Y\""  that first number--whatever that turns out to be--we're gonna take the first row of the matrix \""A\"" and multiply them one at a time with the elements of this vector \""X\""  So I take this first number multiply it by this first number  Then take the second number multiply it by this second number  Take this third number whatever that is  multiply it the third number and so on until you get to the end  And I'm gonna add up the results of these products and the result of paying that out is going to give us this first element of \""Y\""  Then when we want to get the second element of \""Y\""  let's say this element  The way we do that is we take the second row of A and we repeat the whole thing  So we take the second row of A  and multiply it elements-wise  so the elements of X and add up the results of the products and that would give me the second element of Y  And you keep going to get and we going to take the third row of A  multiply element Ys with the vector x  sum up the results and then I get the third element and so on  until I get down to the last row like so  okay? So that's the procedure  Let's do one more example  Here's the example  So let's look at the dimensions  Here  this is a three by four dimensional matrix  This is a four-dimensional vector  or a 4 x 1 matrix  and so the result of this  the result of this product is going to be a three-dimensional vector  Write  you know  the vector  with room for three elements  Let's do the  let's carry out the products  So for the first element  I'm going to take these four numbers and multiply them with the vector X  So I have 1x1  plus 2x3  plus 1x2  plus 5x1  which is equal to - that's 1+6  plus 2+6  which gives me 14  And then for the second element  I'm going to take this row now and multiply it with this vector (0x1)+3  All right  so 0x1+ 3x3 plus 0x2 plus 4x1  which is equal to  let's see that's 9+4  which is 13  And finally  for the last element  I'm going to take this last row  so I have minus one times one  You have minus two  or really there's a plus next to a two I guess  Times three plus zero times two plus zero times one  and so that's going to be minus one minus six  which is going to make this seven  and so that's vector seven  Okay? So my final answer is this vector fourteen  just to write to that without the colors  fourteen  thirteen  negative seven  And as promised  the result here is a three by one matrix  So that's how you multiply a matrix and a vector  I know that a lot just happened on this slide  so if you're not quite sure where all these numbers went  you know  feel free to pause the video you know  and so take a slow careful look at this big calculation that we just did and try to make sure that you understand the steps of what just happened to get us these numbers fourteen  thirteen and eleven  Finally  let me show you a neat trick  Let's say we have a set of four houses so 4 houses with 4 sizes like these  And let's say I have a hypotheses for predicting what is the price of a house  and let's say I want to compute  you know  H of X for each of my 4 houses here  It turns out there's neat way of posing this  applying this hypothesis to all of my houses at the same time  It turns out there's a neat way to pose this as a Matrix Vector multiplication  So  here's how I'm going to do it  I am going to construct a matrix as follows  My matrix is going to be 1111 times  and I'm going to write down the sizes of my four houses here and I'm going to construct a vector as well  And my vector is going to this vector of two elements  that's minus 40 and 0 25  That's these two co-efficients  data 0 and data 1  And what I am going to do is to take matrix and that vector and multiply them together  that times is that multiplication symbol  So what do I get? Well this is a four by two matrix  This is a two by one matrix  So the outcome is going to be a four by one vector  all right  So  let me  so this is going to be a 4 by 1 matrix is the outcome or really a four diminsonal vector  so let me write it as one of my four elements in my four real numbers here  Now it turns out and so this first element of this result  the way I am going to get that is  I am going to take this and multiply it by the vector  And so this is going to be -40 x 1 + 4 25 x 2104  By the way  on the earlier slides I was writing 1 x -40 and 2104 x 0 25  but the order doesn't matter  right? -40 x 1 is the same as 1 x -40  And this first element  of course  is \""H\"" applied to 2104  So it's really the predicted price of my first house  Well  how about the second element? Hope you can see where I am going to get the second element  Right? I'm gonna take this and multiply it by my vector  And so that's gonna be -40 x 1 + 0 25 x 1416  And so this is going be \""H\"" of 1416  Right? And so on for the third and the fourth elements of this 4 x 1 vector  And just there  right? This thing here that I just drew the green box around  that's a real number  OK? That's a single real number  and this thing here that I drew the magenta box around--the purple  magenta color box around--that's a real number  right? And so this thing on the right--this thing on the right overall  this is a 4 by 1 dimensional matrix  was a 4 dimensional vector  And  the neat thing about this is that when you're actually implementing this in software--so when you have four houses and when you want to use your hypothesis to predict the prices  predict the price \""Y\"" of all of these four houses  What this means is that  you know  you can write this in one line of code  When we talk about octave and program languages later  you can actually  you'll actually write this in one line of code  You write prediction equals my  you know  data matrix times parameters  right? Where data matrix is this thing here  and parameters is this thing here  and this times is a matrix vector multiplication  And if you just do this then this variable prediction - sorry for my bad handwriting - then just implement this one line of code assuming you have an appropriate library to do matrix vector multiplication  If you just do this  then prediction becomes this 4 by 1 dimensional vector  on the right  that just gives you all the predicted prices  And your alternative to doing this as a matrix vector multiplication would be to write eomething like   you know  for I equals 1 to 4  right? And you have say a thousand houses it would be for I equals 1 to a thousand or whatever  And then you have to write a prediction  you know  if I equals  and then do a bunch more work over there and it turns out that When you have a large number of houses  if you're trying to predict the prices of not just four but maybe of a thousand houses then it turns out that when you implement this in the computer  implementing it like this  in any of the various languages  This is not only true for Octave  but for Supra Server Java or Python  other high-level  other languages as well  It turns out  that  by writing code in this style on the left  it allows you to not only simplify the code  because  now  you're just writing one line of code rather than the form of a bunch of things inside  But  for subtle reasons  that we will see later  it turns out to be much more computationally efficient to make predictions on all of the prices of all of your houses doing it the way on the left than the way on the right than if you were to write your own formula  I'll say more about this later when we talk about vectorization  but  so  by posing a prediction this way  you get not only a simpler piece of code  but a more efficient one  So  that's it for matrix vector multiplication and we'll make good use of these sorts of operations as we develop the living regression in other models further  But  in the next video we're going to take this and generalize this to the case of matrix matrix multiplication "
8XAvd2Ey2mQ,Matrix Matrix Multiplication  In this video we'll talk about matrix-matrix multiplication  or how to multiply two matrices together  When we talk about the method in linear regression for how to solve for the parameters theta 0 and theta 1 all in one shot  without needing an iterative algorithm like gradient descent  When we talk about that algorithm  it turns out that matrix-matrix multiplication is one of the key steps that you need to know  So let's  as usual  start with an example  Let's say I have two matrices and I want to multiply them together  Let me again just run through this example and then I'll tell you a little bit of what happened  So the first thing I'm gonna do is I'm going to pull out the first column of this matrix on the right  And I'm going to take this matrix on the left and multiply it by a vector that is just this first column  And it turns out  if I do that  I'm going to get the vector 11  9  So this is the same matrix-vector multiplication as you saw in the last video  I worked this out in advance  so I know it's 11  9  And then the second thing I want to do is I'm going to pull out the second column of this matrix on the right  And I'm then going to take this matrix on the left  so take that matrix  and multiply it by that second column on the right  So again  this is a matrix-vector multiplication step which you saw from the previous video  And it turns out that if you multiply this matrix and this vector you get 10  14  And by the way  if you want to practice your matrix-vector multiplication  feel free to pause the video and check this product yourself  Then I'm just gonna take these two results and put them together  and that'll be my answer  So it turns out the outcome of this product is gonna be a two by two matrix  And the way I'm gonna fill in this matrix is just by taking my elements 11  9  and plugging them here  And taking 10  14 and plugging them into the second column  okay? So that was the mechanics of how to multiply a matrix by another matrix  You basically look at the second matrix one column at a time and you assemble the answers  And again  we'll step through this much more carefully in a second  But I just want to point out also  this first example is a 2x3 matrix  Multiply that by a 3x2 matrix  and the outcome of this product turns out to be a 2x2 matrix  And again  we'll see in a second why this was the case  All right  that was the mechanics of the calculation  Let's actually look at the details and look at what exactly happened  Here are the details  I have a matrix A and I want to multiply that with a matrix B and the result will be some new matrix C  It turns out you can only multiply together matrices whose dimensions match  So A is an m x n matrix  so m rows  n columns  And we multiply with an n x o matrix  And it turns out this n here must match this n here  So the number of columns in the first matrix must equal to the number of rows in the second matrix  And the result of this product will be a m x o matrix  like the matrix C here  And in the previous video everything we did corresponded to the special case of o being equal to 1  That was to the case of B being a vector  But now we're gonna deal with the case of values of o larger than 1  So here's how you multiply together the two matrices  What I'm going to do is I'm going to take the first column of B and treat that as a vector  and multiply the matrix A by the first column of B  And the result of that will be a n by 1 vector  and I'm gonna put that over here  Then I'm gonna take the second column of B  right? So this is another n by 1 vector  So this column here  this is n by 1  It's an n-dimensional vector  Gonna multiply this matrix with this n by 1 vector  The result will be a m-dimensional vector  which we'll put there  and so on  And then I'm gonna take the third column  multiply it by this matrix  I get a m-dimensional vector  And so on  until you get to the last column  The matrix times the last column gives you the last column of C  Just to say that again  the ith column of the matrix C is obtained by taking the matrix A and multiplying the matrix A with the ith column of the matrix B for the values of i = 1  2  up through o  So this is just a summary of what we did up there in order to compute the matrix C  Let's look at just one more example  Let's say I want to multiply together these two matrices  So what I'm going to do is first pull out the first column of my second matrix  That was my matrix B on the previous slide and I therefore have this matrix times that vector  And so  oh  let's do this calculation quickly  This is going to be equal to the 1  3 x 0  3  so that gives 1 x 0 + 3 x 3  And the second element is going to be 2  5 x 0  3  so that's gonna be 2 x 0 + 5 x 3  And that is 9  15  Oh  actually let me write that in green  So this is 9  15  And then next I'm going to pull out the second column of this and do the corresponding calculations  So that's this matrix times this vector 1  2  Let's also do this quickly  so that's 1 x 1 + 3 x 2  so that was that row  And let's do the other one  So let's see  that gives me 2 x 1 + 5 x 2 and so that is going to be equal to  lets see  1 x 1 + 3 x 1 is 7 and 2 x 1 + 5 x 2 is 12  So now I have these two and so my outcome  the product of these two matrices  is going to be this goes here and this goes here  So I get 9  15 and 4  12  [It should be 7 12] And you may notice also that the result of multiplying a 2x2 matrix with another 2x2 matrix  the resulting dimension is going to be that first 2 times that second 2  So the result is itself also a 2x2 matrix  Finally  let me show you one more neat trick that you can do with matrix-matrix multiplication  Let's say  as before  that we have four houses whose prices we wanna predict  Only now  we have three competing hypotheses shown here on the right  So if you want to apply all three competing hypotheses to all four of your houses  it turns out you can do that very efficiently using a matrix-matrix multiplication  So here on the left is my usual matrix  same as from the last video where these values are my housing prices [he means housing sizes]\nand I've put 1s here on the left as well  And what I am going to do is construct another matrix where here  the first column is this -40 and 0 25 and the second column is this 200  0 1 and so on  And it turns out that if you multiply these two matrices  what you find is that this first column  I'll draw that in blue  Well  how do you get this first column? Our procedure for matrix-matrix multiplication is  the way you get this first column is you take this matrix and you multiply it by this first column  And we saw in the previous video that this is exactly the predicted housing prices of the first hypothesis  right  of this first hypothesis here  And how about the second column? Well  [INAUDIBLE] second column  The way you get the second column is  well  you take this matrix and you multiply it by this second column  And so the second column turns out to be the predictions of the second hypothesis up there  and similarly for the third column  And so I didn't step through all the details  but hopefully you can just feel free to pause the video and check the math yourself and check that what I just claimed really is true  But it turns out that by constructing these two matrices  what you can therefore do is very quickly apply all 3 hypotheses to all 4 house sizes to get all 12 predicted prices output by your 3 hypotheses on your 4 houses  So with just one matrix multiplication step you managed to make 12 predictions  And even better  it turns out that in order to do that matrix multiplication  there are lots of good linear algebra libraries in order to do this multiplication step for you  And so pretty much any reasonable programming language that you might be using  Certainly all the top ten most popular programming languages will have great linear algebra libraries  And there'll be good linear algebra libraries that are highly optimized in order to do that matrix-matrix multiplication very efficiently  Including taking advantage of any sort of parallel computation that your computer may be capable of  whether your computer has multiple cores or multiple processors  Or within a processor sometimes there's parallelism as well called SIMD parallelism that your computer can take care of  And there are very good free libraries that you can use to do this matrix-matrix multiplication very efficiently  so that you can very efficiently make lots of predictions with lots of hypotheses 
jI_D3Bzf0u4,Matrix Multiplication Properties  Matrix multiplication is really useful  since you can pack a lot of computation into just one matrix multiplication operation  But you should be careful of how you use them  In this video  I wanna tell you about a few properties of matrix multiplication  When working with just real numbers or when working with scalars  multiplication is commutative  And what I mean by that is that if you take 3 times 5  that is equal to 5 times 3  And the ordering of this multiplication doesn't matter  And this is called the commutative property of multiplication of real numbers  It turns out this property  they can reverse the order in which you multiply things  This is not true for matrix multiplication  So concretely  if A and B are matrices  Then in general  A times B is not equal to B times A  So  just be careful of that  Its not okay to arbitrarily reverse the order in which you multiply matrices  Matrix multiplication in not commutative  is the fancy way of saying it  As a concrete example  here are two matrices  This matrix 1 1 0 0 times 0 0 2 0 and if you multiply these two matrices you get this result on the right  Now let's swap around the order of these two matrices  So I'm gonna take this two matrices and just reverse them  It turns out if you multiply these two matrices  you get the second answer on the right  And well clearly  right  these two matrices are not equal to each other  So  in fact  in general if you have a matrix operation like A times B  if A is an m by n matrix  and B is an n by m matrix  just as an example  Then  it turns out that the matrix A times B  right  is going to be an m by m matrix  Whereas the matrix B times A is going to be an n by n matrix  So the dimensions don't even match  right? So if A x B and B x A may not even be the same dimension  In the example on the left  I have all two by two matrices  So the dimensions were the same  but in general  reversing the order of the matrices can even change the dimension of the outcome  So  matrix multiplication is not commutative  Here's the next property I want to talk about  So  when talking about real numbers or scalars  let's say I have 3 x 5 x 2  I can either multiply 5 x 2 first  Then I can compute this as 3 x 10  Or  I can multiply 3 x 5 first  and I can compute this as 15 x 2  And both of these give you the same answer  right? Both of these is equal to 30  So it doesn't matter whether I multiply 5 x 2 first or whether I multiply 3 x 5 first  because sort of  well  3 x (5 x 2) = (3 x 5) x 2  And this is called the associative property of real number multiplication  It turns out that matrix multiplication is associative  So concretely  let's say I have a product of three matrices A x B x C  Then  I can compute this either as A x (B x C) or I can computer this as (A x B) x C  and these will actually give me the same answer  I'm not gonna prove this but you can just take my word for it I guess  So just be clear  what I mean by these two cases  Let's look at the first one  right  This first case  What I mean by that is if you actually wanna compute A x B x C  What you can do is you can first compute B x C  So that D = B x C then compute A x D  And so this here is really computing A x B x C  Or  for this second case  you can compute this as  you can set E = A x B  then compute E times C  And this is then the same as A x B x C  and it turns out that both of these options will give you this guarantee to give you the same answer  And so we say that matrix multiplication thus enjoy the associative property  Okay? And don't worry about the terminology associative and commutative  That's what it's called  but I'm not really going to use this terminology later in this class  so don't worry about memorizing those terms  Finally  I want to tell you about the Identity Matrix  which is a special matrix  So let's again make the analogy to what we know of real numbers  When dealing with real numbers or scalar numbers  the number 1  you can think of it as the identity of multiplication  And what I mean by that is that for any number z  1 x z = z x 1  And that's just equal to the number z for any real number z  So 1 is the identity operation and so it satisfies this equation  So it turns out  that this in the space of matrices there's an identity matrix as well and it's usually denoted I or sometimes we write it as I of n x n if we want to make it explicit to dimensions  So I subscript n x n is the n x n identity matrix  And so that's a different identity matrix for each dimension n  And here are few examples  Here's the 2 x 2 identity matrix  here's the 3 x 3 identity matrix  here's the 4 x 4 matrix  So the identity matrix has the property that it has ones along the diagonals  All right  and so on  And 0 everywhere else  And so  by the way  the 1 x 1 identity matrix is just a number 1  and so the 1 x 1 matrix with just 1 in it  So it's not a very interesting identity matrix  And informally  when I or others are being sloppy  very often we'll write the identity matrices in fine notation  We'll draw square brackets  just write one one one dot dot dot dot one  and then we'll maybe somewhat sloppily write a bunch of zeros there  And these zeroes on the  this big zero and this big zero  that's meant to denote that this matrix is zero everywhere except for the diagonal  So this is just how I might swap you the right D identity matrix  And it turns out that the identity matrix has its property that for any matrix A  A times identity equals I times A equals A so that's a lot like this equation that we have up here  Right? So 1 times z equals z times 1 equals z itself  So I times A equals A times I equals A  Just to make sure we have the dimensions right  So if A is an m by n matrix  then this identity matrix here  that's an n by n identity matrix  And if is and by then  then this identity matrix  right? For matrix multiplication to make sense  that has to be an m by m matrix  Because this m has the match up that m  and in either case  the outcome of this process is you get back the matrix A which is m by n  So whenever we write the identity matrix I  you know  very often the dimension Mention  right  will be implicit from the content  So these two I's  they're actually different dimension matrices  One may be n by n  the other is n by m  But when we want to make the dimension of the matrix explicit  then sometimes we'll write to this I subscript n by n  kind of like we had up here  But very often  the dimension will be implicit  Finally  I just wanna point out that earlier I said that AB is not  in general  equal to BA  Right? For most matrices A and B  this is not true  But when B is the identity matrix  this does hold true  that A times the identity matrix does indeed equal to identity times A is just that you know this is not true for other matrices B in general  So  that's it for the properties of matrix multiplication and special matrices like the identity matrix I want to tell you about  In the next and final video on our linear algebra review  I'm going to quickly tell you about a couple of special matrix operations and after that everything you need to know about linear algebra for this class 
3Uxw7F75_-8,Inverse and Transpose  In this video  I want to tell you about a couple of special matrix operations  called the matrix inverse and the matrix transpose operation  Let's start by talking about matrix inverse  and as usual we'll start by thinking about how it relates to real numbers  In the last video  I said that the number one plays the role of the identity in the space of real numbers because one times anything is equal to itself  It turns out that real numbers have this property that very number have an  that each number has an inverse  for example  given the number three  there exists some number  which happens to be three inverse so that that number times gives you back the identity element one  And so to me  inverse of course this is just one third  And given some other number  maybe twelve there is some number which is the inverse of twelve written as twelve to the minus one  or really this is just one twelve  So that when you multiply these two things together  the product is equal to the identity element one again  Now it turns out that in the space of real numbers  not everything has an inverse  For example the number zero does not have an inverse  right? Because zero's a zero inverse  one over zero that's undefined  Like this one over zero is not well defined  And what we want to do  in the rest of this slide  is figure out what does it mean to compute the inverse of a matrix  Here's the idea  If A is a n by n matrix  and it has an inverse  I will say a bit more about that later  then the inverse is going to be written A to the minus one and A times this inverse  A to the minus one  is going to equal to A inverse times A  is going to give us back the identity matrix  Okay? Only matrices that are m by m for some the idea of M having inverse  So  a matrix is M by M  this is also called a square matrix and it's called square because the number of rows is equal to the number of columns  Right and it turns out only square matrices have inverses  so A is a square matrix  is m by m  on inverse this equation over here  Let's look at a concrete example  so let's say I have a matrix  three  four  two  sixteen  So this is a two by two matrix  so it's a square matrix and so this may just could have an and it turns out that I happen to know the inverse of this matrix is zero point four  minus zero point one  minus zero point zero five  zero zero seven five  And if I take this matrix and multiply these together it turns out what I get is the two by two identity matrix  I  this is I two by two  Okay? And so on this slide  you know this matrix is the matrix A  and this matrix is the matrix A-inverse  And it turns out if that you are computing A times A-inverse  it turns out if you compute A-inverse times A you also get back the identity matrix  So how did I find this inverse or how did I come up with this inverse over here? It turns out that sometimes you can compute inverses by hand but almost no one does that these days  And it turns out there is very good numerical software for taking a matrix and computing its inverse  So again  this is one of those things where there are lots of open source libraries that you can link to from any of the popular programming languages to compute inverses of matrices  Let me show you a quick example  How I actually computed this inverse  and what I did was I used software called Optive  So let me bring that up  We will see a lot about Optive later  Let me just quickly show you an example  Set my matrix A to be equal to that matrix on the left  type three four two sixteen  so that's my matrix A right  This is matrix 34  216 that I have down here on the left  And  the software lets me compute the inverse of A very easily  It's like P over A equals this  And so  this is right  this matrix here on my four minus  on my one  and so on  This given the numerical solution to what is the inverse of A  So let me just write  inverse of A equals P inverse of A over that I can now just verify that A times A inverse the identity is  type A times the inverse of A and the result of that is this matrix and this is one one on the diagonal and essentially ten to the minus seventeen  ten to the minus sixteen  so Up to numerical precision  up to a little bit of round off error that my computer had in finding optimal matrices and these numbers off the diagonals are essentially zero so A times the inverse is essentially the identity matrix  Can also verify the inverse of A times A is also equal to the identity  ones on the diagonals and values that are essentially zero except for a little bit of round dot error on the off diagonals  If a definition that the inverse of a matrix is  I had this caveat first it must always be a square matrix  it had this caveat  that if A has an inverse  exactly what matrices have an inverse is beyond the scope of this linear algebra for review that one intuition you might take away that just as the number zero doesn't have an inverse  it turns out that if A is say the matrix of all zeros  then this matrix A also does not have an inverse because there's no matrix there's no A inverse matrix so that this matrix times some other matrix will give you the identity matrix so this matrix of all zeros  and there are a few other matrices with properties similar to this  That also don't have an inverse  But it turns out that in this review I don't want to go too deeply into what it means matrix have an inverse but it turns out for our machine learning application this shouldn't be an issue or more precisely for the learning algorithms where this may be an to namely whether or not an inverse matrix appears and I will tell when we get to those learning algorithms just what it means for an algorithm to have or not have an inverse and how to fix it in case  Working with matrices that don't have inverses  But the intuition if you want is that you can think of matrices as not have an inverse that is somehow too close to zero in some sense  So  just to wrap up the terminology  matrix that don't have an inverse Sometimes called a singular matrix or degenerate matrix and so this matrix over here is an example zero zero zero matrix  is an example of a matrix that is singular  or a matrix that is degenerate  Finally  the last special matrix operation I want to tell you about is to do matrix transpose  So suppose I have matrix A  if I compute the transpose of A  that's what I get here on the right  This is a transpose which is written and A superscript T  and the way you compute the transpose of a matrix is as follows  To get a transpose I am going to first take the first row of A one to zero  That becomes this first column of this transpose  And then I'm going to take the second row of A  3 5 9  and that becomes the second column  of the matrix A transpose  And another way of thinking about how the computer transposes is as if you're taking this sort of 45 degree axis and you are mirroring or you are flipping the matrix along that 45 degree axis  so here's the more formal definition of a matrix transpose  Let's say A is a m by n matrix  And let's let B equal A transpose and so BA transpose like so  Then B is going to be a n by m matrix with the dimensions reversed so here we have a 2x3 matrix  And so the transpose becomes a 3x2 matrix  and moreover  the BIJ is equal to AJI  So the IJ element of this matrix B is going to be the JI element of that earlier matrix A  So for example  B 1 2 is going to be equal to  look at this matrix  B 1 2 is going to be equal to this element 3 1st row  2nd column  And that equal to this  which is a two one  second row first column  right  which is equal to two and some [It should be 3] of the example B 3 2  right  that's B 3 2 is this element 9  and that's equal to a two three which is this element up here  nine  And so that wraps up the definition of what it means to take the transpose of a matrix and that in fact concludes our linear algebra review  So by now hopefully you know how to add and subtract matrices as well as multiply them and you also know how  what are the definitions of the inverses and transposes of a matrix and these are the main operations used in linear algebra for this course  In case this is the first time you are seeing this material  I know this was a lot of linear algebra material all presented very quickly and it's a lot to absorb but if you there's no need to memorize all the definitions we just went through and if you download the copy of either these slides or of the lecture notes from the course website  and use either the slides or the lecture notes as a reference then you can always refer back to the definitions and to figure out what are these matrix multiplications  transposes and so on definitions  And the lecture notes on the course website also has pointers to additional resources linear algebra which you can use to learn more about linear algebra by yourself  And next with these new tools  We'll be able in the next few videos to develop more powerful forms of linear regression that can view of a lot more data  a lot more features  a lot more training examples and later on after the new regression we'll actually continue using these linear algebra tools to derive more powerful learning algorithims as well
K-_-eGcAJGc,"Multiple Features  in this video we will start to talk about a new version of linear regression that's more powerful  One that works with multiple variables or with multiple features  Here's what I mean  In the original version of linear regression that we developed  we have a single feature x  the size of the house  and we wanted to use that to predict why the price of the house and this was our form of our hypothesis  But now imagine  what if we had not only the size of the house as a feature or as a variable of which to try to predict the price  but that we also knew the number of bedrooms  the number of house and the age of the home and years  It seems like this would give us a lot more information with which to predict the price  To introduce a little bit of notation  we sort of started to talk about this earlier  I'm going to use the variables X subscript 1 X subscript 2 and so on to denote my  in this case  four features and I'm going to continue to use Y to denote the variable  the output variable price that we're trying to predict  Let's introduce a little bit more notation  Now that we have four features I'm going to use lowercase \""n\"" to denote the number of features  So in this example we have n4 because we have  you know  one  two  three  four features  And \""n\"" is different from our earlier notation where we were using \""n\"" to denote the number of examples  So if you have 47 rows \""M\"" is the number of rows on this table or the number of training examples  So I'm also going to use X superscript \""I\"" to denote the input features of the \""I\"" training example  As a concrete example let say X2 is going to be a vector of the features for my second training example  And so X2 here is going to be a vector 1416  3  2  40 since those are my four features that I have to try to predict the price of the second house  So  in this notation  the superscript 2 here  That's an index into my training set  This is not X to the power of 2  Instead  this is  you know  an index that says look at the second row of this table  This refers to my second training example  With this notation X2 is a four dimensional vector  In fact  more generally  this is an in-dimensional feature back there  With this notation  X2 is now a vector and so  I'm going to use also Xi subscript J to denote the value of the J  of feature number J and the training example  So concretely X2 subscript 3  will refer to feature number three in the x factor which is equal to 2 right? That was a 3 over there  just fix my handwriting  So x2 subscript 3 is going to be equal to 2  Now that we have multiple features  let's talk about what the form of our hypothesis should be  Previously this was the form of our hypothesis  where x was our single feature  but now that we have multiple features  we aren't going to use the simple representation any more  Instead  a form of the hypothesis in linear regression is going to be this  can be theta 0 plus theta 1 x1 plus theta 2 x2 plus theta 3 x3 plus theta 4 X4  And if we have N features then rather than summing up over our four features  we would have a sum over our N features  Concretely for a particular setting of our parameters we may have H of X 80 + 0 1 X1 + 0 01x2 + 3x3 - 2x4  This would be one example of a hypothesis and you remember a hypothesis is trying to predict the price of the house in thousands of dollars  just saying that  you know  the base price of a house is maybe 80 000 plus another open 1  so that's an extra  what  hundred dollars per square feet  yeah  plus the price goes up a little bit for each additional floor that the house has  X two is the number of floors  and it goes up further for each additional bedroom the house has  because X three was the number of bedrooms  and the price goes down a little bit with each additional age of the house  With each additional year of the age of the house  Here's the form of a hypothesis rewritten on the slide  And what I'm gonna do is introduce a little bit of notation to simplify this equation  For convenience of notation  let me define x subscript 0 to be equals one  Concretely  this means that for every example i I have a feature vector X superscript I and X superscript I subscript 0 is going to be equal to 1  You can think of this as defining an additional zero feature  So whereas previously I had n features because x1  x2 through xn  I'm now defining an additional sort of zero feature vector that always takes on the value of one  So now my feature vector X becomes this N+1 dimensional vector that is zero index  So this is now a n+1 dimensional feature vector  but I'm gonna index it from 0 and I'm also going to think of my parameters as a vector  So  our parameters here  right that would be our theta zero  theta one  theta two  and so on all the way up to theta n  we're going to gather them up into a parameter vector written theta 0  theta 1  theta 2  and so on  down to theta n  This is another zero index vector  It's of index signed from zero  That is another n plus 1 dimensional vector  So  my hypothesis cannot be written theta 0x0 plus theta 1x1+ up to theta n Xn  And this equation is the same as this on top because  you know  eight zero is equal to one  Underneath and I now take this form of the hypothesis and write this as either transpose x  depending on how familiar you are with inner products of vectors if you write what theta transfers x is what theta transfer and this is theta zero  theta one  up to theta N  So this thing here is theta transpose and this is actually a N plus one by one matrix  [It should be a 1 by (n+1) matrix] It's also called a row vector and you take that and multiply it with the vector X which is X zero  X one  and so on  down to X n  And so  the inner product that is theta transpose X is just equal to this  This gives us a convenient way to write the form of the hypothesis as just the inner product between our parameter vector theta and our theta vector X  And it is this little bit of notation  this little excerpt of the notation convention that let us write this in this compact form  So that's the form of a hypthesis when we have multiple features  And  just to give this another name  this is also called multivariate linear regression  And the term multivariable that's just maybe a fancy term for saying we have multiple features  or multivariables with which to try to predict the value Y "
ZJLWP103fKw,Gradient Descent for Multiple Variables  In the previous video  we talked about the form of the hypothesis for linear regression with multiple features or with multiple variables  In this video  let's talk about how to fit the parameters of that hypothesis  In particular let's talk about how to use gradient descent for linear regression with multiple features  To quickly summarize our notation  this is our formal hypothesis in multivariable linear regression where we've adopted the convention that x0=1  The parameters of this model are theta0 through theta n  but instead of thinking of this as n separate parameters  which is valid  I'm instead going to think of the parameters as theta where theta here is a n+1-dimensional vector  So I'm just going to think of the parameters of this model as itself being a vector  Our cost function is J of theta0 through theta n which is given by this usual sum of square of error term  But again instead of thinking of J as a function of these n+1 numbers  I'm going to more commonly write J as just a function of the parameter vector theta so that theta here is a vector  Here's what gradient descent looks like  We're going to repeatedly update each parameter theta j according to theta j minus alpha times this derivative term  And once again we just write this as J of theta  so theta j is updated as theta j minus the learning rate alpha times the derivative  a partial derivative of the cost function with respect to the parameter theta j  Let's see what this looks like when we implement gradient descent and  in particular  let's go see what that partial derivative term looks like  Here's what we have for gradient descent for the case of when we had N=1 feature  We had two separate update rules for the parameters theta0 and theta1  and hopefully these look familiar to you  And this term here was of course the partial derivative of the cost function with respect to the parameter of theta0  and similarly we had a different update rule for the parameter theta1  There's one little difference which is that when we previously had only one feature  we would call that feature x(i) but now in our new notation we would of course call this x(i)<u>1 to denote our one feature </u> So that was for when we had only one feature  Let's look at the new algorithm for we have more than one feature  where the number of features n may be much larger than one  We get this update rule for gradient descent and  maybe for those of you that know calculus  if you take the definition of the cost function and take the partial derivative of the cost function J with respect to the parameter theta j  you'll find that that partial derivative is exactly that term that I've drawn the blue box around  And if you implement this you will get a working implementation of gradient descent for multivariate linear regression  The last thing I want to do on this slide is give you a sense of why these new and old algorithms are sort of the same thing or why they're both similar algorithms or why they're both gradient descent algorithms  Let's consider a case where we have two features or maybe more than two features  so we have three update rules for the parameters theta0  theta1  theta2 and maybe other values of theta as well  If you look at the update rule for theta0  what you find is that this update rule here is the same as the update rule that we had previously for the case of n = 1  And the reason that they are equivalent is  of course  because in our notational convention we had this x(i)<u>0 = 1 convention  which is</u> why these two term that I've drawn the magenta boxes around are equivalent  Similarly  if you look the update rule for theta1  you find that this term here is equivalent to the term we previously had  or the equation or the update rule we previously had for theta1  where of course we're just using this new notation x(i)<u>1 to denote</u> our first feature  and now that we have more than one feature we can have similar update rules for the other parameters like theta2 and so on  There's a lot going on on this slide so I definitely encourage you if you need to to pause the video and look at all the math on this slide slowly to make sure you understand everything that's going on here  But if you implement the algorithm written up here then you have a working implementation of linear regression with multiple features 
vlFdVYAXIxg,Gradient Descent in Practice I - Feature Scaling  In this video and in the video after this one  I wanna tell you about some of the practical tricks for making gradient descent work well  In this video  I want to tell you about an idea called feature skill  Here's the idea  If you have a problem where you have multiple features  if you make sure that the features are on a similar scale  by which I mean make sure that the different features take on similar ranges of values  then gradient descents can converge more quickly  Concretely let's say you have a problem with two features where X1 is the size of house and takes on values between say zero to two thousand and two is the number of bedrooms  and maybe that takes on values between one and five  If you plot the contours of the cos function J of theta  then the contours may look like this  where  let's see  J of theta is a function of parameters theta zero  theta one and theta two  I'm going to ignore theta zero  so let's about theta 0 and pretend as a function of only theta 1 and theta 2  but if x1 can take on them  you know  much larger range of values and x2 It turns out that the contours of the cause function J of theta can take on this very very skewed elliptical shape  except that with the so 2000 to 5 ratio  it can be even more secure  So  this is very  very tall and skinny ellipses  or these very tall skinny ovals  can form the contours of the cause function J of theta  And if you run gradient descents on this cos-function  your gradients may end up taking a long time and can oscillate back and forth and take a long time before it can finally find its way to the global minimum  In fact  you can imagine if these contours are exaggerated even more when you draw incredibly skinny  tall skinny contours  and it can be even more extreme than  then  gradient descent just have a much harder time taking it's way  meandering around  it can take a long time to find this way to the global minimum  In these settings  a useful thing to do is to scale the features  Concretely if you instead define the feature X one to be the size of the house divided by two thousand  and define X two to be maybe the number of bedrooms divided by five  then the count well as of the cost function J can become much more  much less skewed so the contours may look more like circles  And if you run gradient descent on a cost function like this  then gradient descent  you can show mathematically  you can find a much more direct path to the global minimum rather than taking a much more convoluted path where you're sort of trying to follow a much more complicated trajectory to get to the global minimum  So  by scaling the features so that there are  the consumer ranges of values  In this example  we end up with both features  X one and X two  between zero and one  You can wind up with an implementation of gradient descent  They can convert much faster  More generally  when we're performing feature scaling  what we often want to do is get every feature into approximately a -1 to +1 range and concretely  your feature x0 is always equal to 1  So  that's already in that range  but you may end up dividing other features by different numbers to get them to this range  The numbers -1 and +1 aren't too important  So  if you have a feature  x1 that winds up being between zero and three  that's not a problem  If you end up having a different feature that winds being between -2 and + 0 5  again  this is close enough to minus one and plus one that  you know  that's fine  and that's fine  It's only if you have a different feature  say X 3 that is between  that ranges from -100 tp +100   then  this is a very different values than minus 1 and plus 1  So  this might be a less well-skilled feature and similarly  if your features take on a very  very small range of values so if X 4 takes on values between minus 0 0001 and positive 0 0001  then again this takes on a much smaller range of values than the minus one to plus one range  And again I would consider this feature poorly scaled  So you want the range of values  you know  can be bigger than plus or smaller than plus one  but just not much bigger  like plus 100 here  or too much smaller like 0 00 one over there  Different people have different rules of thumb  But the one that I use is that if a feature takes on the range of values from say minus three the plus 3 how you should think that should be just fine  but maybe it takes on much larger values than plus 3 or minus 3 unless not to worry and if it takes on values from say minus one-third to one-third  You know  I think that's fine too or 0 to one-third or minus one-third to 0  I guess that's typical range of value sector 0 okay  But it will take on a much tinier range of values like x4 here than gain on mine not to worry  So  the take-home message is don't worry if your features are not exactly on the same scale or exactly in the same range of values  But so long as they're all close enough to this gradient descent it should work okay  In addition to dividing by so that the maximum value when performing feature scaling sometimes people will also do what's called mean normalization  And what I mean by that is that you want to take a feature Xi and replace it with Xi minus new i to make your features have approximately 0 mean  And obviously we want to apply this to the future x zero  because the future x zero is always equal to one  so it cannot have an average value of zero  But it concretely for other features if the range of sizes of the house takes on values between 0 to 2000 and if you know  the average size of a house is equal to 1000 then you might use this formula  Size  set the feature X1 to the size minus the average value divided by 2000 and similarly  on average if your houses have one to five bedrooms and if on average a house has two bedrooms then you might use this formula to mean normalize your second feature x2  In both of these cases  you therefore wind up with features x1 and x2  They can take on values roughly between minus  5 and positive  5  Exactly not true - X2 can actually be slightly larger than  5 but  close enough  And the more general rule is that you might take a feature X1 and replace it with X1 minus mu1 over S1 where to define these terms mu1 is the average value of x1 in the training sets and S1 is the range of values of that feature and by range  I mean let's say the maximum value minus the minimum value or for those of you that understand the deviation of the variable is setting S1 to be the standard deviation of the variable would be fine  too  But taking  you know  this max minus min would be fine  And similarly for the second feature  x2  you replace x2 with this sort of subtract the mean of the feature and divide it by the range of values meaning the max minus min  And this sort of formula will get your features  you know  maybe not exactly  but maybe roughly into these sorts of ranges  and by the way  for those of you that are being super careful technically if we're taking the range as max minus min this five here will actually become a four  So if max is 5 minus 1 then the range of their own values is actually equal to 4  but all of these are approximate and any value that gets the features into anything close to these sorts of ranges will do fine  And the feature scaling doesn't have to be too exact  in order to get gradient descent to run quite a lot faster  So  now you know about feature scaling and if you apply this simple trick  it and make gradient descent run much faster and converge in a lot fewer other iterations  That was feature scaling  In the next video  I'll tell you about another trick to make gradient descent work well in practice 
yhVLhoN5_vE,Gradient Descent in Practice II - Learning Rate  In this video  I want to give you more practical tips for getting gradient descent to work  The ideas in this video will center around the learning rate alpha  Concretely  here's the gradient descent update rule  And what I want to do in this video is tell you about what I think of as debugging  and some tips for making sure that gradient descent is working correctly  And second  I wanna tell you how to choose the learning rate alpha or at least how I go about choosing it  Here's something that I often do to make sure that gradient descent is working correctly  The job of gradient descent is to find the value of theta for you that hopefully minimizes the cost function J(theta)  What I often do is therefore plot the cost function J(theta) as gradient descent runs  So the x axis here is a number of iterations of gradient descent and as gradient descent runs you hopefully get a plot that maybe looks like this  Notice that the x axis is number of iterations  Previously we where looking at plots of J(theta) where the x axis  where the horizontal axis  was the parameter vector theta but this is not what this is  Concretely  what this point is  is I'm going to run gradient descent for 100 iterations  And whatever value I get for theta after 100 iterations  I'm going to get some value of theta after 100 iterations  And I'm going to evaluate the cost function J(theta)  For the value of theta I get after 100 iterations  and this vertical height is the value of J(theta)  For the value of theta I got after 100 iterations of gradient descent  And this point here that corresponds to the value of J(theta) for the theta that I get after I've run gradient descent for 200 iterations  So what this plot is showing is  is it's showing the value of your cost function after each iteration of gradient decent  And if gradient is working properly then J(theta) should decrease after every iteration  And one useful thing that this sort of plot can tell you also is that if you look at the specific figure that I've drawn  it looks like by the time you've gotten out to maybe 300 iterations  between 300 and 400 iterations  in this segment it looks like J(theta) hasn't gone down much more  So by the time you get to 400 iterations  it looks like this curve has flattened out here  And so way out here 400 iterations  it looks like gradient descent has more or less converged because your cost function isn't going down much more  So looking at this figure can also help you judge whether or not gradient descent has converged  By the way  the number of iterations the gradient descent takes to converge for a physical application can vary a lot  so maybe for one application  gradient descent may converge after just thirty iterations  For a different application  gradient descent may take 3 000 iterations  for another learning algorithm  it may take 3 million iterations  It turns out to be very difficult to tell in advance how many iterations gradient descent needs to converge  And is usually by plotting this sort of plot  plotting the cost function as we increase in number in iterations  is usually by looking at these plots  But I try to tell if gradient descent has converged  It's also possible to come up with automatic convergence test  namely to have a algorithm try to tell you if gradient descent has converged  And here's maybe a pretty typical example of an automatic convergence test  And such a test may declare convergence if your cost function J(theta) decreases by less than some small value epsilon  some small value 10 to the minus 3 in one iteration  But I find that usually choosing what this threshold is is pretty difficult  And so in order to check your gradient descent's converge I actually tend to look at plots like these  like this figure on the left  rather than rely on an automatic convergence test  Looking at this sort of figure can also tell you  or give you an advance warning  if maybe gradient descent is not working correctly  Concretely  if you plot J(theta) as a function of the number of iterations  Then if you see a figure like this where J(theta) is actually increasing  then that gives you a clear sign that gradient descent is not working  And a theta like this usually means that you should be using learning rate alpha  If J(theta) is actually increasing  the most common cause for that is if you're trying to minimize a function  that maybe looks like this  But if your learning rate is too big then if you start off there  gradient descent may overshoot the minimum and send you there  And if the learning rate is too big  you may overshoot again and it sends you there  and so on  So that  what you really wanted was for it to start here and for it to slowly go downhill  right? But if the learning rate is too big  then gradient descent can instead keep on overshooting the minimum  So that you actually end up getting worse and worse instead of getting to higher values of the cost function J(theta)  So you end up with a plot like this and if you see a plot like this  the fix is usually just to use a smaller value of alpha  Oh  and also  of course  make sure your code doesn't have a bug of it  But usually too large a value of alpha could be a common problem  Similarly sometimes you may also see J(theta) do something like this  it may go down for a while then go up then go down for a while then go up go down for a while go up and so on  And a fix for something like this is also to use a smaller value of alpha  I'm not going to prove it here  but under other assumptions about the cost function J  that does hold true for linear regression  mathematicians have shown that if your learning rate alpha is small enough  then J(theta) should decrease on every iteration  So if this doesn't happen probably means the alpha's too big  you should set it smaller  But of course  you also don't want your learning rate to be too small because if you do that then the gradient descent can be slow to converge  And if alpha were too small  you might end up starting out here  say  and end up taking just minuscule baby steps  And just taking a lot of iterations before you finally get to the minimum  and so if alpha is too small  gradient descent can make very slow progress and be slow to converge  To summarize  if the learning rate is too small  you can have a slow convergence problem  and if the learning rate is too large  J(theta) may not decrease on every iteration and it may not even converge  In some cases if the learning rate is too large  slow convergence is also possible  But the more common problem you see is just that J(theta) may not decrease on every iteration  And in order to debug all of these things  often plotting that J(theta) as a function of the number of iterations can help you figure out what's going on  Concretely  what I actually do when I run gradient descent is I would try a range of values  So just try running gradient descent with a range of values for alpha  like 0 001 and 0 01  So these are factor of ten differences  And for these different values of alpha are just plot J(theta) as a function of number of iterations  and then pick the value of alpha that seems to be causing J(theta) to decrease rapidly  In fact  what I do actually isn't these steps of ten  So this is a scale factor of ten of each step up  What I actually do is try this range of values  And so on  where this is 0 001  I'll then increase the learning rate threefold to get 0 003  And then this step up  this is another roughly threefold increase from 0 003 to 0 01  And so these are  roughly  trying out gradient descents with each value I try being about 3x bigger than the previous value  So what I'll do is try a range of values until I've found one value that's too small and made sure that I've found one value that's too large  And then I'll sort of try to pick the largest possible value  or just something slightly smaller than the largest reasonable value that I found  And when I do that usually it just gives me a good learning rate for my problem  And if you do this too  maybe you'll be able to choose a good learning rate for your implementation of gradient descent 
dAOPcV8jMTo,Features and Polynomial Regression  You now know about linear regression with multiple variables  In this video  I wanna tell you a bit about the choice of features that you have and how you can get different learning algorithm  sometimes very powerful ones by choosing appropriate features  And in particular I also want to tell you about polynomial regression allows you to use the machinery of linear regression to fit very complicated  even very non-linear functions  Let's take the example of predicting the price of the house  Suppose you have two features  the frontage of house and the depth of the house  So  here's the picture of the house we're trying to sell  So  the frontage is defined as this distance is basically the width or the length of how wide your lot is if this that you own  and the depth of the house is how deep your property is  so there's a frontage  there's a depth  called frontage and depth  You might build a linear regression model like this where frontage is your first feature x1 and and depth is your second feature x2  but when you're applying linear regression  you don't necessarily have to use just the features x1 and x2 that you're given  What you can do is actually create new features by yourself  So  if I want to predict the price of a house  what I might do instead is decide that what really determines the size of the house is the area or the land area that I own  So  I might create a new feature  I'm just gonna call this feature x which is frontage  times depth  This is a multiplication symbol  It's a frontage x depth because this is the land area that I own and I might then select my hypothesis as that using just one feature which is my land area  right? Because the area of a rectangle is you know  the product of the length of the size So  depending on what insight you might have into a particular problem  rather than just taking the features [xx] that we happen to have started off with  sometimes by defining new features you might actually get a better model  Closely related to the idea of choosing your features is this idea called polynomial regression  Let's say you have a housing price data set that looks like this  Then there are a few different models you might fit to this  One thing you could do is fit a quadratic model like this  It doesn't look like a straight line fits this data very well  So maybe you want to fit a quadratic model like this where you think the size  where you think the price is a quadratic function and maybe that'll give you  you know  a fit to the data that looks like that  But then you may decide that your quadratic model doesn't make sense because of a quadratic function  eventually this function comes back down and well  we don't think housing prices should go down when the size goes up too high  So then maybe we might choose a different polynomial model and choose to use instead a cubic function  and where we have now a third-order term and we fit that  maybe we get this sort of model  and maybe the green line is a somewhat better fit to the data cause it doesn't eventually come back down  So how do we actually fit a model like this to our data? Using the machinery of multivariant linear regression  we can do this with a pretty simple modification to our algorithm  The form of the hypothesis we  we know how the fit looks like this  where we say H of x is theta zero plus theta one x one plus x two theta X3  And if we want to fit this cubic model that I have boxed in green  what we're saying is that to predict the price of a house  it's theta 0 plus theta 1 times the size of the house plus theta 2 times the square size of the house  So this term is equal to that term  And then plus theta 3 times the cube of the size of the house raises that third term  In order to map these two definitions to each other  well  the natural way to do that is to set the first feature x one to be the size of the house  and set the second feature x two to be the square of the size of the house  and set the third feature x three to be the cube of the size of the house  And  just by choosing my three features this way and applying the machinery of linear regression  I can fit this model and end up with a cubic fit to my data  I just want to point out one more thing  which is that if you choose your features like this  then feature scaling becomes increasingly important  So if the size of the house ranges from one to a thousand  so  you know  from one to a thousand square feet  say  then the size squared of the house will range from one to one million  the square of a thousand  and your third feature x cubed  excuse me you  your third feature x three  which is the size cubed of the house  will range from one two ten to the nine  and so these three features take on very different ranges of values  and it's important to apply feature scaling if you're using gradient descent to get them into comparable ranges of values  Finally  here's one last example of how you really have broad choices in the features you use  Earlier we talked about how a quadratic model like this might not be ideal because  you know  maybe a quadratic model fits the data okay  but the quadratic function goes back down and we really don't want  right  housing prices that go down  to predict that  as the size of housing freezes  But rather than going to a cubic model there  you have  maybe  other choices of features and there are many possible choices  But just to give you another example of a reasonable choice  another reasonable choice might be to say that the price of a house is theta zero plus theta one times the size  and then plus theta two times the square root of the size  right? So the square root function is this sort of function  and maybe there will be some value of theta one  theta two  theta three  that will let you take this model and  for the curve that looks like that  and  you know  goes up  but sort of flattens out a bit and doesn't ever come back down  And  so  by having insight into  in this case  the shape of a square root function  and  into the shape of the data  by choosing different features  you can sometimes get better models  In this video  we talked about polynomial regression  That is  how to fit a polynomial  like a quadratic function  or a cubic function  to your data  Was also throw out this idea  that you have a choice in what features to use  such as that instead of using the frontish and the depth of the house  maybe  you can multiply them together to get a feature that captures the land area of a house  In case this seems a little bit bewildering  that with all these different feature choices  so how do I decide what features to use  Later in this class  we'll talk about some algorithms were automatically choosing what features are used  so you can have an algorithm look at the data and automatically choose for you whether you want to fit a quadratic function  or a cubic function  or something else  But  until we get to those algorithms now I just want you to be aware that you have a choice in what features to use  and by designing different features you can fit more complex functions your data then just fitting a straight line to the data and in particular you can put polynomial functions as well and sometimes by appropriate insight into the feature simply get a much better model for your data 
bgHodJrPnAg,"Normal Equation  In this video  we'll talk about the normal equation  which for some linear regression problems  will give us a much better way to solve for the optimal value of the parameters theta  Concretely  so far the algorithm that we've been using for linear regression is gradient descent where in order to minimize the cost function J of Theta  we would take this iterative algorithm that takes many steps  multiple iterations of gradient descent to converge to the global minimum  In contrast  the normal equation would give us a method to solve for theta analytically  so that rather than needing to run this iterative algorithm  we can instead just solve for the optimal value for theta all at one go  so that in basically one step you get to the optimal value right there  It turns out the normal equation that has some advantages and some disadvantages  but before we get to that and talk about when you should use it  let's get some intuition about what this method does  For this week's planetary example  let's imagine  let's take a very simplified cost function J of Theta  that's just the function of a real number Theta  So  for now  imagine that Theta is just a scalar value or that Theta is just a row value  It's just a number  rather than a vector  Imagine that we have a cost function J that's a quadratic function of this real value parameter Theta  so J of Theta looks like that  Well  how do you minimize a quadratic function? For those of you that know a little bit of calculus  you may know that the way to minimize a function is to take derivatives and to set derivatives equal to zero  So  you take the derivative of J with respect to the parameter of Theta  You get some formula which I am not going to derive  you set that derivative equal to zero  and this allows you to solve for the value of Theda that minimizes J of Theta  That was a simpler case of when data was just real number  In the problem that we are interested in  Theta is no longer just a real number  but  instead  is this n+1-dimensional parameter vector  and  a cost function J is a function of this vector value or Theta 0 through Theta m  And  a cost function looks like this  some square cost function on the right  How do we minimize this cost function J? Calculus actually tells us that  if you  that one way to do so  is to take the partial derivative of J  with respect to every parameter of Theta J in turn  and then  to set all of these to 0  If you do that  and you solve for the values of Theta 0  Theta 1  up to Theta N  then  this would give you that values of Theta to minimize the cost function J  Where  if you actually work through the calculus and work through the solution to the parameters Theta 0 through Theta N  the derivation ends up being somewhat involved  And  what I am going to do in this video  is actually to not go through the derivation  which is kind of long and kind of involved  but what I want to do is just tell you what you need to know in order to implement this process so you can solve for the values of the thetas that corresponds to where the partial derivatives is equal to zero  Or alternatively  or equivalently  the values of Theta is that minimize the cost function J of Theta  I realize that some of the comments I made that made more sense only to those of you that are normally familiar with calculus  So  but if you don't know  if you're less familiar with calculus  don't worry about it  I'm just going to tell you what you need to know in order to implement this algorithm and get it to work  For the example that I want to use as a running example let's say that I have m = 4 training examples  In order to implement this normal equation at big  what I'm going to do is the following  I'm going to take my data set  so here are my four training examples  In this case let's assume that  you know  these four examples is all the data I have  What I am going to do is take my data set and add an extra column that corresponds to my extra feature  x0  that is always takes on this value of 1  What I'm going to do is I'm then going to construct a matrix called X that's a matrix are basically contains all of the features from my training data  so completely here is my here are all my features and we're going to take all those numbers and put them into this matrix \""X\""  okay? So just  you know  copy the data over one column at a time and then I am going to do something similar for y's  I am going to take the values that I'm trying to predict and construct now a vector  like so and call that a vector y  So X is going to be a m by (n+1) - dimensional matrix  and Y is going to be a m-dimensional vector where m is the number of training examples and n is  n is a number of features  n+1  because of this extra feature X0 that I had  Finally if you take your matrix X and you take your vector Y  and if you just compute this  and set theta to be equal to X transpose X inverse times X transpose Y  this would give you the value of theta that minimizes your cost function  There was a lot that happened on the slides and I work through it using one specific example of one dataset  Let me just write this out in a slightly more general form and then let me just  and later on in this video let me explain this equation a little bit more  It is not yet entirely clear how to do this  In a general case  let us say we have M training examples so X1  Y1 up to Xn  Yn and n features  So  each of the training example x(i) may looks like a vector like this  that is a n+1 dimensional feature vector  The way I'm going to construct the matrix \""X\""  this is also called the design matrix is as follows  Each training example gives me a feature vector like this  say  sort of n+1 dimensional vector  The way I am going to construct my design matrix x is only construct the matrix like this  and what I'm going to do is take the first training example  so that's a vector  take its transpose so it ends up being this  you know  long flat thing and make x1 transpose the first row of my design matrix  Then I am going to take my second training example  x2  take the transpose of that and put that as the second row of x and so on  down until my last training example  Take the transpose of that  and that's my last row of my matrix X  And  so  that makes my matrix X  an M by N +1 dimensional matrix  As a concrete example  let's say I have only one feature  really  only one feature other than X zero  which is always equal to 1  So if my feature vectors X-i are equal to this 1  which is X-0  then some real feature  like maybe the size of the house  then my design matrix  X  would be equal to this  For the first row  I'm going to basically take this and take its transpose  So  I'm going to end up with 1  and then X-1-1  For the second row  we're going to end up with 1 and then X-1-2 and so on down to 1  and then X-1-M  And thus  this will be a m by 2-dimensional matrix  So  that's how to construct the matrix X  And  the vector Y--sometimes I might write an arrow on top to denote that it is a vector  but very often I'll just write this as Y  either way  The vector Y is obtained by taking all all the labels  all the correct prices of houses in my training set  and just stacking them up into an M-dimensional vector  and that's Y  Finally  having constructed the matrix X and the vector Y  we then just compute theta as X'(1/X) x X'Y  I just want to make I just want to make sure that this equation makes sense to you and that you know how to implement it  So  you know  concretely  what is this X'(1/X)? Well  X'(1/X) is the inverse of the matrix X'X  Concretely  if you were to say set A to be equal to X' x X  so X' is a matrix  X' x X gives you another matrix  and we call that matrix A  Then  you know  X'(1/X) is just you take this matrix A and you invert it  right! This gives  let's say 1/A  And so that's how you compute this thing  You compute X'X and then you compute its inverse  We haven't yet talked about Octave  We'll do so in the later set of videos  but in the Octave programming language or a similar view  and also the matlab programming language is very similar  The command to compute this quantity  X transpose X inverse times X transpose Y  is as follows  In Octave X prime is the notation that you use to denote X transpose  And so  this expression that's boxed in red  that's computing X transpose times X  pinv is a function for computing the inverse of a matrix  so this computes X transpose X inverse  and then you multiply that by X transpose  and you multiply that by Y  So you end computing that formula which I didn't prove  but it is possible to show mathematically even though I'm not going to do so here  that this formula gives you the optimal value of theta in the sense that if you set theta equal to this  that's the value of theta that minimizes the cost function J of theta for the new regression  One last detail in the earlier video  I talked about the feature skill and the idea of getting features to be on similar ranges of Scales of similar ranges of values of each other  If you are using this normal equation method then feature scaling isn't actually necessary and is actually okay if  say  some feature X one is between zero and one  and some feature X two is between ranges from zero to one thousand and some feature x three ranges from zero to ten to the minus five and if you are using the normal equation method this is okay and there is no need to do features scaling  although of course if you are using gradient descent  then  features scaling is still important  Finally  where should you use the gradient descent and when should you use the normal equation method  Here are some of the their advantages and disadvantages  Let's say you have m training examples and n features  One disadvantage of gradient descent is that  you need to choose the learning rate Alpha  And  often  this means running it few times with different learning rate alphas and then seeing what works best  And so that is sort of extra work and extra hassle  Another disadvantage with gradient descent is it needs many more iterations  So  depending on the details  that could make it slower  although there's more to the story as we'll see in a second  As for the normal equation  you don't need to choose any learning rate alpha  So that  you know  makes it really convenient  makes it simple to implement  You just run it and it usually just works  And you don't need to iterate  so  you don't need to plot J of Theta or check the convergence or take all those extra steps  So far  the balance seems to favor normal the normal equation  Here are some disadvantages of the normal equation  and some advantages of gradient descent  Gradient descent works pretty well  even when you have a very large number of features  So  even if you have millions of features you can run gradient descent and it will be reasonably efficient  It will do something reasonable  In contrast to normal equation  In  in order to solve for the parameters data  we need to solve for this term  We need to compute this term  X transpose  X inverse  This matrix X transpose X  That's an n by n matrix  if you have n features  Because  if you look at the dimensions of X transpose the dimension of X  you multiply  figure out what the dimension of the product is  the matrix X transpose X is an n by n matrix where n is the number of features  and for almost computed implementations the cost of inverting the matrix  rose roughly as the cube of the dimension of the matrix  So  computing this inverse costs  roughly order  and cube time  Sometimes  it's slightly faster than N cube but  it's  you know  close enough for our purposes  So if n the number of features is very large  then computing this quantity can be slow and the normal equation method can actually be much slower  So if n is large then I might usually use gradient descent because we don't want to pay this all in q time  But  if n is relatively small  then the normal equation might give you a better way to solve the parameters  What does small and large mean? Well  if n is on the order of a hundred  then inverting a hundred-by-hundred matrix is no problem by modern computing standards  If n is a thousand  I would still use the normal equation method  Inverting a thousand-by-thousand matrix is actually really fast on a modern computer  If n is ten thousand  then I might start to wonder  Inverting a ten-thousand- by-ten-thousand matrix starts to get kind of slow  and I might then start to maybe lean in the direction of gradient descent  but maybe not quite  n equals ten thousand  you can sort of convert a ten-thousand-by-ten-thousand matrix  But if it gets much bigger than that  then  I would probably use gradient descent  So  if n equals ten to the sixth with a million features  then inverting a million-by-million matrix is going to be very expensive  and I would definitely favor gradient descent if you have that many features  So exactly how large set of features has to be before you convert a gradient descent  it's hard to give a strict number  But  for me  it is usually around ten thousand that I might start to consider switching over to gradient descents or maybe  some other algorithms that we'll talk about later in this class  To summarize  so long as the number of features is not too large  the normal equation gives us a great alternative method to solve for the parameter theta  Concretely  so long as the number of features is less than 1000  you know  I would use  I would usually is used in normal equation method rather than  gradient descent  To preview some ideas that we'll talk about later in this course  as we get to the more complex learning algorithm  for example  when we talk about classification algorithm  like a logistic regression algorithm  We'll see that those algorithm actually    The normal equation method actually do not work for those more sophisticated learning algorithms  and  we will have to resort to gradient descent for those algorithms  So  gradient descent is a very useful algorithm to know  The linear regression will have a large number of features and for some of the other algorithms that we'll see in this course  because  for them  the normal equation method just doesn't apply and doesn't work  But for this specific model of linear regression  the normal equation can give you a alternative that can be much faster  than gradient descent  So  depending on the detail of your algortithm  depending of the detail of the problems and how many features that you have  both of these algorithms are well worth knowing about "
YGnJyipLKrM,Normal Equation Noninvertibility  In this video I want to talk about the Normal equation and non-invertibility  This is a somewhat more advanced concept  but it's something that I've often been asked about  And so I want to talk it here and address it here  But this is a somewhat more advanced concept  so feel free to consider this optional material  And there's a phenomenon that you may run into that may be somewhat useful to understand  but even if you don't understand the normal equation and linear progression  you should really get that to work okay  Here's the issue  For those of you there are  maybe some are more familiar with linear algebra  what some students have asked me is  when computing this Theta equals X transpose X inverse X transpose Y  What if the matrix X transpose X is non-invertible? So for those of you that know a bit more linear algebra you may know that only some matrices are invertible and some matrices do not have an inverse we call those non-invertible matrices  Singular or degenerate matrices  The issue or the problem of x transpose x being non invertible should happen pretty rarely  And in Octave if you implement this to compute theta  it turns out that this will actually do the right thing  I'm getting a little technical now  and I don't want to go into the details  but Octave hast two functions for inverting matrices  One is called pinv  and the other is called inv  And the differences between these two are somewhat technical  One's called the pseudo-inverse  one's called the inverse  But you can show mathematically that so long as you use the pinv function then this will actually compute the value of data that you want even if X transpose X is non-invertible  The specific details between inv  What is the difference between pinv? What is inv? That's somewhat advanced numerical computing concepts  I don't really want to get into  But I thought in this optional video  I'll try to give you little bit of intuition about what it means for X transpose X to be non-invertible  For those of you that know a bit more linear Algebra might be interested  I'm not gonna prove this mathematically but if X transpose X is non-invertible  there usually two most common causes for this  The first cause is if somehow in your learning problem you have redundant features  Concretely  if you're trying to predict housing prices and if x1 is the size of the house in feet  in square feet and x2 is the size of the house in square meters  then you know 1 meter is equal to 3 28 feet Rounded to two decimals  And so your two features will always satisfy the constraint x1 equals 3 28 squared times x2  And you can show for those of you that are somewhat advanced in linear Algebra  but if you're explaining the algebra you can actually show that if your two features are related  are a linear equation like this  Then matrix X transpose X would be non-invertable  The second thing that can cause X transpose X to be non-invertable is if you are training  if you are trying to run the learning algorithm with a lot of features  Concretely  if m is less than or equal to n  For example  if you imagine that you have m = 10 training examples that you have n equals 100 features then you're trying to fit a parameter back to theta which is  you know  n plus one dimensional  So this is 101 dimensional  you're trying to fit 101 parameters from just 10 training examples  This turns out to sometimes work but not always be a good idea  Because as we'll see later  you might not have enough data if you only have 10 examples to fit you know  100 or 101 parameters  We'll see later in this course why this might be too little data to fit this many parameters  But commonly what we do then if m is less than n  is to see if we can either delete some features or to use a technique called regularization which is something that we'll talk about later in this class as well  that will kind of let you fit a lot of parameters  use a lot features  even if you have a relatively small training set  But this regularization will be a later topic in this course  But to summarize if ever you find that x transpose x is singular or alternatively you find it non-invertable  what I would recommend you do is first look at your features and see if you have redundant features like this x1  x2  You're being linearly dependent or being a linear function of each other like so  And if you do have redundant features and if you just delete one of these features  you really don't need both of these features  If you just delete one of these features  that would solve your non-invertibility problem  And so I would first think through my features and check if any are redundant  And if so then keep deleting redundant features until they're no longer redundant  And if your features are not redundant  I would check if I may have too many features  And if that's the case  I would either delete some features if I can bear to use fewer features or else I would consider using regularization  Which is this topic that we'll talk about later  So that's it for the normal equation and what it means for if the matrix X transpose X is non-invertable but this is a problem that you should run that hopefully you run into pretty rarely and if you just implement it in octave using P and using the P n function which is called a pseudo inverse function so you could use a different linear out your alive in Is called a pseudo-inverse but that implementation should just do the right thing  even if X transpose X is non-invertable  which should happen pretty rarely anyways  so this should not be a problem for most implementations of linear regression 
F41ED0tK4TA,Basic Operations  You now know a bunch about machine learning  In this video  I like to teach you a programing language  Octave  in which you'll be able to very quickly implement the the learning algorithms we've seen already  and the learning algorithms we'll see later in this course  In the past  I've tried to teach machine learning using a large variety of different programming languages including C++ Java  Python  NumPy  and also Octave  and what I found was that students were able to learn the most productively learn the most quickly and prototype your algorithms most quickly using a relatively high level language like octave  In fact  what I often see in Silicon Valley is that if even if you need to build  If you want to build a large scale deployment of a learning algorithm  what people will often do is prototype and the language is Octave  Which is a great prototyping language  So you can sort of get your learning algorithms working quickly  And then only if you need to a very large scale deployment of it  Only then spend your time re-implementing the algorithm to C++ Java or some of the language like that  Because all the lessons we've learned is that a time or develop a time  That is your time  The machine learning's time is incredibly valuable  And if you can get your learning algorithms to work more quickly in Octave  Then overall you have a huge time savings by first developing the algorithms in Octave  and then implementing and maybe C++ Java  only after we have the ideas working  The most common prototyping language I see people use for machine learning are  Octave  MATLAB  Python  NumPy  and R  Octave is nice because open sourced  And MATLAB works well too  but it is expensive for to many people  But if you have access to a copy of MATLAB  You can also use MATLAB with this class  If you know Python  NumPy  or if you know R  I do see some people use it  But  what I see is that people usually end up developing somewhat more slowly  and you know  these languages  Because the Python  NumPy syntax is just slightly clunkier than the Octave syntax  And so because of that  and because we are releasing starter code in Octave  I strongly recommend that you not try to do the following exercises in this class in NumPy and R  But that I do recommend that you instead do the programming exercises for this class in octave instead  What I'm going to do in this video is go through a list of commands very  very quickly  and its goal is to quickly show you the range of commands and the range of things you can do in Octave  The course website will have a transcript of everything I do  and so after watching this video you can refer to the transcript posted on the course website when you want find a command  Concretely  what I recommend you do is first watch the tutorial videos  And after watching to the end  then install Octave on your computer  And finally  it goes to the course website  download the transcripts of the things you see in the session  and type in whatever commands seem interesting to you into Octave  so that it's running on your own computer  so you can see it run for yourself  And with that let's get started  Here's my Windows desktop  and I'm going to start up Octave  And I'm now in Octave  And that's my Octave prompt  Let me first show the elementary operations you can do in Octave  So you type in 5 + 6  That gives you the answer of 11  3 - 2  5 x 8  1/2  2^6 is 64  So those are the elementary math operations  You can also do logical operations  So one equals two  This evaluates to false  The percent command here means a comment  So  one equals two  evaluates to false  Which is represents by zero  One not equals to two  This is true  So that returns one  Note that a not equal sign is this tilde equals symbol  And not bang equals  Which is what some other programming languages use  Lets see logical operations one and zero use a double ampersand sign to the logical AND  And that evaluates false  One or zero is the OR operation  And that evaluates to true  And I can XOR one and zero  and that evaluates to one  This thing over on the left  this Octave 324 x equals 11  this is the default Octave prompt  It shows the  what  the version in Octave and so on  If you don't want that prompt  there's a somewhat cryptic command PF quote  greater than  greater than and so on  that you can use to change the prompt  And I guess this quote a string in the middle  Your quote  greater than  greater than  space  That's what I prefer my Octave prompt to look like  So if I hit enter  Oops  excuse me  Like so  PS1 like so  Now my Octave prompt has changed to the greater than  greater than sign Which  you know  looks quite a bit better  Next let's talk about Octave variables  I can take the variable A and assign it to 3  And hit enter  And now A is equal to 3  You want to assign a variable  but you don't want to print out the result  If you put a semicolon  the semicolon suppresses the print output  So to do that  enter  it doesn't print anything  Whereas A equals 3  mix it  print it out  where A equals  3 semicolon doesn't print anything  I can do string assignment  B equals hi Now if I just enter B it prints out the variable B  So B is the string hi C equals 3 greater than colon 1  So  now C evaluates the true  If you want to print out or display a variable  here's how you go about it  Let me set A equals Pi  And if I want to print A I can just type A like so  and it will print it out  For more complex printing there is also the DISP command which stands for Display  Display A just prints out A like so  You can also display strings so  DISP  sprintf  two decimals  percent 0 2  F  comma  A  Like so  And this will print out the string  Two decimals  colon  3 14  This is kind of an old style C syntax  For those of you that have programmed C before  this is essentially the syntax you use to print screen  So the Sprintf generates a string that is less than the 2 decimals  3 1 plus string  This percent 0 2 F means substitute A into here  showing the two digits after the decimal points  And DISP takes the string DISP generates it by the Sprintf command  Sprintf  The Sprintf command  And DISP actually displays the string  And to show you another example  Sprintf six decimals percent 0 6 F comma A  And  this should print Pi with six decimal places  Finally  I was saying  a like so  looks like this  There are useful shortcuts that type type formats long  It causes strings by default  Be displayed to a lot more decimal places  And format short is a command that restores the default of just printing a small number of digits  Okay  that's how you work with variables  Now let's look at vectors and matrices  Let's say I want to assign MAT A to the matrix  Let me show you an example  1  2  semicolon  3  4  semicolon  5  6  This generates a three by two matrix A whose first row is 1  2  Second row 3  4  Third row is 5  6  What the semicolon does is essentially say  go to the next row of the matrix  There are other ways to type this in  Type A 1  2 semicolon 3  4  semicolon  5  6  like so  And that's another equivalent way of assigning A to be the values of this three by two matrix  Similarly you can assign vectors  So V equals 1  2  3  This is actually a row vector  Or this is a 3 by 1 vector  Where that is a fat Y vector  excuse me  not  this is a 1 by 3 matrix  right  Not 3  by 1  If I want to assign this to a column vector  what I would do instead is do v 1 2 3  And this will give me a 3 by 1  There's a 1 by 3 vector  So this will be a column vector  Here's some more useful notation  V equals 1  0 1  2  What this does is it sets V to the bunch of elements that start from 1  And increments and steps of 0 1 until you get up to 2  So if I do this  V is going to be this  you know  row vector  This is what one by eleven matrix really  That's 1  1 1  1 2  1 3 and so on until we get up to two  Now  and I can also set V equals one colon six  and that sets V to be these numbers  1 through 6  okay  Now here are some other ways to generate matrices  Ones 2 3 is a command that generates a matrix that is a two by three matrix that is the matrix of all ones  So if I set that c2 times ones two by three this generates a two by three matrix that is all two's  You can think of this as a shorter way of writing this and c2 2 2's and you can call them 2 2 2  which would also give you the same result  Let's say W equals one's  one by three  so this is going to be a row vector or a row of three one's and similarly you can also say w equals zeroes  one by three  and this generates a matrix  A one by three matrix of all zeros  Just a couple more ways to generate matrices   If I do W equals Rand one by three  this gives me a one by three matrix of all random numbers  If I do Rand three by three  This gives me a three by three matrix of all random numbers drawn from the uniform distribution between zero and one  So every time I do this  I get a different set of random numbers drawn uniformly between zero and one  For those of you that know what a Gaussian random variable is or for those of you that know what a normal random variable is  you can also set W equals Rand N  one by three  And so these are going to be three values drawn from a Gaussian distribution with mean zero and variance or standard deviation equal to one  And you can set more complex things like W equals minus six  plus the square root ten  times  lets say Rand N  one by ten thousand  And I'm going to put a semicolon at the end because I don't really want this printed out  This is going to be a what? Well  it's going to be a vector of  with a hundred thousand  excuse me  ten thousand elements  So  well  actually  you know what? Let's print it out  So this will generate a matrix like this  Right? With 10 000 elements  So that's what W is  And if I now plot a histogram of W with a hist command  I can now  And Octave's print hist command  you know  takes a couple seconds to bring this up  but this is a histogram of my random variable for W  There was minus 6 plus zero ten times this Gaussian random variable  And I can plot a histogram with more buckets  with more bins  with say  50 bins  And this is my histogram of a Gaussian with mean minus 6  Because I have a minus 6 there plus square root 10 times this  So the variance of this Gaussian random variable is 10 on the standard deviation is square root of 10  which is about what? Three point one  Finally  one special command for generator matrix  which is the I command  So I stands for this is maybe a pun on the word identity  It's server set eye 4  This is the 4 by 4 identity matrix  So I equals eye 4  This gives me a 4 by 4 identity matrix  And I equals eye 5  eye 6  That gives me a 6 by 6 identity matrix  i3 is the 3 by 3 identity matrix  Lastly  to wrap up this video  there's one more useful command  Which is the help command  So you can type help i and this brings up the help function for the identity matrix  Hit Q to quit  And you can also type help rand  Brings up documentation for the rand or the random number generation function  Or even help help  which shows you  you know help on the help function  So  those are the basic operations in Octave  And with this you should be able to generate a few matrices  multiply  add things  And use the basic operations in Octave  In the next video  I'd like to start talking about more sophisticated commands and how to use data around and start to process data in Octave 
OC7wDFnCimw,Moving Data Around  In this second tutorial video on Octave  I'd like to start to tell you how to move data around in Octave  So  if you have data for a machine learning problem  how do you load that data in Octave? How do you put it into matrix? How do you manipulate these matrices? How do you save the results? How do you move data around and operate with data? Here's my Octave window as before  picking up from where we left off in the last video  If I type A  that's the matrix so we generate it  right  with this command equals one  two  three  four  five  six  and this is a three by two matrix  The size command in Octave lets you  tells you what is the size of a matrix  So size A returns three  two  It turns out that this size command itself is actually returning a one by two matrix  So you can actually set SZ equals size of A and SZ is now a one by two matrix where the first element of this is three  and the second element of this is two  So  if you just type size of SZ  Does SZ is a one by two matrix whose two elements contain the dimensions of the matrix A  You can also type size A one to give you back the first dimension of A  size of the first dimension of A  So that's the number of rows and size A two to give you back two  which is the number of columns in the matrix A  If you have a vector V  so let's say V equals one  two  three  four  and you type length V  What this does is it gives you the size of the longest dimension  So you can also type length A and because A is a three by two matrix  the longer dimension is of size three  so this should print out three  But usually we apply length only to vectors  So you know  length one  two  three  four  five  rather than apply length to matrices because that's a little more confusing  Now  let's look at how the load data and find data on the file system  When we start an Octave we're usually  we're often in a path that is  you know  the location of where the Octave location is  So the PWD command shows the current directory  or the current path that Octave is in  So right now we're in this maybe somewhat off scale directory  The CD command stands for change directory  so I can go to C /Users/Ang/Desktop  and now I'm in  you know  in my Desktop and if I type ls  ls is  it comes from a Unix or a Linux command  But  ls will list the directories on my desktop and so these are the files that are on my Desktop right now  In fact  on my desktop are two files  Features X and Price Y that's maybe come from a machine learning problem I want to solve  So  here's my desktop  Here's Features X  and Features X is this window  excuse me  is this file with two columns of data  This is actually my housing prices data  So I think  you know  I think I have forty-seven rows in this data set  And so the first house has size two hundred four square feet  has three bedrooms  second house has sixteen hundred square feet  has three bedrooms  and so on  And Price Y is this file that has the prices of the data in my training set  So  Features X and Price Y are just text files with my data  How do I load this data into Octave? Well  I just type the command load Features X dot dat and if I do that  I load the Features X and can load Price Y dot dat  And by the way  there are multiple ways to do this  This command if you put Features X dot dat on that in strings and load it like so  This is a typo there  This is an equivalent command  So you can  this way I'm just putting the file name of the string in the founding in a string and in an Octave use single quotes to represent strings  like so  So that's a string  and we can load the file whose name is given by that string  Now the WHO command now shows me what variables I have in my Octave workspace  So Who shows me whether the variables that Octave has in memory currently  Features X and Price Y are among them  as well as the variables that  you know  we created earlier in this session  So I can type Features X to display features X  And there's my data  And I can type size features X and that's my 47 by two matrix  And some of these size  press Y  that gives me my 47 by one vector  This is a 47 dimensional vector  This is all common vector that has all the prices Y in my training set  Now the who function shows you one of the variables that  in the current workspace  There's also the who S variable that gives you the detailed view  And so this also  with an S at the end this also lists my variables except that it now lists the sizes as well  So A is a three by two matrix and features X as a 47 by 2 matrix  Price Y is a 47 by one matrix  Meaning this is just a vector  And it shows  you know  how many bytes of memory it's taking up  As well as what type of data this is  Double means double position floating point so that just means that these are real values  the floating point numbers  Now if you want to get rid of a variable you can use the clear command  So clear features X and type whose again  You notice that the features X variable has now disappeared  And how do we save data? Let's see  Let's take the variable V and say that it's a price Y 1 colon 10  This sets V to be the first 10 elements of vector Y  So let's type who or whose  Whereas Y was a 47 by 1 vector  V is now 10 by 1  B equals price Y  one column ten that sets it to the just the first ten elements of Y  Let's say I wanna save this to date to disc the command save  hello mat V  This will save the variable V into a file called hello mat  So let's do that  And now a file has appeared on my Desktop  you know  called Hello mat  I happen to have MATLAB installed in this window  which is why  you know  this icon looks like this because Windows is recognized as it's a MATLAB file but don't worry about it if this file looks like it has a different icon on your machine and let's say I clear all my variables  So  if you type clear without anything then this actually deletes all of the variables in your workspace  So there's now nothing left in the workspace  And if I load hello mat  I can now load back my variable v  which is the data that I previously saved into the hello mat file  So  hello mat  what we did just now to save hello mat to view  this save the data in a binary format  a somewhat more compressed binary format  So if v is a lot of data  this  you know  will be somewhat more compressing  Will take off less the space  If you want to save your data in a human readable format then you type save hello text the variable v and then -ascii  So  this will save it as a text or as ascii format of text  And now  once I've done that  I have this file  Hello text has just appeared on my desktop  and if I open this up  we see that this is a text file with my data saved away  So that's how you load and save data  Now let's talk a bit about how to manipulate data  Let's set a equals to that matrix again so is my three by two matrix  So as indexing  So type A 3  2  This indexes into the 3  2 elements of the matrix A  So  this is what  you know  in normally  we will write this as a subscript 3  2 or A subscript  you know  3  2 and so that's the element and third row and second column of A which is the element of six  I can also type A to comma colon to fetch everything in the second row  So  the colon means every element along that row or column  So  a of 2 comma colon is this second row of a  Right  And similarly  if I do a colon comma 2 then this means get everything in the second column of A  So  this gives me 2 4 6  Right this means of A  everything  second column  So  this is my second column A  which is 2 4 6  Now  you can also use somewhat most of the sophisticated index in the operations  So So  we just click each of an example  You do this maybe less often  but let me do this A 1 3 comma colon  This means get all of the elements of A who's first indexes one or three  This means I get everything from the first and third rows of A and from all columns  So  this was the matrix A and so A 1 3 comma colon means get everything from the first row and from the second row and from the third row and the colon means  you know  one both of first and the second columns and so this gives me this 1 2 5 6  Although  you use the source of more subscript index operations maybe somewhat less often  To show you what else we can do  Here's the A matrix and this source A colon  to give me the second column  You can also use this to do assignments  So I can take the second column of A and assign that to 10  11  12  and if I do that I'm now  you know  taking the second column of a and I'm assigning this column vector 10  11  12 to it  So  now a is this matrix that's 1  3  5  And the second column has been replaced by 10  11  12  And here's another operation  Let's set A to be equal to A comma 100  101  102 like so and what this will do is depend another column vector to the right  So  now  oops  I think I made a little mistake  Should have put semicolons there and now A is equals to this  Okay? I hope that makes sense  So this 100  101  102  This is a column vector and what we did was we set A  take A and set it to the original definition  And then we put that column vector to the right and so  we ended up taking the matrix A and--which was these six elements on the left  So we took matrix A and we appended another column vector to the right  which is now why A is a three by three matrix that looks like that  And finally  one neat trick that I sometimes use if you do just a and just a colon like so  This is a somewhat special case syntax  What this means is that put all elements with A into a single column vector and this gives me a 9 by 1 vector  They adjust the other ones are combined together  Just a couple more examples  Let's see  Let's say I set A to be equal to 123456  okay? And let's say I set a B to B equal to 11  12  13  14  15  16  I can create a new matrix C as A B  This just means my Matrix A  Here's my Matrix B and I've set C to be equal to AB  What I'm doing is I'm taking these two matrices and just concatenating onto each other  So the left  matrix A on the left  And I have the matrix B on the right  And that's how I formed this matrix C by putting them together  I can also do C equals A semicolon B  The semi colon notation means that I go put the next thing at the bottom  So  I'll do is a equals semicolon B  It also puts the matrices A and B together except that it now puts them on top of each other  so now I have A on top and B at the bottom and C here is now in 6 by 2 matrix  So  just say the semicolon thing usually means  you know  go to the next line  So  C is comprised by a and then go to the bottom of that and then put b in the bottom and by the way  this A B is the same as A  B and so you know  either of these gives you the same result  So  with that  hopefully you now know how to construct matrices and hopefully starts to show you some of the commands that you use to quickly put together matrices and take matrices and  you know  slam them together to form bigger matrices  and with just a few lines of code  Octave is very convenient in terms of how quickly we can assemble complex matrices and move data around  So that's it for moving data around  In the next video we'll start to talk about how to actually do complex computations on this  on our data  So  hopefully that gives you a sense of how  with just a few commands  you can very quickly move data around in Octave  You know  you load and save vectors and matrices  load and save data  put together matrices to create bigger matrices  index into or select specific elements on the matrices  I know I went through a lot of commands  so I think the best thing for you to do is afterward  to look at the transcript of the things I was typing  You know  look at it  Look at the coursework site and download the transcript of the session from there and look through the transcript and type some of those commands into Octave yourself and start to play with these commands and get it to work  And obviously  you know  there's no point at all to try to memorize all these commands  It's just  but what you should do is  hopefully from this video you have gotten a sense of the sorts of things you can do  So that when later on when you are trying to program a learning algorithms yourself  if you are trying to find a specific command that maybe you think Octave can do because you think you might have seen it here  you should refer to the transcript of the session and look through that in order to find the commands you wanna use  So  that's it for moving data around and in the next video what I'd like to do is start to tell you how to actually do complex computations on our data  and how to compute on the data  and actually start to implement learning algorithms 
vWU1Ct271vQ,Computing on Data  Now that you know how to load and save data in Octave  put your data into matrices and so on  In this video  I'd like to show you how to do computational operations on data  And later on  we'll be using these source of computational operations to implement our learning algorithms  Let's get started  Here's my Octave window  Let me just quickly initialize some variables to use for our example  So set A to be a three by two matrix  and set B to a three by two matrix  and let's set C to a two by two matrix like so  Now let's say I want to multiply two of my matrices  So let's say I want to compute A*C  I just type A*C  so it's a three by two matrix times a two by two matrix  this gives me this three by two matrix  You can also do element wise operations and do A * B and what this will do is it'll take each element of A and multiply it by the corresponding elements B  so that's A  that's B  that's A  * B  So for example  the first element gives 1 times 11  which gives 11  The second element gives 2 time 12 Which gives 24  and so on  So this is element-wise multiplication of two matrices  And in general  the period tends to  is usually used to denote element-wise operations in Octave  So here's a matrix A  and if I do A  ^ 2  this gives me the element wise squaring of A  So 1 squared is 1  2 squared is 4  and so on  Let's set v as a vector  Let's set v as one  two  three as a column vector  You can also do one dot over v to do the element-wise reciprocal of v  so this gives me one over one  one over two  and one over three  and this is where I do the matrices  so one dot over a gives me the element wise inverse of a  And once again  the period here gives us a clue that this an element-wise operation  We can also do things like log(v)  this is a element-wise logarithm of the v E to the V is base E exponentiation of these elements  so this is E  this is E squared EQ  because this was V  and I can also do abs V to take the element-wise absolute value of V  So here  V was our positive  abs  minus one  two minus 3  the element-wise absolute value gives me back these non-negative values  And negative v gives me the minus of v  This is the same as negative one times v  but usually you just write negative v instead of -1*v  And what else can you do? Here's another neat trick  So  let's see  Let's say I want to take v an increment each of its elements by one  Well one way to do it is by constructing a three by one vector that's all ones and adding that to v  So if I do that  this increments v by from 1  2  3 to 2  3  4  The way I did that was  length(v) is 3  so ones(length(v) 1)  this is ones of 3 by 1  so that's ones(3 1) on the right and what I did was v plus ones v by one  which is adding this vector of our ones to v  and so this increments v by one  and another simpler way to do that is to type v plus one  So she has v  and v plus one also means to add one element wise to each of my elements of v  Now  let's talk about more operations  So here's my matrix A  if you want to buy A transposed  the way to do that is to write A prime  that's the apostrophe symbol  it's the left quote  so it's on your keyboard  you have a left quote and a right quote  So this is actually the standard quotation mark  Just type A transpose  this gives me the transpose of my matrix A  And  of course  A transpose  if I transpose that again  then I should get back my matrix A  Some more useful functions  Let's say lower case a is 1 15 2 0 5  so it's 1 by 4 matrix  Let's say val equals max of A this returns the maximum value of A which in this case is 15 and I can do val  ind max(a) and this returns val and ind which are going to be the maximum value of A which is 15  as well as the index  So it was the element number two of A that was 15 so ind is my index into this  Just as a warning  if you do max(A)  where A is a matrix  what this does is this actually does the column wise maximum  But say a little more about this in a second  Still using this example that there for lowercase a  If I do a < 3  this does the element wise operation  Element wise comparison  so the first element of A is less than three so this one  Second element of A is not less than three so this value says zero cuz it's false  The third and fourth elements of A are less than three  so that's just 1 1  So that's the element-wise comparison of all four elements of the variable a < 3  And it returns true or false depending on whether or not there's less than three  Now  if I do find(a < 3)  this will tell me which are the elements of a  the variable a  that are less than 3  and in this case  the first  third and fourth elements are less than 3  For our next example  let me set a to be equal to magic(3)  The magic function returns  let's type help magic  The magic function returns these matrices called magic squares  They have this  you know  mathematical property that all of their rows and columns and diagonals sum up to the same thing  So  you know  it's not actually useful for machine learning as far as I know  but I'm just using this as a convenient way to generate a three by three matrix  And these magic squares have the property that each row  each column  and the diagonals all add up to the same thing  so it's kind of a mathematical construct  I use this magic function only when I'm doing demos or when I'm teaching octave like those in  I don't actually use it for any useful machine learning application  But let's see  if I type RC = find(A > 7) this finds All the elements of A that are greater than equal to seven  and so r  c stands for row and column  So the 1 1 element is greater than 7  the 3 2 element is greater than 7  and the 2 3 element is greater than 7  So let's see  The 2 3 element  for example  is A(2 3)  is 7 is this element out here  and that is indeed greater than equal seven  By the way  I actually don't even memorize myself what these find functions do and what all of these things do myself  And whenever I use the find function  sometimes I forget myself exactly what it does  and now I would type help find to look at the document  Okay  just two more things that I'll quickly show you  One is the sum function  so here's my a  and then type sum(a)  This adds up all the elements of a  and if I want to multiply them together  I type prod(a) prod sends the product  and this returns the product of these four elements of A  Floor(a) rounds down these elements of A  so 0 5 gets rounded down to 0  And ceil  or ceiling(A) gets rounded up to the nearest integer  so 0 5 gets rounded up to 1  You can also  let's see  Let me type rand(3)  this generates a three by three matrix  If i type max(rand(3)  what this does is it takes the element-wise maximum of 3 random 3 by 3 matrices  So you notice all of these numbers tend to be a bit on the large side because each of these is actually the max of a element wise max of two randomly generated matrices  This is my magic number  This is my magic square  three by three A  Let's say I type max A  and then this will be a []  1  what this does is this texts the column wise maximum  So the max of the first column is 8  max of second column is 9  the max of the third column is 7  This 1 means to take the max among the first dimension of 8  In contrast  if I were to type max A  this funny notation  two  then this takes the per row maximum  So the max of the first row is eight  max of second row is seven  max of the third row is nine  and so this allows you to take maxes either per row or per column  And remember the default's to a column wise element  So if you want to find the maximum element in the entire matrix A  you can type max(max(A)) like so  which is 9  Or you can turn A into a vector and type max(A( )) like so and this treats this as a vector and takes the max element of that vector  Finally let's set A to be a 9 by 9 magic square  So remember the magic square has this property that every column and every row sums the same thing  and also the diagonals  so just a nine by nine matrix square  So let me just sum(A  1)  So this does a per column sum  so we'll take each column of A and add them up and this is verified that indeed for a nine by nine matrix square  every column adds up to 369  adds up to the same thing  Now let's do the row wide sum  So the sum(A 2)  and this sums up each row of A  and indeed each row of A also sums up to 369  Now  let's sum the diagonal elements of A and make sure that also sums up to the same thing  So what I'm gonna do is construct a nine by nine identity matrix  that's eye nine  And let me take A and construct  multiply A element wise  so here's my matrix A  I'm going to do A  ^ eye(9)  What this will do is take the element wise product of these two matrices  and so this should Wipe out everything in A  except for the diagonal entries  And now  I'm gonna do sum sum of A of that and this gives me the sum of these diagonal elements  and indeed that is 369  You can sum up the other diagonals as well  So this top left to bottom left  you can sum up the opposite diagonal from bottom left to top right  The commands for this is somewhat more cryptic  you don't really need to know this  I'm just showing you this in case any of you are curious  But let's see  Flipud stands for flip up down  But if you do that  that turns out to sum up the elements in the opposite  So the other diagram  that also sums up to 369  Here  let me show you  Whereas eye(9) is this matrix  Flipup(eye(9))  takes the identity matrix  and flips it vertically  so you end up with  excuse me  flip UD  end up with ones on this opposite diagonal as well  Just one last command and then that's it  and then that'll be it for this video  Let's set A to be the three by three magic square game  If you want to invert a matrix  you type pinv(A)  This is typically called the pseudo-inverse  but it does matter  Just think of it as basically the inverse of A  and that's the inverse of A  And so I can set temp = pinv(A) and temp times A  this is indeed the identity matrix  where it's essentially ones on the diagonals  and zeroes on the off-diagonals  up to a numeric round off  So  that's it for how to do different computational operations on data and matrices  And after running a learning algorithm  often one of the most useful things is to be able to look at your results  so to plot or visualize your result  And in the next video  I'm going to very quickly show you how again with one or two lines of code using Octave  You can quickly visualize your data or plot your data and use that to better understand what you're learning algorithms are doing 
OI4cYnHCHu4,Plotting Data  When developing learning algorithms  very often a few simple plots can give you a better sense of what the algorithm is doing and just sanity check that everything is going okay and the algorithms doing what is supposed to  For example  in an earlier video  I talked about how plotting the cost function J of theta can help you make sure that gradient descent is converging  Often  plots of the data or of all the learning algorithm outputs will also give you ideas for how to improve your learning algorithm  Fortunately  Octave has very simple tools to generate lots of different plots and when I use learning algorithms  I find that plotting the data  plotting the learning algorithm and so on are often an important part of how I get ideas for improving the algorithms and in this video  I'd like to show you some of these Octave tools for plotting and visualizing your data  Here's my Octave window  Let's quickly generate some data for us to plot  So I'm going to set T to be equal to  you know  this array of numbers  Here's T  set of numbers going from 0 up to  98  Let's set y1 equals sine of 2 pie 40 and if I want to plot the sine function  it's very easy  I just type plot T comma Y 1 and hit enter  And up comes this plot where the horizontal axis is the T variable and the vertical axis is y1  which is the sine you saw in the function that we just computed  Let's set y2 to be equal to the cosine of two pi  four T  like so  And if I plot T comma y2  what octave will I do is I'll take my sine plot and it will replace with this cosine function and now  you know  cosine of xi of 1  Now  what if I want to have both the sine and the cosine plots on top of each other? What I'm going to do is I'm going to type plot t y1  So here's my sine function  and then I'm going to use the function hold on  And what hold does it closes octaves to now figures on top of the old one and let me now plot t y2  I'm going to plot the cosine function in a different color  So  let me put there r in quotation marks there and instead of replacing the current figure  I'll plot the cosine function on top and the r indicates the what is an event color  And here additional commands - x label times  to label the X axis  or the horizontal axis  And Y label values A  to label the vertical axis value  and I can also label my two lines with this command  legend sine cosine and this puts this legend up on the upper right showing what the 2 lines are  and finally title my plot is the title at the top of this figure  Lastly  if you want to save this figure  you type print -dpng myplot  png  So PNG is a graphics file format  and if you do this it will let you save this as a file  If I do that  let me actually change directory to  let's see  like that  and then I will print that out  So this will take a while depending on how your Octave configuration is setup  may take a few seconds  but change directory to my desktop and Octave is now taking a few seconds to save this  If I now go to my desktop  Let's hide these windows  Here's myplot png which Octave has saved  and you know  there's the figure saved as the PNG file  Octave can save thousand other formats as well  So  you can type help plot  if you want to see the other file formats  rather than PNG  that you can save figures in  And lastly  if you want to get rid of the plot  the close command causes the figure to go away  As I figure if I type close  that figure just disappeared from my desktop  Octave also lets you specify a figure and numbers  You type figure 1 plots t  y1  That starts up first figure  and that plots t  y1  And then if you want a second figure  you specify a different figure number  So figure two  plot t  y2 like so  and now on my desktop  I actually have 2 figures  So  figure 1 and figure 2 thus 1 plotting the sine function  1 plotting the cosine function  Here's one other neat command that I often use  which is the subplot command  So  we're going to use subplot 1 2 1  What it does it sub-divides the plot into a one-by-two grid with the first 2 parameters are  and it starts to access the first element  That's what the final parameter 1 is  right? So  divide my figure into a one by two grid  and I want to access the first element right now  And so  if I type that in  this product  this figure  is on the left  And if I plot t  y1  it now fills up this first element  And if I I'll do subplot 122  I'm going to start to access the second element and plot t  y2  Well  throw in y2 in the right hand side  or in the second element  And last command  you can also change the axis scales and change axis these to 1 51 minus 1 1 and this sets the x range and y range for the figure on the right  and concretely  it assess the horizontal major values in the figure on the right to make sure 0 5 to 1  and the vertical axis values use the range from minus one to one  And  you know  you don't need to memorize all these commands  If you ever need to change the access or you need to know is that  you know  there's an access command and you can already get the details from the usual octave help command  Finally  just a couple last commands CLF clear is a figure and here's one unique trait  Let's set a to be equal to a 5 by 5 magic squares a  So  a is now this 5 by 5 matrix does a neat trick that I sometimes use to visualize the matrix  which is I can use image sc of a what this will do is plot a five by five matrix  a five by five grid of color  where the different colors correspond to the different values in the A matrix  So concretely  I can also do color bar  Let me use a more sophisticated command  and image sc A color bar color map gray  This is actually running three commands at a time  I'm running image sc then running color bar  then running color map gray  And what this does  is it sets a color map  so a gray color map  and on the right it also puts in this color bar  And so this color bar shows what the different shades of color correspond to  Concretely  the upper left element of the A matrix is 17  and so that corresponds to kind of a mint shade of gray  Whereas in contrast the second element of A--sort of the 1 2 element of A--is 24  Right  so it's A 1 2 is 24  So that corresponds to this square out here  which is nearly a shade of white  And the small value  say A--what is that? A 4 5  you know  is a value 3 over here that corresponds-- you can see on my color bar that it corresponds to a much darker shade in this image  So here's another example  I can plot a larger  you know  here's a magic 15 that gives you a 15 by 15 magic square and this gives me a plot of what my 15 by 15 magic squares values looks like  And finally to wrap up this video  what you've seen me do here is use comma chaining of function calls  Here's how you actually do this  If I type A equals 1  B equals 2  C equals 3  and hit Enter  then this is actually carrying out three commands at the same time  Or really carrying out three commands  one after another  and it prints out all three results  And this is a lot like A equals 1  B equals 2  C equals 3  except that if I use semicolons instead of a comma  it doesn't print out anything  So  this  you know  this thing here we call comma chaining of commands  or comma chaining of function calls  And  it's just another convenient way in Octave to put multiple commands like image sc color bar  colon map to put multi-commands on the same line  So  that's it  You now know how to plot different figures and octave  and in next video the next main piece that I want to tell you about is how to write control statements like if  while  for statements and octave as well as hard to define and use functions
mARX-dNTucY,"Control Statements  for  while  if statement  In this video  I'd like to tell you how to write control statements for your Octave programs  so things like \""for\""  \""while\"" and \""if\"" statements and also how to define and use functions  Here's my Octave window  Let me first show you how to use a \""for\"" loop  I'm going to start by setting v to be a 10 by 1 vector 0  Now  here's I write a \""for\"" loop for I equals 1 to 10  That's for I equals Y colon 10  And let's see  I'm going to set V of I equals two to the power of I  and finally end  The white space does not matter  so I am putting the spaces just to make it look nicely indented  but you know spacing doesn't matter  But if I do this  then the result is that V gets set to  you know  two to the power one  two to the power two  and so on  So this is syntax for I equals one colon 10 that makes I loop through the values one through 10  And by the way  you can also do this by setting your indices equals one to 10  and so the indices in the array from one to 10  You can also write for I equals indices  And this is actually the same as if I equals one to 10  You can do  you know  display I and this would do the same thing  So  that is a \""for\"" loop  if you are familiar with \""break\"" and \""continue\""  there's \""break\"" and \""continue\"" statements  you can also use those inside loops in octave  but first let me show you how a while loop works  So  here's my vector V  Let's write the while loop  I equals 1  while I is less than or equal to 5  let's set V I equals one hundred and increment I by one  end  So this says what? I starts off equal to one and then I'm going to set V I equals one hundred and increment I by one until I is  you know  greater than five  And as a result of that  whereas previously V was this powers of two vector  I've now taken the first five elements of my vector and overwritten them with this value one hundred  So that's a syntax for a while loop  Let's do another example  Y equals one while true and here I wanted to show you how to use a break statement  Let's say V I equals 999 and I equals i+1 if i equals 6 break and end  And this is also our first use of an if statement  so I hope the logic of this makes sense  Since I equals one and  you know  increment loop  While repeatedly set V I equals 1 and increment i by 1  and then when 1 i gets up to 6  do a break which breaks here although the while do and so  the effective is should be to take the first five elements of this vector V and set them to 999  And yes  indeed  we're taking V and overwritten the first five elements with 999  So  this is the syntax for \""if\"" statements  and for \""while\"" statement  and notice the end  We have two ends here  This ends here ends the if statement and the second end here ends the while statement  Now let me show you the more general syntax for how to use an if-else statement  So  let's see  V 1 is equal to 999  let's type V1 equals to 2 for this example  So  let me type if V 1 equals 1 display the value as one  Here's how you write an else statement  or rather here's an else if  V 1 equals 2  This is  if in case that's true in our example  display the value as 2  else display  the value is not one or two  Okay  so that's a if-else if-else statement it ends  And of course  here we've just set v 1 equals 2  so hopefully  yup  displays that the value is 2  And finally  I don't think I talked about this earlier  but if you ever need to exit Octave  you can type the exit command and you hit enter that will cause Octave to quit or the 'q'--quits command also works  Finally  let's talk about functions and how to define them and how to use them  Here's my desktop  and I have predefined a file or pre-saved on my desktop a file called \""squarethisnumber m\""  This is how you define functions in Octave  You create a file called  you know  with your function name and then ending in  m  and when Octave finds this file  it knows that this where it should look for the definition of the function \""squarethisnumber m\""  Let's open up this file  Notice that I'm using the Microsoft program Wordpad to open up this file  I just want to encourage you  if your using Microsoft Windows  to use Wordpad rather than Notepad to open up these files  if you have a different text editor that's fine too  but notepad sometimes messes up the spacing  If you only have Notepad  that should work too  that could work too  but if you have Wordpad as well  I would rather use that or some other text editor  if you have a different text editor for editing your functions  So  here's how you define the function in Octave  Let me just zoom in a little bit  And this file has just three lines in it  The first line says function Y equals square root number of X  this tells Octave that I'm gonna return the value Y  I'm gonna return one value and that the value is going to be saved in the variable Y and moreover  it tells Octave that this function has one argument  which is the argument X  and the way the function body is defined  if Y equals X squared  So  let's try to call this function \""square\""  this number 5  and this actually isn't going to work  and Octave says square this number it's undefined  That's because Octave doesn't know where to find this file  So as usual  let's use PWD  or not in my directory  so let's see this c \\users\\ang\\desktop  That's where my desktop is  Oops  a little typo there  Users ANG desktop and if I now type square root number 5  it returns the answer 25  As kind of an advanced feature  this is only for those of you that know what the term search path means  But so if you want to modify the Octave search path and you could  you just think of this next part as advanced or optional material  Only for those who are either familiar with the concepts of search paths and permit languages  but you can use the term addpath  safety colon  slash users/ANG/desktop to add that directory to the Octave search path so that even if you know  go to some other directory I can still  Octave still knows to look in the users ANG desktop directory for functions so that even though I'm in a different directory now  it still knows where to find the square this number function  Okay? But if you're not familiar with the concept of search path  don't worry about it  Just make sure as you use the CD command to go to the directory of your function before you run it and that actually works just fine  One concept that Octave has that many other programming languages don't is that it can also let you define functions that return multiple values or multiple arguments  So here's an example of that  Define the function called square and cube this number X and what this says is this function returns 2 values  y1 and y2  When I set down  this follows  y1 is squared  y2 is execute  And what this does is this really returns 2 numbers  So  some of you depending on what programming language you use  if you're familiar with  you know  CC++ your offer  Often  we think of the function as return in just one value  But just so the syntax in Octave that should return multiple values  Now back in the Octave window  If I type  you know  a  b equals square and cube this number 5 then a is now equal to 25 and b is equal to the cube of 5 equal to 125  So  this is often convenient if you needed to define a function that returns multiple values  Finally  I'm going to show you just one more sophisticated example of a function  Let's say I have a data set that looks like this  with data points at 1  1  2  2  3  3  And what I'd like to do is to define an octave function to compute the cost function J of theta for different values of theta  First let's put the data into octave  So I set my design matrix to be 1 1 1 2 1 3  So  this is my design matrix x with x0  the first column being the said term and the second term being you know  my the x-values of my three training examples  And let me set y to be 1-2-3 as follows  which were the y axis values  So let's say theta is equal to 0 semicolon 1  Here at my desktop  I've predefined does cost function j and if I bring up the definition of that function it looks as follows  So function j equals cost function j equals x y theta  some commons  specifying the inputs and then vary few steps set m to be the number trading examples thus the number of rows in x  Compute the predictions  predictions equals x times theta and so this is a common that's wrapped around  so this is probably the preceding comment line  Computer script errors by  you know  taking the difference between your predictions and the y values and taking the element of y squaring and then finally computing the cost function J  And Octave knows that J is a value I want to return because J appeared here in the function definition  Feel free by the way to pause this video if you want to look at this function definition for longer and kind of make sure that you understand the different steps  But when I run it in Octave  I run j equals cost function j x y theta  It computes  Oops  made a typo there  It should have been capital X  It computes J equals 0 because if my data set was  you know  123  123 then setting  theta 0 equals 0  theta 1 equals 1  this gives me exactly the 45-degree line that fits my data set perfectly  Whereas in contrast if I set theta equals say 0  0  then this hypothesis is predicting zeroes on everything the same  theta 0 equals 0  theta 1 equals 0 and I compute the cost function then it's 2 333 and that's actually equal to 1 squared  which is my squared error on the first example  plus 2 squared  plus 3 squared and then divided by 2m  which is 2 times number of training examples  which is indeed 2 33 and so  that sanity checks that this function here is  you know  computing the correct cost function and these are the couple examples we tried out on our simple training example  And so that sanity tracks that the cost function J  as defined here  that it is indeed  you know  seeming to compute the correct cost function  at least on our simple training set that we had here with X and Y being this simple training example that we solved  So  now you know how to right control statements like for loops  while loops and if statements in octave as well as how to define and use functions  In the next video  I'm going to just very quickly step you through the logistics of working on and submitting problem sets for this class and how to use our submission system  And finally  after that  in the final octave tutorial video  I wanna tell you about vectorization  which is an idea for how to make your octave programs run much fast "
VmnzH3GZiKw,Vectorization  In this video I like to tell you about the idea of Vectorization  So  whether you using Octave or a similar language like MATLAB or whether you're using Python [INAUDIBLE]  R  Java  C++  all of these languages have either built into them or have regularly and easily accessible difference in numerical linear algebra libraries  They're usually very well written  highly optimized  often sort of developed by people that have PhDs in numerical computing or they're really specialized in numerical computing  And when you're implementing machine learning algorithms  if you're able to take advantage of these linear algebra libraries or these numerical linear algebra libraries  and make some routine calls to them rather than sort of write code yourself to do things that these libraries could be doing  If you do that  then often you get code that  first  is more efficient  so you just run more quickly and take better advantage of any parallel hardware your computer may have and so on  And second  it also means that you end up with less code that you need to write  so it's a simpler implementation that is therefore maybe also more likely to be by free  And as a concrete example  rather than writing code yourself to multiply matrices  if you let Octave do it by typing a times b  that would use a very efficient routine to multiply the two matrices  And there's a bunch of examples like these  where if you use appropriate vectorization implementations you get much simpler code and much more efficient code  Let's look at some examples  Here's our usual hypothesis for linear regression  and if you want to compute h(x)  notice that there's a sum on the right  And so one thing you could do is  compute the sum from j = 0 to j = n yourself  Another way to think of this is to think of h(x) as theta transpose x  and what you can do is  think of this as you are computing this inner product between two vectors where theta is your vector  say  theta 0  theta 1  theta 2  If you have two features  if n equals two  and if you think x as this vector  x0  x1  x2  and these two views can give you two different implementations  Here's what I mean  Here's an unvectorized implementation for how to compute and by unvectorize  I mean without vectorization  We might first initialize prediction just to be 0 0  The prediction's going to eventually be h(x)  and then I'm going to have a for loop for j=1 through n+1  prediction gets incremented by theta(j) * x(j)  So it's kind of this expression over here  By the way  I should mention  in these vectors that I wrote over here  I had these vectors being 0 index  So I had theta 0  theta 1  theta 2  But because MATLAB is one index  theta 0 in that MATLAB  we would end up representing as theta 1 and the second element ends up as theta 2 and this third element may end up as theta 3  just because our vectors in MATLAB are indexed starting from 1  even though I wrote theta and x here  starting indexing from 0  which is why here I have a for loop  j goes from 1 through n+1 rather than j goes through 0 up to n  right? But so this is an unvectorized implementation in that we have for loop that is summing up the n elements of the sum  In contrast  here's how you would write a vectorized implementation  which is that you would think of a x and theta as vectors  You just said prediction = theta' * x  You're just computing like so  So instead of writing all these lines of code with a for loop  you instead just have one line of code  And what this line of code on the right will do is  it will use Octaves highly optimized numerical linear algebra routines to compute this inner product between the two vectors  theta and X  and not only is the vectorized implementation simpler  it will also run much more efficiently  So that was octave  but the issue of vectorization applies to other programming language as well  Lets look on the example in C++  Here's what an unvectorized implementation might look like  We again initialize prediction to 0 0 and then we now how a for loop for j = 0 up to n  Prediction += theta j * x[j]  where again  you have this explicit for loop that you write yourself  In contrast  using a good numerical linear algebra library in C++  you could write a function like  or rather  In contrast  using a good numerical linear algebra library in C++  you can instead write code that might look like this  So depending on the details of your numerical linear algebra library  you might be able to have an object  this is a C++ object  which is vector theta  and a C++ object which is vector x  and you just take theta transpose * x  where this times becomes a C++ sort of overload operator so you can just multiply these two vectors in C++  And depending on the details of your numerical linear algebra library  you might end up using a slightly different syntax  but by relying on the library to do this inner product  you can get a much simpler piece of code and a much more efficient one  Let's now look at a more sophisticated example  Just to remind you  here's our update rule for a gradient descent of a linear regression  And so we update theta j using this rule for all values of j = 0  1  2  and so on  And if I just write out these equations for theta 0  theta 1  theta 2  assuming we have two features  so n = 2  Then these are the updates we perform for theta 0  theta 1  theta 2  where you might remember my saying in an earlier video  that these should be simultaneous updates  So  let's see if we can come up with a vectorizing notation of this  Here are my same three equations written in a slightly smaller font  and you can imagine that one way to implement these three lines of code is to have a for loop that says for j = 0  1 through 2 to update theta j  or something like that  But instead  let's come up with a vectorized implementation and see if we can have a simpler way to basically compress these three lines of code or a for loop that effectively does these three steps one set at a time  Let's see if we can take these three steps and compress them into one line of vectorized code  Here's the idea  What I'm going to do is  I'm going to think of theta as a vector  and I'm gonna update theta as theta- alpha times some other vector delta  where delta's is going to be equal to 1 over m  sum from i = 1 through m  And then this term over on the right  okay? So  let me explain what's going on here  Here  I'm going to treat theta as a vector  so this is n plus one dimensional vector  and I'm saying that theta gets here updated as that's a vector  Rn + 1  Alpha is a real number  and delta  here is a vector  So  this subtraction operation  that's a vector subtraction  okay? Cuz alpha times delta is a vector  and so I'm saying theta gets this vector  alpha times delta subtracted from it  So  what is a vector delta? Well this vector delta  looks like this  and what it's meant to be is really meant to be this thing over here  Concretely  delta will be a n plus one dimensional vector  and the very first element of the vector delta is going to be equal to that  So  if we have the delta  if we index it from 0  if it's delta 0  delta 1  delta 2  what I want is that delta 0 is equal to this first box in green up above  And indeed  you might be able to convince yourself that delta 0 is this 1 of the m sum of ho(x)  x(i) minus y(i) times x(i) 0  So  let's just make sure we're on this same page about how delta really is computed  Delta is 1 over m times this sum over here  and what is this sum? Well  this term over here  that's a real number  and the second term over here  x i  this term over there is a vector  right  because x(i) may be a vector that would be  say  x(i)0  x(i)1  x(i)2  right  and what is the summation? Well  what the summation is saying is that  this term  that is this term over here  this is equal to  (h of(x(1))- y(1)) * x(1) + (h of(x(2))- y(2) x x(2) +  and so on  okay? Because this is summation of i  so as i ranges from i = 1 through m  you get these different terms  and you're summing up these terms here  And the meaning of these terms  this is a lot like if you remember actually from the earlier quiz in this  right  you saw this equation  We said that in order to vectorize this code we will instead said u = 2v + 5w  So we're saying that the vector u is equal to two times the vector v plus five times the vector w  So this is an example of how to add different vectors and this summation's the same thing  This is saying that the summation over here is just some real number  right? That's kinda like the number two or some other number times the vector  x1  So it's kinda like 2v or say some other number times x1  and then plus instead of 5w we instead have some other real number  plus some other vector  and then you add on other vectors  plus dot  dot  dot  plus the other vectors  which is why  over all  this thing over here  that whole quantity  that delta is just some vector  And concretely  the three elements of delta correspond if n = 2  the three elements of delta correspond exactly to this thing  to the second thing  and this third thing  Which is why when you update theta according to theta- alpha delta  we end up carrying exactly the same simultaneous updates as the update rules that we have up top  So  I know that there was a lot that happened on this slide  but again  feel free to pause the video and if you aren't sure what just happened I'd encourage you to step through this slide to make sure you understand why is it that this update here with this definition of delta  right  why is it that that's equal to this update on top? And if it's still not clear  one insight is that  this thing over here  that's exactly the vector x  and so we're just taking all three of these computations  and compressing them into one step with this vector delta  which is why we can come up with a vectorized implementation of this step of the new refresh in this way  So  I hope this step makes sense and do look at the video and see if you can understand it  In case you don't understand quite the equivalence of this map  if you implement this  this turns out to be the right answer anyway  So  even if you didn't quite understand equivalence  if you just implement it this way  you'll be able to get linear regression to work  But if you're able to figure out why these two steps are equivalent  then hopefully that will give you a better understanding of vectorization as well  And finally  if you are implementing linear regression using more than one or two features  so sometimes we use linear regression with 10's or 100's or 1 000's of features  But if you use the vectorized implementation of linear regression  you'll see that will run much faster than if you had  say  your old for loop that was updating theta zero  then theta one  then theta two yourself  So  using a vectorized implementation  you should be able to get a much more efficient implementation of linear regression  And when you vectorize later algorithms that we'll see in this class  there's good trick  whether in Octave or some other language like C++  Java  for getting your code to run more efficiently Vectorization
w-WIQ41cDXI,Classification  In this and the next few videos  I want to start to talk about classification problems  where the variable y that you want to predict is valued  We'll develop an algorithm called logistic regression  which is one of the most popular and most widely used learning algorithms today  Here are some examples of classification problems  Earlier we talked about email spam classification as an example of a classification problem  Another example would be classifying online transactions  So if you have a website that sells stuff and if you want to know if a particular transaction is fraudulent or not  whether someone is using a stolen credit card or has stolen the user's password  There's another classification problem  And earlier we also talked about the example of classifying tumors as cancerous  malignant or as benign tumors  In all of these problems the variable that we're trying to predict is a variable y that we can think of as taking on two values either zero or one  either spam or not spam  fraudulent or not fraudulent  related malignant or benign  Another name for the class that we denote with zero is the negative class  and another name for the class that we denote with one is the positive class  So zero we denote as the benign tumor  and one  positive class we denote a malignant tumor  The assignment of the two classes  spam not spam and so on  The assignment of the two classes to positive and negative to zero and one is somewhat arbitrary and it doesn't really matter but often there is this intuition that a negative class is conveying the absence of something like the absence of a malignant tumor  Whereas one the positive class is conveying the presence of something that we may be looking for  but the definition of which is negative and which is positive is somewhat arbitrary and it doesn't matter that much  For now we're going to start with classification problems with just two classes zero and one  Later one we'll talk about multi class problems as well where therefore y may take on four values zero  one  two  and three  This is called a multiclass classification problem  But for the next few videos  let's start with the two class or the binary classification problem and we'll worry about the multiclass setting later  So how do we develop a classification algorithm? Here's an example of a training set for a classification task for classifying a tumor as malignant or benign  And notice that malignancy takes on only two values  zero or no  one or yes  So one thing we could do given this training set is to apply the algorithm that we already know  Linear regression to this data set and just try to fit the straight line to the data  So if you take this training set and fill a straight line to it  maybe you get a hypothesis that looks like that  right  So that's my hypothesis  H(x) equals theta transpose x  If you want to make predictions one thing you could try doing is then threshold the classifier outputs at 0 5 that is at a vertical axis value 0 5 and if the hypothesis outputs a value that is greater than equal to 0 5 you can take y = 1  If it's less than 0 5 you can take y=0  Let's see what happens if we do that  So 0 5 and so that's where the threshold is and that's using linear regression this way  Everything to the right of this point we will end up predicting as the positive cross  Because the output values is greater than 0 5 on the vertical axis and everything to the left of that point we will end up predicting as a negative value  In this particular example  it looks like linear regression is actually doing something reasonable  Even though this is a classification toss we're interested in  But now let's try changing the problem a bit  Let me extend out the horizontal access a little bit and let's say we got one more training example way out there on the right  Notice that that additional training example  this one out here  it doesn't actually change anything  right  Looking at the training set it's pretty clear what a good hypothesis is  Is that well everything to the right of somewhere around here  to the right of this we should predict this positive  Everything to the left we should probably predict as negative because from this training set  it looks like all the tumors larger than a certain value around here are malignant  and all the tumors smaller than that are not malignant  at least for this training set  But once we've added that extra example over here  if you now run linear regression  you instead get a straight line fit to the data  That might maybe look like this  And if you know threshold hypothesis at 0 5  you end up with a threshold that's around here  so that everything to the right of this point you predict as positive and everything to the left of that point you predict as negative  And this seems a pretty bad thing for linear regression to have done  right  because you know these are our positive examples  these are our negative examples  It's pretty clear we really should be separating the two somewhere around there  but somehow by adding one example way out here to the right  this example really isn't giving us any new information  I mean  there should be no surprise to the learning algorithm  That the example way out here turns out to be malignant  But somehow having that example out there caused linear regression to change its straight-line fit to the data from this magenta line out here to this blue line over here  and caused it to give us a worse hypothesis  So  applying linear regression to a classification problem often isn't a great idea  In the first example  before I added this extra training example  previously linear regression was just getting lucky and it got us a hypothesis that worked well for that particular example  but usually applying linear regression to a data set  you might get lucky but often it isn't a good idea  So I wouldn't use linear regression for classification problems  Here's one other funny thing about what would happen if we were to use linear regression for a classification problem  For classification we know that y is either zero or one  But if you are using linear regression where the hypothesis can output values that are much larger than one or less than zero  even if all of your training examples have labels y equals zero or one  And it seems kind of strange that even though we know that the labels should be zero  one it seems kind of strange if the algorithm can output values much larger than one or much smaller than zero  So what we'll do in the next few videos is develop an algorithm called logistic regression  which has the property that the output  the predictions of logistic regression are always between zero and one  and doesn't become bigger than one or become less than zero  And by the way  logistic regression is  and we will use it as a classification algorithm  is some  maybe sometimes confusing that the term regression appears in this name even though logistic regression is actually a classification algorithm  But that's just a name it was given for historical reasons  So don't be confused by that logistic regression is actually a classification algorithm that we apply to settings where the label y is discrete value  when it's either zero or one  So hopefully you now know why  if you have a classification problem  using linear regression isn't a good idea  In the next video  we'll start working out the details of the logistic regression algorithm 
X0oVLO5huc0,Hypothesis Representation  Let's start talking about logistic regression  In this video  I'd like to show you the hypothesis representation  That is  what is the function we're going to use to represent our hypothesis when we have a classification problem? Earlier  we said that we would like our classifier to output values that are between 0 and 1  So we'd like to come up with a hypothesis that satisfies this property  that is  predictions are maybe between 0 and 1  When we were using linear regression  this was the form of a hypothesis  where h(x) is theta transpose x  For logistic regression  I'm going to modify this a little bit and make the hypothesis g of theta transpose x  Where I'm going to define the function g as follows  G(z)  z is a real number  is equal to one over one plus e to the negative z  This is called the sigmoid function  or the logistic function  and the term logistic function  that's what gives rise to the name logistic regression  And by the way  the terms sigmoid function and logistic function are basically synonyms and mean the same thing  So the two terms are basically interchangeable  and either term can be used to refer to this function g  And if we take these two equations and put them together  then here's just an alternative way of writing out the form of my hypothesis  I'm saying that h(x) Is 1 over 1 plus e to the negative theta transpose x  And all I've do is I've taken this variable z  z here is a real number  and plugged in theta transpose x  So I end up with theta transpose x in place of z there  Lastly  let me show you what the sigmoid function looks like  We're gonna plot it on this figure here  The sigmoid function  g(z)  also called the logistic function  it looks like this  It starts off near 0 and then it rises until it crosses 0 5 and the origin  and then it flattens out again like so  So that's what the sigmoid function looks like  And you notice that the sigmoid function  while it asymptotes at one and asymptotes at zero  as a z axis  the horizontal axis is z  As z goes to minus infinity  g(z) approaches zero  And as g(z) approaches infinity  g(z) approaches one  And so because g(z) upwards values are between zero and one  we also have that h(x) must be between zero and one  Finally  given this hypothesis representation  what we need to do  as before  is fit the parameters theta to our data  So given a training set we need to a pick a value for the parameters theta and this hypothesis will then let us make predictions  We'll talk about a learning algorithm later for fitting the parameters theta  but first let's talk a bit about the interpretation of this model  Here's how I'm going to interpret the output of my hypothesis  h(x)  When my hypothesis outputs some number  I am going to treat that number as the estimated probability that y is equal to one on a new input  example x  Here's what I mean  here's an example  Let's say we're using the tumor classification example  so we may have a feature vector x  which is this x zero equals one as always  And then one feature is the size of the tumor  Suppose I have a patient come in and they have some tumor size and I feed their feature vector x into my hypothesis  And suppose my hypothesis outputs the number 0 7  I'm going to interpret my hypothesis as follows  I'm gonna say that this hypothesis is telling me that for a patient with features x  the probability that y equals 1 is 0 7  In other words  I'm going to tell my patient that the tumor  sadly  has a 70 percent chance  or a 0 7 chance of being malignant  To write this out slightly more formally  or to write this out in math  I'm going to interpret my hypothesis output as  P of y=1 given x parameterized by theta  So for those of you that are familiar with probability  this equation may make sense  If you're a little less familiar with probability  then here's how I read this expression  This is the probability that y is equal to one  Given x  given that my patient has features x  so given my patient has a particular tumor size represented by my features x  And this probability is parameterized by theta  So I'm basically going to count on my hypothesis to give me estimates of the probability that y is equal to 1  Now  since this is a classification task  we know that y must be either 0 or 1  right? Those are the only two values that y could possibly take on  either in the training set or for new patients that may walk into my office  or into the doctor's office in the future  So given h(x)  we can therefore compute the probability that y = 0 as well  completely because y must be either 0 or 1  We know that the probability of y = 0 plus the probability of y = 1 must add up to 1  This first equation looks a little bit more complicated  It's basically saying that probability of y=0 for a particular patient with features x  and given our parameters theta  Plus the probability of y=1 for that same patient with features x and given theta parameters theta must add up to one  If this equation looks a little bit complicated  feel free to mentally imagine it without that x and theta  And this is just saying that the product of y equals zero plus the product of y equals one  must be equal to one  And we know this to be true because y has to be either zero or one  and so the chance of y equals zero  plus the chance that y is one  Those two must add up to one  And so if you just take this term and move it to the right hand side  then you end up with this equation  That says probability that y equals zero is 1 minus probability of y equals 1  and thus if our hypothesis feature of x gives us that term  You can therefore quite simply compute the probability or compute the estimated probability that y is equal to 0 as well  So  you now know what the hypothesis representation is for logistic regression and we're seeing what the mathematical formula is  defining the hypothesis for logistic regression  In the next video  I'd like to try to give you better intuition about what the hypothesis function looks like  And I wanna tell you about something called the decision boundary  And we'll look at some visualizations together to try to get a better sense of what this hypothesis function of logistic regression really looks like 
8z4zUjsZZrs,Decision Boundary  In the last video  we talked about the hypothesis representation for logistic regression  What Id like to do now is tell you about something called the decision boundary  and this will give us a better sense of what the logistic regressions hypothesis function is computing  To recap  this is what we wrote out last time  where we said that the hypothesis is represented as h of x equals g of theta transpose x  where g is this function called the sigmoid function  which looks like this  It slowly increases from zero to one  asymptoting at one  What I want to do now is try to understand better when this hypothesis will make predictions that y is equal to 1 versus when it might make predictions that y is equal to 0  And understand better what hypothesis function looks like particularly when we have more than one feature  Concretely  this hypothesis is outputting estimates of the probability that y is equal to one  given x and parameterized by theta  So if we wanted to predict is y equal to one or is y equal to zero  here's something we might do  Whenever the hypothesis outputs that the probability of y being one is greater than or equal to 0 5  so this means that if there is more likely to be y equals 1 than y equals 0  then let's predict y equals 1  And otherwise  if the probability  the estimated probability of y being over 1 is less than 0 5  then let's predict y equals 0  And I chose a greater than or equal to here and less than here If h of x is equal to 0 5 exactly  then you could predict positive or negative  but I probably created a loophole here  so we default maybe to predicting positive if h of x is 0 5  but that's a detail that really doesn't matter that much  What I want to do is understand better when is it exactly that h of x will be greater than or equal to 0 5  so that we'll end up predicting y is equal to 1  If we look at this plot of the sigmoid function  we'll notice that the sigmoid function  g of z is greater than or equal to 0 5 whenever z is greater than or equal to zero  So is in this half of the figure that g takes on values that are 0 5 and higher  This notch here  that's 0 5  and so when z is positive  g of z  the sigmoid function is greater than or equal to 0 5  Since the hypothesis for logistic regression is h of x equals g of theta and transpose x  this is therefore going to be greater than or equal to 0 5  whenever theta transpose x is greater than or equal to 0  So what we're shown  right  because here theta transpose x takes the role of z  So what we're shown is that a hypothesis is gonna predict y equals 1 whenever theta transpose x is greater than or equal to 0  Let's now consider the other case of when a hypothesis will predict y is equal to 0  Well  by similar argument  h(x) is going to be less than 0 5 whenever g(z) is less than 0 5 because the range of values of z that cause g(z) to take on values less than 0 5  well  that's when z is negative  So when g(z) is less than 0 5  a hypothesis will predict that y is equal to 0  And by similar argument to what we had earlier  h(x) is equal to g of theta transpose x and so we'll predict y equals 0 whenever this quantity theta transpose x is less than 0  To summarize what we just worked out  we saw that if we decide to predict whether y=1 or y=0 depending on whether the estimated probability is greater than or equal to 0 5  or whether less than 0 5  then that's the same as saying that when we predict y=1 whenever theta transpose x is greater than or equal to 0  And we'll predict y is equal to 0 whenever theta transpose x is less than 0  Let's use this to better understand how the hypothesis of logistic regression makes those predictions  Now  let's suppose we have a training set like that shown on the slide  And suppose a hypothesis is h of x equals g of theta zero plus theta one x one plus theta two x two  We haven't talked yet about how to fit the parameters of this model  We'll talk about that in the next video  But suppose that via a procedure to specified  We end up choosing the following values for the parameters  Let's say we choose theta 0 equals 3  theta 1 equals 1  theta 2 equals 1  So this means that my parameter vector is going to be theta equals minus 3  1  1  So  when given this choice of my hypothesis parameters  let's try to figure out where a hypothesis would end up predicting y equals one and where it would end up predicting y equals zero  Using the formulas that we were taught on the previous slide  we know that y equals one is more likely  that is the probability that y equals one is greater than or equal to 0 5  whenever theta transpose x is greater than zero  And this formula that I just underlined  -3 + x1 + x2  is  of course  theta transpose x when theta is equal to this value of the parameters that we just chose  So for any example  for any example which features x1 and x2 that satisfy this equation  that minus 3 plus x1 plus x2 is greater than or equal to 0  our hypothesis will think that y equals 1  the small x will predict that y is equal to 1  We can also take -3 and bring this to the right and rewrite this as x1+x2 is greater than or equal to 3  so equivalently  we found that this hypothesis would predict y=1 whenever x1+x2 is greater than or equal to 3  Let's see what that means on the figure  if I write down the equation  X1 + X2 = 3  this defines the equation of a straight line and if I draw what that straight line looks like  it gives me the following line which passes through 3 and 3 on the x1 and the x2 axis  So the part of the infospace  the part of the X1 X2 plane that corresponds to when X1 plus X2 is greater than or equal to 3  that's going to be this right half thing  that is everything to the up and everything to the upper right portion of this magenta line that I just drew  And so  the region where our hypothesis will predict y = 1  is this region  just really this huge region  this half space over to the upper right  And let me just write that down  I'm gonna call this the y = 1 region  And  in contrast  the region where x1 + x2 is less than 3  that's when we will predict that y is equal to 0  And that corresponds to this region  And there's really a half plane  but that region on the left is the region where our hypothesis will predict y = 0  I wanna give this line  this magenta line that I drew a name  This line  there  is called the decision boundary  And concretely  this straight line  X1 plus X equals 3  That corresponds to the set of points  so that corresponds to the region where H of X is equal to 0 5 exactly and the decision boundary that is this straight line  that's the line that separates the region where the hypothesis predicts Y equals 1 from the region where the hypothesis predicts that y is equal to zero  And just to be clear  the decision boundary is a property of the hypothesis including the parameters theta zero  theta one  theta two  And in the figure I drew a training set  I drew a data set  in order to help the visualization  But even if we take away the data set this decision boundary and the region where we predict y =1 versus y = 0  that's a property of the hypothesis and of the parameters of the hypothesis and not a property of the data set  Later on  of course  we'll talk about how to fit the parameters and there we'll end up using the training set  using our data  To determine the value of the parameters  But once we have particular values for the parameters theta0  theta1  theta2 then that completely defines the decision boundary and we don't actually need to plot a training set in order to plot the decision boundary  Let's now look at a more complex example where as usual  I have crosses to denote my positive examples and Os to denote my negative examples  Given a training set like this  how can I get logistic regression to fit the sort of data? Earlier when we were talking about polynomial regression or when we're talking about linear regression  we talked about how we could add extra higher order polynomial terms to the features  And we can do the same for logistic regression  Concretely  let's say my hypothesis looks like this where I've added two extra features  x1 squared and x2 squared  to my features  So that I now have five parameters  theta zero through theta four  As before  we'll defer to the next video  our discussion on how to automatically choose values for the parameters theta zero through theta four  But let's say that varied procedure to be specified  I end up choosing theta zero equals minus one  theta one equals zero  theta two equals zero  theta three equals one and theta four equals one  What this means is that with this particular choose of parameters  my parameter effect theta theta looks like minus one  zero  zero  one  one  Following our earlier discussion  this means that my hypothesis will predict that y=1 whenever -1 + x1 squared + x2 squared is greater than or equal to 0  This is whenever theta transpose times my theta transfers  my features is greater than or equal to zero  And if I take minus 1 and just bring this to the right  I'm saying that my hypothesis will predict that y is equal to 1 whenever x1 squared plus x2 squared is greater than or equal to 1  So what does this decision boundary look like? Well  if you were to plot the curve for x1 squared plus x2 squared equals 1 Some of you will recognize that  that is the equation for circle of radius one  centered around the origin  So that is my decision boundary  And everything outside the circle  I'm going to predict as y=1  So out here is my y equals 1 region  we'll predict y equals 1 out here and inside the circle is where I'll predict y is equal to 0  So by adding these more complex  or these polynomial terms to my features as well  I can get more complex decision boundaries that don't just try to separate the positive and negative examples in a straight line that I can get in this example  a decision boundary that's a circle  Once again  the decision boundary is a property  not of the trading set  but of the hypothesis under the parameters  So  so long as we're given my parameter vector theta  that defines the decision boundary  which is the circle  But the training set is not what we use to define the decision boundary  The training set may be used to fit the parameters theta  We'll talk about how to do that later  But  once you have the parameters theta  that is what defines the decisions boundary  Let me put back the training set just for visualization  And finally let's look at a more complex example  So can we come up with even more complex decision boundaries then this? If I have even higher polynomial terms so things like X1 squared  X1 squared X2  X1 squared equals squared and so on  And have much higher polynomials  then it's possible to show that you can get even more complex decision boundaries and the regression can be used to find decision boundaries that may  for example  be an ellipse like that or maybe a little bit different setting of the parameters maybe you can get instead a different decision boundary which may even look like some funny shape like that  Or for even more complete examples maybe you can also get this decision boundaries that could look like more complex shapes like that where everything in here you predict y = 1 and everything outside you predict y = 0  So this higher autopolynomial features you can a very complex decision boundaries  So  with these visualizations  I hope that gives you a sense of what's the range of hypothesis functions we can represent using the representation that we have for logistic regression  Now that we know what h(x) can represent  what I'd like to do next in the following video is talk about how to automatically choose the parameters theta so that given a training set we can automatically fit the parameters to our data 
LRGpZI_qPSk,Cost Function  In this video  we'll talk about how to fit the parameters of theta for the logistic compression  In particular  I'd like to define the optimization objective  or the cost function that we'll use to fit the parameters  Here's the supervised learning problem of fitting logistic regression model  We have a training set of m training examples and as usual  each of our examples is represented by a that's n plus one dimensional  and as usual we have x o equals one  First feature or a zero feature is always equal to one  And because this is a computation problem  our training set has the property that every label y is either 0 or 1  This is a hypothesis  and the parameters of a hypothesis is this theta over here  And the question that I want to talk about is given this training set  how do we choose  or how do we fit the parameter's theta? Back when we were developing the linear regression model  we used the following cost function  I've written this slightly differently where instead of 1 over 2m  I've taken a one-half and put it inside the summation instead  Now I want to use an alternative way of writing out this cost function  Which is that instead of writing out this square of return here  let's write in here costs of h of x  y and I'm going to define that total cost of h of x  y to be equal to this  Just equal to this one-half of the squared error  So now we can see more clearly that the cost function is a sum over my training set  which is 1 over n times the sum of my training set of this cost term here  And to simplify this equation a little bit more  it's going to be convenient to get rid of those superscripts  So just define cost of h of x comma y to be equal to one half of this squared error  And interpretation of this cost function is that  this is the cost I want my learning algorithm to have to pay if it outputs that value  if its prediction is h of x  and the actual label was y  So just cross off the superscripts  right  and no surprise for linear regression the cost we've defined is that or the cost of this is that is one-half times the square difference between what I predicted and the actual value that we have  0 for y  Now this cost function worked fine for linear regression  But here  we're interested in logistic regression  If we could minimize this cost function that is plugged into J here  that will work okay  But it turns out that if we use this particular cost function  this would be a non-convex function of the parameter's data  Here's what I mean by non-convex  Have some cross function j of theta and for logistic regression  this function h here has a nonlinearity that is one over one plus e to the negative theta transpose  So this is a pretty complicated nonlinear function  And if you take the function  plug it in here  And then take this cost function and plug it in there and then plot what j of theta looks like  You find that j of theta can look like a function that's like this with many local optima  And the formal term for this is that this is a non-convex function  And you can kind of tell  if you were to run gradient descent on this sort of function It is not guaranteed to converge to the global minimum  Whereas in contrast what we would like is to have a cost function j of theta that is convex  that is a single bow-shaped function that looks like this so that if you run theta in the we would be guaranteed that would converge to the global minimum  And the problem with using this great cost function is that because of this very nonlinear function that appears in the middle here  J of theta ends up being a nonconvex function if you were to define it as a square cost function  So what we'd like to do is  instead of come up with a different cost function  that is convex  and so that we can apply a great algorithm  like gradient descent and be guaranteed to find the global minimum  Here's the cost function that we're going to use for logistic regression  We're going to say that the cost  or the penalty that the algorithm pays  if it upwards the value of h(x)  so if this is some number like 0 7  it predicts the value h of x  And the actual cost label turns out to be y  The cost is going to be -log(h(x)) if y = 1 and -log(1- h(x)) if y = 0  This looks like a pretty complicated function  but let's plot this function to gain some intuition about what it's doing  Let's start off with the case of y = 1  If y = 1  then the cost function is -log(h(x))  And if we plot that  so let's say that the horizontal axis is h(x)  so we know that a hypothesis is going to output a value between 0 and 1  Right  so h(x)  that varies between 0 and 1  If you plot what this cost function looks like  you find that it looks like this  One way to see why the plot looks like this is because if you were to plot log z with z on the horizontal axis  then that looks like that  And it approaches minus infinity  right? So this is what the log function looks like  And this is 0  this is 1  Here  z is of course playing the role of h of x  And so -log z will look like this  Just flipping the sign  minus log z  and we're interested only in the range of when this function goes between zero and one  so get rid of that  And so we're just left with  you know  this part of the curve  and that's what this curve on the left looks like  Now  this cost function has a few interesting and desirable properties  First  you notice that if y is equal to 1 and h(x) is equal to 1  in other words  if the hypothesis exactly predicts h equals 1 and y is exactly equal to what it predicted  then the cost = 0 right? That corresponds to the curve doesn't actually flatten out  The curve is still going  First  notice that if h(x) = 1  if that hypothesis predicts that y = 1 and if indeed y = 1 then the cost = 0  That corresponds to this point down here  right? If h(x) = 1 and we're only considering the case of y = 1 here  But if h(x) = 1 then the cost is down here  is equal to 0  And that's where we'd like it to be because if we correctly predict the output y  then the cost is 0  But now notice also that as h(x) approaches 0  so as the output of a hypothesis approaches 0  the cost blows up and it goes to infinity  And what this does is this captures the intuition that if a hypothesis of 0  that's like saying a hypothesis saying the chance of y equals 1 is equal to 0  It's kinda like our going to our medical patients and saying the probability that you have a malignant tumor  the probability that y=1  is zero  So  it's like absolutely impossible that your tumor is malignant  But if it turns out that the tumor  the patient's tumor  actually is malignant  so if y is equal to one  even after we told them  that the probability of it happening is zero  So it's absolutely impossible for it to be malignant  But if we told them this with that level of certainty and we turn out to be wrong  then we penalize the learning algorithm by a very  very large cost  And that's captured by having this cost go to infinity if y equals 1 and h(x) approaches 0  This slide consider the case of y equals 1  Let's look at what the cost function looks like for y equals 0  If y is equal to 0  then the cost looks like this  it looks like this expression over here  and if you plot the function  -log(1-z)  what you get is the cost function actually looks like this  So it goes from 0 to 1  something like that and so if you plot the cost function for the case of y equals 0  you find that it looks like this  And what this curve does is it now goes up and it goes to plus infinity as h of x goes to 1 because as I was saying  that if y turns out to be equal to 0  But we predicted that y is equal to 1 with almost certainly  probably 1  then we end up paying a very large cost  And conversely  if h of x is equal to 0 and y equals 0  then the hypothesis melted  The protected y of z is equal to 0  and it turns out y is equal to 0  so at this point  the cost function is going to be 0  In this video  we will define the cost function for a single train example  The topic of convexity analysis is now beyond the scope of this course  but it is possible to show that with a particular choice of cost function  this will give a convex optimization problem  Overall cost function j of theta will be convex and local optima free  In the next video we're gonna take these ideas of the cost function for a single training example and develop that further  and define the cost function for the entire training set  And we'll also figure out a simpler way to write it than we have been using so far  and based on that we'll work out grading descent  and that will give us logistic compression algorithm 
KHGxigA8Jko,Simplified Cost Function and Gradient Descent  In this video  we'll figure out a slightly simpler way to write the cost function than we have been using so far  And we'll also figure out how to apply gradient descent to fit the parameters of logistic regression  So  by the end of this  video you know how to implement a fully working version of logistic regression  Here's our cost function for logistic regression  Our overall cost function is 1 over m times the sum over the trading set of the cost of making different predictions on the different examples of labels y i  And this is the cost of a single example that we worked out earlier  And just want to remind you that for classification problems in our training sets  and in fact even for examples  now that our training set y is always equal to zero or one  right? That's sort of part of the mathematical definition of y  Because y is either zero or one  we'll be able to come up with a simpler way to write this cost function  And in particular  rather than writing out this cost function on two separate lines with two separate cases  so y equals one and y equals zero  I'm going to show you a way to take these two lines and compress them into one equation  And this would make it more convenient to write out a cost function and derive gradient descent  Concretely  we can write out the cost function as follows  We say that cost of H(x)  y  I'm gonna write this as -y times log h(x)- (1-y) times log (1-h(x))  And I'll show you in a second that this expression  no  this equation  is an equivalent way  or more compact way  of writing out this definition of the cost function that we have up here  Let's see why that's the case  We know that there are only two possible cases  Y must be zero or one  So let's suppose Y equals one  If y is equal to 1  than this equation is saying that the cost is equal to  well if y is equal to 1  then this thing here is equal to 1  And 1 minus y is going to be equal to 0  right  So if y is equal to 1  then 1 minus y is 1 minus 1  which is therefore 0  So the second term gets multiplied by 0 and goes away  And we're left with only this first term  which is y times log- y times log (h(x))  Y is 1 so that's equal to -log h(x)  And this equation is exactly what we have up here for if y = 1  The other case is if y = 0  And if that's the case  then our writing of the cos function is saying that  well  if y is equal to 0  then this term here would be equal to zero  Whereas 1 minus y  if y is equal to zero would be equal to 1  because 1 minus y becomes 1 minus zero which is just equal to 1  And so the cost function simplifies to just this last term here  right? Because the fist term over here gets multiplied by zero  and so it disappears  and so it's just left with this last term  which is -log (1- h(x))  And you can verify that this term here is just exactly what we had for when y is equal to 0  So this shows that this definition for the cost is just a more compact way of taking both of these expressions  the cases y =1 and y = 0  and writing them in a more convenient form with just one line  We can therefore write all our cost functions for logistic regression as follows  It is this 1 over m of the sum of these cost functions  And plugging in the definition for the cost that we worked out earlier  we end up with this  And we just put the minus sign outside  And why do we choose this particular function  while it looks like there could be other cost functions we could have chosen  Although I won't have time to go into great detail of this in this course  this cost function can be derived from statistics using the principle of maximum likelihood estimation  Which is an idea in statistics for how to efficiently find parameters' data for different models  And it also has a nice property that it is convex  So this is the cost function that essentially everyone uses when fitting logistic regression models  If you don't understand the terms that I just said  if you don't know what the principle of maximum likelihood estimation is  don't worry about it  But it's just a deeper rationale and justification behind this particular cost function than I have time to go into in this class  Given this cost function  in order to fit the parameters  what we're going to do then is try to find the parameters theta that minimize J of theta  So if we try to minimize this  this would give us some set of parameters theta  Finally  if we're given a new example with some set of features x  we can then take the thetas that we fit to our training set and output our prediction as this  And just to remind you  the output of my hypothesis I'm going to interpret as the probability that y is equal to one  And given the input x and parameterized by theta  But just  you can think of this as just my hypothesis as estimating the probability that y is equal to one  So all that remains to be done is figure out how to actually minimize J of theta as a function of theta so that we can actually fit the parameters to our training set  The way we're going to minimize the cost function is using gradient descent  Here's our cost function and if we want to minimize it as a function of theta  here's our usual template for graded descent where we repeatedly update each parameter by taking  updating it as itself minus learning ray alpha times this derivative term  If you know some calculus  feel free to take this term and try to compute the derivative yourself and see if you can simplify it to the same answer that I get  But even if you don't know calculus don't worry about it  If you actually compute this  what you get is this equation  and just write it out here  It's sum from i equals one through m of essentially the error times xij  So if you take this partial derivative term and plug it back in here  we can then write out our gradient descent algorithm as follows  And all I've done is I took the derivative term for the previous slide and plugged it in there  So if you have n features  you would have a parameter vector theta  which with parameters theta 0  theta 1  theta 2  down to theta n  And you will use this update to simultaneously update all of your values of theta  Now  if you take this update rule and compare it to what we were doing for linear regression  You might be surprised to realize that  well  this equation was exactly what we had for linear regression  In fact  if you look at the earlier videos  and look at the update rule  the Gradient Descent rule for linear regression  It looked exactly like what I drew here inside the blue box  So are linear regression and logistic regression different algorithms or not? Well  this is resolved by observing that for logistic regression  what has changed is that the definition for this hypothesis has changed  So as whereas for linear regression  we had h(x) equals theta transpose X  now this definition of h(x) has changed  And is instead now one over one plus e to the negative transpose x  So even though the update rule looks cosmetically identical  because the definition of the hypothesis has changed  this is actually not the same thing as gradient descent for linear regression  In an earlier video  when we were talking about gradient descent for linear regression  we had talked about how to monitor a gradient descent to make sure that it is converging  I usually apply that same method to logistic regression  too to monitor a gradient descent  to make sure it's converging correctly  And hopefully  you can figure out how to apply that technique to logistic regression yourself  When implementing logistic regression with gradient descent  we have all of these different parameter values  theta zero down to theta n  that we need to update using this expression  And one thing we could do is have a for loop  So for i equals zero to n  or for i equals one to n plus one  So update each of these parameter values in turn  But of course rather than using a for loop  ideally we would also use a vector rise implementation  So that a vector rise implementation can update all of these m plus one parameters all in one fell swoop  And to check your own understanding  you might see if you can figure out how to do the vector rise implementation with this algorithm as well  So  now you you know how to implement gradient descents for logistic regression  There was one last idea that we had talked about earlier  for linear regression  which was feature scaling  We saw how feature scaling can help gradient descent converge faster for linear regression  The idea of feature scaling also applies to gradient descent for logistic regression  And yet we have features that are on very different scale  then applying feature scaling can also make grading descent run faster for logistic regression  So that's it  you now know how to implement logistic regression and this is a very powerful  and probably the most widely used  classification algorithm in the world  And you now know how we get it to work for yourself 
SWs4P1VlBbQ,Advanced Optimization  In the last video  we talked about gradient descent for minimizing the cost function J of theta for logistic regression  In this video  I'd like to tell you about some advanced optimization algorithms and some advanced optimization concepts  Using some of these ideas  we'll be able to get logistic regression to run much more quickly than it's possible with gradient descent  And this will also let the algorithms scale much better to very large machine learning problems  such as if we had a very large number of features  Here's an alternative view of what gradient descent is doing  We have some cost function J and we want to minimize it  So what we need to is  we need to write code that can take as input the parameters theta and they can compute two things  J of theta and these partial derivative terms for  you know  J equals 0  1 up to N  Given code that can do these two things  what gradient descent does is it repeatedly performs the following update  Right? So given the code that we wrote to compute these partial derivatives  gradient descent plugs in here and uses that to update our parameters theta  So another way of thinking about gradient descent is that we need to supply code to compute J of theta and these derivatives  and then these get plugged into gradient descents  which can then try to minimize the function for us  For gradient descent  I guess technically you don't actually need code to compute the cost function J of theta  You only need code to compute the derivative terms  But if you think of your code as also monitoring convergence of some such  we'll just think of ourselves as providing code to compute both the cost function and the derivative terms  So  having written code to compute these two things  one algorithm we can use is gradient descent  But gradient descent isn't the only algorithm we can use  And there are other algorithms  more advanced  more sophisticated ones  that  if we only provide them a way to compute these two things  then these are different approaches to optimize the cost function for us  So conjugate gradient BFGS and L-BFGS are examples of more sophisticated optimization algorithms that need a way to compute J of theta  and need a way to compute the derivatives  and can then use more sophisticated strategies than gradient descent to minimize the cost function  The details of exactly what these three algorithms is well beyond the scope of this course  And in fact you often end up spending  you know  many days  or a small number of weeks studying these algorithms  If you take a class and advance the numerical computing  But let me just tell you about some of their properties  These three algorithms have a number of advantages  One is that  with any of this algorithms you usually do not need to manually pick the learning rate alpha  So one way to think of these algorithms is that given is the way to compute the derivative and a cost function  You can think of these algorithms as having a clever inter-loop  And  in fact  they have a clever inter-loop called a line search algorithm that automatically tries out different values for the learning rate alpha and automatically picks a good learning rate alpha so that it can even pick a different learning rate for every iteration  And so then you don't need to choose it yourself  These algorithms actually do more sophisticated things than just pick a good learning rate  and so they often end up converging much faster than gradient descent  These algorithms actually do more sophisticated things than just pick a good learning rate  and so they often end up converging much faster than gradient descent  but detailed discussion of exactly what they do is beyond the scope of this course  In fact  I actually used to have used these algorithms for a long time  like maybe over a decade  quite frequently  and it was only  you know  a few years ago that I actually figured out for myself the details of what conjugate gradient  BFGS and O-BFGS do  So it is actually entirely possible to use these algorithms successfully and apply to lots of different learning problems without actually understanding the inter-loop of what these algorithms do  If these algorithms have a disadvantage  I'd say that the main disadvantage is that they're quite a lot more complex than gradient descent  And in particular  you probably should not implement these algorithms - conjugate gradient  L-BGFS  BFGS - yourself unless you're an expert in numerical computing  Instead  just as I wouldn't recommend that you write your own code to compute square roots of numbers or to compute inverses of matrices  for these algorithms also what I would recommend you do is just use a software library  So  you know  to take a square root what all of us do is use some function that someone else has written to compute the square roots of our numbers  And fortunately  Octave and the closely related language MATLAB - we'll be using that - Octave has a very good  Has a pretty reasonable library implementing some of these advanced optimization algorithms  And so if you just use the built-in library  you know  you get pretty good results  I should say that there is a difference between good and bad implementations of these algorithms  And so  if you're using a different language for your machine learning application  if you're using C  C++  Java  and so on  you might want to try out a couple of different libraries to make sure that you find a good library for implementing these algorithms  Because there is a difference in performance between a good implementation of  you know  contour gradient or LPFGS versus less good implementation of contour gradient or LPFGS  So now let's explain how to use these algorithms  I'm going to do so with an example  Let's say that you have a problem with two parameters equals theta zero and theta one  And let's say your cost function is J of theta equals theta one minus five squared  plus theta two minus five squared  So with this cost function  You know the value for theta 1 and theta 2  If you want to minimize J of theta as a function of theta  The value that minimizes it is going to be theta 1 equals 5  theta 2 equals equals five  Now  again  I know some of you know more calculus than others  but the derivatives of the cost function J turn out to be these two expressions  I've done the calculus  So if you want to apply one of the advanced optimization algorithms to minimize cost function J  So  you know  if we didn't know the minimum was at 5  5  but if you want to have a cost function 5 the minimum numerically using something like gradient descent but preferably more advanced than gradient descent  what you would do is implement an octave function like this  so we implement a cost function  cost function theta function like that  and what this does is that it returns two arguments  the first J-val  is how we would compute the cost function J  And so this says J-val equals  you know  theta one minus five squared plus theta two minus five squared  So it's just computing this cost function over here  And the second argument that this function returns is gradient  So gradient is going to be a two by one vector  and the two elements of the gradient vector correspond to the two partial derivative terms over here  Having implemented this cost function  you would  you can then call the advanced optimization function called the fminunc - it stands for function minimization unconstrained in Octave -and the way you call this is as follows  You set a few options  This is a options as a data structure that stores the options you want  So grant up on  this sets the gradient objective parameter to on  It just means you are indeed going to provide a gradient to this algorithm  I'm going to set the maximum number of iterations to  let's say  one hundred  We're going give it an initial guess for theta  There's a 2 by 1 vector  And then this command calls fminunc  This at symbol presents a pointer to the cost function that we just defined up there  And if you call this  this will compute  you know  will use one of the more advanced optimization algorithms  And if you want to think it as just like gradient descent  But automatically choosing the learning rate alpha for so you don't have to do so yourself  But it will then attempt to use the sort of advanced optimization algorithms  Like gradient descent on steroids  To try to find the optimal value of theta for you  Let me actually show you what this looks like in Octave  So I've written this cost function of theta function exactly as we had it on the previous line  It computes J-val which is the cost function  And it computes the gradient with the two elements being the partial derivatives of the cost function with respect to  you know  the two parameters  theta one and theta two  Now let's switch to my Octave window  I'm gonna type in those commands I had just now  So  options equals optimset  This is the notation for setting my parameters on my options  for my optimization algorithm  Grant option on  maxIter  100 so that says 100 iterations  and I am going to provide the gradient to my algorithm  Let's say initial theta equals zero's two by one  So that's my initial guess for theta  And now I have of theta  function val exit flag equals fminunc constraint  A pointer to the cost function  and provide my initial guess  And the options like so  And if I hit enter this will run the optimization algorithm  And it returns pretty quickly  This funny formatting that's because my line  you know  my code wrapped around  So  this funny thing is just because my command line had wrapped around  But what this says is that numerically renders  you know  think of it as gradient descent on steroids  they found the optimal value of a theta is theta 1 equals 5  theta 2 equals 5  exactly as we're hoping for  The function value at the optimum is essentially 10 to the minus 30  So that's essentially zero  which is also what we're hoping for  And the exit flag is 1  and this shows what the convergence status of this  And if you want you can do help fminunc to read the documentation for how to interpret the exit flag  But the exit flag let's you verify whether or not this algorithm thing has converged  So that's how you run these algorithms in Octave  I should mention  by the way  that for the Octave implementation  this value of theta  your parameter vector of theta  must be in rd for d greater than or equal to 2  So if theta is just a real number  So  if it is not at least a two-dimensional vector or some higher than two-dimensional vector  this fminunc may not work  so and if in case you have a one-dimensional function that you use to optimize  you can look in the octave documentation for fminunc for additional details  So  that's how we optimize our trial example of this simple quick driving cost function  However  we apply this to let's just say progression  In logistic regression we have a parameter vector theta  and I'm going to use a mix of octave notation and sort of math notation  But I hope this explanation will be clear  but our parameter vector theta comprises these parameters theta 0 through theta n because octave indexes  vectors using indexing from 1  you know  theta 0 is actually written theta 1 in octave  theta 1 is gonna be written  So  if theta 2 in octave and that's gonna be a written theta n+1  right? And that's because Octave indexes is vectors starting from index of 1 and so the index of 0  So what we need to do then is write a cost function that captures the cost function for logistic regression  Concretely  the cost function needs to return J-val  which is  you know  J-val as you need some codes to compute J of theta and we also need to give it the gradient  So  gradient 1 is going to be some code to compute the partial derivative in respect to theta 0  the next partial derivative respect to theta 1 and so on  Once again  this is gradient 1  gradient 2 and so on  rather than gradient 0  gradient 1 because octave indexes is vectors starting from one rather than from zero  But the main concept I hope you take away from this slide is  that what you need to do  is write a function that returns the cost function and returns the gradient  And so in order to apply this to logistic regression or even to linear regression  if you want to use these optimization algorithms for linear regression  What you need to do is plug in the appropriate code to compute these things over here  So  now you know how to use these advanced optimization algorithms  Because  using  because for these algorithms  you're using a sophisticated optimization library  it makes the just a little bit more opaque and so just maybe a little bit harder to debug  But because these algorithms often run much faster than gradient descent  often quite typically whenever I have a large machine learning problem  I will use these algorithms instead of using gradient descent  And with these ideas  hopefully  you'll be able to get logistic progression and also linear regression to work on much larger problems  So  that's it for advanced optimization concepts  And in the next and final video on Logistic Regression  I want to tell you how to take the logistic regression algorithm that you already know about and make it work also on multi-class classification problems 
rs84dWPEFGU,Multiclass Classification  One-vs-all  In this video we'll talk about how to get logistic regression to work for multiclass classification problems  And in particular I want to tell you about an algorithm called one-versus-all classification  What's a multiclass classification problem? Here are some examples  Lets say you want a learning algorithm to automatically put your email into different folders or to automatically tag your emails so you might have different folders or different tags for work email  email from your friends  email from your family  and emails about your hobby  And so here we have a classification problem with four classes which we might assign to the classes y = 1  y =2  y =3  and y = 4 too  And another example  for medical diagnosis  if a patient comes into your office with maybe a stuffy nose  the possible diagnosis could be that they're not ill  Maybe that's y = 1  Or they have a cold  2  Or they have a flu  And a third and final example if you are using machine learning to classify the weather  you know maybe you want to decide that the weather is sunny  cloudy  rainy  or snow  or if it's gonna be snow  and so in all of these examples  y can take on a small number of values  maybe one to three  one to four and so on  and these are multiclass classification problems  And by the way  it doesn't really matter whether we index is at 0  1  2  3  or as 1  2  3  4  I tend to index my classes starting from 1 rather than starting from 0  but either way we're off and it really doesn't matter  Whereas previously for a binary classification problem  our data sets look like this  For a multi-class classification problem our data sets may look like this where here I'm using three different symbols to represent our three classes  So the question is given the data set with three classes where this is an example of one class  that's an example of a different class  and that's an example of yet a third class  How do we get a learning algorithm to work for the setting? We already know how to do binary classification using a regression  We know how to you know maybe fit a straight line to set for the positive and negative classes  You see an idea called one-vs-all classification  We can then take this and make it work for multi-class classification as well  Here's how a one-vs-all classification works  And this is also sometimes called one-vs-rest  Let's say we have a training set like that shown on the left  where we have three classes of y equals 1  we denote that with a triangle  if y equals 2  the square  and if y equals three  then the cross  What we're going to do is take our training set and turn this into three separate binary classification problems  I'll turn this into three separate two class classification problems  So let's start with class one which is the triangle  We're gonna essentially create a new sort of fake training set where classes two and three get assigned to the negative class  And class one gets assigned to the positive class  You want to create a new training set like that shown on the right  and we're going to fit a classifier which I'm going to call h subscript theta superscript one of x where here the triangles are the positive examples and the circles are the negative examples  So think of the triangles being assigned the value of one and the circles assigned the value of zero  And we're just going to train a standard logistic regression classifier and maybe that will give us a position boundary that looks like that  Okay? This superscript one here stands for class one  so we're doing this for the triangles of class one  Next we do the same thing for class two  Gonna take the squares and assign the squares as the positive class  and assign everything else  the triangles and the crosses  as a negative class  And then we fit a second logistic regression classifier and call this h of x superscript two  where the superscript two denotes that we're now doing this  treating the square class as the positive class  And maybe we get classified like that  And finally  we do the same thing for the third class and fit a third classifier h super script three of x  and maybe this will give us a decision bounty of the visible cross fire  This separates the positive and negative examples like that  So to summarize  what we've done is  we've fit three classifiers  So  for i = 1  2  3  we'll fit a classifier x super script i subscript theta of x  Thus trying to estimate what is the probability that y is equal to class i  given x and parametrized by theta  Right? So in the first instance for this first one up here  this classifier was learning to recognize the triangles  So it's thinking of the triangles as a positive clause  so x superscript one is essentially trying to estimate what is the probability that the y is equal to one  given that x is parametrized by theta  And similarly  this is treating the square class as a positive class and so it's trying to estimate the probability that y = 2 and so on  So we now have three classifiers  each of which was trained to recognize one of the three classes  Just to summarize  what we've done is we want to train a logistic regression classifier h superscript i of x for each class i to predict the probability that y is equal to i  Finally to make a prediction  when we're given a new input x  and we want to make a prediction  What we do is we just run all three of our classifiers on the input x and we then pick the class i that maximizes the three  So we just basically pick the classifier  I think whichever one of the three classifiers is most confident and so the most enthusiastically says that it thinks it has the right clause  So whichever value of i gives us the highest probability we then predict y to be that value  So that's it for multi-class classification and one-vs-all method  And with this little method you can now take the logistic regression classifier and make it work on multi-class classification problems as well
oCL8hrh95Uc,The Problem of Overfitting  By now  you've seen a couple different learning algorithms  linear regression and logistic regression  They work well for many problems  but when you apply them to certain machine learning applications  they can run into a problem called overfitting that can cause them to perform very poorly  What I'd like to do in this video is explain to you what is this overfitting problem  and in the next few videos after this  we'll talk about a technique called regularization  that will allow us to ameliorate or to reduce this overfitting problem and get these learning algorithms to maybe work much better  So what is overfitting? Let's keep using our running example of predicting housing prices with linear regression where we want to predict the price as a function of the size of the house  One thing we could do is fit a linear function to this data  and if we do that  maybe we get that sort of straight line fit to the data  But this isn't a very good model  Looking at the data  it seems pretty clear that as the size of the housing increases  the housing prices plateau  or kind of flattens out as we move to the right and so this algorithm does not fit the training and we call this problem underfitting  and another term for this is that this algorithm has high bias  Both of these roughly mean that it's just not even fitting the training data very well  The term is kind of a historical or technical one  but the idea is that if a fitting a straight line to the data  then  it's as if the algorithm has a very strong preconception  or a very strong bias that housing prices are going to vary linearly with their size and despite the data to the contrary  Despite the evidence of the contrary is preconceptions still are bias  still closes it to fit a straight line and this ends up being a poor fit to the data  Now  in the middle  we could fit a quadratic functions enter and  with this data set  we fit the quadratic function  maybe  we get that kind of curve and  that works pretty well  And  at the other extreme  would be if we were to fit  say a fourth other polynomial to the data  So  here we have five parameters  theta zero through theta four  and  with that  we can actually fill a curve that process through all five of our training examples  You might get a curve that looks like this  That  on the one hand  seems to do a very good job fitting the training set and  that is processed through all of my data  at least  But  this is still a very wiggly curve  right? So  it's going up and down all over the place  and  we don't actually think that's such a good model for predicting housing prices  So  this problem we call overfitting  and  another term for this is that this algorithm has high variance   The term high variance is another historical or technical one  But  the intuition is that  if we're fitting such a high order polynomial  then  the hypothesis can fit  you know  it's almost as if it can fit almost any function and this face of possible hypothesis is just too large  it's too variable  And we don't have enough data to constrain it to give us a good hypothesis so that's called overfitting  And in the middle  there isn't really a name but I'm just going to write  you know  just right  Where a second degree polynomial  quadratic function seems to be just right for fitting this data  To recap a bit the problem of over fitting comes when if we have too many features  then to learn hypothesis may fit the training side very well  So  your cost function may actually be very close to zero or may be even zero exactly  but you may then end up with a curve like this that  you know tries too hard to fit the training set  so that it even fails to generalize to new examples and fails to predict prices on new examples as well  and here the term generalized refers to how well a hypothesis applies even to new examples  That is to data to houses that it has not seen in the training set  On this slide  we looked at over fitting for the case of linear regression  A similar thing can apply to logistic regression as well  Here is a logistic regression example with two features X1 and x2  One thing we could do  is fit logistic regression with just a simple hypothesis like this  where  as usual  G is my sigmoid function  And if you do that  you end up with a hypothesis  trying to use  maybe  just a straight line to separate the positive and the negative examples  And this doesn't look like a very good fit to the hypothesis  So  once again  this is an example of underfitting or of the hypothesis having high bias  In contrast  if you were to add to your features these quadratic terms  then  you could get a decision boundary that might look more like this  And  you know  that's a pretty good fit to the data  Probably  about as good as we could get  on this training set  And  finally  at the other extreme  if you were to fit a very high-order polynomial  if you were to generate lots of high-order polynomial terms of speeches  then  logistical regression may contort itself  may try really hard to find a decision boundary that fits your training data or go to great lengths to contort itself  to fit every single training example well  And  you know  if the features X1 and X2 offer predicting  maybe  the cancer to the  you know  cancer is a malignant  benign breast tumors  This doesn't  this really doesn't look like a very good hypothesis  for making predictions  And so  once again  this is an instance of overfitting and  of a hypothesis having high variance and not really  and  being unlikely to generalize well to new examples  Later  in this course  when we talk about debugging and diagnosing things that can go wrong with learning algorithms  we'll give you specific tools to recognize when overfitting and  also  when underfitting may be occurring  But  for now  lets talk about the problem of  if we think overfitting is occurring  what can we do to address it? In the previous examples  we had one or two dimensional data so  we could just plot the hypothesis and see what was going on and select the appropriate degree polynomial  So  earlier for the housing prices example  we could just plot the hypothesis and  you know  maybe see that it was fitting the sort of very wiggly function that goes all over the place to predict housing prices  And we could then use figures like these to select an appropriate degree polynomial  So plotting the hypothesis  could be one way to try to decide what degree polynomial to use  But that doesn't always work  And  in fact more often we may have learning problems that where we just have a lot of features  And there is not just a matter of selecting what degree polynomial  And  in fact  when we have so many features  it also becomes much harder to plot the data and it becomes much harder to visualize it  to decide what features to keep or not  So concretely  if we're trying predict housing prices sometimes we can just have a lot of different features  And all of these features seem  you know  maybe they seem kind of useful  But  if we have a lot of features  and  very little training data  then  over fitting can become a problem  In order to address over fitting  there are two main options for things that we can do  The first option is  to try to reduce the number of features  Concretely  one thing we could do is manually look through the list of features  and  use that to try to decide which are the more important features  and  therefore  which are the features we should keep  and  which are the features we should throw out  Later in this course  where also talk about model selection algorithms  Which are algorithms for automatically deciding which features to keep and  which features to throw out  This idea of reducing the number of features can work well  and  can reduce over fitting  And  when we talk about model selection  we'll go into this in much greater depth  But  the disadvantage is that  by throwing away some of the features  is also throwing away some of the information you have about the problem  For example  maybe  all of those features are actually useful for predicting the price of a house  so  maybe  we don't actually want to throw some of our information or throw some of our features away  The second option  which we'll talk about in the next few videos  is regularization  Here  we're going to keep all the features  but we're going to reduce the magnitude or the values of the parameters theta J  And  this method works well  we'll see  when we have a lot of features  each of which contributes a little bit to predicting the value of Y  like we saw in the housing price prediction example  Where we could have a lot of features  each of which are  you know  somewhat useful  so  maybe  we don't want to throw them away  So  this subscribes the idea of regularization at a very high level  And  I realize that  all of these details probably don't make sense to you yet  But  in the next video  we'll start to formulate exactly how to apply regularization and  exactly what regularization means  And  then we'll start to figure out  how to use this  to make how learning algorithms work well and avoid overfitting 
PfCkp4X2gV0,Cost Function  In this video  I'd like to convey to you  the main intuitions behind how regularization works  And  we'll also write down the cost function that we'll use  when we were using regularization  With the hand drawn examples that we have on these slides  I think I'll be able to convey part of the intuition  But  an even better way to see for yourself  how regularization works  is if you implement it  and  see it work for yourself  And  if you do the appropriate exercises after this  you get the chance to self see regularization in action for yourself  So  here is the intuition  In the previous video  we saw that  if we were to fit a quadratic function to this data  it gives us a pretty good fit to the data  Whereas  if we were to fit an overly high order degree polynomial  we end up with a curve that may fit the training set very well  but  really not be a  but overfit the data poorly  and  not generalize well  Consider the following  suppose we were to penalize  and  make the parameters theta 3 and theta 4 really small  Here's what I mean  here is our optimization objective  or here is our optimization problem  where we minimize our usual squared error cause function  Let's say I take this objective and modify it and add to it  plus 1000 theta 3 squared  plus 1000 theta 4 squared  1000 I am just writing down as some huge number  Now  if we were to minimize this function  the only way to make this new cost function small is if theta 3 and data 4 are small  right? Because otherwise  if you have a thousand times theta 3  this new cost functions gonna be big  So when we minimize this new function we are going to end up with theta 3 close to 0 and theta 4 close to 0  and as if we're getting rid of these two terms over there  And if we do that  well then  if theta 3 and theta 4 close to 0 then we are being left with a quadratic function  and  so  we end up with a fit to the data  that's  you know  quadratic function plus maybe  tiny contributions from small terms  theta 3  theta 4  that they may be very close to 0  And  so  we end up with essentially  a quadratic function  which is good  Because this is a much better hypothesis  In this particular example  we looked at the effect of penalizing two of the parameter values being large  More generally  here is the idea behind regularization  The idea is that  if we have small values for the parameters  then  having small values for the parameters  will somehow  will usually correspond to having a simpler hypothesis  So  for our last example  we penalize just theta 3 and theta 4 and when both of these were close to zero  we wound up with a much simpler hypothesis that was essentially a quadratic function  But more broadly  if we penalize all the parameters usually that  we can think of that  as trying to give us a simpler hypothesis as well because when  you know  these parameters are as close as you in this example  that gave us a quadratic function  But more generally  it is possible to show that having smaller values of the parameters corresponds to usually smoother functions as well for the simpler  And which are therefore  also  less prone to overfitting  I realize that the reasoning for why having all the parameters be small  Why that corresponds to a simpler hypothesis  I realize that reasoning may not be entirely clear to you right now  And it is kind of hard to explain unless you implement yourself and see it for yourself  But I hope that the example of having theta 3 and theta 4 be small and how that gave us a simpler hypothesis  I hope that helps explain why  at least give some intuition as to why this might be true  Lets look at the specific example  For housing price prediction we may have our hundred features that we talked about where may be x1 is the size  x2 is the number of bedrooms  x3 is the number of floors and so on  And we may we may have a hundred features  And unlike the polynomial example  we don't know  right  we don't know that theta 3  theta 4  are the high order polynomial terms  So  if we have just a bag  if we have just a set of a hundred features  it's hard to pick in advance which are the ones that are less likely to be relevant  So we have a hundred or a hundred one parameters  And we don't know which ones to pick  we don't know which parameters to try to pick  to try to shrink  So  in regularization  what we're going to do  is take our cost function  here's my cost function for linear regression  And what I'm going to do is  modify this cost function to shrink all of my parameters  because  you know  I don't know which one or two to try to shrink  So I am going to modify my cost function to add a term at the end  Like so we have square brackets here as well  When I add an extra regularization term at the end to shrink every single parameter and so this term we tend to shrink all of my parameters theta 1  theta 2  theta 3 up to theta 100  By the way  by convention the summation here starts from one so I am not actually going penalize theta zero being large  That sort of the convention that  the sum I equals one through N  rather than I equals zero through N  But in practice  it makes very little difference  and  whether you include  you know  theta zero or not  in practice  make very little difference to the results  But by convention  usually  we regularize only theta through theta 100  Writing down our regularized optimization objective  our regularized cost function again  Here it is  Here's J of theta where  this term on the right is a regularization term and lambda here is called the regularization parameter and what lambda does  is it controls a trade off between two different goals  The first goal  capture it by the first goal objective  is that we would like to train  is that we would like to fit the training data well  We would like to fit the training set well  And the second goal is  we want to keep the parameters small  and that's captured by the second term  by the regularization objective  And by the regularization term  And what lambda  the regularization parameter does is the controls the trade of between these two goals  between the goal of fitting the training set well and the goal of keeping the parameter plan small and therefore keeping the hypothesis relatively simple to avoid overfitting  For our housing price prediction example  whereas  previously  if we had fit a very high order polynomial  we may have wound up with a very  sort of wiggly or curvy function like this  If you still fit a high order polynomial with all the polynomial features in there  but instead  you just make sure  to use this sole of regularized objective  then what you can get out is in fact a curve that isn't quite a quadratic function  but is much smoother and much simpler and maybe a curve like the magenta line that  you know  gives a much better hypothesis for this data  Once again  I realize it can be a bit difficult to see why strengthening the parameters can have this effect  but if you implement yourselves with regularization you will be able to see this effect firsthand  In regularized linear regression  if the regularization parameter monitor is set to be very large  then what will happen is we will end up penalizing the parameters theta 1  theta 2  theta 3  theta 4 very highly  That is  if our hypothesis is this is one down at the bottom  And if we end up penalizing theta 1  theta 2  theta 3  theta 4 very heavily  then we end up with all of these parameters close to zero  right? Theta 1 will be close to zero  theta 2 will be close to zero  Theta three and theta four will end up being close to zero  And if we do that  it's as if we're getting rid of these terms in the hypothesis so that we're just left with a hypothesis that will say that  It says that  well  housing prices are equal to theta zero  and that is akin to fitting a flat horizontal straight line to the data  And this is an example of underfitting  and in particular this hypothesis  this straight line it just fails to fit the training set well  It's just a fat straight line  it doesn't go  you know  go near  It doesn't go anywhere near most of the training examples  And another way of saying this is that this hypothesis has too strong a preconception or too high bias that housing prices are just equal to theta zero  and despite the clear data to the contrary  you know chooses to fit a sort of  flat line  just a flat horizontal line  I didn't draw that very well  This just a horizontal flat line to the data  So for regularization to work well  some care should be taken  to choose a good choice for the regularization parameter lambda as well  And when we talk about multi-selection later in this course  we'll talk about a way  a variety of ways for automatically choosing the regularization parameter lambda as well  So  that's the idea of the high regularization and the cost function reviews in order to use regularization In the next two videos  lets take these ideas and apply them to linear regression and to logistic regression  so that we can then get them to avoid overfitting 
ExLO-_btIGM,Regularized Linear Regression  For linear regression  we have previously worked out two learning algorithms  One based on gradient descent and one based on the normal equation  In this video  we'll take those two algorithms and generalize them to the case of regularized linear regression  Here's the optimization objective that we came up with last time for regularized linear regression  This first part is our usual objective for linear regression  And we now have this additional regularization term  where lambda is our regularization parameter  and we like to find parameters theta that minimizes this cost function  this regularized cost function  J of theta  Previously  we were using gradient descent for the original cost function without the regularization term  And we had the following algorithm  for regular linear regression  without regularization  we would repeatedly update the parameters theta J as follows for J equals 0  1  2  up through n  Let me take this and just write the case for theta 0 separately  So I'm just going to write the update for theta 0 separately than for the update for the parameters 1  2  3  and so on up to n  And so this is  I haven't changed anything yet  right  This is just writing the update for theta 0 separately from the updates for theta 1  theta 2  theta 3  up to theta n  And the reason I want to do this is you may remember that for our regularized linear regression  we penalize the parameters theta 1  theta 2  and so on up to theta n  But we don't penalize theta 0  So  when we modify this algorithm for regularized linear regression  we're going to end up treating theta zero slightly differently  Concretely  if we want to take this algorithm and modify it to use the regular rise objective  all we need to do is take this term at the bottom and modify it as follows  We'll take this term and add minus lambda over m times theta j  And if you implement this  then you have gradient descent for trying to minimize the regularized cost function  j of theta  And concretely  I'm not gonna do the calculus to prove it  but concretely if you look at this term  this term hat I've written in square brackets  if you know calculus it's possible to prove that that term is the partial derivative with respect to J of theta using the new definition of J of theta with the regularization term  And similarly  this term up on top which I'm drawing the cyan box  that's still the partial derivative respect of theta zero of J of theta  If you look at the update for theta j  it's possible to show something very interesting  Concretely  theta j gets updated as theta j minus alpha times  and then you have this other term here that depends on theta J  So if you group all the terms together that depend on theta j  you can show that this update can be written equivalently as follows  And all I did was add theta j here is  so theta j times 1  And this term is  right  lambda over m  there's also an alpha here  so you end up with alpha lambda over m multiplied into theta j  And this term here  1 minus alpha times lambda m  is a pretty interesting term  It has a pretty interesting effect  Concretely this term  1 minus alpha times lambda over m  is going to be a number that is usually a little bit less than one  because alpha times lambda over m is going to be positive  and usually if your learning rate is small and if m is large  this is usually pretty small  So this term here is gonna be a number that's usually a little bit less than 1  so think of it as a number like 0 99  let's say  And so the effect of our update to theta j is  we're going to say that theta j gets replaced by theta j times 0 99  right? So theta j times 0 99 has the effect of shrinking theta j a little bit towards zero  So this makes theta j a bit smaller  And more formally  this makes the square norm of theta j a little bit smaller  And then after that  the second term here  that's actually exactly the same as the original gradient descent update that we had  before we added all this regularization stuff  So  hopefully this gradient descent  hopefully this update makes sense  When we're using a regularized linear regression and what we're doing is on every iteration we're multiplying theta j by a number that's a little bit less then one  so its shrinking the parameter a little bit  and then we're performing a similar update as before  Of course that's just the intuition behind what this particular update is doing  Mathematically what it's doing is it's exactly gradient descent on the cost function J of theta that we defined on the previous slide that uses the regularization term  Gradient descent was just one of our two algorithms for fitting a linear regression model  The second algorithm was the one based on the normal equation  where what we did was we created the design matrix X where each row corresponded to a separate training example  And we created a vector y  so this is a vector  that's an m dimensional vector  And that contained the labels from my training set  So whereas X is an m by (n+1) dimensional matrix  y is an m dimensional vector  And in order to minimize the cost function J  we found that one way to do so is to set theta to be equal to this  Right  you have X transpose X  inverse  X transpose Y  I'm leaving room here to fill in stuff of course  And what this value for theta does is this minimizes the cost function J of theta  when we were not using regularization  Now that we are using regularization  if you were to derive what the minimum is  and just to give you a sense of how to derive the minimum  the way you derive it is you take partial derivatives with respect to each parameter  Set this to zero  and then do a bunch of math and you can then show that it's a formula like this that minimizes the cost function  And concretely  if you are using regularization  then this formula changes as follows  Inside this parenthesis  you end up with a matrix like this  0  1  1  1  and so on  1  until the bottom  So this thing over here is a matrix whose upper left-most entry is 0  There are ones on the diagonals  and then zeros everywhere else in this matrix  Because I'm drawing this rather sloppily  But as a example  if n = 2  then this matrix is going to be a three by three matrix  More generally  this matrix is an (n+1) by (n+1) dimensional matrix  So if n = 2  then that matrix becomes something that looks like this  It would be 0  and then 1s on the diagonals  and then 0s on the rest of the diagonals  And once again  I'm not going to show this derivation  which is frankly somewhat long and involved  but it is possible to prove that if you are using the new definition of J of theta  with the regularization objective  then this new formula for theta is the one that we give you  the global minimum of J of theta  So finally I want to just quickly describe the issue of non-invertibility  This is relatively advanced material  so you should consider this as optional  And feel free to skip it  or if you listen to it and positive it doesn't really make sense  don't worry about it either  But earlier when I talked about the normal equation method  we also had an optional video on the non-invertibility issue  So this is another optional part to this  sort of an add-on to that earlier optional video on non-invertibility  Now  consider a setting where m  the number of examples  is less than or equal to n  the number of features  If you have fewer examples than features  than this matrix  X transpose X will be non-invertible  or singular  Or the other term for this is the matrix will be degenerate  And if you implement this in Octave anyway and you use the pinv function to take the pseudo inverse  it will kind of do the right thing  but it's not clear that it would give you a very good hypothesis  even though numerically the Octave pinv function will give you a result that kinda makes sense  But if you were doing this in a different language  and if you were taking just the regular inverse  which in Octave denoted with the function inv  we're trying to take the regular inverse of X transpose X  Then in this setting  you find that X transpose X is singular  is non-invertible  and if you're doing this in different program language and using some linear algebra library to try to take the inverse of this matrix  it just might not work because that matrix is non-invertible or singular  Fortunately  regularization also takes care of this for us  And concretely  so long as the regularization parameter lambda is strictly greater than 0  it is actually possible to prove that this matrix  X transpose X plus lambda times this funny matrix here  it is possible to prove that this matrix will not be singular and that this matrix will be invertible  So using regularization also takes care of any non-invertibility issues of the X transpose X matrix as well  So you now know how to implement regularized linear regression  Using this you'll be able to avoid overfitting even if you have lots of features in a relatively small training set  And this should let you get linear regression to work much better for many problems  In the next video we'll take this regularization idea and apply it to logistic regression  So that you'd be able to get logistic regression to avoid overfitting and perform much better as well 
SP5JZXBIQ9c,Regularized Logistic Regression  For logistic regression  we previously talked about two types of optimization algorithms  We talked about how to use gradient descent to optimize as cost function J of theta  And we also talked about advanced optimization methods  Ones that require that you provide a way to compute your cost function J of theta and that you provide a way to compute the derivatives  In this video  we'll show how you can adapt both of those techniques  both gradient descent and the more advanced optimization techniques in order to have them work for regularized logistic regression  So  here's the idea  We saw earlier that Logistic Regression can also be prone to overfitting if you fit it with a very  sort of  high order polynomial features like this  Where G is the sigmoid function and in particular you end up with a hypothesis  you know  whose decision bound to be just sort of an overly complex and extremely contortive function that really isn't such a great hypothesis for this training set  and more generally if you have logistic regression with a lot of features  Not necessarily polynomial ones  but just with a lot of features you can end up with overfitting  This was our cost function for logistic regression  And if we want to modify it to use regularization  all we need to do is add to it the following term plus londer over 2M  sum from J equals 1  and as usual sum from J equals 1  Rather than the sum from J equals 0  of theta J squared  And this has to effect therefore  of penalizing the parameters theta 1 theta 2 and so on up to theta N from being too large  And if you do this  then it will the have the effect that even though you're fitting a very high order polynomial with a lot of parameters  So long as you apply regularization and keep the parameters small you're more likely to get a decision boundary  You know  that maybe looks more like this  It looks more reasonable for separating the positive and the negative examples  So  when using regularization even when you have a lot of features  the regularization can help take care of the overfitting problem  How do we actually implement this? Well  for the original gradient descent algorithm  this was the update we had  We will repeatedly perform the following update to theta J  This slide looks a lot like the previous one for linear regression  But what I'm going to do is write the update for theta 0 separately  So  the first line is for update for theta 0 and a second line is now my update for theta 1 up to theta N  Because I'm going to treat theta 0 separately  And in order to modify this algorithm  to use a regularized cos function  all I need to do is pretty similar to what we did for linear regression is actually to just modify this second update rule as follows  And  once again  this  you know  cosmetically looks identical what we had for linear regression  But of course is not the same algorithm as we had  because now the hypothesis is defined using this  So this is not the same algorithm as regularized linear regression  Because the hypothesis is different  Even though this update that I wrote down  It actually looks cosmetically the same as what we had earlier  We're working out gradient descent for regularized linear regression  And of course  just to wrap up this discussion  this term here in the square brackets  so this term here  this term is  of course  the new partial derivative for respect of theta J of the new cost function J of theta  Where J of theta here is the cost function we defined on a previous slide that does use regularization  So  that's gradient descent for regularized linear regression  Let's talk about how to get regularized linear regression to work using the more advanced optimization methods  And just to remind you for those methods what we needed to do was to define the function that's called the cost function  that takes us input the parameter vector theta and once again in the equations we've been writing here we used 0 index vectors  So we had theta 0 up to theta N  But because Octave indexes the vectors starting from 1  Theta 0 is written in Octave as theta 1  Theta 1 is written in Octave as theta 2  and so on down to theta N plus 1  And what we needed to do was provide a function  Let's provide a function called cost function that we would then pass in to what we have  what we saw earlier  We will use the fminunc and then you know at cost function  and so on  right  But the F min  u and c was the F min unconstrained and this will work with fminunc was what will take the cost function and minimize it for us  So the two main things that the cost function needed to return were first J-val  And for that  we need to write code to compute the cost function J of theta  Now  when we're using regularized logistic regression  of course the cost function j of theta changes and  in particular  now a cost function needs to include this additional regularization term at the end as well  So  when you compute j of theta be sure to include that term at the end  And then  the other thing that this cost function thing needs to derive with a gradient  So gradient one needs to be set to the partial derivative of J of theta with respect to theta zero  gradient two needs to be set to that  and so on  Once again  the index is off by one  Right  because of the indexing from one Octave users  And looking at these terms  This term over here  We actually worked this out on a previous slide is actually equal to this  It doesn't change  Because the derivative for theta zero doesn't change  Compared to the version without regularization  And the other terms do change  And in particular the derivative respect to theta one  We worked this out on the previous slide as well  Is equal to  you know  the original term and then minus londer M times theta 1  Just so we make sure we pass this correctly  And we can add parentheses here  Right  so the summation doesn't extend  And similarly  you know  this other term here looks like this  with this additional term that we had on the previous slide  that corresponds to the gradient from their regularization objective  So if you implement this cost function and pass this into fminunc or to one of those advanced optimization techniques  that will minimize the new regularized cost function J of theta  And the parameters you get out will be the ones that correspond to logistic regression with regularization  So  now you know how to implement regularized logistic regression  When I walk around Silicon Valley  I live here in Silicon Valley  there are a lot of engineers that are frankly  making a ton of money for their companies using machine learning algorithms  And I know we've only been  you know  studying this stuff for a little while  But if you understand linear regression  the advanced optimization algorithms and regularization  by now  frankly  you probably know quite a lot more machine learning than many  certainly now  but you probably know quite a lot more machine learning right now than frankly  many of the Silicon Valley engineers out there having very successful careers  You know  making tons of money for the companies  Or building products using machine learning algorithms  So  congratulations  You've actually come a long ways  And you can actually  you actually know enough to apply this stuff and get to work for many problems  So congratulations for that  But of course  there's still a lot more that we want to teach you  and in the next set of videos after this  we'll start to talk about a very powerful cause of non-linear classifier  So whereas linear regression  logistic regression  you know  you can form polynomial terms  but it turns out that there are much more powerful nonlinear quantifiers that can then sort of polynomial regression  And in the next set of videos after this one  I'll start telling you about them  So that you have even more powerful learning algorithms than you have now to apply to different problems Regularized Logistic Regression
d1DG-wHrlCs,"Non-linear Hypotheses  In this and in the next set of videos  I'd like to tell you about a learning algorithm called a Neural Network  We're going to first talk about the representation and then in the next set of videos talk about learning algorithms for it  Neutral networks is actually a pretty old idea  but had fallen out of favor for a while  But today  it is the state of the art technique for many different machine learning problems  So why do we need yet another learning algorithm? We already have linear regression and we have logistic regression  so why do we need  you know  neural networks? In order to motivate the discussion of neural networks  let me start by showing you a few examples of machine learning problems where we need to learn complex non-linear hypotheses  Consider a supervised learning classification problem where you have a training set like this  If you want to apply logistic regression to this problem  one thing you could do is apply logistic regression with a lot of nonlinear features like that  So here  g as usual is the sigmoid function  and we can include lots of polynomial terms like these  And  if you include enough polynomial terms then  you know  maybe you can get a hypotheses that separates the positive and negative examples  This particular method works well when you have only  say  two features - x1 and x2 - because you can then include all those polynomial terms of x1 and x2  But for many interesting machine learning problems would have a lot more features than just two  We've been talking for a while about housing prediction  and suppose you have a housing classification problem rather than a regression problem  like maybe if you have different features of a house  and you want to predict what are the odds that your house will be sold within the next six months  so that will be a classification problem  And as we saw we can come up with quite a lot of features  maybe a hundred different features of different houses  For a problem like this  if you were to include all the quadratic terms  all of these  even all of the quadratic that is the second or the polynomial terms  there would be a lot of them  There would be terms like x1 squared  x1x2  x1x3  you know  x1x4 up to x1x100 and then you have x2 squared  x2x3 and so on  And if you include just the second order terms  that is  the terms that are a product of  you know  two of these terms  x1 times x1 and so on  then  for the case of n equals 100  you end up with about five thousand features  And  asymptotically  the number of quadratic features grows roughly as order n squared  where n is the number of the original features  like x1 through x100 that we had  And its actually closer to n squared over two  So including all the quadratic features doesn't seem like it's maybe a good idea  because that is a lot of features and you might up overfitting the training set  and it can also be computationally expensive  you know  to be working with that many features  One thing you could do is include only a subset of these  so if you include only the features x1 squared  x2 squared  x3 squared  up to maybe x100 squared  then the number of features is much smaller  Here you have only 100 such quadratic features  but this is not enough features and certainly won't let you fit the data set like that on the upper left  In fact  if you include only these quadratic features together with the original x1  and so on  up to x100 features  then you can actually fit very interesting hypotheses  So  you can fit things like  you know  access a line of the ellipses like these  but you certainly cannot fit a more complex data set like that shown here  So 5000 features seems like a lot  if you were to include the cubic  or third order known of each others  the x1  x2  x3  You know  x1 squared  x2  x10 and x11  x17 and so on  You can imagine there are gonna be a lot of these features  In fact  they are going to be order and cube such features and if any is 100 you can compute that  you end up with on the order of about 170 000 such cubic features and so including these higher auto-polynomial features when your original feature set end is large this really dramatically blows up your feature space and this doesn't seem like a good way to come up with additional features with which to build none many classifiers when n is large  For many machine learning problems  n will be pretty large  Here's an example  Let's consider the problem of computer vision  And suppose you want to use machine learning to train a classifier to examine an image and tell us whether or not the image is a car  Many people wonder why computer vision could be difficult  I mean when you and I look at this picture it is so obvious what this is  You wonder how is it that a learning algorithm could possibly fail to know what this picture is  To understand why computer vision is hard let's zoom into a small part of the image like that area where the little red rectangle is  It turns out that where you and I see a car  the computer sees that  What it sees is this matrix  or this grid  of pixel intensity values that tells us the brightness of each pixel in the image  So the computer vision problem is to look at this matrix of pixel intensity values  and tell us that these numbers represent the door handle of a car  Concretely  when we use machine learning to build a car detector  what we do is we come up with a label training set  with  let's say  a few label examples of cars and a few label examples of things that are not cars  then we give our training set to the learning algorithm trained a classifier and then  you know  we may test it and show the new image and ask  \""What is this new thing?\""  And hopefully it will recognize that that is a car  To understand why we need nonlinear hypotheses  let's take a look at some of the images of cars and maybe non-cars that we might feed to our learning algorithm  Let's pick a couple of pixel locations in our images  so that's pixel one location and pixel two location  and let's plot this car  you know  at the location  at a certain point  depending on the intensities of pixel one and pixel two  And let's do this with a few other images  So let's take a different example of the car and you know  look at the same two pixel locations and that image has a different intensity for pixel one and a different intensity for pixel two  So  it ends up at a different location on the figure  And then let's plot some negative examples as well  That's a non-car  that's a non-car   And if we do this for more and more examples using the pluses to denote cars and minuses to denote non-cars  what we'll find is that the cars and non-cars end up lying in different regions of the space  and what we need therefore is some sort of non-linear hypotheses to try to separate out the two classes  What is the dimension of the feature space? Suppose we were to use just 50 by 50 pixel images  Now that suppose our images were pretty small ones  just 50 pixels on the side  Then we would have 2500 pixels  and so the dimension of our feature size will be N equals 2500 where our feature vector x is a list of all the pixel testings  you know  the pixel brightness of pixel one  the brightness of pixel two  and so on down to the pixel brightness of the last pixel where  you know  in a typical computer representation  each of these may be values between say 0 to 255 if it gives us the grayscale value  So we have n equals 2500  and that's if we were using grayscale images  If we were using RGB images with separate red  green and blue values  we would have n equals 7500  So  if we were to try to learn a nonlinear hypothesis by including all the quadratic features  that is all the terms of the form  you know  Xi times Xj  while with the 2500 pixels we would end up with a total of three million features  And that's just too large to be reasonable  the computation would be very expensive to find and to represent all of these three million features per training example  So  simple logistic regression together with adding in maybe the quadratic or the cubic features - that's just not a good way to learn complex nonlinear hypotheses when n is large because you just end up with too many features  In the next few videos  I would like to tell you about Neural Networks  which turns out to be a much better way to learn complex hypotheses  complex nonlinear hypotheses even when your input feature space  even when n is large  And along the way I'll also get to show you a couple of fun videos of historically important applications of Neural networks as well that I hope those videos that we'll see later will be fun for you to watch as well "
fV2k2ivttL0,Neurons and the Brain  Neural Networks are a pretty old algorithm that was originally motivated by the goal of having machines that can mimic the brain  Now in this class  of course I'm teaching Neural Networks to you because they work really well for different machine learning problems and not  certainly not because they're logically motivated  In this video  I'd like to give you some of the background on Neural Networks  So that we can get a sense of what we can expect them to do  Both in the sense of applying them to modern day machinery problems  as well as for those of you that might be interested in maybe the big AI dream of someday building truly intelligent machines  Also  how Neural Networks might pertain to that  The origins of Neural Networks was as algorithms that try to mimic the brain and those a sense that if we want to build learning systems while why not mimic perhaps the most amazing learning machine we know about  which is perhaps the brain  Neural Networks came to be very widely used throughout the 1980's and 1990's and for various reasons as popularity diminished in the late 90's  But more recently  Neural Networks have had a major recent resurgence  One of the reasons for this resurgence is that Neural Networks are computationally some what more expensive algorithm and so  it was only  you know  maybe somewhat more recently that computers became fast enough to really run large scale Neural Networks and because of that as well as a few other technical reasons which we'll talk about later  modern Neural Networks today are the state of the art technique for many applications  So  when you think about mimicking the brain while one of the human brain does tell me same things  right? The brain can learn to see process images than to hear  learn to process our sense of touch  We can  you know  learn to do math  learn to do calculus  and the brain does so many different and amazing things  It seems like if you want to mimic the brain it seems like you have to write lots of different pieces of software to mimic all of these different fascinating  amazing things that the brain tell us  but does is this fascinating hypothesis that the way the brain does all of these different things is not worth like a thousand different programs  but instead  the way the brain does it is worth just a single learning algorithm  This is just a hypothesis but let me share with you some of the evidence for this  This part of the brain  that little red part of the brain  is your auditory cortex and the way you're understanding my voice now is your ear is taking the sound signal and routing the sound signal to your auditory cortex and that's what's allowing you to understand my words  Neuroscientists have done the following fascinating experiments where you cut the wire from the ears to the auditory cortex and you re-wire  in this case an animal's brain  so that the signal from the eyes to the optic nerve eventually gets routed to the auditory cortex  If you do this it turns out  the auditory cortex will learn to see  And this is in every single sense of the word see as we know it  So  if you do this to the animals  the animals can perform visual discrimination task and as they can look at images and make appropriate decisions based on the images and they're doing it with that piece of brain tissue  Here's another example  That red piece of brain tissue is your somatosensory cortex  That's how you process your sense of touch  If you do a similar re-wiring process then the somatosensory cortex will learn to see  Because of this and other similar experiments  these are called neuro-rewiring experiments  There's this sense that if the same piece of physical brain tissue can process sight or sound or touch then maybe there is one learning algorithm that can process sight or sound or touch  And instead of needing to implement a thousand different programs or a thousand different algorithms to do  you know  the thousand wonderful things that the brain does  maybe what we need to do is figure out some approximation or to whatever the brain's learning algorithm is and implement that and that the brain learned by itself how to process these different types of data  To a surprisingly large extent  it seems as if we can plug in almost any sensor to almost any part of the brain and so  within the reason  the brain will learn to deal with it  Here are a few more examples  On the upper left is an example of learning to see with your tongue  The way it works is--this is actually a system called BrainPort undergoing FDA trials now to help blind people see--but the way it works is  you strap a grayscale camera to your forehead  facing forward  that takes the low resolution grayscale image of what's in front of you and you then run a wire to an array of electrodes that you place on your tongue so that each pixel gets mapped to a location on your tongue where maybe a high voltage corresponds to a dark pixel and a low voltage corresponds to a bright pixel and  even as it does today  with this sort of system you and I will be able to learn to see  you know  in tens of minutes with our tongues  Here's a second example of human echo location or human sonar  So there are two ways you can do this  You can either snap your fingers  or click your tongue  I can't do that very well  But there are blind people today that are actually being trained in schools to do this and learn to interpret the pattern of sounds bouncing off your environment - that's sonar  So  if after you search on YouTube  there are actually videos of this amazing kid who tragically because of cancer had his eyeballs removed  so this is a kid with no eyeballs  But by snapping his fingers  he can walk around and never hit anything  He can ride a skateboard  He can shoot a basketball into a hoop and this is a kid with no eyeballs  Third example is the Haptic Belt where if you have a strap around your waist  ring up buzzers and always have the northmost one buzzing  You can give a human a direction sense similar to maybe how birds can  you know  sense where north is  And  some of the bizarre example  but if you plug a third eye into a frog  the frog will learn to use that eye as well  So  it's pretty amazing to what extent is as if you can plug in almost any sensor to the brain and the brain's learning algorithm will just figure out how to learn from that data and deal with that data  And there's a sense that if we can figure out what the brain's learning algorithm is  and  you know  implement it or implement some approximation to that algorithm on a computer  maybe that would be our best shot at  you know  making real progress towards the AI  the artificial intelligence dream of someday building truly intelligent machines  Now  of course  I'm not teaching Neural Networks  you know  just because they might give us a window into this far-off AI dream  even though I'm personally  that's one of the things that I personally work on in my research life  But the main reason I'm teaching Neural Networks in this class is because it's actually a very effective state of the art technique for modern day machine learning applications  So  in the next few videos  we'll start diving into the technical details of Neural Networks so that you can apply them to modern-day machine learning applications and get them to work well on problems  But for me  you know  one of the reasons the excite me is that maybe they give us this window into what we might do if we're also thinking of what algorithms might someday be able to learn in a manner similar to humankind 
cP1fN6xf3nI,Model Representation I  In this video  I want to start telling you about how we represent neural networks  In other words  how we represent our hypothesis or how we represent our model when using neural networks  Neural networks were developed as simulating neurons or networks of neurons in the brain  So  to explain the hypothesis representation let's start by looking at what a single neuron in the brain looks like  Your brain and mine is jam packed full of neurons like these and neurons are cells in the brain  And two things to draw attention to are that first  The neuron has a cell body  like so  and moreover  the neuron has a number of input wires  and these are called the dendrites  You think of them as input wires  and these receive inputs from other locations  And a neuron also has an output wire called an Axon  and this output wire is what it uses to send signals to other neurons  so to send messages to other neurons  So  at a simplistic level what a neuron is  is a computational unit that gets a number of inputs through it input wires and does some computation and then it says outputs via its axon to other nodes or to other neurons in the brain  Here's a illustration of a group of neurons  The way that neurons communicate with each other is with little pulses of electricity  they are also called spikes but that just means pulses of electricity  So here is one neuron and what it does is if it wants a send a message what it does is sends a little pulse of electricity  Varis axon to some different neuron and here  this axon that is this open wire  connects to the dendrites of this second neuron over here  which then accepts this incoming message that some computation  And they  in turn  decide to send out this message on this axon to other neurons  and this is the process by which all human thought happens  It's these Neurons doing computations and passing messages to other neurons as a result of what other inputs they've got  And  by the way  this is how our senses and our muscles work as well  If you want to move one of your muscles the way that where else in your neuron may send this electricity to your muscle and that causes your muscles to contract and your eyes  some senses like your eye must send a message to your brain while it does it senses hosts electricity entity to a neuron in your brain like so  In a neuro network  or rather  in an artificial neuron network that we've implemented on the computer  we're going to use a very simple model of what a neuron does we're going to model a neuron as just a logistic unit  So  when I draw a yellow circle like that  you should think of that as a playing a role analysis  who's maybe the body of a neuron  and we then feed the neuron a few inputs who's various dendrites or input wiles  And the neuron does some computation  And output some value on this output wire  or in the biological neuron  this is an axon  And whenever I draw a diagram like this  what this means is that this represents a computation of h of x equals one over one plus e to the negative theta transpose x  where as usual  x and theta are our parameter vectors  like so  So this is a very simple  maybe a vastly oversimplified model  of the computations that the neuron does  where it gets a number of inputs  x1  x2  x3 and it outputs some value computed like so  When I draw a neural network  usually I draw only the input nodes x1  x2  x3  Sometimes when it's useful to do so  I'll draw an extra node for x0  This x0 now that's sometimes called the bias unit or the bias neuron  but because x0 is already equal to 1  sometimes  I draw this  sometimes I won't just depending on whatever is more notationally convenient for that example  Finally  one last bit of terminology when we talk about neural networks  sometimes we'll say that this is a neuron or an artificial neuron with a Sigmoid or logistic activation function  So this activation function in the neural network terminology  This is just another term for that function for that non-linearity g(z) = 1 over 1+e to the -z  And whereas so far I've been calling theta the parameters of the model  I'll mostly continue to use that terminology  Here  it's a copy to the parameters  but in neural networks  in the neural network literature sometimes you might hear people talk about weights of a model and weights just means exactly the same thing as parameters of a model  But I'll mostly continue to use the terminology parameters in these videos  but sometimes  you might hear others use the weights terminology  So  this little diagram represents a single neuron  What a neural network is  is just a group of this different neurons strong together  Completely  here we have input units x1  x2  x3 and once again  sometimes you can draw this extra note x0 and Sometimes not  just flow that in here  And here we have three neurons which have written 81  82  83  I'll talk about those indices later  And once again we can if we want add in just a0 and add the mixture bias unit there  There's always a value of 1  And then finally we have this third node and the final layer  and there's this third node that outputs the value that the hypothesis h(x) computes  To introduce a bit more terminology  in a neural network  the first layer  this is also called the input layer because this is where we Input our features  x1  x2  x3  The final layer is also called the output layer because that layer has a neuron  this one over here  that outputs the final value computed by a hypothesis  And then  layer 2 in between  this is called the hidden layer  The term hidden layer isn't a great terminology  but this ideation is that  you know  you supervised early  where you get to see the inputs and get to see the correct outputs  where there's a hidden layer of values you don't get to observe in the training setup  It's not x  and it's not y  and so we call those hidden  And they try to see neural nets with more than one hidden layer but in this example  we have one input layer  Layer 1  one hidden layer  Layer 2  and one output layer  Layer 3  But basically  anything that isn't an input layer and isn't an output layer is called a hidden layer  So I want to be really clear about what this neural network is doing  Let's step through the computational steps that are and body represented by this diagram  To explain these specific computations represented by a neural network  here's a little bit more notation  I'm going to use a superscript j subscript i to denote the activation of neuron i or of unit i in layer j  So completely this gave superscript to sub group one  that's the activation of the first unit in layer two  in our hidden layer  And by activation I just mean the value that's computed by and as output by a specific  In addition  new network is parametrize by these matrixes  theta super script j Where theta j is going to be a matrix of weights controlling the function mapping form one layer  maybe the first layer to the second layer  or from the second layer to the third layer  So here are the computations that are represented by this diagram  This first hidden unit here has it's value computed as follows  there's a is a21 is equal to the sigma function of the sigma activation function  also called the logistics activation function  apply to this sort of linear combination of these inputs  And then this second hidden unit has this activation value computer as sigmoid of this  And similarly for this third hidden unit is computed by that formula  So here we have 3 theta 1 which is matrix of parameters governing our mapping from our three different units  our hidden units  Theta 1 is going to be a 3  Theta 1 is going to be a 3x4-dimensional matrix  And more generally  if a network has SJU units in there j and sj + 1 units and sj + 1 then the matrix theta j which governs the function mapping from there sj + 1  That will have to mention sj +1 by sj + 1 I'll just be clear about this notation right  This is Subscript j + 1 and that's s subscript j  and then this whole thing  plus 1  this whole thing (sj + 1)  okay? So that's s subscript j + 1 by  So that's s subscript j + 1 by sj + 1 where this plus one is not part of the subscript  Okay  so we talked about what the three hidden units do to compute their values  Finally  there's a loss of this final and after that we have one more unit which computer h of x and that's equal can also be written as a(3)1 and that's equal to this  And you notice that I've written this with a superscript two here  because theta of superscript two is the matrix of parameters  or the matrix of weights that controls the function that maps from the hidden units  that is the layer two units to the one layer three unit  that is the output unit  To summarize  what we've done is shown how a picture like this over here defines an artificial neural network which defines a function h that maps with x's input values to hopefully to some space that provisions y  And these hypothesis are parameterized by parameters denoting with a capital theta so that  as we vary theta  we get different hypothesis and we get different functions  Mapping say from x to y  So this gives us a mathematical definition of how to represent the hypothesis in the neural network  In the next few videos what I would like to do is give you more intuition about what these hypothesis representations do  as well as go through a few examples and talk about how to compute them efficiently 
8BHpKzahwPM,Model Representation II  In the last video  we gave a mathematical definition of how to represent or how to compute the hypotheses used by Neural Network  In this video  I like show you how to actually carry out that computation efficiently  and that is show you a vector rise implementation  And second  and more importantly  I want to start giving you intuition about why these neural network representations might be a good idea and how they can help us to learn complex nonlinear hypotheses  Consider this neural network  Previously we said that the sequence of steps that we need in order to compute the output of a hypotheses is these equations given on the left where we compute the activation values of the three hidden uses and then we use those to compute the final output of our hypotheses h of x  Now  I'm going to define a few extra terms  So  this term that I'm underlining here  I'm going to define that to be z superscript 2 subscript 1  So that we have that a(2)1  which is this term is equal to g of z to 1  And by the way  these superscript 2  you know  what that means is that the z2 and this a2 as well  the superscript 2 in parentheses means that these are values associated with layer 2  that is with the hidden layer in the neural network  Now this term here I'm going to similarly define as z(2)2  And finally  this last term here that I'm underlining  let me define that as z(2)3  So that similarly we have a(2)3 equals g of z(2)3  So these z values are just a linear combination  a weighted linear combination  of the input values x0  x1  x2  x3 that go into a particular neuron  Now if you look at this block of numbers  you may notice that that block of numbers corresponds suspiciously similar to the matrix vector operation  matrix vector multiplication of x1 times the vector x  Using this observation we're going to be able to vectorize this computation of the neural network  Concretely  let's define the feature vector x as usual to be the vector of x0  x1  x2  x3 where x0 as usual is always equal 1 and that defines z2 to be the vector of these z-values  you know  of z(2)1 z(2)2  z(2)3  And notice that  there  z2 this is a three dimensional vector  We can now vectorize the computation of a(2)1  a(2)2  a(2)3 as follows  We can just write this in two steps  We can compute z2 as theta 1 times x and that would give us this vector z2  and then a2 is g of z2 and just to be clear z2 here  This is a three-dimensional vector and a2 is also a three-dimensional vector and thus this activation g  This applies the sigmoid function element-wise to each of the z2's elements  And by the way  to make our notation a little more consistent with what we'll do later  in this input layer we have the inputs x  but we can also thing it is as in activations of the first layers  So  if I defined a1 to be equal to x  So  the a1 is vector  I can now take this x here and replace this with z2 equals theta1 times a1 just by defining a1 to be activations in my input layer  Now  with what I've written so far I've now gotten myself the values for a1  a2  a3  and really I should put the superscripts there as well  But I need one more value  which is I also want this a(0)2 and that corresponds to a bias unit in the hidden layer that goes to the output there  Of course  there was a bias unit here too that  you know  it just didn't draw under here but to take care of this extra bias unit  what we're going to do is add an extra a0 superscript 2  that's equal to one  and after taking this step we now have that a2 is going to be a four dimensional feature vector because we just added this extra  you know  a0 which is equal to 1 corresponding to the bias unit in the hidden layer  And finally  to compute the actual value output of our hypotheses  we then simply need to compute z3  So z3 is equal to this term here that I'm just underlining  This inner term there is z3  And z3 is stated 2 times a2 and finally my hypotheses output h of x which is a3 that is the activation of my one and only unit in the output layer  So  that's just the real number  You can write it as a3 or as a(3)1 and that's g of z3  This process of computing h of x is also called forward propagation and is called that because we start of with the activations of the input-units and then we sort of forward-propagate that to the hidden layer and compute the activations of the hidden layer and then we sort of forward propagate that and compute the activations of the output layer  but this process of computing the activations from the input then the hidden then the output layer  and that's also called forward propagation and what we just did is we just worked out a vector wise implementation of this procedure  So  if you implement it using these equations that we have on the right  these would give you an efficient way or both of the efficient way of computing h of x  This forward propagation view also helps us to understand what Neural Networks might be doing and why they might help us to learn interesting nonlinear hypotheses  Consider the following neural network and let's say I cover up the left path of this picture for now  If you look at what's left in this picture  This looks a lot like logistic regression where what we're doing is we're using that note  that's just the logistic regression unit and we're using that to make a prediction h of x  And concretely  what the hypotheses is outputting is h of x is going to be equal to g which is my sigmoid activation function times theta 0 times a0 is equal to 1 plus theta 1 plus theta 2 times a2 plus theta 3 times a3 whether values a1  a2  a3 are those given by these three given units  Now  to be actually consistent to my early notation  Actually  we need to  you know  fill in these superscript 2's here everywhere and I also have these indices 1 there because I have only one output unit  but if you focus on the blue parts of the notation  This is  you know  this looks awfully like the standard logistic regression model  except that I now have a capital theta instead of lower case theta  And what this is doing is just logistic regression  But where the features fed into logistic regression are these values computed by the hidden layer  Just to say that again  what this neural network is doing is just like logistic regression  except that rather than using the original features x1  x2  x3  is using these new features a1  a2  a3  Again  we'll put the superscripts there  you know  to be consistent with the notation  And the cool thing about this  is that the features a1  a2  a3  they themselves are learned as functions of the input  Concretely  the function mapping from layer 1 to layer 2  that is determined by some other set of parameters  theta 1  So it's as if the neural network  instead of being constrained to feed the features x1  x2  x3 to logistic regression  It gets to learn its own features  a1  a2  a3  to feed into the logistic regression and as you can imagine depending on what parameters it chooses for theta 1  You can learn some pretty interesting and complex features and therefore you can end up with a better hypotheses than if you were constrained to use the raw features x1  x2 or x3 or if you will constrain to say choose the polynomial terms  you know  x1  x2  x3  and so on  But instead  this algorithm has the flexibility to try to learn whatever features at once  using these a1  a2  a3 in order to feed into this last unit that's essentially a logistic regression here  I realized this example is described as a somewhat high level and so I'm not sure if this intuition of the neural network  you know  having more complex features will quite make sense yet  but if it doesn't yet in the next two videos I'm going to go through a specific example of how a neural network can use this hidden there to compute more complex features to feed into this final output layer and how that can learn more complex hypotheses  So  in case what I'm saying here doesn't quite make sense  stick with me for the next two videos and hopefully out there working through those examples this explanation will make a little bit more sense  But just the point O  You can have neural networks with other types of diagrams as well  and the way that neural networks are connected  that's called the architecture  So the term architecture refers to how the different neurons are connected to each other  This is an example of a different neural network architecture and once again you may be able to get this intuition of how the second layer  here we have three heading units that are computing some complex function maybe of the input layer  and then the third layer can take the second layer's features and compute even more complex features in layer three so that by the time you get to the output layer  layer four  you can have even more complex features of what you are able to compute in layer three and so get very interesting nonlinear hypotheses  By the way  in a network like this  layer one  this is called an input layer  Layer four is still our output layer  and this network has two hidden layers  So anything that's not an input layer or an output layer is called a hidden layer  So  hopefully from this video you've gotten a sense of how the feed forward propagation step in a neural network works where you start from the activations of the input layer and forward propagate that to the first hidden layer  then the second hidden layer  and then finally the output layer  And you also saw how we can vectorize that computation  In the next  I realized that some of the intuitions in this video of how  you know  other certain layers are computing complex features of the early layers  I realized some of that intuition may be still slightly abstract and kind of a high level  And so what I would like to do in the two videos is work through a detailed example of how a neural network can be used to compute nonlinear functions of the input and hope that will give you a good sense of the sorts of complex nonlinear hypotheses we can get out of Neural Networks 
GxSCRLF37z4,Examples and Intuitions I  In this and the next video I want to work through a detailed example showing how a neural network can compute a complex non linear function of the input  And hopefully this will give you a good sense of why neural networks can be used to learn complex non linear hypotheses  Consider the following problem where we have features X1 and X2 that are binary values  So  either 0 or 1  So  X1 and X2 can each take on only one of two possible values  In this example  I've drawn only two positive examples and two negative examples  That you can think of this as a simplified version of a more complex learning problem where we may have a bunch of positive examples in the upper right and lower left and a bunch of negative examples denoted by the circles  And what we'd like to do is learn a non-linear division of boundary that may need to separate the positive and negative examples  So  how can a neural network do this and rather than using the example and the variable to use this maybe easier to examine example on the left  Concretely what this is  is really computing the type of label y equals x 1 x or x 2  Or actually this is actually the x 1 x nor x 2 function where x nor is the alternative notation for not x 1 or x 2  So  x 1 x or x 2 that's true only if exactly 1 of x 1 or x 2 is equal to 1  It turns out that these specific examples in the works out a little bit better if we use the XNOR example instead  These two are the same of course  This means not x1 or x2 and so  we're going to have positive examples of either both are true or both are false and what have as y equals 1  y equals 1  And we're going to have y equals 0 if only one of them is true and we're going to figure out if we can get a neural network to fit to this sort of training set  In order to build up to a network that fits the XNOR example we're going to start with a slightly simpler one and show a network that fits the AND function  Concretely  let's say we have input x1 and x2 that are again binaries so  it's either 0 or 1 and let's say our target labels y = x1 AND x2  This is a logical AND  So  can we get a one-unit network to compute this logical AND function? In order to do so  I'm going to actually draw in the bias unit as well the plus one unit  Now let me just assign some values to the weights or parameters of this network  I'm gonna write down the parameters on this diagram here  -30 here  +20 and + 20  And what this mean is just that I'm assigning a value of -30 to the value associated with X0 this +1 going into this unit and a parameter value of +20 that multiplies to X1 a value of +20 for the parameter that multiplies into x 2  So  concretely it's the same that the hypothesis h(x)=g(-30+20 X1 plus 20 X2  So  sometimes it's just convenient to draw these weights  Draw these parameters up here in the diagram within and of course this- 30  This is actually theta 1 of 1 0  This is theta 1 of 1 1 and that's theta 1 of 1 2 but it's just easier to think about it as associating these parameters with the edges of the network  Let's look at what this little single neuron network will compute  Just to remind you the sigmoid activation function g(z) looks like this  It starts from 0 rises smoothly crosses 0 5 and then it asymptotic as 1 and to give you some landmarks  if the horizontal axis value z is equal to 4 6 then the sigmoid function is equal to 0 99  This is very close to 1 and kind of symmetrically  if it's -4 6 then the sigmoid function there is 0 01 which is very close to 0  Let's look at the four possible input values for x1 and x2 and look at what the hypotheses will output in that case  If x1 and x2 are both equal to 0  If you look at this  if x1 x2 are both equal to 0 then the hypothesis of g of -30  So  this is a very far to the left of this diagram so it will be very close to 0  If x 1 equals 0 and x equals 1  then this formula here evaluates the g that is the sigma function applied to -10  and again that's you know to the far left of this plot and so  that's again very close to 0  This is also g of minus 10 that is f x 1 is equal to 1 and x 2 0  this minus 30 plus 20 which is minus 10 and finally if x 1 equals 1 x 2 equals 1 then you have g of minus 30 plus 20 plus 20  So  that's g of positive 10 which is there for very close to 1  And if you look in this column this is exactly the logical and function  So  this is computing h of x is approximately x 1 and x 2  In other words it outputs one If and only if x2  x1 and x2  are both equal to 1  So  by writing out our little truth table like this we manage to figure what's the logical function that our neural network computes  This network showed here computes the OR function  Just to show you how I worked that out  If you are write out the hypothesis that this confusing g of -10 + 20 x 1 + 20 x 2 and so you fill in these values  You find that's g of minus 10 which is approximately 0  g of 10 which is approximately 1 and so on and these are approximately 1 and approximately 1 and these numbers are essentially the logical OR function  So  hopefully with this you now understand how single neurons in a neural network can be used to compute logical functions like AND and OR and so on  In the next video we'll continue building on these examples and work through a more complex example  We'll get to show you how a neural network now with multiple layers of units can be used to compute more complex functions like the XOR function or the XNOR function 
U8fLEJREP44,Examples and Intuitions II  In this video I'd like to keep working through our example to show how a Neural Network can compute complex non linear hypothesis  In the last video we saw how a Neural Network can be used to compute the functions x1 AND x2  and the function x1 OR x2 when x1 and x2 are binary  that is when they take on values 0 1  We can also have a network to compute negation  that is to compute the function not x1  Let me just write down the ways associated with this network  We have only one input feature x1 in this case and the bias unit +1  And if I associate this with the weights plus 10 and -20  then my hypothesis is computing this h(x) equals sigmoid (10- 20 x1)  So when x1 is equal to 0  my hypothesis would be computing g(10- 20 x 0) is just 10  And so that's approximately 1  and when x is equal to 1  this will be g(-10) which is approximately equal to 0  And if you look at what these values are  that's essentially the not x1 function  Cells include negations  the general idea is to put that large negative weight in front of the variable you want to negate  Minus 20 multiplied by x1 and that's the general idea of how you end up negating x1  And so in an example that I hope that you can figure out yourself  If you want to compute a function like this NOT x1 AND NOT x2  part of that will probably be putting large negative weights in front of x1 and x2  but it should be feasible  So you get a neural network with just one output unit to compute this as well  All right  so this logical function  NOT x1 AND NOT x2  is going to be equal to 1 if and only if x1 equals x2 equals 0  All right since this is a logical function  this says NOT x1 means x1 must be 0 and NOT x2  that means x2 must be equal to 0 as well  So this logical function is equal to 1 if and only if both x1 and x2 are equal to 0 and hopefully you should be able to figure out how to make a small neural network to compute this logical function as well  Now  taking the three pieces that we have put together as the network for computing x1 AND x2  and the network computing for computing NOT x1 AND NOT x2  And one last network computing for computing x1 OR x2  we should be able to put these three pieces together to compute this x1 XNOR x2 function  And just to remind you if this is x1  x2  this function that we want to compute would have negative examples here and here  and we'd have positive examples there and there  And so clearly this will need a non linear decision boundary in order to separate the positive and negative examples  Let's draw the network  I'm going to take my input +1  x1  x2 and create my first hidden unit here  I'm gonna call this a 21 cuz that's my first hidden unit  And I'm gonna copy the weight over from the red network  the x1 and x2  As well so then -30  20  20  Next let me create a second hidden unit which I'm going to call a 2 2  That is the second hidden unit of layer two  I'm going to copy over the cyan that's work in the middle  so I'm gonna have the weights 10 -20 -20  And so  let's pull some of the truth table values  For the red network  we know that was computing the x1 and x2  and so this will be approximately 0 0 0 1  depending on the values of x1 and x2  and for a 2 2  the cyan network  What do we know? The function NOT x1 AND NOT x2  that outputs 1 0 0 0  for the 4 values of x1 and x2  Finally  I'm going to create my output node  my output unit that is a 3 1  This is one more output h(x) and I'm going to copy over the old network for that  And I'm going to need a +1 bias unit here  so you draw that in  And I'm going to copy over the weights from the green networks  So that's -10  20  20 and we know earlier that this computes the OR function  So let's fill in the truth table entries  So the first entry is 0 OR 1 which can be 1 that makes 0 OR 0 which is 0  0 OR 0 which is 0  1 OR 0 and that falls to 1  And thus h(x) is equal to 1 when either both x1 and x2 are zero or when x1 and x2 are both 1 and concretely h(x) outputs 1 exactly at these two locations and then outputs 0 otherwise  And thus will this neural network  which has a input layer  one hidden layer  and one output layer  we end up with a nonlinear decision boundary that computes this XNOR function  And the more general intuition is that in the input layer  we just have our four inputs  Then we have a hidden layer  which computed some slightly more complex functions of the inputs that its shown here this is slightly more complex functions  And then by adding yet another layer we end up with an even more complex non linear function  And this is a sort of intuition about why neural networks can compute pretty complicated functions  That when you have multiple layers you have relatively simple function of the inputs of the second layer  But the third layer I can build on that to complete even more complex functions  and then the layer after that can compute even more complex functions  To wrap up this video  I want to show you a fun example of an application of a the Neural Network that captures this intuition of the deeper layers computing more complex features  I want to show you a video of that customer a good friend of mine Yann LeCunj  Yann is a professor at New York University  NYU and he was one of the early pioneers of Neural Network reasearch and is sort of a legend in the field now and his ideas are used in all sorts of products and applications throughout the world now  So I wanna show you a video from some of his early work in which he was using a neural network to recognize handwriting  to do handwritten digit recognition  You might remember early in this class  at the start of this class I said that one of the earliest successes of neural networks was trying to use it to read zip codes to help USPS Laws and read postal codes  So this is one of the attempts  this is one of the algorithms used to try to address that problem  In the video that I'll show you this area here is the input area that shows a canvasing character shown to the network  This column here shows a visualization of the features computed by sort of the first hidden layer of the network  So that the first hidden layer of the network and so the first hidden layer  this visualization shows different features  Different edges and lines and so on detected  This is a visualization of the next hidden layer  It's kinda harder to see  harder to understand the deeper  hidden layers  and that's a visualization of why the next hidden layer is confusing  You probably have a hard time seeing what's going on much beyond the first hidden layer  but then finally  all of these learned features get fed to the upper layer  And shown over here is the final answer  it's the final predictive value for what handwritten digit the neural network thinks it is being shown  So let's take a look at the video  [MUSIC] So I hope you enjoyed the video and that this hopefully gave you some intuition about the source of pretty complicated functions neural networks can learn  In which it takes its input this image  just takes this input  the raw pixels and the first hidden layer computes some set of features  The next hidden layer computes even more complex features and even more complex features  And these features can then be used by essentially the final layer of the logistic classifiers to make accurate predictions without the numbers that the network sees 
20J8lr0M1EY,"Multiclass Classification  In this video  I want to tell you about how to use neural networks to do multiclass classification where we may have more than one category that we're trying to distinguish amongst  In the last part of the last video  where we had the handwritten digit recognition problem  that was actually a multiclass classification problem because there were ten possible categories for recognizing the digits from 0 through 9 and so  if you want us to fill you in on the details of how to do that  The way we do multiclass classification in a neural network is essentially an extension of the one versus all method  So  let's say that we have a computer vision example  where instead of just trying to recognize cars as in the original example that I started off with  but let's say that we're trying to recognize  you know  four categories of objects and given an image we want to decide if it is a pedestrian  a car  a motorcycle or a truck  If that's the case  what we would do is we would build a neural network with four output units so that our neural network now outputs a vector of four numbers  So  the output now is actually needing to be a vector of four numbers and what we're going to try to do is get the first output unit to classify  is the image a pedestrian  yes or no  The second unit to classify  is the image a car  yes or no  This unit to classify  is the image a motorcycle  yes or no  and this would classify  is the image a truck  yes or no  And thus  when the image is of a pedestrian  we would ideally want the network to output 1  0  0  0  when it is a car we want it to output 0  1  0  0  when this is a motorcycle  we get it to or rather  we want it to output 0  0  1  0 and so on  So this is just like the \""one versus all\"" method that we talked about when we were describing logistic regression  and here we have essentially four logistic regression classifiers  each of which is trying to recognize one of the four classes that we want to distinguish amongst  So  rearranging the slide of it  here's our neural network with four output units and those are what we want h of x to be when we have the different images  and the way we're going to represent the training set in these settings is as follows  So  when we have a training set with different images of pedestrians  cars  motorcycles and trucks  what we're going to do in this example is that whereas previously we had written out the labels as y being an integer from 1  2  3 or 4  Instead of representing y this way  we're going to instead represent y as follows  namely Yi will be either 1  0  0  0 or 0  1  0  0 or 0  0  1  0 or 0  0  0  1 depending on what the corresponding image Xi is  And so one training example will be one pair Xi colon Yi where Xi is an image with  you know one of the four objects and Yi will be one of these vectors  And hopefully  we can find a way to get our Neural Networks to output some value  So  the h of x is approximately y and both h of x and Yi  both of these are going to be in our example  four dimensional vectors when we have four classes  So  that's how you get neural network to do multiclass classification  This wraps up our discussion on how to represent Neural Networks that is on our hypotheses representation  In the next set of videos  let's start to talk about how take a training set and how to automatically learn the parameters of the neural network "
#NAME?,Cost Function  Neural networks are one of the most powerful learning algorithms that we have today  In this and in the next few videos  I'd like to start talking about a learning algorithm for fitting the parameters of a neural network given a training set  As with the discussion of most of our learning algorithms  we're going to begin by talking about the cost function for fitting the parameters of the network  I'm going to focus on the application of neural networks to classification problems  So suppose we have a network like that shown on the left  And suppose we have a training set like this is x I  y I pairs of M training example  I'm going to use upper case L to denote the total number of layers in this network  So for the network shown on the left we would have capital L equals 4  I'm going to use S subscript L to denote the number of units  that is the number of neurons  Not counting the bias unit in their L of the network  So for example  we would have a S one  which is equal there  equals S three unit  S two in my example is five units  And the output layer S four  which is also equal to S L because capital L is equal to four  The output layer in my example under that has four units  We're going to consider two types of classification problems  The first is Binary classification  where the labels y are either 0 or 1  In this case  we will have 1 output unit  so this Neural Network unit on top has 4 output units  but if we had binary classification we would have only one output unit that computes h(x)  And the output of the neural network would be h(x) is going to be a real number  And in this case the number of output units  S L  where L is again the index of the final layer  Cuz that's the number of layers we have in the network so the number of units we have in the output layer is going to be equal to 1  In this case to simplify notation later  I'm also going to set K=1 so you can think of K as also denoting the number of units in the output layer  The second type of classification problem we'll consider will be multi-class classification problem where we may have K distinct classes  So our early example had this representation for y if we have 4 classes  and in this case we will have capital K output units and our hypothesis or output vectors that are K dimensional  And the number of output units will be equal to K  And usually we would have K greater than or equal to 3 in this case  because if we had two causes  then we don't need to use the one verses all method  We use the one verses all method only if we have K greater than or equals V classes  so having only two classes we will need to use only one upper unit  Now let's define the cost function for our neural network  The cost function we use for the neural network is going to be a generalization of the one that we use for logistic regression  For logistic regression we used to minimize the cost function J(theta) that was minus 1/m of this cost function and then plus this extra regularization term here  where this was a sum from J=1 through n  because we did not regularize the bias term theta0  For a neural network  our cost function is going to be a generalization of this  Where instead of having basically just one  which is the compression output unit  we may instead have K of them  So here's our cost function  Our new network now outputs vectors in R K where R might be equal to 1 if we have a binary classification problem  I'm going to use this notation h(x) subscript i to denote the ith output  That is  h(x) is a k-dimensional vector and so this subscript i just selects out the ith element of the vector that is output by my neural network  My cost function J(theta) is now going to be the following  Is - 1 over M of a sum of a similar term to what we have for logistic regression  except that we have the sum from K equals 1 through K  This summation is basically a sum over my K output  A unit  So if I have four output units  that is if the final layer of my neural network has four output units  then this is a sum from k equals one through four of basically the logistic regression algorithm's cost function but summing that cost function over each of my four output units in turn  And so you notice in particular that this applies to Yk Hk  because we're basically taking the K upper units  and comparing that to the value of Yk which is that one of those vectors saying what cost it should be  And finally  the second term here is the regularization term  similar to what we had for the logistic regression  This summation term looks really complicated  but all it's doing is it's summing over these terms theta j i l for all values of i j and l  Except that we don't sum over the terms corresponding to these bias values like we have for logistic progression  Completely  we don't sum over the terms responding to where i is equal to 0  So that is because when we're computing the activation of a neuron  we have terms like these  Theta i 0  Plus theta i1  x1 plus and so on  Where I guess put in a two there  this is the first hit in there  And so the values with a zero there  that corresponds to something that multiplies into an x0 or an a0  And so this is kinda like a bias unit and by analogy to what we were doing for logistic progression  we won't sum over those terms in our regularization term because we don't want to regularize them and string their values as zero  But this is just one possible convention  and even if you were to sum over i equals 0 up to Sl  it would work about the same and doesn't make a big difference  But maybe this convention of not regularizing the bias term is just slightly more common  So that's the cost function we're going to use for our neural network  In the next video we'll start to talk about an algorithm for trying to optimize the cost function 
FE3RZsnn2Mo,Backpropagation Algorithm  In the previous video  we talked about a cost function for the neural network  In this video  let's start to talk about an algorithm  for trying to minimize the cost function  In particular  we'll talk about the back propagation algorithm  Here's the cost function that we wrote down in the previous video  What we'd like to do is try to find parameters theta to try to minimize j of theta  In order to use either gradient descent or one of the advance optimization algorithms  What we need to do therefore is to write code that takes this input the parameters theta and computes j of theta and these partial derivative terms  Remember  that the parameters in the the neural network of these things  theta superscript l subscript ij  that's the real number and so  these are the partial derivative terms we need to compute  In order to compute the cost function j of theta  we just use this formula up here and so  what I want to do for the most of this video is focus on talking about how we can compute these partial derivative terms  Let's start by talking about the case of when we have only one training example  so imagine  if you will that our entire training set comprises only one training example which is a pair xy  I'm not going to write x1y1 just write this  Write a one training example as xy and let's tap through the sequence of calculations we would do with this one training example  The first thing we do is we apply forward propagation in order to compute whether a hypotheses actually outputs given the input  Concretely  the called the a(1) is the activation values of this first layer that was the input there  So  I'm going to set that to x and then we're going to compute z(2) equals theta(1) a(1) and a(2) equals g  the sigmoid activation function applied to z(2) and this would give us our activations for the first middle layer  That is for layer two of the network and we also add those bias terms  Next we apply 2 more steps of this four and propagation to compute a(3) and a(4) which is also the upwards of a hypotheses h of x  So this is our vectorized implementation of forward propagation and it allows us to compute the activation values for all of the neurons in our neural network  Next  in order to compute the derivatives  we're going to use an algorithm called back propagation  The intuition of the back propagation algorithm is that for each note we're going to compute the term delta superscript l subscript j that's going to somehow represent the error of note j in the layer l  So  recall that a superscript l subscript j that does the activation of the j of unit in layer l and so  this delta term is in some sense going to capture our error in the activation of that neural duo  So  how we might wish the activation of that note is slightly different  Concretely  taking the example neural network that we have on the right which has four layers  And so capital L is equal to 4  For each output unit  we're going to compute this delta term  So  delta for the j of unit in the fourth layer is equal to just the activation of that unit minus what was the actual value of 0 in our training example  So  this term here can also be written h of x subscript j  right  So this delta term is just the difference between when a hypotheses output and what was the value of y in our training set whereas y subscript j is the j of element of the vector value y in our labeled training set  And by the way  if you think of delta a and y as vectors then you can also take those and come up with a vectorized implementation of it  which is just delta 4 gets set as a4 minus y  Where here  each of these delta 4 a4 and y  each of these is a vector whose dimension is equal to the number of output units in our network  So we've now computed the era term's delta 4 for our network  What we do next is compute the delta terms for the earlier layers in our network  Here's a formula for computing delta 3 is delta 3 is equal to theta 3 transpose times delta 4  And this dot times  this is the element y's multiplication operation that we know from MATLAB  So delta 3 transpose delta 4  that's a vector  g prime z3 that's also a vector and so dot times is in element y's multiplication between these two vectors  This term g prime of z3  that formally is actually the derivative of the activation function g evaluated at the input values given by z3  If you know calculus  you can try to work it out yourself and see that you can simplify it to the same answer that I get  But I'll just tell you pragmatically what that means  What you do to compute this g prime  these derivative terms is just a3 dot times1 minus A3 where A3 is the vector of activations  1 is the vector of ones and A3 is again the activation the vector of activation values for that layer  Next you apply a similar formula to compute delta 2 where again that can be computed using a similar formula  Only now it is a2 like so and I then prove it here but you can actually  it's possible to prove it if you know calculus that this expression is equal to mathematically  the derivative of the g function of the activation function  which I'm denoting by g prime  And finally  that's it and there is no delta1 term  because the first layer corresponds to the input layer and that's just the feature we observed in our training sets  so that doesn't have any error associated with that  It's not like  you know  we don't really want to try to change those values  And so we have delta terms only for layers 2  3 and for this example  The name back propagation comes from the fact that we start by computing the delta term for the output layer and then we go back a layer and compute the delta terms for the third hidden layer and then we go back another step to compute delta 2 and so  we're sort of back propagating the errors from the output layer to layer 3 to their to hence the name back complication  Finally  the derivation is surprisingly complicated  surprisingly involved but if you just do this few steps steps of computation it is possible to prove viral frankly some what complicated mathematical proof  It's possible to prove that if you ignore authorization then the partial derivative terms you want are exactly given by the activations and these delta terms  This is ignoring lambda or alternatively the regularization term lambda will equal to 0  We'll fix this detail later about the regularization term  but so by performing back propagation and computing these delta terms  you can  you know  pretty quickly compute these partial derivative terms for all of your parameters  So this is a lot of detail  Let's take everything and put it all together to talk about how to implement back propagation to compute derivatives with respect to your parameters  And for the case of when we have a large training set  not just a training set of one example  here's what we do  Suppose we have a training set of m examples like that shown here  The first thing we're going to do is we're going to set these delta l subscript i j  So this triangular symbol? That's actually the capital Greek alphabet delta   The symbol we had on the previous slide was the lower case delta  So the triangle is capital delta  We're gonna set this equal to zero for all values of l i j  Eventually  this capital delta l i j will be used to compute the partial derivative term  partial derivative respect to theta l i j of J of theta  So as we'll see in a second  these deltas are going to be used as accumulators that will slowly add things in order to compute these partial derivatives  Next  we're going to loop through our training set  So  we'll say for i equals 1 through m and so for the i iteration  we're going to working with the training example xi  yi  So the first thing we're going to do is set a1 which is the activations of the input layer  set that to be equal to xi is the inputs for our i training example  and then we're going to perform forward propagation to compute the activations for layer two  layer three and so on up to the final layer  layer capital L  Next  we're going to use the output label yi from this specific example we're looking at to compute the error term for delta L for the output there  So delta L is what a hypotheses output minus what the target label was? And then we're going to use the back propagation algorithm to compute delta L minus 1  delta L minus 2  and so on down to delta 2 and once again there is now delta 1 because we don't associate an error term with the input layer  And finally  we're going to use these capital delta terms to accumulate these partial derivative terms that we wrote down on the previous line  And by the way  if you look at this expression  it's possible to vectorize this too  Concretely  if you think of delta ij as a matrix  indexed by subscript ij  Then  if delta L is a matrix we can rewrite this as delta L  gets updated as delta L plus lower case delta L plus one times aL transpose  So that's a vectorized implementation of this that automatically does an update for all values of i and j  Finally  after executing the body of the four-loop we then go outside the four-loop and we compute the following  We compute capital D as follows and we have two separate cases for j equals zero and j not equals zero  The case of j equals zero corresponds to the bias term so when j equals zero that's why we're missing is an extra regularization term  Finally  while the formal proof is pretty complicated what you can show is that once you've computed these D terms  that is exactly the partial derivative of the cost function with respect to each of your perimeters and so you can use those in either gradient descent or in one of the advanced authorization algorithms  So that's the back propagation algorithm and how you compute derivatives of your cost function for a neural network  I know this looks like this was a lot of details and this was a lot of steps strung together  But both in the programming assignments write out and later in this video  we'll give you a summary of this so we can have all the pieces of the algorithm together so that you know exactly what you need to implement if you want to implement back propagation to compute the derivatives of your neural network's cost function with respect to those parameters 
S8qwzHUiRoE,Backpropagation Intuition  In the previous video  we talked about the backpropagation algorithm  To a lot of people seeing it for the first time  their first impression is often that wow this is a really complicated algorithm  and there are all these different steps  and I'm not sure how they fit together  And it's kinda this black box of all these complicated steps  In case that's how you're feeling about backpropagation  that's actually okay  Backpropagation maybe unfortunately is a less mathematically clean  or less mathematically simple algorithm  compared to linear regression or logistic regression  And I've actually used backpropagation  you know  pretty successfully for many years  And even today I still don't sometimes feel like I have a very good sense of just what it's doing  or intuition about what back propagation is doing  If  for those of you that are doing the programming exercises  that will at least mechanically step you through the different steps of how to implement back prop  So you'll be able to get it to work for yourself  And what I want to do in this video is look a little bit more at the mechanical steps of backpropagation  and try to give you a little more intuition about what the mechanical steps the back prop is doing to hopefully convince you that  you know  it's at least a reasonable algorithm  In case even after this video in case back propagation still seems very black box and kind of like a  too many complicated steps and a little bit magical to you  that's actually okay  And Even though I've used back prop for many years  sometimes this is a difficult algorithm to understand  but hopefully this video will help a little bit  In order to better understand backpropagation  let's take another closer look at what forward propagation is doing  Here's a neural network with two input units that is not counting the bias unit  and two hidden units in this layer  and two hidden units in the next layer  And then  finally  one output unit  Again  these counts two  two  two  are not counting these bias units on top  In order to illustrate forward propagation  I'm going to draw this network a little bit differently  And in particular I'm going to draw this neuro-network with the nodes drawn as these very fat ellipsis  so that I can write text in them  When performing forward propagation  we might have some particular example  Say some example x i comma y i  And it'll be this x i that we feed into the input layer  So this maybe x i 2 and x i 2 are the values we set the input layer to  And when we forward propagated to the first hidden layer here  what we do is compute z (2) 1 and z (2) 2  So these are the weighted sum of inputs of the input units  And then we apply the sigmoid of the logistic function  and the sigmoid activation function applied to the z value  Here's are the activation values  So that gives us a (2) 1 and a (2) 2  And then we forward propagate again to get here z (3) 1  Apply the sigmoid of the logistic function  the activation function to that to get a (3) 1  And similarly  like so until we get z (4) 1  Apply the activation function  This gives us a (4)1  which is the final output value of the neural network  Let's erase this arrow to give myself some more space  And if you look at what this computation really is doing  focusing on this hidden unit  let's say  We have to add this weight  Shown in magenta there is my weight theta (2) 1 0  the indexing is not important  And this way here  which I'm highlighting in red  that is theta (2) 1 1 and this weight here  which I'm drawing in cyan  is theta (2) 1 2  So the way we compute this value  z(3)1 is  z(3)1 is as equal to this magenta weight times this value  So that's theta (2) 10 x 1  And then plus this red weight times this value  so that's theta(2) 11 times a(2)1  And finally this cyan weight times this value  which is therefore plus theta(2)12 times a(2)1  And so that's forward propagation  And it turns out that as we'll see later in this video  what backpropagation is doing is doing a process very similar to this  Except that instead of the computations flowing from the left to the right of this network  the computations since their flow from the right to the left of the network  And using a very similar computation as this  And I'll say in two slides exactly what I mean by that  To better understand what backpropagation is doing  let's look at the cost function  It's just the cost function that we had for when we have only one output unit  If we have more than one output unit  we just have a summation you know over the output units indexed by k there  If you have only one output unit then this is a cost function  And we do forward propagation and backpropagation on one example at a time  So let's just focus on the single example  x (i) y (i) and focus on the case of having one output unit  So y (i) here is just a real number  And let's ignore regularization  so lambda equals 0  And this final term  that regularization term  goes away  Now if you look inside the summation  you find that the cost term associated with the training example  that is the cost associated with the training example x(i)  y(i)  That's going to be given by this expression  So  the cost to live off examplie i is written as follows  And what this cost function does is it plays a role similar to the squared arrow  So  rather than looking at this complicated expression  if you want you can think of cost of i being approximately the square difference between what the neural network outputs  versus what is the actual value  Just as in logistic repression  we actually prefer to use the slightly more complicated cost function using the log  But for the purpose of intuition  feel free to think of the cost function as being the sort of the squared error cost function  And so this cost(i) measures how well is the network doing on correctly predicting example i  How close is the output to the actual observed label y(i)? Now let's look at what backpropagation is doing  One useful intuition is that backpropagation is computing these delta superscript l subscript j terms  And we can think of these as the quote error of the activation value that we got for unit j in the layer  in the lth layer  More formally  for  and this is maybe only for those of you who are familiar with calculus  More formally  what the delta terms actually are is this  they're the partial derivative with respect to z l j  that is this weighted sum of inputs that were confusing these z terms  Partial derivatives with respect to these things of the cost function  So concretely  the cost function is a function of the label y and of the value  this h of x output value neural network  And if we could go inside the neural network and just change those z l j values a little bit  then that will affect these values that the neural network is outputting  And that will end up changing the cost function  And again really  this is only for those of you who are expert in Calculus  If you're comfortable with partial derivatives  what these delta terms are is they turn out to be the partial derivative of the cost function  with respect to these intermediate terms that were confusing  And so they're a measure of how much would we like to change the neural network's weights  in order to affect these intermediate values of the computation  So as to affect the final output of the neural network h(x) and therefore affect the overall cost  In case this lost part of this partial derivative intuition  in case that doesn't make sense  Don't worry about the rest of this  we can do without really talking about partial derivatives  But let's look in more detail about what backpropagation is doing  For the output layer  the first set's this delta term  delta (4) 1  as y (i) if we're doing forward propagation and back propagation on this training example i  That says y(i) minus a(4)1  So this is really the error  right? It's the difference between the actual value of y minus what was the value predicted  and so we're gonna compute delta(4)1 like so  Next we're gonna do  propagate these values backwards  I'll explain this in a second  and end up computing the delta terms for the previous layer  We're gonna end up with delta(3)1  Delta(3)2  And then we're gonna propagate this further backward  and end up computing delta(2)1 and delta(2)2  Now the backpropagation calculation is a lot like running the forward propagation algorithm  but doing it backwards  So here's what I mean  Let's look at how we end up with this value of delta(2)2  So we have delta(2)2  And similar to forward propagation  let me label a couple of the weights  So this weight  which I'm going to draw in cyan  Let's say that weight is theta(2)1 2  and this one down here when we highlight this in red  That is going to be let's say theta(2) of 2 2  So if we look at how delta(2)2  is computed  how it's computed with this note  It turns out that what we're going to do  is gonna take this value and multiply it by this weight  and add it to this value multiplied by that weight  So it's really a weighted sum of these delta values  weighted by the corresponding edge strength  So completely  let me fill this in  this delta(2)2 is going to be equal to  Theta(2)1 2 is that magenta lay times delta(3)1  Plus  and the thing I had in red  that's theta (2)2 times delta (3)2  So it's really literally this red wave times this value  plus this magenta weight times this value  And that's how we wind up with that value of delta  And just as another example  let's look at this value  How do we get that value? Well it's a similar process  If this weight  which I'm gonna highlight in green  if this weight is equal to  say  delta (3) 1 2  Then we have that delta (3) 2 is going to be equal to that green weight  theta (3) 12 times delta (4) 1  And by the way  so far I've been writing the delta values only for the hidden units  but excluding the bias units  Depending on how you define the backpropagation algorithm  or depending on how you implement it  you know  you may end up implementing something that computes delta values for these bias units as well  The bias units always output the value of plus one  and they are just what they are  and there's no way for us to change the value  And so  depending on your implementation of back prop  the way I usually implement it  I do end up computing these delta values  but we just discard them  we don't use them  Because they don't end up being part of the calculation needed to compute a derivative  So hopefully that gives you a little better intuition about what back propegation is doing  In case of all of this still seems sort of magical  sort of black box  in a later video  in the putting it together video  I'll try to get a little bit more intuition about what backpropagation is doing  But unfortunately this is a difficult algorithm to try to visualize and understand what it is really doing  But fortunately I've been  I guess many people have been using very successfully for many years  And if you implement the algorithm you can have a very effective learning algorithm  Even though the inner workings of exactly how it works can be harder to visualize 
XRaDIbBAb98,Implementation Note  Unrolling Parameters  In the previous video  we talked about how to use back propagation to compute the derivatives of your cost function  In this video  I want to quickly tell you about one implementational detail of unrolling your parameters from matrices into vectors  which we need in order to use the advanced optimization routines  Concretely  let's say you've implemented a cost function that takes this input  you know  parameters theta and returns the cost function and returns derivatives  Then you can pass this to an advanced authorization algorithm by fminunc and fminunc isn't the only one by the way  There are also other advanced authorization algorithms  But what all of them do is take those input pointedly the cost function  and some initial value of theta  And both  and these routines assume that theta and the initial value of theta  that these are parameter vectors  maybe Rn or Rn plus 1  But these are vectors and it also assumes that  you know  your cost function will return as a second return value this gradient which is also Rn and Rn plus 1  So also a vector  This worked fine when we were using logistic progression but now that we're using a neural network our parameters are no longer vectors  but instead they are these matrices where for a full neural network we would have parameter matrices theta 1  theta 2  theta 3 that we might represent in Octave as these matrices theta 1  theta 2  theta 3  And similarly these gradient terms that were expected to return  Well  in the previous video we showed how to compute these gradient matrices  which was capital D1  capital D2  capital D3  which we might represent an octave as matrices D1  D2  D3  In this video I want to quickly tell you about the idea of how to take these matrices and unroll them into vectors  So that they end up being in a format suitable for passing into as theta here off for getting out for a gradient there  Concretely  let's say we have a neural network with one input layer with ten units  hidden layer with ten units and one output layer with just one unit  so s1 is the number of units in layer one and s2 is the number of units in layer two  and s3 is a number of units in layer three  In this case  the dimension of your matrices theta and D are going to be given by these expressions  For example  theta one is going to a 10 by 11 matrix and so on  So in if you want to convert between these matrices  vectors  What you can do is take your theta 1  theta 2  theta 3  and write this piece of code and this will take all the elements of your three theta matrices and take all the elements of theta one  all the elements of theta 2  all the elements of theta 3  and unroll them and put all the elements into a big long vector  Which is thetaVec and similarly the second command would take all of your D matrices and unroll them into a big long vector and call them DVec  And finally if you want to go back from the vector representations to the matrix representations  What you do to get back to theta one say is take thetaVec and pull out the first 110 elements  So theta 1 has 110 elements because it's a 10 by 11 matrix so that pulls out the first 110 elements and then you can use the reshape command to reshape those back into theta 1  And similarly  to get back theta 2 you pull out the next 110 elements and reshape it  And for theta 3  you pull out the final eleven elements and run reshape to get back the theta 3  Here's a quick Octave demo of that process  So for this example let's set theta 1 equal to be ones of 10 by 11  so it's a matrix of all ones  And just to make this easier seen  let's set that to be 2 times ones  10 by 11 and let's set theta 3 equals 3 times 1's of 1 by 11  So this is 3 separate matrices  theta 1  theta 2  theta 3  We want to put all of these as a vector  ThetaVec equals theta 1  theta 2 theta 3  Right  that's a colon in the middle and like so and now thetavec is going to be a very long vector  That's 231 elements  If I display it  I find that this very long vector with all the elements of the first matrix  all the elements of the second matrix  then all the elements of the third matrix  And if I want to get back my original matrices  I can do reshape thetaVec  Let's pull out the first 110 elements and reshape them to a 10 by 11 matrix  This gives me back theta 1  And if I then pull out the next 110 elements  So that's indices 111 to 220  I get back all of my 2's  And if I go from 221 up to the last element  which is element 231  and reshape to 1 by 11  I get back theta 3  To make this process really concrete  here's how we use the unrolling idea to implement our learning algorithm  Let's say that you have some initial value of the parameters theta 1  theta 2  theta 3  What we're going to do is take these and unroll them into a long vector we're gonna call initial theta to pass in to fminunc as this initial setting of the parameters theta  The other thing we need to do is implement the cost function  Here's my implementation of the cost function  The cost function is going to give us input  thetaVec  which is going to be all of my parameters vectors that in the form that's been unrolled into a vector  So the first thing I'm going to do is I'm going to use thetaVec and I'm going to use the reshape functions  So I'll pull out elements from thetaVec and use reshape to get back my original parameter matrices  theta 1  theta 2  theta 3  So these are going to be matrices that I'm going to get  So that gives me a more convenient form in which to use these matrices so that I can run forward propagation and back propagation to compute my derivatives  and to compute my cost function j of theta  And finally  I can then take my derivatives and unroll them  to keeping the elements in the same ordering as I did when I unroll my thetas  But I'm gonna unroll D1  D2  D3  to get gradientVec which is now what my cost function can return  It can return a vector of these derivatives  So  hopefully  you now have a good sense of how to convert back and forth between the matrix representation of the parameters versus the vector representation of the parameters  The advantage of the matrix representation is that when your parameters are stored as matrices it's more convenient when you're doing forward propagation and back propagation and it's easier when your parameters are stored as matrices to take advantage of the  sort of  vectorized implementations  Whereas in contrast the advantage of the vector representation  when you have like thetaVec or DVec is that when you are using the advanced optimization algorithms  Those algorithms tend to assume that you have all of your parameters unrolled into a big long vector  And so with what we just went through  hopefully you can now quickly convert between the two as needed 
KiA8BnstHLQ,Gradient Checking  In the last few videos we talked about how to do forward propagation and back propagation in a neural network in order to compute derivatives  But back prop as an algorithm has a lot of details and can be a little bit tricky to implement  And one unfortunate property is that there are many ways to have subtle bugs in back prop  So that if you run it with gradient descent or some other optimizational algorithm  it could actually look like it's working  And your cost function  J of theta may end up decreasing on every iteration of gradient descent  But this could prove true even though there might be some bug in your implementation of back prop  So that it looks J of theta is decreasing  but you might just wind up with a neural network that has a higher level of error than you would with a bug free implementation  And you might just not know that there was this subtle bug that was giving you worse performance  So  what can we do about this? There's an idea called gradient checking that eliminates almost all of these problems  So  today every time I implement back propagation or a similar gradient to a [INAUDIBLE] on a neural network or any other reasonably complex model  I always implement gradient checking  And if you do this  it will help you make sure and sort of gain high confidence that your implementation of four prop and back prop or whatever is 100% correct  And from what I've seen this pretty much eliminates all the problems associated with a sort of a buggy implementation as a back prop  And in the previous videos I asked you to take on faith that the formulas I gave for computing the deltas and the vs and so on  I asked you to take on faith that those actually do compute the gradients of the cost function  But once you implement numerical gradient checking  which is the topic of this video  you'll be able to absolute verify for yourself that the code you're writing does indeed  is indeed computing the derivative of the cross function J  So here's the idea  consider the following example  Suppose that I have the function J of theta and I have some value theta and for this example gonna assume that theta is just a real number  And let's say that I want to estimate the derivative of this function at this point and so the derivative is equal to the slope of that tangent one  Here's how I'm going to numerically approximate the derivative  or rather here's a procedure for numerically approximating the derivative  I'm going to compute theta plus epsilon  so now we move it to the right  And I'm gonna compute theta minus epsilon and I'm going to look at those two points  And connect them by a straight line And I'm gonna connect these two points by a straight line  and I'm gonna use the slope of that little red line as my approximation to the derivative  Which is  the true derivative is the slope of that blue line over there  So  you know it seems like it would be a pretty good approximation  Mathematically  the slope of this red line is this vertical height divided by this horizontal width  So this point on top is the J of (Theta plus Epsilon)  This point here is J (Theta minus Epsilon)  so this vertical difference is J (Theta plus Epsilon) minus J of theta minus epsilon and this horizontal distance is just 2 epsilon  So my approximation is going to be that the derivative respect of theta of J of theta at this value of theta  that that's approximately J of theta plus epsilon minus J of theta minus epsilon over 2 epsilon  Usually  I use a pretty small value for epsilon  expect epsilon to be maybe on the order of 10 to the minus 4  There's usually a large range of different values for epsilon that work just fine  And in fact  if you let epsilon become really small  then mathematically this term here  actually mathematically  it becomes the derivative  It becomes exactly the slope of the function at this point  It's just that we don't want to use epsilon that's too  too small  because then you might run into numerical problems  So I usually use epsilon around ten to the minus four  And by the way some of you may have seen an alternative formula for s meeting the derivative which is this formula  This one on the right is called a one-sided difference  whereas the formula on the left  that's called a two-sided difference  The two sided difference gives us a slightly more accurate estimate  so I usually use that  rather than this one sided difference estimate  So  concretely  when you implement an octave  is you implemented the following  you implement call to compute gradApprox  which is going to be our approximation derivative as just here this formula  J of theta plus epsilon minus J of theta minus epsilon divided by 2 times epsilon  And this will give you a numerical estimate of the gradient at that point  And in this example it seems like it's a pretty good estimate  Now on the previous slide  we considered the case of when theta was a rolled number  Now let's look at a more general case of when theta is a vector parameter  so let's say theta is an R n  And it might be an unrolled version of the parameters of our neural network  So theta is a vector that has n elements  theta 1 up to theta n  We can then use a similar idea to approximate all the partial derivative terms  Concretely the partial derivative of a cost function with respect to the first parameter  theta one  that can be obtained by taking J and increasing theta one  So you have J of theta one plus epsilon and so on  Minus J of this theta one minus epsilon and divide it by two epsilon  The partial derivative respect to the second parameter theta two  is again this thing except that you would take J of here you're increasing theta two by epsilon  and here you're decreasing theta two by epsilon and so on down to the derivative  With respect of theta n would give you increase and decrease theta and by epsilon over there  So  these equations give you a way to numerically approximate the partial derivative of J with respect to any one of your parameters theta i  Completely  what you implement is therefore the following  We implement the following in octave to numerically compute the derivatives  We say  for i = 1 n  where n is the dimension of our parameter of vector theta  And I usually do this with the unrolled version of the parameter  So theta is just a long list of all of my parameters in my neural network  say  I'm gonna set thetaPlus = theta  then increase thetaPlus of the (i) element by epsilon  And so this is basically thetaPlus is equal to theta except for thetaPlus(i) which is now incremented by epsilon  Epsilon  so theta plus is equal to  write theta 1  theta 2 and so on  Then theta I has epsilon added to it and then we go down to theta N  So this is what theta plus is  And similar these two lines set theta minus to something similar except that this instead of theta I plus Epsilon  this now becomes theta I minus Epsilon  And then finally you implement this gradApprox (i) and this would give you your approximation to the partial derivative respect of theta i of J of theta  And the way we use this in our neural network implementation is  we would implement this four loop to compute the top partial derivative of the cost function for respect to every parameter in that network  and we can then take the gradient that we got from backprop  So DVec was the derivative we got from backprop  All right  so backprop  backpropogation  was a relatively efficient way to compute a derivative or a partial derivative  Of a cost function with respect to all our parameters  And what I usually do is then  take my numerically computed derivative that is this gradApprox that we just had from up here  And make sure that that is equal or approximately equal up to small values of numerical round up  that it's pretty close  So the DVec that I got from backprop  And if these two ways of computing the derivative give me the same answer  or give me any similar answers  up to a few decimal places  then I'm much more confident that my implementation of backprop is correct  And when I plug these DVec vectors into gradient assent or some advanced optimization algorithm  I can then be much more confident that I'm computing the derivatives correctly  and therefore that hopefully my code will run correctly and do a good job optimizing J of theta  Finally  I wanna put everything together and tell you how to implement this numerical gradient checking  Here's what I usually do  First thing I do is implement back propagation to compute DVec  So there's a procedure we talked about in the earlier video to compute DVec which may be our unrolled version of these matrices  So then what I do  is implement a numerical gradient checking to compute gradApprox  So this is what I described earlier in this video and in the previous slide  Then should make sure that DVec and gradApprox give similar values  you know let's say up to a few decimal places  And finally and this is the important step  before you start to use your code for learning  for seriously training your network  it's important to turn off gradient checking and to no longer compute this gradApprox thing using the numerical derivative formulas that we talked about earlier in this video  And the reason for that is the numeric code gradient checking code  the stuff we talked about in this video  that's a very computationally expensive  that's a very slow way to try to approximate the derivative  Whereas In contrast  the back propagation algorithm that we talked about earlier  that is the thing we talked about earlier for computing  You know  D1  D2  D3 for Dvec  Backprop is much more computationally efficient way of computing for derivatives  So once you've verified that your implementation of back propagation is correct  you should turn off gradient checking and just stop using that  So just to reiterate  you should be sure to disable your gradient checking code before running your algorithm for many iterations of gradient descent or for many iterations of the advanced optimization algorithms  in order to train your classifier  Concretely  if you were to run the numerical gradient checking on every single iteration of gradient descent  Or if you were in the inner loop of your costFunction  then your code would be very slow  Because the numerical gradient checking code is much slower than the backpropagation algorithm  than the backpropagation method where  you remember  we were computing delta(4)  delta(3)  delta(2)  and so on  That was the backpropagation algorithm  That is a much faster way to compute derivates than gradient checking  So when you're ready  once you've verified the implementation of back propagation is correct  make sure you turn off or you disable your gradient checking code while you train your algorithm  or else you code could run very slowly  So  that's how you take gradients numericaly  and that's how you can verify tha implementation of back propagation is correct  Whenever I implement back propagation or similar gradient discerning algorithm for a complicated mode l I always use gradient checking and this really helps me make sure that my code is correct 
vS0khTah3zU,Random Initialization  In the previous video  we've put together almost all the pieces you need in order to implement and train in your network  There's just one last idea I need to share with you  which is the idea of random initialization  When you're running an algorithm of gradient descent  or also the advanced optimization algorithms  we need to pick some initial value for the parameters theta  So for the advanced optimization algorithm  it assumes you will pass it some initial value for the parameters theta  Now let's consider a gradient descent  For that  we'll also need to initialize theta to something  and then we can slowly take steps to go downhill using gradient descent  To go downhill  to minimize the function j of theta  So what can we set the initial value of theta to? Is it possible to set the initial value of theta to the vector of all zeros? Whereas this worked okay when we were using logistic regression  initializing all of your parameters to zero actually does not work when you are trading on your own network  Consider trading the follow Neural network  and let's say we initialize all the parameters of the network to 0  And if you do that  then what you  what that means is that at the initialization  this blue weight  colored in blue is gonna equal to that weight  so they're both 0  And this weight that I'm coloring in in red  is equal to that weight  colored in red  and also this weight  which I'm coloring in green is going to equal to the value of that weight  And what that means is that both of your hidden units  A1 and A2  are going to be computing the same function of your inputs  And thus you end up with for every one of your training examples  you end up with A 2 1 equals A 2 2  And moreover because I'm not going to show this in too much detail  but because these outgoing weights are the same you can also show that the delta values are also gonna be the same  So concretely you end up with delta 1 1  delta 2 1 equals delta 2 2  and if you work through the map further  what you can show is that the partial derivatives with respect to your parameters will satisfy the following  that the partial derivative of the cost function with respected to breaking out the derivatives respect to these two blue waves in your network  You find that these two partial derivatives are going to be equal to each other  And so what this means is that even after say one greater descent update  you're going to update  say  this first blue rate was learning rate times this  and you're gonna update the second blue rate with some learning rate times this  And what this means is that even after one created the descent update  those two blue rates  those two blue color parameters will end up the same as each other  So there'll be some nonzero value  but this value would equal to that value  And similarly  even after one gradient descent update  this value would equal to that value  There'll still be some non-zero values  just that the two red values are equal to each other  And similarly  the two green ways  Well  they'll both change values  but they'll both end up with the same value as each other  So after each update  the parameters corresponding to the inputs going into each of the two hidden units are identical  That's just saying that the two green weights are still the same  the two red weights are still the same  the two blue weights are still the same  and what that means is that even after one iteration of say  gradient descent and descent  You find that your two headed units are still computing exactly the same functions of the inputs  You still have the a1(2) = a2(2)  And so you're back to this case  And as you keep running greater descent  the blue waves   the two blue waves  will stay the same as each other  The two red waves will stay the same as each other and the two green waves will stay the same as each other  And what this means is that your neural network really can compute very interesting functions  right? Imagine that you had not only two hidden units  but imagine that you had many  many hidden units  Then what this is saying is that all of your headed units are computing the exact same feature  All of your hidden units are computing the exact same function of the input  And this is a highly redundant representation because you find the logistic progression unit  It really has to see only one feature because all of these are the same  And this prevents you and your network from doing something interesting  In order to get around this problem  the way we initialize the parameters of a neural network therefore is with random initialization  Concretely  the problem was saw on the previous slide is something called the problem of symmetric ways  that's the ways are being the same  So this random initialization is how we perform symmetry breaking  So what we do is we initialize each value of theta to a random number between minus epsilon and epsilon  So this is a notation to b numbers between minus epsilon and plus epsilon  So my weight for my parameters are all going to be randomly initialized between minus epsilon and plus epsilon  The way I write code to do this in octave is I've said Theta1 should be equal to this  So this rand 10 by 11  that's how you compute a random 10 by 11 dimensional matrix  All the values are between 0 and 1  so these are going to be raw numbers that take on any continuous values between 0 and 1  And so if you take a number between zero and one  multiply it by two times INIT_EPSILON then minus INIT_EPSILON  then you end up with a number that's between minus epsilon and plus epsilon  And the so that leads us  this epsilon here has nothing to do with the epsilon that we were using when we were doing gradient checking  So when numerical gradient checking  there we were adding some values of epsilon and theta  This is your unrelated value of epsilon  We just wanted to notate init epsilon just to distinguish it from the value of epsilon we were using in gradient checking  And similarly if you want to initialize theta2 to a random 1 by 11 matrix you can do so using this piece of code here  So to summarize  to create a neural network what you should do is randomly initialize the waves to small values close to zero  between -epsilon and +epsilon say  And then implement back propagation  do great in checking  and use either great in descent or 1b advanced optimization algorithms to try to minimize j(theta) as a function of the parameters theta starting from just randomly chosen initial value for the parameters  And by doing symmetry breaking  which is this process  hopefully great gradient descent or the advanced optimization algorithms will be able to find a good value of theta 
qhRq627zvow,Putting It Together  So  it's taken us a lot of videos to get through the neural network learning algorithm  In this video  what I'd like to do is try to put all the pieces together  to give a overall summary or a bigger picture view  of how all the pieces fit together and of the overall process of how to implement a neural network learning algorithm  When training a neural network  the first thing you need to do is pick some network architecture and by architecture I just mean connectivity pattern between the neurons  So  you know  we might choose between say  a neural network with three input units and five hidden units and four output units versus one of 3  5 hidden  5 hidden  4 output and here are 3  5  5  5 units in each of three hidden layers and four open units  and so these choices of how many hidden units in each layer and how many hidden layers  those are architecture choices  So  how do you make these choices? Well first  the number of input units well that's pretty well defined  And once you decides on the fix set of features x the number of input units will just be  you know  the dimension of your features x(i) would be determined by that  And if you are doing multiclass classifications the number of output of this will be determined by the number of classes in your classification problem  And just a reminder if you have a multiclass classification where y takes on say values between 1 and 10  so that you have ten possible classes  Then remember to right  your output y as these were the vectors  So instead of clause one  you recode it as a vector like that  or for the second class you recode it as a vector like that  So if one of these apples takes on the fifth class  you know  y equals 5  then what you're showing to your neural network is not actually a value of y equals 5  instead here at the upper layer which would have ten output units  you will instead feed to the vector which you know with one in the fifth position and a bunch of zeros down here  So the choice of number of input units and number of output units is maybe somewhat reasonably straightforward  And as for the number of hidden units and the number of hidden layers  a reasonable default is to use a single hidden layer and so this type of neural network shown on the left with just one hidden layer is probably the most common  Or if you use more than one hidden layer  again the reasonable default will be to have the same number of hidden units in every single layer  So here we have two hidden layers and each of these hidden layers have the same number five of hidden units and here we have  you know  three hidden layers and each of them has the same number  that is five hidden units  Rather than doing this sort of network architecture on the left would be a perfect ably reasonable default  And as for the number of hidden units - usually  the more hidden units the better  it's just that if you have a lot of hidden units  it can become more computationally expensive  but very often  having more hidden units is a good thing  And usually the number of hidden units in each layer will be maybe comparable to the dimension of x  comparable to the number of features  or it could be any where from same number of hidden units of input features to maybe so that three or four times of that  So having the number of hidden units is comparable  You know  several times  or some what bigger than the number of input features is often a useful thing to do So  hopefully this gives you one reasonable set of default choices for neural architecture and and if you follow these guidelines  you will probably get something that works well  but in a later set of videos where I will talk specifically about advice for how to apply algorithms  I will actually say a lot more about how to choose a neural network architecture  Or actually have quite a lot I want to say later to make good choices for the number of hidden units  the number of hidden layers  and so on  Next  here's what we need to implement in order to trade in neural network  there are actually six steps that I have  I have four on this slide and two more steps on the next slide  First step is to set up the neural network and to randomly initialize the values of the weights  And we usually initialize the weights to small values near zero  Then we implement forward propagation so that we can input any excellent neural network and compute h of x which is this output vector of the y values  We then also implement code to compute this cost function j of theta  And next we implement back-prop  or the back-propagation algorithm  to compute these partial derivatives terms  partial derivatives of j of theta with respect to the parameters  Concretely  to implement back prop  Usually we will do that with a fore loop over the training examples  Some of you may have heard of advanced  and frankly very advanced factorization methods where you don't have a four-loop over the m-training examples  that the first time you're implementing back prop there should almost certainly the four loop in your code  where you're iterating over the examples  you know  x1  y1  then so you do forward prop and back prop on the first example  and then in the second iteration of the four-loop  you do forward propagation and back propagation on the second example  and so on  Until you get through the final example  So there should be a four-loop in your implementation of back prop  at least the first time implementing it  And then there are frankly somewhat complicated ways to do this without a four-loop  but I definitely do not recommend trying to do that much more complicated version the first time you try to implement back prop  So concretely  we have a four-loop over my m-training examples and inside the four-loop we're going to perform fore prop and back prop using just this one example  And what that means is that we're going to take x(i)  and feed that to my input layer  perform forward-prop  perform back-prop and that will if all of these activations and all of these delta terms for all of the layers of all my units in the neural network then still inside this four-loop  let me draw some curly braces just to show the scope with the four-loop  this is in octave code of course  but it's more a sequence Java code  and a four-loop encompasses all this  We're going to compute those delta terms  which are is the formula that we gave earlier  Plus  you know  delta l plus one times a  l transpose of the code  And then finally  outside the having computed these delta terms  these accumulation terms  we would then have some other code and then that will allow us to compute these partial derivative terms  Right and these partial derivative terms have to take into account the regularization term lambda as well  And so  those formulas were given in the earlier video  So  how do you done that you now hopefully have code to compute these partial derivative terms  Next is step five  what I do is then use gradient checking to compare these partial derivative terms that were computed  So  I've compared the versions computed using back propagation versus the partial derivatives computed using the numerical estimates as using numerical estimates of the derivatives  So  I do gradient checking to make sure that both of these give you very similar values  Having done gradient checking just now reassures us that our implementation of back propagation is correct  and is then very important that we disable gradient checking  because the gradient checking code is computationally very slow  And finally  we then use an optimization algorithm such as gradient descent  or one of the advanced optimization methods such as LB of GS  contract gradient has embodied into fminunc or other optimization methods  We use these together with back propagation  so back propagation is the thing that computes these partial derivatives for us  And so  we know how to compute the cost function  we know how to compute the partial derivatives using back propagation  so we can use one of these optimization methods to try to minimize j of theta as a function of the parameters theta  And by the way  for neural networks  this cost function j of theta is non-convex  or is not convex and so it can theoretically be susceptible to local minima  and in fact algorithms like gradient descent and the advance optimization methods can  in theory  get stuck in local optima  but it turns out that in practice this is not usually a huge problem and even though we can't guarantee that these algorithms will find a global optimum  usually algorithms like gradient descent will do a very good job minimizing this cost function j of theta and get a very good local minimum  even if it doesn't get to the global optimum  Finally  gradient descents for a neural network might still seem a little bit magical  So  let me just show one more figure to try to get that intuition about what gradient descent for a neural network is doing  This was actually similar to the figure that I was using earlier to explain gradient descent  So  we have some cost function  and we have a number of parameters in our neural network  Right here I've just written down two of the parameter values  In reality  of course  in the neural network  we can have lots of parameters with these  Theta one  theta two--all of these are matrices  right? So we can have very high dimensional parameters but because of the limitations the source of parts we can draw  I'm pretending that we have only two parameters in this neural network  Although obviously we have a lot more in practice  Now  this cost function j of theta measures how well the neural network fits the training data  So  if you take a point like this one  down here  that's a point where j of theta is pretty low  and so this corresponds to a setting of the parameters  There's a setting of the parameters theta  where  you know  for most of the training examples  the output of my hypothesis  that may be pretty close to y(i) and if this is true than that's what causes my cost function to be pretty low  Whereas in contrast  if you were to take a value like that  a point like that corresponds to  where for many training examples  the output of my neural network is far from the actual value y(i) that was observed in the training set  So points like this on the line correspond to where the hypothesis  where the neural network is outputting values on the training set that are far from y(i)  So  it's not fitting the training set well  whereas points like this with low values of the cost function corresponds to where j of theta is low  and therefore corresponds to where the neural network happens to be fitting my training set well  because I mean this is what's needed to be true in order for j of theta to be small  So what gradient descent does is we'll start from some random initial point like that one over there  and it will repeatedly go downhill  And so what back propagation is doing is computing the direction of the gradient  and what gradient descent is doing is it's taking little steps downhill until hopefully it gets to  in this case  a pretty good local optimum  So  when you implement back propagation and use gradient descent or one of the advanced optimization methods  this picture sort of explains what the algorithm is doing  It's trying to find a value of the parameters where the output values in the neural network closely matches the values of the y(i)'s observed in your training set  So  hopefully this gives you a better sense of how the many different pieces of neural network learning fit together  In case even after this video  in case you still feel like there are  like  a lot of different pieces and it's not entirely clear what some of them do or how all of these pieces come together  that's actually okay  Neural network learning and back propagation is a complicated algorithm  And even though I've seen the math behind back propagation for many years and I've used back propagation  I think very successfully  for many years  even today I still feel like I don't always have a great grasp of exactly what back propagation is doing sometimes  And what the optimization process looks like of minimizing j if theta  Much this is a much harder algorithm to feel like I have a much less good handle on exactly what this is doing compared to say  linear regression or logistic regression  Which were mathematically and conceptually much simpler and much cleaner algorithms  But so in case if you feel the same way  you know  that's actually perfectly okay  but if you do implement back propagation  hopefully what you find is that this is one of the most powerful learning algorithms and if you implement this algorithm  implement back propagation  implement one of these optimization methods  you find that back propagation will be able to fit very complex  powerful  non-linear functions to your data  and this is one of the most effective learning algorithms we have today 
honXKsVP-wk,Autonomous Driving  In this video  I'd like to show you a fun and historically important example of neural networks learning of using a neural network for autonomous driving  That is getting a car to learn to drive itself  The video that I'll showed a minute was something that I'd gotten from Dean Pomerleau  who was a colleague who works out in Carnegie Mellon University out on the east coast of the United States  And in part of the video you see visualizations like this  And I want to tell what a visualization looks like before starting the video  Down here on the lower left is the view seen by the car of what's in front of it  And so here you kinda see a road that's maybe going a bit to the left  and then going a little bit to the right  And up here on top  this first horizontal bar shows the direction selected by the human driver  And this location of this bright white band that shows the steering direction selected by the human driver where you know here far to the left corresponds to steering hard left  here corresponds to steering hard to the right  And so this location which is a little bit to the left  a little bit left of center means that the human driver at this point was steering slightly to the left  And this second bot here corresponds to the steering direction selected by the learning algorithm and again the location of this sort of white band means that the neural network was here selecting a steering direction that's slightly to the left  And in fact before the neural network starts leaning initially  you see that the network outputs a grey band  like a grey  like a uniform grey band throughout this region and sort of a uniform gray fuzz corresponds to the neural network having been randomly initialized  And initially having no idea how to drive the car  Or initially having no idea of what direction to steer in  And is only after it has learned for a while  that will then start to output like a solid white band in just a small part of the region corresponding to choosing a particular steering direction  And that corresponds to when the neural network becomes more confident in selecting a band in one particular location  rather than outputting a sort of light gray fuzz  but instead outputting a white band that's more constantly selecting one's steering direction  >> ALVINN is a system of artificial neural networks that learns to steer by watching a person drive  ALVINN is designed to control the NAVLAB 2  a modified Army Humvee who had put sensors  computers  and actuators for autonomous navigation experiments  The initial step in configuring ALVINN is creating a network just here  During training  a person drives the vehicle while ALVINN watches  Once every two seconds  ALVINN digitizes a video image of the road ahead  and records the person's steering direction  This training image is reduced in resolution to 30 by 32 pixels and provided as input to ALVINN's three layered network  Using the back propagation learning algorithm ALVINN is training to output the same steering direction as the human driver for that image  Initially the network steering response is random  After about two minutes of training the network learns to accurately imitate the steering reactions of the human driver  This same training procedure is repeated for other road types  After the networks have been trained the operator pushes the run switch and ALVINN begins driving  Twelve times per second  ALVINN digitizes the image and feeds it to its neural networks  Each network  running in parallel  produces a steering direction  and a measure of its' confidence in its' response  The steering direction  from the most confident network  in this network training for the one lane road  is used to control the vehicle  Suddenly an intersection appears ahead of the vehicle  As the vehicle approaches the intersection the confidence of the lone lane network decreases  As it crosses the intersection and the two lane road ahead comes into view  the confidence of the two lane network rises  When its' confidence rises the two lane network is selected to steer  Safely guiding the vehicle into its lane onto the two lane road  >> So that was autonomous driving using the neural network  Of course there are more recently more modern attempts to do autonomous driving  There are few projects in the US and Europe and so on  that are giving more robust driving controllers than this  but I think it's still pretty remarkable and pretty amazing how instant neural network trained with backpropagation can actually learn to drive a car somewhat well 
ok-M86mjfAQ,"Deciding What to Try Next  By now you have seen a lot of different learning algorithms  And if you've been following along these videos you should consider yourself an expert on many state-of-the-art machine learning techniques  But even among people that know a certain learning algorithm  There's often a huge difference between someone that really knows how to powerfully and effectively apply that algorithm  versus someone that's less familiar with some of the material that I'm about to teach and who doesn't really understand how to apply these algorithms and can end up wasting a lot of their time trying things out that don't really make sense  What I would like to do is make sure that if you are developing machine learning systems  that you know how to choose one of the most promising avenues to spend your time pursuing  And on this and the next few videos I'm going to give a number of practical suggestions  advice  guidelines on how to do that  And concretely what we'd focus on is the problem of  suppose you are developing a machine learning system or trying to improve the performance of a machine learning system  how do you go about deciding what are the proxy avenues to try next? To explain this  let's continue using our example of learning to predict housing prices  And let's say you've implement and regularize linear regression  Thus minimizing that cost function j  Now suppose that after you take your learn parameters  if you test your hypothesis on the new set of houses  suppose you find that this is making huge errors in this prediction of the housing prices  The question is what should you then try mixing in order to improve the learning algorithm? There are many things that one can think of that could improve the performance of the learning algorithm  One thing they could try  is to get more training examples  And concretely  you can imagine  maybe  you know  setting up phone surveys  going door to door  to try to get more data on how much different houses sell for  And the sad thing is I've seen a lot of people spend a lot of time collecting more training examples  thinking oh  if we have twice as much or ten times as much training data  that is certainly going to help  right? But sometimes getting more training data doesn't actually help and in the next few videos we will see why  and we will see how you can avoid spending a lot of time collecting more training data in settings where it is just not going to help  Other things you might try are to well maybe try a smaller set of features  So if you have some set of features such as x1  x2  x3 and so on  maybe a large number of features  Maybe you want to spend time carefully selecting some small subset of them to prevent overfitting  Or maybe you need to get additional features  Maybe the current set of features aren't informative enough and you want to collect more data in the sense of getting more features  And once again this is the sort of project that can scale up the huge projects can you imagine getting phone surveys to find out more houses  or extra land surveys to find out more about the pieces of land and so on  so a huge project  And once again it would be nice to know in advance if this is going to help before we spend a lot of time doing something like this  We can also try adding polynomial features things like x2 square x2 square and product features x1  x2  We can still spend quite a lot of time thinking about that and we can also try other things like decreasing lambda  the regularization parameter or increasing lambda  Given a menu of options like these  some of which can easily scale up to six month or longer projects  Unfortunately  the most common method that people use to pick one of these is to go by gut feeling  In which what many people will do is sort of randomly pick one of these options and maybe say  \""Oh  lets go and get more training data \"" And easily spend six months collecting more training data or maybe someone else would rather be saying  \""Well  let's go collect a lot more features on these houses in our data set \"" And I have a lot of times  sadly seen people spend  you know  literally 6 months doing one of these avenues that they have sort of at random only to discover six months later that that really wasn't a promising avenue to pursue  Fortunately  there is a pretty simple technique that can let you very quickly rule out half of the things on this list as being potentially promising things to pursue  And there is a very simple technique  that if you run  can easily rule out many of these options  and potentially save you a lot of time pursuing something that's just is not going to work  In the next two videos after this  I'm going to first talk about how to evaluate learning algorithms  And in the next few videos after that  I'm going to talk about these techniques  which are called the machine learning diagnostics  And what a diagnostic is  is a test you can run  to get insight into what is or isn't working with an algorithm  and which will often give you insight as to what are promising things to try to improve a learning algorithm's performance  We'll talk about specific diagnostics later in this video sequence  But I should mention in advance that diagnostics can take time to implement and can sometimes  you know  take quite a lot of time to implement and understand but doing so can be a very good use of your time when you are developing learning algorithms because they can often save you from spending many months pursuing an avenue that you could have found out much earlier just was not going to be fruitful  So in the next few videos  I'm going to first talk about how evaluate your learning algorithms and after that I'm going to talk about some of these diagnostics which will hopefully let you much more effectively select more of the useful things to try mixing if your goal to improve the machine learning system "
MViI7GPG9xo,"Evaluating a Hypothesis  In this video  I would like to talk about how to evaluate a hypothesis that has been learned by your algorithm  In later videos  we will build on this to talk about how to prevent in the problems of overfitting and underfitting as well  When we fit the parameters of our learning algorithm we think about choosing the parameters to minimize the training error  One might think that getting a really low value of training error might be a good thing  but we have already seen that just because a hypothesis has low training error  that doesn't mean it is necessarily a good hypothesis  And we've already seen the example of how a hypothesis can overfit  And therefore fail to generalize the new examples not in the training set  So how do you tell if the hypothesis might be overfitting  In this simple example we could plot the hypothesis h of x and just see what was going on  But in general for problems with more features than just one feature  for problems with a large number of features like these it becomes hard or may be impossible to plot what the hypothesis looks like and so we need some other way to evaluate our hypothesis  The standard way to evaluate a learned hypothesis is as follows  Suppose we have a data set like this  Here I have just shown 10 training examples  but of course usually we may have dozens or hundreds or maybe thousands of training examples  In order to make sure we can evaluate our hypothesis  what we are going to do is split the data we have into two portions  The first portion is going to be our usual training set and the second portion is going to be our test set  and a pretty typical split of this all the data we have into a training set and test set might be around say a 70%  30% split  Worth more today to grade the training set and relatively less to the test set  And so now  if we have some data set  we run a sine of say 70% of the data to be our training set where here \""m\"" is as usual our number of training examples and the remainder of our data might then be assigned to become our test set  And here  I'm going to use the notation m subscript test to denote the number of test examples  And so in general  this subscript test is going to denote examples that come from a test set so that x1 subscript test  y1 subscript test is my first test example which I guess in this example might be this example over here  Finally  one last detail whereas here I've drawn this as though the first 70% goes to the training set and the last 30% to the test set  If there is any sort of ordinary to the data  That should be better to send a random 70% of your data to the training set and a random 30% of your data to the test set  So if your data were already randomly sorted  you could just take the first 70% and last 30% that if your data were not randomly ordered  it would be better to randomly shuffle or to randomly reorder the examples in your training set  Before you know sending the first 70% in the training set and the last 30% of the test set  Here then is a fairly typical procedure for how you would train and test the learning algorithm and the learning regression  First  you learn the parameters theta from the training set so you minimize the usual training error objective j of theta  where j of theta here was defined using that 70% of all the data you have  There is only the training data  And then you would compute the test error  And I am going to denote the test error as j subscript test  And so what you do is take your parameter theta that you have learned from the training set  and plug it in here and compute your test set error  Which I am going to write as follows  So this is basically the average squared error as measured on your test set  It's pretty much what you'd expect  So if we run every test example through your hypothesis with parameter theta and just measure the squared error that your hypothesis has on your m subscript test  test examples  And of course  this is the definition of the test set error if we are using linear regression and using the squared error metric  How about if we were doing a classification problem and say using logistic regression instead  In that case  the procedure for training and testing say logistic regression is pretty similar first we will do the parameters from the training data  that first 70% of the data  And it will compute the test error as follows  It's the same objective function as we always use but we just logistic regression  except that now is define using our m subscript test  test examples  While this definition of the test set error j subscript test is perfectly reasonable  Sometimes there is an alternative test sets metric that might be easier to interpret  and that's the misclassification error  It's also called the zero one misclassification error  with zero one denoting that you either get an example right or you get an example wrong  Here's what I mean  Let me define the error of a prediction  That is h of x  And given the label y as equal to one if my hypothesis outputs the value greater than equal to five and Y is equal to zero or if my hypothesis outputs a value of less than 0 5 and y is equal to one  right  so both of these cases basic respond to if your hypothesis mislabeled the example assuming your threshold at an 0 5  So either thought it was more likely to be 1  but it was actually 0  or your hypothesis stored was more likely to be 0  but the label was actually 1  And otherwise  we define this error function to be zero  If your hypothesis basically classified the example y correctly  We could then define the test error  using the misclassification error metric to be one of the m tests of sum from i equals one to m subscript test of the error of h of x(i) test comma y(i)  And so that's just my way of writing out that this is exactly the fraction of the examples in my test set that my hypothesis has mislabeled  And so that's the definition of the test set error using the misclassification error of the 0 1 misclassification metric  So that's the standard technique for evaluating how good a learned hypothesis is  In the next video  we will adapt these ideas to helping us do things like choose what features like the degree polynomial to use with the learning algorithm or choose the regularization parameter for learning algorithm "
Pu_ZzCKw1VQ,Model Selection and Train/Validation/Test Sets  Suppose you're left to decide what degree of polynomial to fit to a data set  So that what features to include that gives you a learning algorithm  Or suppose you'd like to choose the regularization parameter longer for learning algorithm  How do you do that? This account model selection process  Browsers  and in our discussion of how to do this  we'll talk about not just how to split your data into the train and test sets  but how to switch data into what we discover is called the train  validation  and test sets  We'll see in this video just what these things are  and how to use them to do model selection  We've already seen a lot of times the problem of overfitting  in which just because a learning algorithm fits a training set well  that doesn't mean it's a good hypothesis  More generally  this is why the training set's error is not a good predictor for how well the hypothesis will do on new example  Concretely  if you fit some set of parameters  Theta0  theta1  theta2  and so on  to your training set  Then the fact that your hypothesis does well on the training set  Well  this doesn't mean much in terms of predicting how well your hypothesis will generalize to new examples not seen in the training set  And a more general principle is that once your parameter is what fit to some set of data  Maybe the training set  maybe something else  Then the error of your hypothesis as measured on that same data set  such as the training error  that's unlikely to be a good estimate of your actual generalization error  That is how well the hypothesis will generalize to new examples  Now let's consider the model selection problem  Let's say you're trying to choose what degree polynomial to fit to data  So  should you choose a linear function  a quadratic function  a cubic function? All the way up to a 10th-order polynomial  So it's as if there's one extra parameter in this algorithm  which I'm going to denote d  which is  what degree of polynomial  Do you want to pick  So it's as if  in addition to the theta parameters  it's as if there's one more parameter  d  that you're trying to determine using your data set  So  the first option is d equals one  if you fit a linear function  We can choose d equals two  d equals three  all the way up to d equals 10  So  we'd like to fit this extra sort of parameter which I'm denoting by d  And concretely let's say that you want to choose a model  that is choose a degree of polynomial  choose one of these 10 models  And fit that model and also get some estimate of how well your fitted hypothesis was generalize to new examples  Here's one thing you could do  What you could  first take your first model and minimize the training error  And this would give you some parameter vector theta  And you could then take your second model  the quadratic function  and fit that to your training set and this will give you some other  Parameter vector theta  In order to distinguish between these different parameter vectors  I'm going to use a superscript one superscript two there where theta superscript one just means the parameters I get by fitting this model to my training data  And theta superscript two just means the parameters I get by fitting this quadratic function to my training data and so on  By fitting a cubic model I get parenthesis three up to  well  say theta 10  And one thing we ccould do is that take these parameters and look at test error  So I can compute on my test set J test of one  J test of theta two  and so on  J test of theta three  and so on  So I'm going to take each of my hypotheses with the corresponding parameters and just measure the performance of on the test set  Now  one thing I could do then is  in order to select one of these models  I could then see which model has the lowest test set error  And let's just say for this example that I ended up choosing the fifth order polynomial  So  this seems reasonable so far  But now let's say I want to take my fifth hypothesis  this  this  fifth order model  and let's say I want to ask  how well does this model generalize? One thing I could do is look at how well my fifth order polynomial hypothesis had done on my test set  But the problem is this will not be a fair estimate of how well my hypothesis generalizes  And the reason is what we've done is we've fit this extra parameter d  that is this degree of polynomial  And what fits that parameter d  using the test set  namely  we chose the value of d that gave us the best possible performance on the test set  And so  the performance of my parameter vector theta5  on the test set  that's likely to be an overly optimistic estimate of generalization error  Right  so  that because I had fit this parameter d to my test set is no longer fair to evaluate my hypothesis on this test set  because I fit my parameters to this test set  I've chose the degree d of polynomial using the test set  And so my hypothesis is likely to do better on this test set than it would on new examples that it hasn't seen before  and that's which is  which is what I really care about  So just to reiterate  on the previous slide  we saw that if we fit some set of parameters  you know  say theta0  theta1  and so on  to some training set  then the performance of the fitted model on the training set is not predictive of how well the hypothesis will generalize to new examples  Is because these parameters were fit to the training set  so they're likely to do well on the training set  even if the parameters don't do well on other examples  And  in the procedure I just described on this line  we just did the same thing  And specifically  what we did was  we fit this parameter d to the test set  And by having fit the parameter to the test set  this means that the performance of the hypothesis on that test set may not be a fair estimate of how well the hypothesis is  is likely to do on examples we haven't seen before  To address this problem  in a model selection setting  if we want to evaluate a hypothesis  this is what we usually do instead  Given the data set  instead of just splitting into a training test set  what we're going to do is then split it into three pieces  And the first piece is going to be called the training set as usual  So let me call this first part the training set  And the second piece of this data  I'm going to call the cross validation set  [SOUND] Cross validation  And the cross validation  as V-D  Sometimes it's also called the validation set instead of cross validation set  And then the loss can be to call the usual test set  And the pretty  pretty typical ratio at which to split these things will be to send 60% of your data's  your training set  maybe 20% to your cross validation set  and 20% to your test set  And these numbers can vary a little bit but this integration be pretty typical  And so our training sets will now be only maybe 60% of the data  and our cross-validation set  or our validation set  will have some number of examples  I'm going to denote that m subscript cv  So that's the number of cross-validation examples  Following our early notational convention I'm going to use xi cv comma y i cv  to denote the i cross validation example  And finally we also have a test set over here with our m subscript test being the number of test examples  So  now that we've defined the training validation or cross validation and test sets  We can also define the training error  cross validation error  and test error  So here's my training error  and I'm just writing this as J subscript train of theta  This is pretty much the same things  These are the same thing as the J of theta that I've been writing so far  this is just a training set error you know  as measuring a training set and then J subscript cv my cross validation error  this is pretty much what you'd expect  just like the training error you've set measure it on a cross validation data set  and here's my test set error same as before  So when faced with a model selection problem like this  what we're going to do is  instead of using the test set to select the model  we're instead going to use the validation set  or the cross validation set  to select the model  Concretely  we're going to first take our first hypothesis  take this first model  and say  minimize the cross function  and this would give me some parameter vector theta for the new model  And  as before  I'm going to put a superscript 1  just to denote that this is the parameter for the new model  We do the same thing for the quadratic model  Get some parameter vector theta two  Get some para  parameter vector theta three  and so on  down to theta ten for the polynomial  And what I'm going to do is  instead of testing these hypotheses on the test set  I'm instead going to test them on the cross validation set  And measure J subscript cv  to see how well each of these hypotheses do on my cross validation set  And then I'm going to pick the hypothesis with the lowest cross validation error  So for this example  let's say for the sake of argument  that it was my 4th order polynomial  that had the lowest cross validation error  So in that case I'm going to pick this fourth order polynomial model  And finally  what this means is that that parameter d  remember d was the degree of polynomial  right? So d equals two  d equals three  all the way up to d equals 10  What we've done is we'll fit that parameter d and we'll say d equals four  And we did so using the cross-validation set  And so this degree of polynomial  so the parameter  is no longer fit to the test set  and so we've not saved away the test set  and we can use the test set to measure  or to estimate the generalization error of the model that was selected  By the of them  So  that was model selection and how you can take your data  split it into a training  validation  and test set  And use your cross validation data to select the model and evaluate it on the test set  One final note  I should say that in  The machine learning  as of this practice today  there aren't many people that will do that early thing that I talked about  and said that  you know  it isn't such a good idea  of selecting your model using this test set  And then using the same test set to report the error as though selecting your degree of polynomial on the test set  and then reporting the error on the test set as though that were a good estimate of generalization error  That sort of practice is unfortunately many  many people do do it  If you have a massive  massive test that is maybe not a terrible thing to do  but many practitioners  most practitioners that machine learnimg tend to advise against that  And it's considered better practice to have separate train validation and test sets  I just warned you to sometimes people to do  you know  use the same data for the purpose of the validation set  and for the purpose of the test set  You need a training set and a test set  and that's good  that's practice  though you will see some people do it  But  if possible  I would recommend against doing that yourself 
3ye2OXj32DM,Diagnosing Bias vs  Variance  If you run a learning algorithm and it doesn't do as long as you are hoping  almost all the time  it will be because you have either a high bias problem or a high variance problem  in other words  either an underfitting problem or an overfitting problem  In this case  it's very important to figure out which of these two problems is bias or variance or a bit of both that you actually have  Because knowing which of these two things is happening would give a very strong indicator for whether the useful and promising ways to try to improve your algorithm  In this video  I'd like to delve more deeply into this bias and variance issue and understand them better as was figure out how to look in a learning algorithm and evaluate or diagnose whether we might have a bias problem or a variance problem since this will be critical to figuring out how to improve the performance of a learning algorithm that you will implement  So  you've already seen this figure a few times where if you fit two simple hypothesis like a straight line that underfits the data  if you fit a two complex hypothesis  then that might fit the training set perfectly but overfit the data and this may be hypothesis of some intermediate level of complexities of some maybe degree two polynomials or not too low and not too high degree that's like just right and gives you the best generalization error over these options  Now that we're armed with the notion of chain training and validation in test sets  we can understand the concepts of bias and variance a little bit better  Concretely  let's let our training error and cross validation error be defined as in the previous videos  Just say the squared error  the average squared error  as measured on the training sets or as measured on the cross validation set  Now  let's plot the following figure  On the horizontal axis I'm going to plot the degree of polynomial  So  as I go to the right I'm going to be fitting higher and higher order polynomials  So where the left of this figure where maybe d equals one  we're going to be fitting very simple functions whereas we're here on the right of the horizontal axis  I have much larger values of ds  of a much higher degree polynomial  So here  that's going to correspond to fitting much more complex functions to your training set  Let's look at the training error and the cross validation error and plot them on this figure  Let's start with the training error  As we increase the degree of the polynomial  we're going to be able to fit our training set better and better and so if d equals one  then there is high training error  if we have a very high degree of polynomial our training error is going to be really low  maybe even 0 because will fit the training set really well  So  as we increase the degree of polynomial  we find typically that the training error decreases  So I'm going to write J subscript train of theta there  because our training error tends to decrease with the degree of the polynomial that we fit to the data  Next  let's look at the cross-validation error or for that matter  if we look at the test set error  we'll get a pretty similar result as if we were to plot the cross validation error  So  we know that if d equals one  we're fitting a very simple function and so we may be underfitting the training set and so it's going to be very high cross-validation error  If we fit an intermediate degree polynomial  we had d equals two in our example in the previous slide  we're going to have a much lower cross-validation error because we're finding a much better fit to the data  Conversely  if d were too high  So if d took on say a value of four  then we're again overfitting  and so we end up with a high value for cross-validation error  So  if you were to vary this smoothly and plot a curve  you might end up with a curve like that where that's JCV of theta  Again  if you plot J test of theta you get something very similar  So  this sort of plot also helps us to better understand the notions of bias and variance  Concretely  suppose you have applied a learning algorithm and it's not performing as well as you are hoping  so if your cross-validation set error or your test set error is high  how can we figure out if the learning algorithm is suffering from high bias or suffering from high variance? So  the setting of a cross-validation error being high corresponds to either this regime or this regime  So  this regime on the left corresponds to a high bias problem  That is  if you are fitting a overly low order polynomial such as a d equals one when we really needed a higher order polynomial to fit to data  whereas in contrast this regime corresponds to a high variance problem  That is  if d the degree of polynomial was too large for the data set that we have  and this figure gives us a clue for how to distinguish between these two cases  Concretely  for the high bias case  that is the case of underfitting  what we find is that both the cross validation error and the training error are going to be high  So  if your algorithm is suffering from a bias problem  the training set error will be high and you might find that the cross validation error will also be high  It might be close  maybe just slightly higher  than the training error  So  if you see this combination  that's a sign that your algorithm may be suffering from high bias  In contrast  if your algorithm is suffering from high variance  then if you look here  we'll notice that J train  that is the training error  is going to be low  That is  you're fitting the training set very well  whereas your cross validation error assuming that this is  say  the squared error which we're trying to minimize say  whereas in contrast your error on a cross validation set or your cross function or cross validation set will be much bigger than your training set error  So  this is a double greater than sign  That's the map symbol for much greater thans  denoted by two greater than signs  So if you see this combination of values  then that's a clue that your learning algorithm may be suffering from high variance and might be overfitting  The key that distinguishes these two cases is  if you have a high bias problem  your training set error will also be high is your hypothesis just not fitting the training set well  If you have a high variance problem  your training set error will usually be low  that is much lower than your cross-validation error  So hopefully that gives you a somewhat better understanding of the two problems of bias and variance  I still have a lot more to say about bias and variance in the next few videos  but what we'll see later is that by diagnosing whether a learning algorithm may be suffering from high bias or high variance  I'll show you even more details on how to do that in later videos  But we'll see that by figuring out whether a learning algorithm may be suffering from high bias or high variance or combination of both  that that would give us much better guidance for what might be promising things to try in order to improve the performance of a learning algorithm 
V4mP2pQyou0,Regularization and Bias/Variance  You've seen how regularization can help prevent over-fitting  But how does it affect the bias and variances of a learning algorithm? In this video I'd like to go deeper into the issue of bias and variances and talk about how it interacts with and is affected by the regularization of your learning algorithm  Suppose we're fitting a high auto polynomial  like that showed here  but to prevent over fitting we need to use regularization  like that shown here  So we have this regularization term to try to keep the values of the prem to small  And as usual  the regularizations comes from J = 1 to m  rather than j = 0 to m  Let's consider three cases  The first is the case of the very large value of the regularization parameter lambda  such as if lambda were equal to 10 000  Some huge value  In this case  all of these parameters  theta 1  theta 2  theta 3  and so on would be heavily penalized and so we end up with most of these parameter values being closer to zero  And the hypothesis will be roughly h of x  just equal or approximately equal to theta zero  So we end up with a hypothesis that more or less looks like that  more or less a flat  constant straight line  And so this hypothesis has high bias and it badly under fits this data set  so the horizontal straight line is just not a very good model for this data set  At the other extreme is if we have a very small value of lambda  such as if lambda were equal to zero  In that case  given that we're fitting a high order polynomial  this is a usual over-fitting setting  In that case  given that we're fitting a high-order polynomial  basically  without regularization or with very minimal regularization  we end up with our usual high-variance  over fitting setting  This is basically if lambda is equal to zero  we're just fitting with our regularization  so that over fits the hypothesis  And it's only if we have some intermediate value of longer that is neither too large nor too small that we end up with parameters data that give us a reasonable fit to this data  So  how can we automatically choose a good value for the regularization parameter? Just to reiterate  here's our model  and here's our learning algorithm's objective  For the setting where we're using regularization  let me define J train(theta) to be something different  to be the optimization objective  but without the regularization term  Previously  in an earlier video  when we were not using regularization I define J train of data to be the same as J of theta as the cause function but when we're using regularization when the six well under term we're going to define J train my training set to be just my sum of squared errors on the training set or my average squared error on the training set without taking into account that regularization  And similarly I'm then also going to define the cross validation sets error and to test that error as before to be the average sum of squared errors on the cross validation in the test sets so just to summarize my definitions of J train J CU and J test are just the average square there one half of the other square record on the training validation of the test set without the extra regularization term  So  this is how we can automatically choose the regularization parameter lambda  So what I usually do is maybe have some range of values of lambda I want to try out  So I might be considering not using regularization or here are a few values I might try lambda considering lambda = 0 01  0 02  0 04  and so on  And I usually set these up in multiples of two  until some maybe larger value if I were to do these in multiples of 2 I'd end up with a 10 24  It's 10 exactly  but this is close enough  And the three to four decimal places won't effect your result that much  So  this gives me maybe 12 different models  And I'm trying to select a month corresponding to 12 different values of the regularization of the parameter lambda  And of course you can also go to values less than 0 01 or values larger than 10 but I've just truncated it here for convenience  Given the issue of these 12 models  what we can do is then the following  we can take this first model with lambda equals zero and minimize my cost function J of data and this will give me some parameter of active data  And similar to the earlier video  let me just denote this as theta super script one  And then I can take my second model with lambda set to 0 01 and minimize my cost function now using lambda equals 0 01 of course  To get some different parameter vector theta  Let me denote that theta(2)  And for that I end up with theta(3)  So if part for my third model  And so on until for my final model with lambda set to 10 or 10 24  I end up with this theta(12)  Next  I can talk all of these hypotheses  all of these parameters and use my cross validation set to validate them so I can look at my first model  my second model  fit to these different values of the regularization parameter  and evaluate them with my cross validation set based in measure the average square error of each of these square vector parameters theta on my cross validation sets  And I would then pick whichever one of these 12 models gives me the lowest error on the trans validation set  And let's say  for the sake of this example  that I end up picking theta 5  the 5th order polynomial  because that has the lowest cause validation error  Having done that  finally what I would do if I wanted to report each test set error  is to take the parameter theta 5 that I've selected  and look at how well it does on my test set  So once again  here is as if we've fit this parameter  theta  to my cross-validation set  which is why I'm setting aside a separate test set that I'm going to use to get a better estimate of how well my parameter vector  theta  will generalize to previously unseen examples  So that's model selection applied to selecting the regularization parameter lambda  The last thing I'd like to do in this video is get a better understanding of how cross validation and training error vary as we vary the regularization parameter lambda  And so just a reminder right  that was our original cost on j of theta  But for this purpose we're going to define training error without using a regularization parameter  and cross validation error without using the regularization parameter  And what I'd like to do is plot this Jtrain and plot this Jcv  meaning just how well does my hypothesis do on the training set and how does my hypothesis do when it cross validation sets  As I vary my regularization parameter lambda  So as we saw earlier if lambda is small then we're not using much regularization and we run a larger risk of over fitting whereas if lambda is large that is if we were on the right part of this horizontal axis then  with a large value of lambda  we run the higher risk of having a biased problem  so if you plot J train and J cv  what you find is that  for small values of lambda  you can fit the trading set relatively way cuz you're not regularizing  So  for small values of lambda  the regularization term basically goes away  and you're just minimizing pretty much just gray arrows  So when lambda is small  you end up with a small value for Jtrain  whereas if lambda is large  then you have a high bias problem  and you might not feel your training that well  so you end up the value up there  So Jtrain of theta will tend to increase when lambda increases  because a large value of lambda corresponds to high bias where you might not even fit your trainings that well  whereas a small value of lambda corresponds to  if you can really fit a very high degree polynomial to your data  let's say  After the cost validation error we end up with a figure like this  where over here on the right  if we have a large value of lambda  we may end up under fitting  and so this is the bias regime  And so the cross validation error will be high  Let me just leave all of that to this Jcv (theta) because so  with high bias  we won't be fitting  we won't be doing well in cross validation sets  whereas here on the left  this is the high variance regime  where we have two smaller value with longer  then we may be over fitting the data  And so by over fitting the data  then the cross validation error will also be high  And so  this is what the cross validation error and what the trading error may look like on a trading stance as we vary the regularization parameter lambda  And so once again  it will often be some intermediate value of lambda that is just right or that works best In terms of having a small cross validation error or a small test theta  And whereas the curves I've drawn here are somewhat cartoonish and somewhat idealized so on the real data set the curves you get may end up looking a little bit more messy and just a little bit more noisy then this  For some data sets you will really see these for sorts of trends and by looking at a plot of the hold-out cross validation error you can either manual  automatically try to select a point that minimizes the cross validation error and select the value of lambda corresponding to low cross validation error  When I'm trying to pick the regularization parameter lambda for learning algorithm  often I find that plotting a figure like this one shown here helps me understand better what's going on and helps me verify that I am indeed picking a good value for the regularization parameter monitor  So hopefully that gives you more insight into regularization and it's effects on the bias and variance of a learning algorithm  By now you've seen bias and variance from a lot of different perspectives  And what we like to do in the next video is take all the insights we've gone through and build on them to put together a diagnostic that's called learning curves  which is a tool that I often use to diagnose if the learning algorithm may be suffering from a bias problem or a variance problem  or a little bit of both 
ujT6nt_hHEc,Learning Curves  In this video  I'd like to tell you about learning curves  Learning curves is often a very useful thing to plot  If either you wanted to sanity check that your algorithm is working correctly  or if you want to improve the performance of the algorithm  And learning curves is a tool that I actually use very often to try to diagnose if a physical learning algorithm may be suffering from bias  sort of variance problem or a bit of both  Here's what a learning curve is  To plot a learning curve  what I usually do is plot j train which is  say  average squared error on my training set or Jcv which is the average squared error on my cross validation set  And I'm going to plot that as a function of m  that is as a function of the number of training examples I have  And so m is usually a constant like maybe I just have  you know  a 100 training examples but what I'm going to do is artificially with use my training set exercise  So  I deliberately limit myself to using only  say  10 or 20 or 30 or 40 training examples and plot what the training error is and what the cross validation is for this smallest training set exercises  So let's see what these plots may look like  Suppose I have only one training example like that shown in this this first example here and let's say I'm fitting a quadratic function  Well  I have only one training example  I'm going to be able to fit it perfectly right? You know  just fit the quadratic function  I'm going to have 0 error on the one training example  If I have two training examples  Well the quadratic function can also fit that very well  So  even if I am using regularization  I can probably fit this quite well  And if I am using no neural regularization  I'm going to fit this perfectly and if I have three training examples again  Yeah  I can fit a quadratic function perfectly so if m equals 1 or m equals 2 or m equals 3  my training error on my training set is going to be 0 assuming I'm not using regularization or it may slightly large in 0 if I'm using regularization and by the way if I have a large training set and I'm artificially restricting the size of my training set in order to J train  Here if I set M equals 3  say  and I train on only three examples  then  for this figure I am going to measure my training error only on the three examples that actually fit my data too and so even I have to say a 100 training examples but if I want to plot what my training error is the m equals 3  What I'm going to do is to measure the training error on the three examples that I've actually fit to my hypothesis 2  And not all the other examples that I have deliberately omitted from the training process  So just to summarize what we've seen is that if the training set size is small then the training error is going to be small as well  Because you know  we have a small training set is going to be very easy to fit your training set very well may be even perfectly now say we have m equals 4 for example  Well then a quadratic function can be a longer fit this data set perfectly and if I have m equals 5 then you know  maybe quadratic function will fit to stay there so so  then as my training set gets larger  It becomes harder and harder to ensure that I can find the quadratic function that process through all my examples perfectly  So in fact as the training set size grows what you find is that my average training error actually increases and so if you plot this figure what you find is that the training set error that is the average error on your hypothesis grows as m grows and just to repeat when the intuition is that when m is small when you have very few training examples  It's pretty easy to fit every single one of your training examples perfectly and so your error is going to be small whereas when m is larger then gets harder all the training examples perfectly and so your training set error becomes more larger now  how about the cross validation error  Well  the cross validation is my error on this cross validation set that I haven't seen and so  you know  when I have a very small training set  I'm not going to generalize well  just not going to do well on that  So  right  this hypothesis here doesn't look like a good one  and it's only when I get a larger training set that  you know  I'm starting to get hypotheses that maybe fit the data somewhat better  So your cross validation error and your test set error will tend to decrease as your training set size increases because the more data you have  the better you do at generalizing to new examples  So  just the more data you have  the better the hypothesis you fit  So if you plot j train  and Jcv this is the sort of thing that you get  Now let's look at what the learning curves may look like if we have either high bias or high variance problems  Suppose your hypothesis has high bias and to explain this I'm going to use a  set an example  of fitting a straight line to data that  you know  can't really be fit well by a straight line  So we end up with a hypotheses that maybe looks like that  Now let's think what would happen if we were to increase the training set size  So if instead of five examples like what I've drawn there  imagine that we have a lot more training examples  Well what happens  if you fit a straight line to this  What you find is that  you end up with you know  pretty much the same straight line  I mean a straight line that just cannot fit this data and getting a ton more data  well the straight line isn't going to change that much  This is the best possible straight-line fit to this data  but the straight line just can't fit this data set that well  So  if you plot across validation error  this is what it will look like  Option on the left  if you have already a miniscule training set size like you know  maybe just one training example and is not going to do well  But by the time you have reached a certain number of training examples  you have almost fit the best possible straight line  and even if you end up with a much larger training set size  a much larger value of m  you know  you're basically getting the same straight line  and so  the cross-validation error - let me label that - or test set error or plateau out  or flatten out pretty soon  once you reached beyond a certain the number of training examples  unless you pretty much fit the best possible straight line  And how about training error? Well  the training error will again be small  And what you find in the high bias case is that the training error will end up close to the cross validation error  because you have so few parameters and so much data  at least when m is large  The performance on the training set and the cross validation set will be very similar  And so  this is what your learning curves will look like  if you have an algorithm that has high bias  And finally  the problem with high bias is reflected in the fact that both the cross validation error and the training error are high  and so you end up with a relatively high value of both Jcv and the j train  This also implies something very interesting  which is that  if a learning algorithm has high bias  as we get more and more training examples  that is  as we move to the right of this figure  we'll notice that the cross validation error isn't going down much  it's basically fattened up  and so if learning algorithms are really suffering from high bias  Getting more training data by itself will actually not help that much and as our figure example in the figure on the right  here we had only five training  examples  and we fill certain straight line  And when we had a ton more training data  we still end up with roughly the same straight line  And so if the learning algorithm has high bias give me a lot more training data  That doesn't actually help you get a much lower cross validation error or test set error  So knowing if your learning algorithm is suffering from high bias seems like a useful thing to know because this can prevent you from wasting a lot of time collecting more training data where it might just not end up being helpful  Next let us look at the setting of a learning algorithm that may have high variance  Let us just look at the training error in a around if you have very smart training set like five training examples shown on the figure on the right and if we're fitting say a very high order polynomial  and I've written a hundredth degree polynomial which really no one uses  but just an illustration  And if we're using a fairly small value of lambda  maybe not zero  but a fairly small value of lambda  then we'll end up  you know  fitting this data very well that with a function that overfits this  So  if the training set size is small  our training error  that is  j train of theta will be small  And as this training set size increases a bit  you know  we may still be overfitting this data a little bit but it also becomes slightly harder to fit this data set perfectly  and so  as the training set size increases  we'll find that j train increases  because it is just a little harder to fit the training set perfectly when we have more examples  but the training set error will still be pretty low  Now  how about the cross validation error? Well  in high variance setting  a hypothesis is overfitting and so the cross validation error will remain high  even as we get you know  a moderate number of training examples and  so maybe  the cross validation error may look like that  And the indicative diagnostic that we have a high variance problem  is the fact that there's this large gap between the training error and the cross validation error  And looking at this figure  If we think about adding more training data  that is  taking this figure and extrapolating to the right  we can kind of tell that  you know the two curves  the blue curve and the magenta curve  are converging to each other  And so  if we were to extrapolate this figure to the right  then it seems it likely that the training error will keep on going up and the cross-validation error would keep on going down  And the thing we really care about is the cross-validation error or the test set error  right? So in this sort of figure  we can tell that if we keep on adding training examples and extrapolate to the right  well our cross validation error will keep on coming down  And  so  in the high variance setting  getting more training data is  indeed  likely to help  And so again  this seems like a useful thing to know if your learning algorithm is suffering from a high variance problem  because that tells you  for example that it may be be worth your while to see if you can go and get some more training data  Now  on the previous slide and this slide  I've drawn fairly clean fairly idealized curves  If you plot these curves for an actual learning algorithm  sometimes you will actually see  you know  pretty much curves  like what I've drawn here  Although  sometimes you see curves that are a little bit noisier and a little bit messier than this  But plotting learning curves like these can often tell you  can often help you figure out if your learning algorithm is suffering from bias  or variance or even a little bit of both  So when I'm trying to improve the performance of a learning algorithm  one thing that I'll almost always do is plot these learning curves  and usually this will give you a better sense of whether there is a bias or variance problem  And in the next video we'll see how this can help suggest specific actions is to take  or to not take  in order to try to improve the performance of your learning algorithm 
G3OwDpYTJPE,Deciding What to Do Next Revisited  We've talked about how to evaluate learning algorithms  talked about model selection  talked a lot about bias and variance  So how does this help us figure out what are potentially fruitful  potentially not fruitful things to try to do to improve the performance of a learning algorithm  Let's go back to our original motivating example and go for the result  So here is our earlier example of maybe having fit regularized linear regression and finding that it doesn't work as well as we're hoping  We said that we had this menu of options  So is there some way to figure out which of these might be fruitful options? The first thing all of this was getting more training examples  What this is good for  is this helps to fix high variance  And concretely  if you instead have a high bias problem and don't have any variance problem  then we saw in the previous video that getting more training examples  while maybe just isn't going to help much at all  So the first option is useful only if you  say  plot the learning curves and figure out that you have at least a bit of a variance  meaning that the cross-validation error is  you know  quite a bit bigger than your training set error  How about trying a smaller set of features? Well  trying a smaller set of features  that's again something that fixes high variance  And in other words  if you figure out  by looking at learning curves or something else that you used  that have a high bias problem  then for goodness sakes  don't waste your time trying to carefully select out a smaller set of features to use  Because if you have a high bias problem  using fewer features is not going to help  Whereas in contrast  if you look at the learning curves or something else you figure out that you have a high variance problem  then  indeed trying to select out a smaller set of features  that might indeed be a very good use of your time  How about trying to get additional features  adding features  usually  not always  but usually we think of this as a solution for fixing high bias problems  So if you are adding extra features it's usually because your current hypothesis is too simple  and so we want to try to get additional features to make our hypothesis better able to fit the training set  And similarly  adding polynomial features  this is another way of adding features and so there is another way to try to fix the high bias problem  And  if concretely if your learning curves show you that you still have a high variance problem  then  you know  again this is maybe a less good use of your time  And finally  decreasing and increasing lambda  This are quick and easy to try  I guess these are less likely to be a waste of  you know  many months of your life  But decreasing lambda  you already know fixes high bias  In case this isn't clear to you  you know  I do encourage you to pause the video and think through this that convince yourself that decreasing lambda helps fix high bias  whereas increasing lambda fixes high variance  And if you aren't sure why this is the case  do pause the video and make sure you can convince yourself that this is the case  Or take a look at the curves that we were plotting at the end of the previous video and try to make sure you understand why these are the case  Finally  let us take everything we have learned and relate it back to neural networks and so  here is some practical advice for how I usually choose the architecture or the connectivity pattern of the neural networks I use  So  if you are fitting a neural network  one option would be to fit  say  a pretty small neural network with you know  relatively few hidden units  maybe just one hidden unit  If you're fitting a neural network  one option would be to fit a relatively small neural network with  say  relatively few  maybe only one hidden layer and maybe only a relatively few number of hidden units  So  a network like this might have relatively few parameters and be more prone to underfitting  The main advantage of these small neural networks is that the computation will be cheaper  An alternative would be to fit a  maybe relatively large neural network with either more hidden units--there's a lot of hidden in one there--or with more hidden layers  And so these neural networks tend to have more parameters and therefore be more prone to overfitting  One disadvantage  often not a major one but something to think about  is that if you have a large number of neurons in your network  then it can be more computationally expensive  Although within reason  this is often hopefully not a huge problem  The main potential problem of these much larger neural networks is that it could be more prone to overfitting and it turns out if you're applying neural network very often using a large neural network often it's actually the larger  the better but if it's overfitting  you can then use regularization to address overfitting  usually using a larger neural network by using regularization to address is overfitting that's often more effective than using a smaller neural network  And the main possible disadvantage is that it can be more computationally expensive  And finally  one of the other decisions is  say  the number of hidden layers you want to have  right? So  do you want one hidden layer or do you want three hidden layers  as we've shown here  or do you want two hidden layers? And usually  as I think I said in the previous video  using a single hidden layer is a reasonable default  but if you want to choose the number of hidden layers  one other thing you can try is find yourself a training cross-validation  and test set split and try training neural networks with one hidden layer or two hidden layers or three hidden layers and see which of those neural networks performs best on the cross-validation sets  You take your three neural networks with one  two and three hidden layers  and compute the cross validation error at Jcv and all of them and use that to select which of these is you think the best neural network  So  that's it for bias and variance and ways like learning curves  who tried to diagnose these problems  As far as what you think is implied  for one might be truthful or not truthful things to try to improve the performance of a learning algorithm  If you understood the contents of the last few videos and if you apply them you actually be much more effective already and getting learning algorithms to work on problems and even a large fraction  maybe the majority of practitioners of machine learning here in Silicon Valley today doing these things as their full-time jobs  So I hope that these pieces of advice on by experience in diagnostics will help you to much effectively and powerfully apply learning and get them to work very well 
GzpaDTbXMiQ,"Prioritizing What to Work On  In the next few videos I'd like to talk about machine learning system design  These videos will touch on the main issues that you may face when designing a complex machine learning system  and will actually try to give advice on how to strategize putting together a complex machine learning system  In case this next set of videos seems a little disjointed that's because these videos will touch on a range of the different issues that you may come across when designing complex learning systems  And even though the next set of videos may seem somewhat less mathematical  I think that this material may turn out to be very useful  and potentially huge time savers when you're building big machine learning systems  Concretely  I'd like to begin with the issue of prioritizing how to spend your time on what to work on  and I'll begin with an example on spam classification  Let's say you want to build a spam classifier  Here are a couple of examples of obvious spam and non-spam emails  if the one on the left tried to sell things  And notice how spammers will deliberately misspell words  like Vincent with a 1 there  and mortgages  And on the right as maybe an obvious example of non-stamp email  actually email from my younger brother  Let's say we have a labeled training set of some number of spam emails and some non-spam emails denoted with labels y equals 1 or 0  how do we build a classifier using supervised learning to distinguish between spam and non-spam? In order to apply supervised learning  the first decision we must make is how do we want to represent x  that is the features of the email  Given the features x and the labels y in our training set  we can then train a classifier  for example using logistic regression  Here's one way to choose a set of features for our emails  We could come up with  say  a list of maybe a hundred words that we think are indicative of whether e-mail is spam or non-spam  for example  if a piece of e-mail contains the word 'deal' maybe it's more likely to be spam if it contains the word 'buy' maybe more likely to be spam  a word like 'discount' is more likely to be spam  whereas if a piece of email contains my name  Andrew  maybe that means the person actually knows who I am and that might mean it's less likely to be spam  And maybe for some reason I think the word \""now\"" may be indicative of non-spam because I get a lot of urgent emails  and so on  and maybe we choose a hundred words or so  Given a piece of email  we can then take this piece of email and encode it into a feature vector as follows  I'm going to take my list of a hundred words and sort them in alphabetical order say  It doesn't have to be sorted  But  you know  here's a  here's my list of words  just count and so on  until eventually I'll get down to now  and so on and given a piece of e-mail like that shown on the right  I'm going to check and see whether or not each of these words appears in the e-mail and then I'm going to define a feature vector x where in this piece of an email on the right  my name doesn't appear so I'm gonna put a zero there  The word \""by\"" does appear  so I'm gonna put a one there and I'm just gonna put one's or zeroes  I'm gonna put a one even though the word \""by\"" occurs twice  I'm not gonna recount how many times the word occurs  The word \""due\"" appears  I put a one there  The word \""discount\"" doesn't appear  at least not in this this little short email  and so on  The word \""now\"" does appear and so on  So I put ones and zeroes in this feature vector depending on whether or not a particular word appears  And in this example my feature vector would have to mention one hundred  if I have a hundred  if if I chose a hundred words to use for this representation and each of my features Xj will basically be 1 if you have a particular word that  we'll call this word j  appears in the email and Xj would be zero otherwise  Okay  So that gives me a feature representation of a piece of email  By the way  even though I've described this process as manually picking a hundred words  in practice what's most commonly done is to look through a training set  and in the training set depict the most frequently occurring n words where n is usually between ten thousand and fifty thousand  and use those as your features  So rather than manually picking a hundred words  here you look through the training examples and pick the most frequently occurring words like ten thousand to fifty thousand words  and those form the features that you are going to use to represent your email for spam classification  Now  if you're building a spam classifier one question that you may face is  what's the best use of your time in order to make your spam classifier have higher accuracy  you have lower error  One natural inclination is going to collect lots of data  Right? And in fact there's this tendency to think that  well the more data we have the better the algorithm will do  And in fact  in the email spam domain  there are actually pretty serious projects called Honey Pot Projects  which create fake email addresses and try to get these fake email addresses into the hands of spammers and use that to try to collect tons of spam email  and therefore you know  get a lot of spam data to train learning algorithms  But we've already seen in the previous sets of videos that getting lots of data will often help  but not all the time  But for most machine learning problems  there are a lot of other things you could usually imagine doing to improve performance  For spam  one thing you might think of is to develop more sophisticated features on the email  maybe based on the email routing information  And this would be information contained in the email header  So  when spammers send email  very often they will try to obscure the origins of the email  and maybe use fake email headers  Or send email through very unusual sets of computer service  Through very unusual routes  in order to get the spam to you  And some of this information will be reflected in the email header  And so one can imagine  looking at the email headers and trying to develop more sophisticated features to capture this sort of email routing information to identify if something is spam  Something else you might consider doing is to look at the email message body  that is the email text  and try to develop more sophisticated features  For example  should the word 'discount' and the word 'discounts' be treated as the same words or should we have treat the words 'deal' and 'dealer' as the same word? Maybe even though one is lower case and one in capitalized in this example  Or do we want more complex features about punctuation because maybe spam is using exclamation marks a lot more  I don't know  And along the same lines  maybe we also want to develop more sophisticated algorithms to detect and maybe to correct to deliberate misspellings  like mortgage  medicine  watches  Because spammers actually do this  because if you have watches with a 4 in there then well  with the simple technique that we talked about just now  the spam classifier might not equate this as the same thing as the word \""watches \"" and so it may have a harder time realizing that something is spam with these deliberate misspellings  And this is why spammers do it  While working on a machine learning problem  very often you can brainstorm lists of different things to try  like these  By the way  I've actually worked on the spam problem myself for a while  And I actually spent quite some time on it  And even though I kind of understand the spam problem  I actually know a bit about it  I would actually have a very hard time telling you of these four options which is the best use of your time so what happens  frankly what happens far too often is that a research group or product group will randomly fixate on one of these options  And sometimes that turns out not to be the most fruitful way to spend your time depending  you know  on which of these options someone ends up randomly fixating on  By the way  in fact  if you even get to the stage where you brainstorm a list of different options to try  you're probably already ahead of the curve  Sadly  what most people do is instead of trying to list out the options of things you might try  what far too many people do is wake up one morning and  for some reason  just  you know  have a weird gut feeling that  \""Oh let's have a huge honeypot project to go and collect tons more data\"" and for whatever strange reason just sort of wake up one morning and randomly fixate on one thing and just work on that for six months  But I think we can do better  And in particular what I'd like to do in the next video is tell you about the concept of error analysis and talk about the way where you can try to have a more systematic way to choose amongst the options of the many different things you might work  and therefore be more likely to select what is actually a good way to spend your time  you know for the next few weeks  or next few days or the next few months "
dBeyFiabVIk,Error Analysis  In the last video I talked about how  when faced with a machine learning problem  there are often lots of different ideas for how to improve the algorithm  In this video  let's talk about the concept of error analysis  Which will hopefully give you a way to more systematically make some of these decisions  If you're starting work on a machine learning problem  or building a machine learning application  It's often considered very good practice to start  not by building a very complicated system with lots of complex features and so on  But to instead start by building a very simple algorithm that you can implement quickly  And when I start with a learning problem what I usually do is spend at most one day  like literally at most 24 hours  To try to get something really quick and dirty  Frankly not at all sophisticated system but get something really quick and dirty running  and implement it and then test it on my cross-validation data  Once you've done that you can then plot learning curves  this is what we talked about in the previous set of videos  But plot learning curves of the training and test errors to try to figure out if you're learning algorithm maybe suffering from high bias or high variance  or something else  And use that to try to decide if having more data  more features  and so on are likely to help  And the reason that this is a good approach is often  when you're just starting out on a learning problem  there's really no way to tell in advance  Whether you need more complex features  or whether you need more data  or something else  And it's just very hard to tell in advance  that is  in the absence of evidence  in the absence of seeing a learning curve  It's just incredibly difficult to figure out where you should be spending your time  And it's often by implementing even a very  very quick and dirty implementation  And by plotting learning curves  that helps you make these decisions  So if you like you can to think of this as a way of avoiding whats sometimes called premature optimization in computer programming  And this idea that says we should let evidence guide our decisions on where to spend our time rather than use gut feeling  which is often wrong  In addition to plotting learning curves  one other thing that's often very useful to do is what's called error analysis  And what I mean by that is that when building say a spam classifier  I will often look at my cross validation set and manually look at the emails that my algorithm is making errors on  So look at the spam e-mails and non-spam e-mails that the algorithm is misclassifying and see if you can spot any systematic patterns in what type of examples it is misclassifying  And often  by doing that  this is the process that will inspire you to design new features  Or they'll tell you what are the current things or current shortcomings of the system  And give you the inspiration you need to come up with improvements to it  Concretely  here's a specific example  Let's say you've built a spam classifier and you have 500 examples in your cross validation set  And let's say in this example that the algorithm has a very high error rate  And this classifies 100 of these cross validation examples  So what I do is manually examine these 100 errors and manually categorize them  Based on things like what type of email it is  what cues or what features you think might have helped the algorithm classify them correctly  So  specifically  by what type of email it is  if I look through these 100 errors  I might find that maybe the most common types of spam emails in these classifies are maybe emails on pharma or pharmacies  trying to sell drugs  Maybe emails that are trying to sell replicas such as fake watches  fake random things  maybe some emails trying to steal passwords   These are also called phishing emails  that's another big category of emails  and maybe other categories  So in terms of classify what type of email it is  I would actually go through and count up my hundred emails  Maybe I find that 12 of them is label emails  or pharma emails  and maybe 4 of them are emails trying to sell replicas  that sell fake watches or something  And maybe I find that 53 of them are these what's called phishing emails  basically emails trying to persuade you to give them your password  And 31 emails are other types of emails  And it's by counting up the number of emails in these different categories that you might discover  for example  That the algorithm is doing really  particularly poorly on emails trying to steal passwords  And that may suggest that it might be worth your effort to look more carefully at that type of email and see if you can come up with better features to categorize them correctly  And  also what I might do is look at what cues or what additional features might have helped the algorithm classify the emails  So let's say that some of our hypotheses about things or features that might help us classify emails better are  Trying to detect deliberate misspellings versus unusual email routing versus unusual spamming punctuation  Such as if people use a lot of exclamation marks  And once again I would manually go through and let's say I find five cases of this and 16 of this and 32 of this and a bunch of other types of emails as well  And if this is what you get on your cross validation set  then it really tells you that maybe deliberate spellings is a sufficiently rare phenomenon that maybe it's not worth all the time trying to write algorithms that detect that  But if you find that a lot of spammers are using  you know  unusual punctuation  then maybe that's a strong sign that it might actually be worth your while to spend the time to develop more sophisticated features based on the punctuation  So this sort of error analysis  which is really the process of manually examining the mistakes that the algorithm makes  can often help guide you to the most fruitful avenues to pursue  And this also explains why I often recommend implementing a quick and dirty implementation of an algorithm  What we really want to do is figure out what are the most difficult examples for an algorithm to classify  And very often for different algorithms  for different learning algorithms they'll often find similar categories of examples difficult  And by having a quick and dirty implementation  that's often a quick way to let you identify some errors and quickly identify what are the hard examples  So that you can focus your effort on those  Lastly  when developing learning algorithms  one other useful tip is to make sure that you have a numerical evaluation of your learning algorithm  And what I mean by that is you if you're developing a learning algorithm  it's often incredibly helpful  If you have a way of evaluating your learning algorithm that just gives you back a single real number  maybe accuracy  maybe error  But the single real number that tells you how well your learning algorithm is doing  I'll talk more about this specific concept in later videos  but here's a specific example  Let's say we're trying to decide whether or not we should treat words like discount  discounts  discounted  discounting as the same word  So you know maybe one way to do that is to just look at the first few characters in the word like  you know  If you just look at the first few characters of a word  then you figure out that maybe all of these words roughly have similar meanings  In natural language processing  the way that this is done is actually using a type of software called stemming software  And if you ever want to do this yourself  search on a web-search engine for the porter stemmer  and that would be one reasonable piece of software for doing this sort of stemming  which will let you treat all these words  discount  discounts  and so on  as the same word  But using a stemming software that basically looks at the first few alphabets of a word  more of less  it can help  but it can hurt  And it can hurt because for example  the software may mistake the words universe and university as being the same thing  Because  you know  these two words start off with the same alphabets  So if you're trying to decide whether or not to use stemming software for a spam cross classifier  it's not always easy to tell  And in particular  error analysis may not actually be helpful for deciding if this sort of stemming idea is a good idea  Instead  the best way to figure out if using stemming software is good to help your classifier is if you have a way to very quickly just try it and see if it works  And in order to do this  having a way to numerically evaluate your algorithm is going to be very helpful  Concretely  maybe the most natural thing to do is to look at the cross validation error of the algorithm's performance with and without stemming  So  if you run your algorithm without stemming and end up with 5 percent classification error  And you rerun it and you end up with 3 percent classification error  then this decrease in error very quickly allows you to decide that it looks like using stemming is a good idea  For this particular problem  there's a very natural  single  real number evaluation metric  namely the cross validation error  We'll see later examples where coming up with this sort of single  real number evaluation metric will need a little bit more work  But as we'll see in a later video  doing so would also then let you make these decisions much more quickly of say  whether or not to use stemming  And  just as one more quick example  let's say that you're also trying to decide whether or not to distinguish between upper versus lower case  So  you know  as the word  mom  were upper case  and versus lower case m  should that be treated as the same word or as different words? Should this be treated as the same feature  or as different features? And so  once again  because we have a way to evaluate our algorithm  If you try this down here  if I stopped distinguishing upper and lower case  maybe I end up with 3 2 percent error  And I find that therefore  this does worse than if I use only stemming  So  this let's me very quickly decide to go ahead and to distinguish or to not distinguish between upper and lowercase  So when you're developing a learning algorithm  very often you'll be trying out lots of new ideas and lots of new versions of your learning algorithm  If every time you try out a new idea  if you end up manually examining a bunch of examples again to see if it got better or worse  that's gonna make it really hard to make decisions on  Do you use stemming or not? Do you distinguish upper and lower case or not? But by having a single real number evaluation metric  you can then just look and see  oh  did the arrow go up or did it go down? And you can use that to much more rapidly try out new ideas and almost right away tell if your new idea has improved or worsened the performance of the learning algorithm  And this will let you often make much faster progress  So the recommended  strongly recommended the way to do error analysis is on the cross validations there rather than the test set  But  you know  there are people that will do this on the test set  even though that's definitely a less mathematic appropriate  certainly a less recommended way to  thing to do than to do error analysis on your cross validation set  Set to wrap up this video  when starting on a new machine learning problem  what I almost always recommend is to implement a quick and dirty implementation of your learning out of them  And I've almost never seen anyone spend too little time on this quick and dirty implementation  I've pretty much only ever seen people spend much too much time building their first  supposedly  quick and dirty implementation  So really  don't worry about it being too quick  or don't worry about it being too dirty  But really  implement something as quickly as you can  And once you have the initial implementation  this is then a powerful tool for deciding where to spend your time next  Because first you can look at the errors it makes  and do this sort of error analysis to see what other mistakes it makes  and use that to inspire further development  And second  assuming your quick and dirty implementation incorporated a single real number evaluation metric  This can then be a vehicle for you to try out different ideas and quickly see if the different ideas you're trying out are improving the performance of your algorithm  And therefore let you  maybe much more quickly make decisions about what things to fold in and what things to incorporate into your learning algorithm 
umRDw1LLjJM,"Error Metrics for Skewed Classes  In the previous video  I talked about error analysis and the importance of having error metrics  that is of having a single real number evaluation metric for your learning algorithm to tell how well it's doing  In the context of evaluation and of error metrics  there is one important case  where it's particularly tricky to come up with an appropriate error metric  or evaluation metric  for your learning algorithm  That case is the case of what's called skewed classes  Let me tell you what that means  Consider the problem of cancer classification  where we have features of medical patients and we want to decide whether or not they have cancer  So this is like the malignant versus benign tumor classification example that we had earlier  So let's say y equals 1 if the patient has cancer and y equals 0 if they do not  We have trained the progression classifier and let's say we test our classifier on a test set and find that we get 1 percent error  So  we're making 99% correct diagnosis  Seems like a really impressive result  right  We're correct 99% percent of the time  But now  let's say we find out that only 0 5 percent of patients in our training test sets actually have cancer  So only half a percent of the patients that come through our screening process have cancer  In this case  the 1% error no longer looks so impressive  And in particular  here's a piece of code  here's actually a piece of non learning code that takes this input of features x and it ignores it  It just sets y equals 0 and always predicts  you know  nobody has cancer and this algorithm would actually get 0 5 percent error  So this is even better than the 1% error that we were getting just now and this is a non learning algorithm that you know  it is just predicting y equals 0 all the time  So this setting of when the ratio of positive to negative examples is very close to one of two extremes  where  in this case  the number of positive examples is much  much smaller than the number of negative examples because y equals one so rarely  this is what we call the case of skewed classes  We just have a lot more of examples from one class than from the other class  And by just predicting y equals 0 all the time  or maybe our predicting y equals 1 all the time  an algorithm can do pretty well  So the problem with using classification error or classification accuracy as our evaluation metric is the following  Let's say you have one joining algorithm that's getting 99 2% accuracy  So  that's a 0 8% error  Let's say you make a change to your algorithm and you now are getting 99 5% accuracy  That is 0 5% error  So  is this an improvement to the algorithm or not? One of the nice things about having a single real number evaluation metric is this helps us to quickly decide if we just need a good change to the algorithm or not  By going from 99 2% accuracy to 99 5% accuracy  You know  did we just do something useful or did we just replace our code with something that just predicts y equals zero more often? So  if you have very skewed classes it becomes much harder to use just classification accuracy  because you can get very high classification accuracies or very low errors  and it's not always clear if doing so is really improving the quality of your classifier because predicting y equals 0 all the time doesn't seem like a particularly good classifier  But just predicting y equals 0 more often can bring your error down to  you know  maybe as low as 0 5%  When we're faced with such a skewed classes therefore we would want to come up with a different error metric or a different evaluation metric  One such evaluation metric are what's called precision recall  Let me explain what that is  Let's say we are evaluating a classifier on the test set  For the examples in the test set the actual class of that example in the test set is going to be either one or zero  right  if there is a binary classification problem  And what our learning algorithm will do is it will  you know  predict some value for the class and our learning algorithm will predict the value for each example in my test set and the predicted value will also be either one or zero  So let me draw a two by two table as follows  depending on a full of these entries depending on what was the actual class and what was the predicted class  If we have an example where the actual class is one and the predicted class is one then that's called an example that's a true positive  meaning our algorithm predicted that it's positive and in reality the example is positive  If our learning algorithm predicted that something is negative  class zero  and the actual class is also class zero then that's what's called a true negative  We predicted zero and it actually is zero  To find the other two boxes  if our learning algorithm predicts that the class is one but the actual class is zero  then that's called a false positive  So that means our algorithm for the patient is cancelled out in reality if the patient does not  And finally  the last box is a zero  one  That's called a false negative because our algorithm predicted zero  but the actual class was one  And so  we have this little sort of two by two table based on what was the actual class and what was the predicted class  So here's a different way of evaluating the performance of our algorithm  We're going to compute two numbers  The first is called precision - and what that says is  of all the patients where we've predicted that they have cancer  what fraction of them actually have cancer? So let me write this down  the precision of a classifier is the number of true positives divided by the number that we predicted as positive  right? So of all the patients that we went to those patients and we told them  \""We think you have cancer \"" Of all those patients  what fraction of them actually have cancer? So that's called precision  And another way to write this would be true positives and then in the denominator is the number of predicted positives  and so that would be the sum of the  you know  entries in this first row of the table  So it would be true positives divided by true positives  I'm going to abbreviate positive as POS and then plus false positives  again abbreviating positive using POS  So that's called precision  and as you can tell high precision would be good  That means that all the patients that we went to and we said  \""You know  we're very sorry  We think you have cancer \"" high precision means that of that group of patients most of them we had actually made accurate predictions on them and they do have cancer  The second number we're going to compute is called recall  and what recall say is  if all the patients in  let's say  in the test set or the cross-validation set  but if all the patients in the data set that actually have cancer  what fraction of them that we correctly detect as having cancer  So if all the patients have cancer  how many of them did we actually go to them and you know  correctly told them that we think they need treatment  So  writing this down  recall is defined as the number of positives  the number of true positives  meaning the number of people that have cancer and that we correctly predicted have cancer and we take that and divide that by  divide that by the number of actual positives  so this is the right number of actual positives of all the people that do have cancer  What fraction do we directly flag and you know  send the treatment  So  to rewrite this in a different form  the denominator would be the number of actual positives as you know  is the sum of the entries in this first column over here  And so writing things out differently  this is therefore  the number of true positives  divided by the number of true positives plus the number of false negatives  And so once again  having a high recall would be a good thing  So by computing precision and recall this will usually give us a better sense of how well our classifier is doing  And in particular if we have a learning algorithm that predicts y equals zero all the time  if it predicts no one has cancer  then this classifier will have a recall equal to zero  because there won't be any true positives and so that's a quick way for us to recognize that  you know  a classifier that predicts y equals 0 all the time  just isn't a very good classifier  And more generally  even for settings where we have very skewed classes  it's not possible for an algorithm to sort of \""cheat\"" and somehow get a very high precision and a very high recall by doing some simple thing like predicting y equals 0 all the time or predicting y equals 1 all the time  And so we're much more sure that a classifier of a high precision or high recall actually is a good classifier  and this gives us a more useful evaluation metric that is a more direct way to actually understand whether  you know  our algorithm may be doing well  So one final note in the definition of precision and recall  that we would define precision and recall  usually we use the convention that y is equal to 1  in the presence of the more rare class  So if we are trying to detect  rare conditions such as cancer  hopefully that's a rare condition  precision and recall are defined setting y equals 1  rather than y equals 0  to be sort of that the presence of that rare class that we're trying to detect  And by using precision and recall  we find  what happens is that even if we have very skewed classes  it's not possible for an algorithm to you know  \""cheat\"" and predict y equals 1 all the time  or predict y equals 0 all the time  and get high precision and recall  And in particular  if a classifier is getting high precision and high recall  then we are actually confident that the algorithm has to be doing well  even if we have very skewed classes  So for the problem of skewed classes precision recall gives us more direct insight into how the learning algorithm is doing and this is often a much better way to evaluate our learning algorithms  than looking at classification error or classification accuracy  when the classes are very skewed "
sx0UPzztC5o,Trading Off Precision and Recall  In the last video  we talked about precision and recall as an evaluation metric for classification problems with skewed constants  For many applications  we'll want to somehow control the trade-off between precision and recall  Let me tell you how to do that and also show you some even more effective ways to use precision and recall as an evaluation metric for learning algorithms  As a reminder  here are the definitions of precision and recall from the previous video  Let's continue our cancer classification example  where y equals 1 if the patient has cancer and y equals 0 otherwise  And let's say we're trained in logistic regression classifier which outputs probability between 0 and 1  So  as usual  we're going to predict 1  y equals 1  if h(x) is greater or equal to 0 5  And predict 0 if the hypothesis outputs a value less than 0 5  And this classifier may give us some value for precision and some value for recall  But now  suppose we want to predict that the patient has cancer only if we're very confident that they really do  Because if you go to a patient and you tell them that they have cancer  it's going to give them a huge shock  What we give is a seriously bad news  and they may end up going through a pretty painful treatment process and so on  And so maybe we want to tell someone that we think they have cancer only if they are very confident  One way to do this would be to modify the algorithm  so that instead of setting this threshold at 0 5  we might instead say that we will predict that y is equal to 1 only if h(x) is greater or equal to 0 7  So this is like saying  we'll tell someone they have cancer only if we think there's a greater than or equal to  70% chance that they have cancer  And  if you do this  then you're predicting someone has cancer only when you're more confident and so you end up with a classifier that has higher precision  Because all of the patients that you're going to and saying  we think you have cancer  although those patients are now ones that you're pretty confident actually have cancer  And so a higher fraction of the patients that you predict have cancer will actually turn out to have cancer because making those predictions only if we're pretty confident  But in contrast this classifier will have lower recall because now we're going to make predictions  we're going to predict y = 1 on a smaller number of patients  Now  can even take this further  Instead of setting the threshold at 0 7  we can set this at 0 9  Now we'll predict y=1 only if we are more than 90% certain that the patient has cancer  And so  a large fraction of those patients will turn out to have cancer  And so this would be a higher precision classifier will have lower recall because we want to correctly detect that those patients have cancer  Now consider a different example  Suppose we want to avoid missing too many actual cases of cancer  so we want to avoid false negatives  In particular  if a patient actually has cancer  but we fail to tell them that they have cancer then that can be really bad  Because if we tell a patient that they don't have cancer  then they're not going to go for treatment  And if it turns out that they have cancer  but we fail to tell them they have cancer  well  they may not get treated at all  And so that would be a really bad outcome because they die because we told them that they don't have cancer  They fail to get treated  but it turns out they actually have cancer  So  suppose that  when in doubt  we want to predict that y=1  So  when in doubt  we want to predict that they have cancer so that at least they look further into it  and these can get treated in case they do turn out to have cancer  In this case  rather than setting higher probability threshold  we might instead take this value and instead set it to a lower value  So maybe 0 3 like so  right? And by doing so  we're saying that  you know what  if we think there's more than a 30% chance that they have cancer we better be more conservative and tell them that they may have cancer so that they can seek treatment if necessary  And in this case what we would have is going to be a higher recall classifier  because we're going to be correctly flagging a higher fraction of all of the patients that actually do have cancer  But we're going to end up with lower precision because a higher fraction of the patients that we said have cancer  a high fraction of them will turn out not to have cancer after all  And by the way  just as a sider  when I talk about this to other students  I've been told before  it's pretty amazing  some of my students say  is how I can tell the story both ways  Why we might want to have higher precision or higher recall and the story actually seems to work both ways  But I hope the details of the algorithm is true and the more general principle is depending on where you want  whether you want higher precision- lower recall  or higher recall- lower precision  You can end up predicting y=1 when h(x) is greater than some threshold  And so in general  for most classifiers there is going to be a trade off between precision and recall  and as you vary the value of this threshold that we join here  you can actually plot out some curve that trades off precision and recall  Where a value up here  this would correspond to a very high value of the threshold  maybe threshold equals 0 99  So that's saying  predict y=1 only if we're more than 99% confident  at least 99% probability this one  So that would be a high precision  relatively low recall  Where as the point down here  will correspond to a value of the threshold that's much lower  maybe equal 0 01  meaning  when in doubt at all  predict y=1  and if you do that  you end up with a much lower precision  higher recall classifier  And as you vary the threshold  if you want you can actually trace of a curve for your classifier to see the range of different values you can get for precision recall  And by the way  the precision-recall curve can look like many different shapes  Sometimes it will look like this  sometimes it will look like that  Now there are many different possible shapes for the precision-recall curve  depending on the details of the classifier  So  this raises another interesting question which is  is there a way to choose this threshold automatically? Or more generally  if we have a few different algorithms or a few different ideas for algorithms  how do we compare different precision recall numbers? Concretely  suppose we have three different learning algorithms  So actually  maybe these are three different learning algorithms  maybe these are the same algorithm but just with different values for the threshold  How do we decide which of these algorithms is best? One of the things we talked about earlier is the importance of a single real number evaluation metric  And that is the idea of having a number that just tells you how well is your classifier doing  But by switching to the precision recall metric we've actually lost that  We now have two real numbers  And so we often  we end up face the situations like if we trying to compare Algorithm 1 and Algorithm 2  we end up asking ourselves  is the precision of 0 5 and a recall of 0 4  was that better or worse than a precision of 0 7 and recall of 0 1? And  if every time you try out a new algorithm you end up having to sit around and think  well  maybe 0 5/0 4 is better than 0 7/0 1  or maybe not  I don't know  If you end up having to sit around and think and make these decisions  that really slows down your decision making process for what changes are useful to incorporate into your algorithm  Whereas in contrast  if we have a single real number evaluation metric like a number that just tells us is algorithm 1 or is algorithm 2 better  then that helps us to much more quickly decide which algorithm to go with  It helps us as well to much more quickly evaluate different changes that we may be contemplating for an algorithm  So how can we get a single real number evaluation metric? One natural thing that you might try is to look at the average precision and recall  So  using P and R to denote precision and recall  what you could do is just compute the average and look at what classifier has the highest average value  But this turns out not to be such a good solution  because similar to the example we had earlier it turns out that if we have a classifier that predicts y=1 all the time  then if you do that you can get a very high recall  but you end up with a very low value of precision  Conversely  if you have a classifier that predicts y equals zero  almost all the time  that is that it predicts y=1 very sparingly  this corresponds to setting a very high threshold using the notation of the previous y  Then you can actually end up with a very high precision with a very low recall  So  the two extremes of either a very high threshold or a very low threshold  neither of that will give a particularly good classifier  And the way we recognize that is by seeing that we end up with a very low precision or a very low recall  And if you just take the average of (P+R)/2 from this example  the average is actually highest for Algorithm 3  even though you can get that sort of performance by predicting y=1 all the time and that's just not a very good classifier  right? You predict y=1 all the time  just normal useful classifier  but all it does is prints out y=1  And so Algorithm 1 or Algorithm 2 would be more useful than Algorithm 3  But in this example  Algorithm 3 has a higher average value of precision recall than Algorithms 1 and 2  So we usually think of this average of precision and recall as not a particularly good way to evaluate our learning algorithm  In contrast  there's a different way for combining precision and recall  This is called the F Score and it uses that formula  And so in this example  here are the F Scores  And so we would tell from these F Scores  it looks like Algorithm 1 has the highest F Score  Algorithm 2 has the second highest  and Algorithm 3 has the lowest  And so  if we go by the F Score we would pick probably Algorithm 1 over the others  The F Score  which is also called the F1 Score  is usually written F1 Score that I have here  but often people will just say F Score  either term is used  Is a little bit like taking the average of precision and recall  but it gives the lower value of precision and recall  whichever it is  it gives it a higher weight  And so  you see in the numerator here that the F Score takes a product of precision and recall  And so if either precision is 0 or recall is equal to 0  the F Score will be equal to 0  So in that sense  it kind of combines precision and recall  but for the F Score to be large  both precision and recall have to be pretty large  I should say that there are many different possible formulas for combing precision and recall  This F Score formula is really maybe a  just one out of a much larger number of possibilities  but historically or traditionally this is what people in Machine Learning seem to use  And the term F Score  it doesn't really mean anything  so don't worry about why it's called F Score or F1 Score  But this usually gives you the effect that you want because if either a precision is zero or recall is zero  this gives you a very low F Score  and so to have a high F Score  you kind of need a precision or recall to be one  And concretely  if P=0 or R=0  then this gives you that the F Score = 0  Whereas a perfect F Score  so if precision equals one and recall equals 1  that will give you an F Score  that's equal to 1 times 1 over 2 times 2  so the F Score will be equal to 1  if you have perfect precision and perfect recall  And intermediate values between 0 and 1  this usually gives a reasonable rank ordering of different classifiers  So in this video  we talked about the notion of trading off between precision and recall  and how we can vary the threshold that we use to decide whether to predict y=1 or y=0  So it's the threshold that says  do we need to be at least 70% confident or 90% confident  or whatever before we predict y=1  And by varying the threshold  you can control a trade off between precision and recall  We also talked about the F Score  which takes precision and recall  and again  gives you a single real number evaluation metric  And of course  if your goal is to automatically set that threshold to decide what's really y=1 and y=0  one pretty reasonable way to do that would also be to try a range of different values of thresholds  So you try a range of values of thresholds and evaluate these different thresholds on  say  your cross-validation set and then to pick whatever value of threshold gives you the highest F Score on your crossvalidation [INAUDIBLE]  And that be a pretty reasonable way to automatically choose the threshold for your classifier as well 
AN5I6fFxyfs,"Data For Machine Learning  In the previous video  we talked about evaluation metrics  In this video  I'd like to switch tracks a bit and touch on another important aspect of machine learning system design  which will often come up  which is the issue of how much data to train on  Now  in some earlier videos  I had cautioned against blindly going out and just spending lots of time collecting lots of data  because it's only sometimes that that would actually help  But it turns out that under certain conditions  and I will say in this video what those conditions are  getting a lot of data and training on a certain type of learning algorithm  can be a very effective way to get a learning algorithm to do very good performance  And this arises often enough that if those conditions hold true for your problem and if you're able to get a lot of data  this could be a very good way to get a very high performance learning algorithm  So in this video  let's talk more about that  Let me start with a story  Many  many years ago  two researchers that I know  Michelle Banko and Eric Broule ran the following fascinating study  They were interested in studying the effect of using different learning algorithms versus trying them out on different training set sciences  they were considering the problem of classifying between confusable words  so for example  in the sentence  for breakfast I ate  should it be to  two or too? Well  for this example  for breakfast I ate two  2 eggs  So  this is one example of a set of confusable words and that's a different set  So they took machine learning problems like these  sort of supervised learning problems to try to categorize what is the appropriate word to go into a certain position in an English sentence  They took a few different learning algorithms which were  you know  sort of considered state of the art back in the day  when they ran the study in 2001  so they took a variance  roughly a variance on logistic regression called the Perceptron  They also took some of their algorithms that were fairly out back then but somewhat less used now so when the algorithm also very similar to which is a regression but different in some ways  much used somewhat less  used not too much right now took what's called a memory based learning algorithm again used somewhat less now  But I'll talk a little bit about that later  And they used a naive based algorithm  which is something they'll actually talk about in this course  The exact algorithms of these details aren't important  Think of this as  you know  just picking four different classification algorithms and really the exact algorithms aren't important  But what they did was they varied the training set size and tried out these learning algorithms on the range of training set sizes and that's the result they got  And the trends are very clear right first most of these outer rooms give remarkably similar performance  And second  as the training set size increases  on the horizontal axis is the training set size in millions go from you know a hundred thousand up to a thousand million that is a billion training examples  The performance of the algorithms all pretty much monotonically increase and the fact that if you pick any algorithm may be pick a \""inferior algorithm\"" but if you give that \""inferior algorithm\"" more data  then from these examples  it looks like it will most likely beat even a \""superior algorithm\""  So since this original study which is very influential  there's been a range of many different studies showing similar results that show that many different learning algorithms you know tend to  can sometimes  depending on details  can give pretty similar ranges of performance  but what can really drive performance is you can give the algorithm a ton of training data  And this is  results like these has led to a saying in machine learning that often in machine learning it's not who has the best algorithm that wins  it's who has the most data So when is this true and when is this not true? Because we have a learning algorithm for which this is true then getting a lot of data is often maybe the best way to ensure that we have an algorithm with very high performance rather than you know  debating worrying about exactly which of these items to use  Let's try to lay out a set of assumptions under which having a massive training set we think will be able to help  Let's assume that in our machine learning problem  the features x have sufficient information with which we can use to predict y accurately  For example  if we take the confusable words all of them that we had on the previous slide  Let's say that it features x capture what are the surrounding words around the blank that we're trying to fill in  So the features capture then we want to have  sometimes for breakfast I have black eggs  Then yeah that is pretty much information to tell me that the word I want in the middle is TWO and that is not word TO and its not the word TOO  So the features capture  you know  one of these surrounding words then that gives me enough information to pretty unambiguously decide what is the label y or in other words what is the word that I should be using to fill in that blank out of this set of three confusable words  So that's an example what the future ex has sufficient information for specific y  For a counter example  Consider a problem of predicting the price of a house from only the size of the house and from no other features  So if you imagine I tell you that a house is  you know  500 square feet but I don't give you any other features  I don't tell you that the house is in an expensive part of the city  Or if I don't tell you that the house  the number of rooms in the house  or how nicely furnished the house is  or whether the house is new or old  If I don't tell you anything other than that this is a 500 square foot house  well there's so many other factors that would affect the price of a house other than just the size of a house that if all you know is the size  it's actually very difficult to predict the price accurately  So that would be a counter example to this assumption that the features have sufficient information to predict the price to the desired level of accuracy  The way I think about testing this assumption  one way I often think about it is  how often I ask myself  Given the input features x  given the features  given the same information available as well as learning algorithm  If we were to go to human expert in this domain  Can a human experts actually or can human expert confidently predict the value of y  For this first example if we go to  you know an expert human English speaker  You go to someone that speaks English well  right  then a human expert in English just read most people like you and me will probably we would probably be able to predict what word should go in here  to a good English speaker can predict this well  and so this gives me confidence that x allows us to predict y accurately  but in contrast if we go to an expert in human prices  Like maybe an expert realtor  right  someone who sells houses for a living  If I just tell them the size of a house and I tell them what the price is well even an expert in pricing or selling houses wouldn't be able to tell me and so this is fine that for the housing price example knowing only the size doesn't give me enough information to predict the price of the house  So  let's say  this assumption holds  Let's see then  when having a lot of data could help  Suppose the features have enough information to predict the value of y  And let's suppose we use a learning algorithm with a large number of parameters so maybe logistic regression or linear regression with a large number of features  Or one thing that I sometimes do  one thing that I often do actually is using neural network with many hidden units  That would be another learning algorithm with a lot of parameters  So these are all powerful learning algorithms with a lot of parameters that can fit very complex functions  So  I'm going to call these  I'm going to think of these as low-bias algorithms because you know we can fit very complex functions and because we have a very powerful learning algorithm  they can fit very complex functions  Chances are  if we run these algorithms on the data sets  it will be able to fit the training set well  and so hopefully the training error will be slow  Now let's say  we use a massive  massive training set  in that case  if we have a huge training set  then hopefully even though we have a lot of parameters but if the training set is sort of even much larger than the number of parameters then hopefully these albums will be unlikely to overfit  Right because we have such a massive training set and by unlikely to overfit what that means is that the training error will hopefully be close to the test error  Finally putting these two together that the train set error is small and the test set error is close to the training error what this two together imply is that hopefully the test set error will also be small  Another way to think about this is that in order to have a high performance learning algorithm we want it not to have high bias and not to have high variance  So the bias problem we're going to address by making sure we have a learning algorithm with many parameters and so that gives us a low bias alorithm and by using a very large training set  this ensures that we don't have a variance problem here  So hopefully our algorithm will have no variance and so is by pulling these two together  that we end up with a low bias and a low variance learning algorithm and this allows us to do well on the test set  And fundamentally it's a key ingredients of assuming that the features have enough information and we have a rich class of functions that's why it guarantees low bias  and then it having a massive training set that that's what guarantees more variance  So this gives us a set of conditions rather hopefully some understanding of what's the sort of problem where if you have a lot of data and you train a learning algorithm with lot of parameters  that might be a good way to give a high performance learning algorithm and really  I think the key test that I often ask myself are first  can a human experts look at the features x and confidently predict the value of y  Because that's sort of a certification that y can be predicted accurately from the features x and second  can we actually get a large training set  and train the learning algorithm with a lot of parameters in the training set and if you can't do both then that's more often give you a very kind performance learning algorithm "
IRbggk6J0TA,Optimization Objective  By now  you've seen a range of difference learning algorithms  With supervised learning  the performance of many supervised learning algorithms will be pretty similar  and what matters less often will be whether you use learning algorithm a or learning algorithm b  but what matters more will often be things like the amount of data you create these algorithms on  as well as your skill in applying these algorithms  Things like your choice of the features you design to give to the learning algorithms  and how you choose the colorization parameter  and things like that  But  there's one more algorithm that is very powerful and is very widely used both within industry and academia  and that's called the support vector machine  And compared to both logistic regression and neural networks  the Support Vector Machine  or SVM sometimes gives a cleaner  and sometimes more powerful way of learning complex non-linear functions  And so let's take the next videos to talk about that  Later in this course  I will do a quick survey of a range of different supervisory algorithms just as a very briefly describe them  But the support vector machine  given its popularity and how powerful it is  this will be the last of the supervisory algorithms that I'll spend a significant amount of time on in this course as with our development other learning algorithms  we're gonna start by talking about the optimization objective  So  let's get started on this algorithm  In order to describe the support vector machine  I'm actually going to start with logistic regression  and show how we can modify it a bit  and get what is essentially the support vector machine  So in logistic regression  we have our familiar form of the hypothesis there and the sigmoid activation function shown on the right  And in order to explain some of the math  I'm going to use z to denote theta transpose axiom  Now let's think about what we would like logistic regression to do  If we have an example with y equals one and by this I mean an example in either the training set or the test set or the cross-validation set  but when y is equal to one then we're sort of hoping that h of x will be close to one  Right  we're hoping to correctly classify that example  And what having x subscript 1  what that means is that theta transpose x must be must larger than 0  So there's greater than  greater than sign that means much  much greater than 0  And that's because it is z  the theta of transpose x is when z is much bigger than 0 is far to the right of the sphere  That the outputs of logistic progression becomes close to one  Conversely  if we have an example where y is equal to zero  then what we're hoping for is that the hypothesis will output a value close to zero  And that corresponds to theta transpose x of z being much less than zero because that corresponds to a hypothesis of putting a value close to zero  If you look at the cost function of logistic regression  what you'll find is that each example (x y) contributes a term like this to the overall cost function  right? So for the overall cost function  we will also have a sum over all the chain examples and the 1 over m term  that this expression here  that's the term that a single training example contributes to the overall objective function so we can just rush them  Now if I take the definition for the fall of my hypothesis and plug it in over here  then what I get is that each training example contributes this term  ignoring the one over M but it contributes that term to my overall cost function for logistic regression  Now let's consider two cases of when y is equal to one and when y is equal to zero  In the first case  let's suppose that y is equal to 1  In that case  only this first term in the objective matters  because this one minus y term would be equal to zero if y is equal to one  So when y is equal to one  when in our example x comma y  when y is equal to 1 what we get is this term   Minus log one over one  plus E to the negative Z where as similar to the last line I'm using Z to denote data transposed X and of course in a cost I should have this minus line that we just had if Y is equal to one so that's equal to one I just simplify in a way in the expression that I have written down here  And if we plot this function as a function of z  what you find is that you get this curve shown on the lower left of the slide  And thus  we also see that when z is equal to large  that is  when theta transpose x is large  that corresponds to a value of z that gives us a fairly small value  a very  very small contribution to the consumption  And this kinda explains why  when logistic regression sees a positive example  with y=1  it tries to set theta transport x to be very large because that corresponds to this term  in the cross function  being small  Now  to fill the support vec machine  here's what we're going to do  We're gonna take this cross function  this minus log 1 over 1 plus e to negative z  and modify it a little bit  Let me take this point 1 over here  and let me draw the cross functions you're going to use  The new pass functions can be flat from here on out  and then we draw something that grows as a straight line  similar to logistic regression  But this is going to be a straight line at this portion  So the curve that I just drew in magenta  and the curve I just drew purple and magenta  so if it's pretty close approximation to the cross function used by logistic regression  Except it is now made up of two line segments  there's this flat portion on the right  and then there's this straight line portion on the left  And don't worry too much about the slope of the straight line portion  It doesn't matter that much  But that's the new cost function we're going to use for when y is equal to one  and you can imagine it should do something pretty similar to logistic regression  But turns out  that this will give the support vector machine computational advantages and give us  later on  an easier optimization problem that would be easier for software to solve  We just talked about the case of y equals one  The other case is if y is equal to zero  In that case  if you look at the cost  then only the second term will apply because the first term goes away  right? If y is equal to zero  then you have a zero here  so you're left only with the second term of the expression above  And so the cost of an example  or the contribution of the cost function  is going to be given by this term over here  And if you plot that as a function of z  to have pure z on the horizontal axis  you end up with this one  And for the support vector machine  once again  we're going to replace this blue line with something similar and at the same time we replace it with a new cost  this flat out here  this 0 out here  And that then grows as a straight line  like so  So let me give these two functions names  This function on the left I'm going to call cost subscript 1 of z  and this function of the right I'm gonna call cost subscript 0 of z  And the subscript just refers to the cost corresponding to when y is equal to 1  versus when y Is equal to zero  Armed with these definitions  we're now ready to build a support vector machine  Here's the cost function  j of theta  that we have for logistic regression  In case this equation looks a bit unfamiliar  it's because previously we had a minus sign outside  but here what I did was I instead moved the minus signs inside these expressions  so it just makes it look a little different  For the support vector machine what we're going to do is essentially take this and replace this with cost1 of z  that is cost1 of theta transpose x  And we're going to take this and replace it with cost0 of z  that is cost0 of theta transpose x  Where the cost one function is what we had on the previous slide that looks like this  And the cost zero function  again what we had on the previous slide  and it looks like this  So what we have for the support vector machine is a minimization problem of one over M  the sum of Y I times cost one  theta transpose X I  plus one minus Y I times cause zero of theta transpose X I  and then plus my usual regularization parameter  Like so  Now  by convention  for the support of vector machine  we're actually write things slightly different  We re-parameterize this just very slightly differently  First  we're going to get rid of the 1 over m terms  and this just this happens to be a slightly different convention that people use for support vector machines compared to or just a progression  But here's what I mean  You're one way to do this  we're just gonna get rid of these one over m terms and this should give you me the same optimal value of beta right? Because one over m is just as constant so whether I solved this minimization problem with one over n in front or not  I should end up with the same optimal value for theta  Here's what I mean  to give you an example  suppose I had a minimization problem  Minimize over a long number U of U minus five squared plus one  Well  the minimum of this happens to be U equals five  Now if I were to take this objective function and multiply it by 10  So here my minimization problem is min over U  10 U minus five squared plus 10  Well the value of U that minimizes this is still U equals five right? So multiply something that you're minimizing over  by some constant  10 in this case  it does not change the value of U that gives us  that minimizes this function  So the same way  what I've done is by crossing out the M is all I'm doing is multiplying my objective function by some constant M and it doesn't change the value of theta  That achieves the minimum  The second bit of notational change  which is just  again  the more standard convention when using SVMs instead of logistic regression  is the following  So for logistic regression  we add two terms to the objective function  The first is this term  which is the cost that comes from the training set and the second is this row  which is the regularization term  And what we had was we had a  we control the trade-off between these by saying  what we want is A plus  and then my regularization parameter lambda  And then times some other term B  where I guess I'm using your A to denote this first term  and I'm using B to denote the second term  maybe without the lambda  And instead of prioritizing this as A plus lambda B  and so what we did was by setting different values for this regularization parameter lambda  we could trade off the relative weight between how much we wanted the training set well  that is  minimizing A  versus how much we care about keeping the values of the parameter small  so that will be  the parameter is B for the support vector machine  just by convention  we're going to use a different parameter  So instead of using lambda here to control the relative waiting between the first and second terms  We're instead going to use a different parameter which by convention is called C and is set to minimize C times a + B  So for logistic regression  if we set a very large value of lambda  that means you will give B a very high weight  Here is that if we set C to be a very small value  then that responds to giving B a much larger rate than C  than A  So this is just a different way of controlling the trade off  it's just a different way of prioritizing how much we care about optimizing the first term  versus how much we care about optimizing the second term  And if you want you can think of this as the parameter C playing a role similar to 1 over lambda  And it's not that it's two equations or these two expressions will be equal  This equals 1 over lambda  that's not the case  It's rather that if C is equal to 1 over lambda  then these two optimization objectives should give you the same value the same optimal value for theta so we just filling that in I'm gonna cross out lambda here and write in the constant C there  So that gives us our overall optimization objective function for the support vector machine  And if you minimize that function  then what you have is the parameters learned by the SVM  Finally unlike logistic regression  the support vector machine doesn't output the probability is that what we have is we have this cost function  that we minimize to get the parameter's data  and what a support vector machine does is it just makes a prediction of y being equal to one or zero  directly  So the hypothesis will predict one if theta transpose x is greater or equal to zero  and it will predict zero otherwise and so having learned the parameters theta  this is the form of the hypothesis for the support vector machine  So that was a mathematical definition of what a support vector machine does  In the next few videos  let's try to get back to intuition about what this optimization objective leads to and whether the source of the hypotheses SVM will learn and we'll also talk about how to modify this just a little bit to the complex nonlinear functions 
6WLwtYLeaoM,Large Margin Intuition  Sometimes people talk about support vector machines  as large margin classifiers  in this video I'd like to tell you what that means  and this will also give us a useful picture of what an SVM hypothesis may look like  Here's my cost function for the support vector machine where here on the left I've plotted my cost 1 of z function that I used for positive examples and on the right I've plotted my zero of 'Z' function  where I have 'Z' here on the horizontal axis  Now  let's think about what it takes to make these cost functions small  If you have a positive example  so if y is equal to 1  then cost 1 of Z is zero only when Z is greater than or equal to 1  So in other words  if you have a positive example  we really want theta transpose x to be greater than or equal to 1 and conversely if y is equal to zero  look this cost zero of z function  then it's only in this region where z is less than equal to 1 we have the cost is zero as z is equals to zero  and this is an interesting property of the support vector machine right  which is that  if you have a positive example so if y is equal to one  then all we really need is that theta transpose x is greater than equal to zero  And that would mean that we classify correctly because if theta transpose x is greater than zero our hypothesis will predict zero  And similarly  if you have a negative example  then really all you want is that theta transpose x is less than zero and that will make sure we got the example right  But the support vector machine wants a bit more than that  It says  you know  don't just barely get the example right  So then don't just have it just a little bit bigger than zero  What i really want is for this to be quite a lot bigger than zero say maybe bit greater or equal to one and I want this to be much less than zero  Maybe I want it less than or equal to -1  And so this builds in an extra safety factor or safety margin factor into the support vector machine  Logistic regression does something similar too of course  but let's see what happens or let's see what the consequences of this are  in the context of the support vector machine  Concretely  what I'd like to do next is consider a case case where we set this constant C to be a very large value  so let's imagine we set C to a very large value  may be a hundred thousand  some huge number  Let's see what the support vector machine will do  If C is very  very large  then when minimizing this optimization objective  we're going to be highly motivated to choose a value  so that this first term is equal to zero  So let's try to understand the optimization problem in the context of  what would it take to make this first term in the objective equal to zero  because you know  maybe we'll set C to some huge constant  and this will hope  this should give us additional intuition about what sort of hypotheses a support vector machine learns  So we saw already that whenever you have a training example with a label of y=1 if you want to make that first term zero  what you need is is to find a value of theta so that theta transpose x i is greater than or equal to 1  And similarly  whenever we have an example  with label zero  in order to make sure that the cost  cost zero of Z  in order to make sure that cost is zero we need that theta transpose x i is less than or equal to -1  So  if we think of our optimization problem as now  really choosing parameters and show that this first term is equal to zero  what we're left with is the following optimization problem  We're going to minimize that first term zero  so C times zero  because we're going to choose parameters so that's equal to zero  plus one half and then you know that second term and this first term is 'C' times zero  so let's just cross that out because I know that's going to be zero  And this will be subject to the constraint that theta transpose x(i) is greater than or equal to one  if y(i) Is equal to one and theta transpose x(i) is less than or equal to minus one whenever you have a negative example and it turns out that when you solve this optimization problem  when you minimize this as a function of the parameters theta you get a very interesting decision boundary  Concretely  if you look at a data set like this with positive and negative examples  this data is linearly separable and by that  I mean that there exists  you know  a straight line  altough there is many a different straight lines  they can separate the positive and negative examples perfectly  For example  here is one decision boundary that separates the positive and negative examples  but somehow that doesn't look like a very natural one  right? Or by drawing an even worse one  you know here's another decision boundary that separates the positive and negative examples but just barely  But neither of those seem like particularly good choices  The Support Vector Machines will instead choose this decision boundary  which I'm drawing in black  And that seems like a much better decision boundary than either of the ones that I drew in magenta or in green  The black line seems like a more robust separator  it does a better job of separating the positive and negative examples  And mathematically  what that does is  this black decision boundary has a larger distance  That distance is called the margin  when I draw up this two extra blue lines  we see that the black decision boundary has some larger minimum distance from any of my training examples  whereas the magenta and the green lines they come awfully close to the training examples  and then that seems to do a less a good job separating the positive and negative classes than my black line  And so this distance is called the margin of the support vector machine and this gives the SVM a certain robustness  because it tries to separate the data with as a large a margin as possible  So the support vector machine is sometimes also called a large margin classifier and this is actually a consequence of the optimization problem we wrote down on the previous slide  I know that you might be wondering how is it that the optimization problem I wrote down in the previous slide  how does that lead to this large margin classifier  I know I haven't explained that yet  And in the next video I'm going to sketch a little bit of the intuition about why that optimization problem gives us this large margin classifier  But this is a useful feature to keep in mind if you are trying to understand what are the sorts of hypothesis that an SVM will choose  That is  trying to separate the positive and negative examples with as big a margin as possible  I want to say one last thing about large margin classifiers in this intuition  so we wrote out this large margin classification setting in the case of when C  that regularization concept  was very large  I think I set that to a hundred thousand or something  So given a dataset like this  maybe we'll choose that decision boundary that separate the positive and negative examples on large margin  Now  the SVM is actually sligthly more sophisticated than this large margin view might suggest  And in particular  if all you're doing is use a large margin classifier then your learning algorithms can be sensitive to outliers  so lets just add an extra positive example like that shown on the screen  If he had one example then it seems as if to separate data with a large margin  maybe I'll end up learning a decision boundary like that  right? that is the magenta line and it's really not clear that based on the single outlier based on a single example and it's really not clear that it's actually a good idea to change my decision boundary from the black one over to the magenta one  So  if C  if the regularization parameter C were very large  then this is actually what SVM will do  it will change the decision boundary from the black to the magenta one but if C were reasonably small if you were to use the C  not too large then you still end up with this black decision boundary  And of course if the data were not linearly separable so if you had some positive examples in here  or if you had some negative examples in here then the SVM will also do the right thing  And so this picture of a large margin classifier that's really  that's really the picture that gives better intuition only for the case of when the regulations parameter C is very large  and just to remind you this corresponds C plays a role similar to one over Lambda  where Lambda is the regularization parameter we had previously  And so it's only of one over Lambda is very large or equivalently if Lambda is very small that you end up with things like this Magenta decision boundary  but in practice when applying support vector machines  when C is not very very large like that  it can do a better job ignoring the few outliers like here  And also do fine and do reasonable things even if your data is not linearly separable  But when we talk about bias and variance in the context of support vector machines which will do a little bit later  hopefully all of of this trade-offs involving the regularization parameter will become clearer at that time  So I hope that gives some intuition about how this support vector machine functions as a large margin classifier that tries to separate the data with a large margin  technically this picture of this view is true only when the parameter C is very large  which is a useful way to think about support vector machines  There was one missing step in this video which is  why is it that the optimization problem we wrote down on these slides  how does that actually lead to the large margin classifier  I didn't do that in this video  in the next video I will sketch a little bit more of the math behind that to explain that separate reasoning of how the optimization problem we wrote out results in a large margin classifier 
9aZ9k_JjYGY,Mathematics Behind Large Margin Classification  In this video  I'd like to tell you a bit about the math behind large margin classification  This video is optional  so please feel free to skip it  It may also give you better intuition about how the optimization problem of the support vex machine  how that leads to large margin classifiers  In order to get started  let me first remind you of a couple of properties of what vector inner products look like  Let's say I have two vectors U and V  that look like this  So both two dimensional vectors  Then let's see what U transpose V looks like  And U transpose V is also called the inner products between the vectors U and V  Use a two dimensional vector  so I can on plot it on this figure  So let's say that's the vector U  And what I mean by that is if on the horizontal axis that value takes whatever value U1 is and on the vertical axis the height of that is whatever U2 is the second component of the vector U  Now  one quantity that will be nice to have is the norm of the vector U  So  these are  you know  double bars on the left and right that denotes the norm or length of U  So this just means  really the euclidean length of the vector U  And this is Pythagoras theorem is just equal to U1 squared plus U2 squared square root  right? And this is the length of the vector U  That's a real number  Just say you know  what is the length of this  what is the length of this vector down here  What is the length of this arrow that I just drew  is the normal view? Now let's go back and look at the vector V because we want to compute the inner product  So V will be some other vector with  you know  some value V1  V2  And so  the vector V will look like that  towards V like so  Now let's go back and look at how to compute the inner product between U and V  Here's how you can do it  Let me take the vector V and project it down onto the vector U  So I'm going to take a orthogonal projection or a 90 degree projection  and project it down onto U like so  And what I'm going to do measure length of this red line that I just drew here  So  I'm going to call the length of that red line P  So  P is the length or is the magnitude of the projection of the vector V onto the vector U  Let me just write that down  So  P is the length of the projection of the vector V onto the vector U  And it is possible to show that unit product U transpose V  that this is going to be equal to P times the norm or the length of the vector U  So  this is one way to compute the inner product  And if you actually do the geometry figure out what P is and figure out what the norm of U is  This should give you the same way  the same answer as the other way of computing unit product  Right  Which is if you take U transpose V then U transposes this U1 U2  its a one by two matrix  1 times V  And so this should actually give you U1  V1 plus U2  V2  And so the theorem of linear algebra that these two formulas give you the same answer  And by the way  U transpose V is also equal to V transpose U  So if you were to do the same process in reverse  instead of projecting V onto U  you could project U onto V  Then  you know  do the same process  but with the rows of U and V reversed  And you would actually  you should actually get the same number whatever that number is  And just to clarify what's going on in this equation the norm of U is a real number and P is also a real number  And so U transpose V is the regular multiplication as two real numbers of the length of P times the normal view  Just one last detail  which is if you look at the norm of P  P is actually signed so to the right  And it can either be positive or negative  So let me say what I mean by that  if U is a vector that looks like this and V is a vector that looks like this  So if the angle between U and V is greater than ninety degrees  Then if I project V onto U  what I get is a projection it looks like this and so that length P  And in this case  I will still have that U transpose V is equal to P times the norm of U  Except in this example P will be negative  So  you know  in inner products if the angle between U and V is less than ninety degrees  then P is the positive length for that red line whereas if the angle of this angle of here is greater than 90 degrees then P here will be negative of the length of the super line of that little line segment right over there  So the inner product between two vectors can also be negative if the angle between them is greater than 90 degrees  So that's how vector inner products work  We're going to use these properties of vector inner product to try to understand the support vector machine optimization objective over there  Here is the optimization objective for the support vector machine that we worked out earlier  Just for the purpose of this slide I am going to make one simplification or once just to make the objective easy to analyze and what I'm going to do is ignore the indeceptrums  So  we'll just ignore theta 0 and set that to be equal to 0  To make things easier to plot  I'm also going to set N the number of features to be equal to 2  So  we have only 2 features  X1 and X2  Now  let's look at the objective function  The optimization objective of the SVM  What we have only two features  When N is equal to 2  This can be written  one half of theta one squared plus theta two squared  Because we only have two parameters  theta one and thetaa two  What I'm going to do is rewrite this a bit  I'm going to write this as one half of theta one squared plus theta two squared and the square root squared  And the reason I can do that  is because for any number  you know  W  right  the square roots of W and then squared  that's just equal to W  So square roots and squared should give you the same thing  What you may notice is that this term inside is that's equal to the norm or the length of the vector theta and what I mean by that is that if we write out the vector theta like this  as you know theta one  theta two  Then this term that I've just underlined in red  that's exactly the length  or the norm  of the vector theta  We are calling the definition of the norm of the vector that we have on the previous line  And in fact this is actually equal to the length of the vector theta  whether you write it as theta zero  theta 1  theta 2  That is  if theta zero is equal to zero  as I assume here  Or just the length of theta 1  theta 2  but for this line I am going to ignore theta 0  So let me just  you know  treat theta as this  let me just write theta  the normal theta as this theta 1  theta 2 only  but the math works out either way  whether we include theta zero here or not  So it's not going to matter for the rest of our derivation  And so finally this means that my optimization objective is equal to one half of the norm of theta squared  So all the support vector machine is doing in the optimization objective is it's minimizing the squared norm of the square length of the parameter vector theta  Now what I'd like to do is look at these terms  theta transpose X and understand better what they're doing  So given the parameter vector theta and given and example x  what is this is equal to? And on the previous slide  we figured out what U transpose V looks like  with different vectors U and V  And so we're going to take those definitions  you know  with theta and X(i) playing the roles of U and V  And let's see what that picture looks like  So  let's say I plot  Let's say I look at just a single training example  Let's say I have a positive example the drawing was across there and let's say that is my example X(i)  what that really means is plotted on the horizontal axis some value X(i) 1 and on the vertical axis X(i) 2  That's how I plot my training examples  And although we haven't been really thinking of this as a vector  what this really is  this is a vector from the origin from 0  0 out to the location of this training example  And now let's say we have a parameter vector and I'm going to plot that as vector  as well  What I mean by that is if I plot theta 1 here and theta 2 there so what is the inner product theta transpose X(i)  While using our earlier method  the way we compute that is we take my example and project it onto my parameter vector theta  And then I'm going to look at the length of this segment that I'm coloring in  in red  And I'm going to call that P superscript I to denote that this is a projection of the i-th training example onto the parameter vector theta  And so what we have is that theta transpose X(i) is equal to following what we have on the previous slide  this is going to be equal to P times the length of the norm of the vector theta  And this is of course also equal to theta 1 x1 plus theta 2 x2  So each of these is  you know  an equally valid way of computing the inner product between theta and X(i)  Okay  So where does this leave us? What this means is that  this constrains that theta transpose X(i) be greater than or equal to one or less than minus one  What this means is that it can replace the use of constraints that P(i) times X be greater than or equal to one  Because theta transpose X(i) is equal to P(i) times the norm of theta  So writing that into our optimization objective  This is what we get where I have  instead of theta transpose X(i)  I now have this P(i) times the norm of theta  And just to remind you we worked out earlier too that this optimization objective can be written as one half times the norm of theta squared  So  now let's consider the training example that we have at the bottom and for now  continuing to use the simplification that theta 0 is equal to 0  Let's see what decision boundary the support vector machine will choose  Here's one option  let's say the support vector machine were to choose this decision boundary  This is not a very good choice because it has very small margins  This decision boundary comes very close to the training examples  Let's see why the support vector machine will not do this  For this choice of parameters it's possible to show that the parameter vector theta is actually at 90 degrees to the decision boundary  And so  that green decision boundary corresponds to a parameter vector theta that points in that direction  And by the way  the simplification that theta 0 equals 0 that just means that the decision boundary must pass through the origin  (0 0) over there  So now  let's look at what this implies for the optimization objective  Let's say that this example here  Let's say that's my first example  you know  X1  If we look at the projection of this example onto my parameters theta  That's the projection  And so that little red line segment  That is equal to P1  And that is going to be pretty small  right  And similarly  if this example here  if this happens to be X2  that's my second example  Then  if I look at the projection of this this example onto theta  You know  Then  let me draw this one in magenta  This little magenta line segment  that's going to be P2  That's the projection of the second example onto my  onto the direction of my parameter vector theta which goes like this  And so  this little projection line segment is getting pretty small  P2 will actually be a negative number  right so P2 is in the opposite direction  This vector has greater than 90 degree angle with my parameter vector theta  it's going to be less than 0  And so what we're finding is that these terms P(i) are going to be pretty small numbers  So if we look at the optimization objective and see  well  for positive examples we need P(i) times the norm of theta to be bigger than either one  But if P(i) over here  if P1 over here is pretty small  that means that we need the norm of theta to be pretty large  right? If P1 of theta is small and we want P1 you know times in all of theta to be bigger than either one  well the only way for that to be true for the profit that these two numbers to be large if P1 is small  as we said we want the norm of theta to be large  And similarly for our negative example  we need P2 times the norm of theta to be less than or equal to minus one  And we saw in this example already that P2 is going pretty small negative number  and so the only way for that to happen as well is for the norm of theta to be large  but what we are doing in the optimization objective is we are trying to find a setting of parameters where the norm of theta is small  and so you know  so this doesn't seem like such a good direction for the parameter vector and theta  In contrast  just look at a different decision boundary  Here  let's say  this SVM chooses that decision boundary  Now the is going to be very different  If that is the decision boundary  here is the corresponding direction for theta  So  with the direction boundary you know  that vertical line that corresponds to it is possible to show using linear algebra that the way to get that green decision boundary is have the vector of theta be at 90 degrees to it  and now if you look at the projection of your data onto the vector x  lets say its before this example is my example of x1  So when I project this on to x  or onto theta  what I find is that this is P1  That length there is P1  The other example  that example is and I do the same projection and what I find is that this length here is a P2 really that is going to be less than 0  And you notice that now P1 and P2  these lengths of the projections are going to be much bigger  and so if we still need to enforce these constraints that P1 of the norm of theta is phase number one because P1 is so much bigger now  The normal can be smaller  And so  what this means is that by choosing the decision boundary shown on the right instead of on the left  the SVM can make the norm of the parameters theta much smaller  So  if we can make the norm of theta smaller and therefore make the squared norm of theta smaller  which is why the SVM would choose this hypothesis on the right instead  And this is how the SVM gives rise to this large margin certification effect  Mainly  if you look at this green line  if you look at this green hypothesis we want the projections of my positive and negative examples onto theta to be large  and the only way for that to hold true this is if surrounding the green line  There's this large margin  there's this large gap that separates positive and negative examples is really the magnitude of this gap  The magnitude of this margin is exactly the values of P1  P2  P3 and so on  And so by making the margin large  by these tyros P1  P2  P3 and so on that's the SVM can end up with a smaller value for the norm of theta which is what it is trying to do in the objective  And this is why this machine ends up with enlarge margin classifiers because itss trying to maximize the norm of these P1 which is the distance from the training examples to the decision boundary  Finally  we did this whole derivation using this simplification that the parameter theta 0 must be equal to 0  The effect of that as I mentioned briefly  is that if theta 0 is equal to 0 what that means is that we are entertaining decision boundaries that pass through the origins of decision boundaries pass through the origin like that  if you allow theta zero to be non 0 then what that means is that you entertain the decision boundaries that did not cross through the origin  like that one I just drew  And I'm not going to do the full derivation that  It turns out that this same large margin proof works in pretty much in exactly the same way  And there's a generalization of this argument that we just went through them long ago through that shows that even when theta 0 is non 0  what the SVM is trying to do when you have this optimization objective  Which again corresponds to the case of when C is very large  But it is possible to show that  you know  when theta is not equal to 0 this support vector machine is still finding is really trying to find the large margin separator that between the positive and negative examples  So that explains how this support vector machine is a large margin classifier  In the next video we will start to talk about how to take some of these SVM ideas and start to apply them to build a complex nonlinear classifiers 
Kn5ipjXJvpQ,Kernels I  In this video  I'd like to start adapting support vector machines in order to develop complex nonlinear classifiers  The main technique for doing that is something called kernels  Let's see what this kernels are and how to use them  If you have a training set that looks like this  and you want to find a nonlinear decision boundary to distinguish the positive and negative examples  maybe a decision boundary that looks like that  One way to do so is to come up with a set of complex polynomial features  right? So  set of features that looks like this  so that you end up with a hypothesis X that predicts 1 if you know that theta 0 and plus theta 1 X1 plus dot dot dot all those polynomial features is greater than 0  and predict 0  otherwise  And another way of writing this  to introduce a level of new notation that I'll use later  is that we can think of a hypothesis as computing a decision boundary using this  So  theta 0 plus theta 1 f1 plus theta 2  f2 plus theta 3  f3 plus and so on  Where I'm going to use this new denotation f1  f2  f3 and so on to denote these new sort of features that I'm computing  so f1 is just X1  f2 is equal to X2  f3 is equal to this one here  So  X1X2  So  f4 is equal to X1 squared where f5 is to be x2 squared and so on and we seen previously that coming up with these high order polynomials is one way to come up with lots more features  the question is  is there a different choice of features or is there better sort of features than this high order polynomials because you know it's not clear that this high order polynomial is what we want  and what we talked about computer vision talk about when the input is an image with lots of pixels  We also saw how using high order polynomials becomes very computationally expensive because there are a lot of these higher order polynomial terms  So  is there a different or a better choice of the features that we can use to plug into this sort of hypothesis form  So  here is one idea for how to define new features f1  f2  f3  On this line I am going to define only three new features  but for real problems we can get to define a much larger number  But here's what I'm going to do in this phase of features X1  X2  and I'm going to leave X0 out of this  the interceptor X0  but in this phase X1 X2  I'm going to just  you know  manually pick a few points  and then call these points l1  we are going to pick a different point  let's call that l2 and let's pick the third one and call this one l3  and for now let's just say that I'm going to choose these three points manually  I'm going to call these three points line ups  so line up one  two  three  What I'm going to do is define my new features as follows  given an example X  let me define my first feature f1 to be some measure of the similarity between my training example X and my first landmark and this specific formula that I'm going to use to measure similarity is going to be this is E to the minus the length of X minus l1  squared  divided by two sigma squared  So  depending on whether or not you watched the previous optional video  this notation  you know  this is the length of the vector W  And so  this thing here  this X minus l1  this is actually just the euclidean distance squared  is the euclidean distance between the point x and the landmark l1  We will see more about this later  But that's my first feature  and my second feature f2 is going to be  you know  similarity function that measures how similar X is to l2 and the game is going to be defined as the following function  This is E to the minus of the square of the euclidean distance between X and the second landmark  that is what the enumerator is and then divided by 2 sigma squared and similarly f3 is  you know  similarity between X and l3  which is equal to  again  similar formula  And what this similarity function is  the mathematical term for this  is that this is going to be a kernel function  And the specific kernel I'm using here  this is actually called a Gaussian kernel  And so this formula  this particular choice of similarity function is called a Gaussian kernel  But the way the terminology goes is that  you know  in the abstract these different similarity functions are called kernels and we can have different similarity functions and the specific example I'm giving here is called the Gaussian kernel  We'll see other examples of other kernels  But for now just think of these as similarity functions  And so  instead of writing similarity between X and l  sometimes we also write this a kernel denoted you know  lower case k between x and one of my landmarks all right  So let's see what a criminals actually do and why these sorts of similarity functions  why these expressions might make sense  So let's take my first landmark  My landmark l1  which is one of those points I chose on my figure just now  So the similarity of the kernel between x and l1 is given by this expression  Just to make sure  you know  we are on the same page about what the numerator term is  the numerator can also be written as a sum from J equals 1 through N on sort of the distance  So this is the component wise distance between the vector X and the vector l  And again for the purpose of these slides I'm ignoring X0  So just ignoring the intercept term X0  which is always equal to 1  So  you know  this is how you compute the kernel with similarity between X and a landmark  So let's see what this function does  Suppose X is close to one of the landmarks  Then this euclidean distance formula and the numerator will be close to 0  right  So  that is this term here  the distance was great  the distance using X and 0 will be close to zero  and so f1  this is a simple feature  will be approximately E to the minus 0 and then the numerator squared over 2 is equal to squared so that E to the 0  E to minus 0  E to 0 is going to be close to one  And I'll put the approximation symbol here because the distance may not be exactly 0  but if X is closer to landmark this term will be close to 0 and so f1 would be close 1  Conversely  if X is far from 01 then this first feature f1 will be E to the minus of some large number squared  divided divided by two sigma squared and E to the minus of a large number is going to be close to 0  So what these features do is they measure how similar X is from one of your landmarks and the feature f is going to be close to one when X is close to your landmark and is going to be 0 or close to zero when X is far from your landmark  Each of these landmarks  On the previous line  I drew three landmarks  l1  l2 l3  Each of these landmarks  defines a new feature f1  f2 and f3  That is  given the the training example X  we can now compute three new features  f1  f2  and f3  given  you know  the three landmarks that I wrote just now  But first  let's look at this exponentiation function  let's look at this similarity function and plot in some figures and just  you know  understand better what this really looks like  For this example  let's say I have two features X1 and X2  And let's say my first landmark  l1 is at a location  3 5  So and let's say I set sigma squared equals one for now  If I plot what this feature looks like  what I get is this figure  So the vertical axis  the height of the surface is the value of f1 and down here on the horizontal axis are  if I have some training example  and there is x1 and there is x2  Given a certain training example  the training example here which shows the value of x1 and x2 at a height above the surface  shows the corresponding value of f1 and down below this is the same figure I had showed  using a quantifiable plot  with x1 on horizontal axis  x2 on horizontal axis and so  this figure on the bottom is just a contour plot of the 3D surface  You notice that when X is equal to 3 5 exactly  then we the f1 takes on the value 1  because that's at the maximum and X moves away as X goes further away then this feature takes on values that are close to 0  And so  this is really a feature  f1 measures  you know  how close X is to the first landmark and if varies between 0 and one depending on how close X is to the first landmark l1  Now the other was due on this slide is show the effects of varying this parameter sigma squared  So  sigma squared is the parameter of the Gaussian kernel and as you vary it  you get slightly different effects  Let's set sigma squared to be equal to 0 5 and see what we get  We set sigma square to 0 5  what you find is that the kernel looks similar  except for the width of the bump becomes narrower  The contours shrink a bit too  So if sigma squared equals to 0 5 then as you start from X equals 3 5 and as you move away  then the feature f1 falls to zero much more rapidly and conversely  if you has increase since where three in that case and as I move away from  you know l  So this point here is really l  right  that's l1 is at location 3 5  right  So it's shown up here  And if sigma squared is large  then as you move away from l1  the value of the feature falls away much more slowly  So  given this definition of the features  let's see what source of hypothesis we can learn  Given the training example X  we are going to compute these features f1  f2  f3 and a hypothesis is going to predict one when theta 0 plus theta 1 f1 plus theta 2 f2  and so on is greater than or equal to 0  For this particular example  let's say that I've already found a learning algorithm and let's say that  you know  somehow I ended up with these values of the parameter  So if theta 0 equals minus 0 5  theta 1 equals 1  theta 2 equals 1  and theta 3 equals 0 And what I want to do is consider what happens if we have a training example that takes has location at this magenta dot  right where I just drew this dot over here  So let's say I have a training example X  what would my hypothesis predict? Well  If I look at this formula  Because my training example X is close to l1  we have that f1 is going to be close to 1 the because my training example X is far from l2 and l3 I have that  you know  f2 would be close to 0 and f3 will be close to 0  So  if I look at that formula  I have theta 0 plus theta 1 times 1 plus theta 2 times some value  Not exactly 0  but let's say close to 0  Then plus theta 3 times something close to 0  And this is going to be equal to plugging in these values now  So  that gives minus 0 5 plus 1 times 1 which is 1  and so on  Which is equal to 0 5 which is greater than or equal to 0  So  at this point  we're going to predict Y equals 1  because that's greater than or equal to zero  Now let's take a different point  Now lets' say I take a different point  I'm going to draw this one in a different color  in cyan say  for a point out there  if that were my training example X  then if you make a similar computation  you find that f1  f2  Ff3 are all going to be close to 0  And so  we have theta 0 plus theta 1  f1  plus so on and this will be about equal to minus 0 5  because theta 0 is minus 0 5 and f1  f2  f3 are all zero  So this will be minus 0 5  this is less than zero  And so  at this point out there  we're going to predict Y equals zero  And if you do this yourself for a range of different points  be sure to convince yourself that if you have a training example that's close to L2  say  then at this point we'll also predict Y equals one  And in fact  what you end up doing is  you know  if you look around this boundary  this space  what we'll find is that for points near l1 and l2 we end up predicting positive  And for points far away from l1 and l2  that's for points far away from these two landmarks  we end up predicting that the class is equal to 0  As so  what we end up doing is that the decision boundary of this hypothesis would end up looking something like this where inside this red decision boundary would predict Y equals 1 and outside we predict Y equals 0  And so this is how with this definition of the landmarks and of the kernel function  We can learn pretty complex non-linear decision boundary  like what I just drew where we predict positive when we're close to either one of the two landmarks  And we predict negative when we're very far away from any of the landmarks  And so this is part of the idea of kernels of and how we use them with the support vector machine  which is that we define these extra features using landmarks and similarity functions to learn more complex nonlinear classifiers  So hopefully that gives you a sense of the idea of kernels and how we could use it to define new features for the Support Vector Machine  But there are a couple of questions that we haven't answered yet  One is  how do we get these landmarks? How do we choose these landmarks? And another is  what other similarity functions  if any  can we use other than the one we talked about  which is called the Gaussian kernel  In the next video we give answers to these questions and put everything together to show how support vector machines with kernels can be a powerful way to learn complex nonlinear functions 
iEPU99cTzXo,Kernels II  In the last video  we started to talk about the kernels idea and how it can be used to define new features for the support vector machine  In this video  I'd like to throw in some of the missing details and  also  say a few words about how to use these ideas in practice  Such as  how they pertain to  for example  the bias variance trade-off in support vector machines  In the last video  I talked about the process of picking a few landmarks  You know  l1  l2  l3 and that allowed us to define the similarity function also called the kernel or in this example if you have this similarity function this is a Gaussian kernel  And that allowed us to build this form of a hypothesis function  But where do we get these landmarks from? Where do we get l1  l2  l3 from? And it seems  also  that for complex learning problems  maybe we want a lot more landmarks than just three of them that we might choose by hand  So in practice this is how the landmarks are chosen which is that given the machine learning problem  We have some data set of some some positive and negative examples  So  this is the idea here which is that we're gonna take the examples and for every training example that we have  we are just going to call it  We're just going to put landmarks as exactly the same locations as the training examples  So if I have one training example if that is x1  well then I'm going to choose this is my first landmark to be at xactly the same location as my first training example  And if I have a different training example x2  Well we're going to set the second landmark to be the location of my second training example  On the figure on the right  I used red and blue dots just as illustration  the color of this figure  the color of the dots on the figure on the right is not significant  But what I'm going to end up with using this method is I'm going to end up with m landmarks of l1  l2 down to l(m) if I have m training examples with one landmark per location of my per location of each of my training examples  And this is nice because it is saying that my features are basically going to measure how close an example is to one of the things I saw in my training set  So  just to write this outline a little more concretely  given m training examples  I'm going to choose the the location of my landmarks to be exactly near the locations of my m training examples  When you are given example x  and in this example x can be something in the training set  it can be something in the cross validation set  or it can be something in the test set  Given an example x we are going to compute  you know  these features as so f1  f2  and so on  Where l1 is actually equal to x1 and so on  And these then give me a feature vector  So let me write f as the feature vector  I'm going to take these f1  f2 and so on  and just group them into feature vector  Take those down to fm  And  you know  just by convention  If we want  we can add an extra feature f0  which is always equal to 1  So this plays a role similar to what we had previously  For x0  which was our interceptor  So  for example  if we have a training example x(i)  y(i)  the features we would compute for this training example will be as follows  given x(i)  we will then map it to  you know  f1(i)  Which is the similarity  I'm going to abbreviate as SIM instead of writing out the whole word similarity  right? And f2(i) equals the similarity between x(i) and l2  and so on  down to fm(i) equals the similarity between x(i) and l(m)  And somewhere in the middle  Somewhere in this list  you know  at the i-th component  I will actually have one feature component which is f subscript i(i)  which is going to be the similarity between x and l(i)  Where l(i) is equal to x(i)  and so you know fi(i) is just going to be the similarity between x and itself  And if you're using the Gaussian kernel this is actually e to the minus 0 over 2 sigma squared and so  this will be equal to 1 and that's okay  So one of my features for this training example is going to be equal to 1  And then similar to what I have above  I can take all of these m features and group them into a feature vector  So instead of representing my example  using  you know  x(i) which is this what R(n) plus R(n) one dimensional vector  Depending on whether you can set terms  is either R(n) or R(n) plus 1  We can now instead represent my training example using this feature vector f  I am going to write this f superscript i  Which is going to be taking all of these things and stacking them into a vector  So  f1(i) down to fm(i) and if you want and well  usually we'll also add this f0(i)  where f0(i) is equal to 1  And so this vector here gives me my new feature vector with which to represent my training example  So given these kernels and similarity functions  here's how we use a simple vector machine  If you already have a learning set of parameters theta  then if you given a value of x and you want to make a prediction  What we do is we compute the features f  which is now an R(m) plus 1 dimensional feature vector  And we have m here because we have m training examples and thus m landmarks and what we do is we predict 1 if theta transpose f is greater than or equal to 0  Right  So  if theta transpose f  of course  that's just equal to theta 0  f0 plus theta 1  f1 plus dot dot dot  plus theta m f(m)  And so my parameter vector theta is also now going to be an m plus 1 dimensional vector  And we have m here because where the number of landmarks is equal to the training set size  So m was the training set size and now  the parameter vector theta is going to be m plus one dimensional  So that's how you make a prediction if you already have a setting for the parameter's theta  How do you get the parameter's theta? Well you do that using the SVM learning algorithm  and specifically what you do is you would solve this minimization problem  You've minimized the parameter's theta of C times this cost function which we had before  Only now  instead of looking there instead of making predictions using theta transpose x(i) using our original features  x(i)  Instead we've taken the features x(i) and replace them with a new features so we are using theta transpose f(i) to make a prediction on the i'f training examples and we see that  you know  in both places here and it's by solving this minimization problem that you get the parameters for your Support Vector Machine  And one last detail is because this optimization problem we really have n equals m features  That is here  The number of features we have  Really  the effective number of features we have is dimension of f  So that n is actually going to be equal to m  So  if you want to  you can think of this as a sum  this really is a sum from j equals 1 through m  And then one way to think about this  is you can think of it as n being equal to m  because if f isn't a new feature  then we have m plus 1 features  with the plus 1 coming from the interceptor  And here  we still do sum from j equal 1 through n  because similar to our earlier videos on regularization  we still do not regularize the parameter theta zero  which is why this is a sum for j equals 1 through m instead of j equals zero though m  So that's the support vector machine learning algorithm  That's one sort of  mathematical detail aside that I should mention  which is that in the way the support vector machine is implemented  this last term is actually done a little bit differently  So you don't really need to know about this last detail in order to use support vector machines  and in fact the equations that are written down here should give you all the intuitions that should need  But in the way the support vector machine is implemented  you know  that term  the sum of j of theta j squared right? Another way to write this is this can be written as theta transpose theta if we ignore the parameter theta 0  So theta 1 down to theta m  Ignoring theta 0  Then this sum of j of theta j squared that this can also be written theta transpose theta  And what most support vector machine implementations do is actually replace this theta transpose theta  will instead  theta transpose times some matrix inside  that depends on the kernel you use  times theta  And so this gives us a slightly different distance metric  We'll use a slightly different measure instead of minimizing exactly the norm of theta squared means that minimize something slightly similar to it  That's like a rescale version of the parameter vector theta that depends on the kernel  But this is kind of a mathematical detail  That allows the support vector machine software to run much more efficiently  And the reason the support vector machine does this is with this modification  It allows it to scale to much bigger training sets  Because for example  if you have a training set with 10 000 training examples  Then  you know  the way we define landmarks  we end up with 10 000 landmarks  And so theta becomes 10 000 dimensional  And maybe that works  but when m becomes really  really big then solving for all of these parameters  you know  if m were 50 000 or a 100 000 then solving for all of these parameters can become expensive for the support vector machine optimization software  thus solving the minimization problem that I drew here  So kind of as mathematical detail  which again you really don't need to know about  It actually modifies that last term a little bit to optimize something slightly different than just minimizing the norm squared of theta squared  of theta  But if you want  you can feel free to think of this as an kind of a n implementational detail that does change the objective a bit  but is done primarily for reasons of computational efficiency  so usually you don't really have to worry about this  And by the way  in case your wondering why we don't apply the kernel's idea to other algorithms as well like logistic regression  it turns out that if you want  you can actually apply the kernel's idea and define the source of features using landmarks and so on for logistic regression  But the computational tricks that apply for support vector machines don't generalize well to other algorithms like logistic regression  And so  using kernels with logistic regression is going too very slow  whereas  because of computational tricks  like that embodied and how it modifies this and the details of how the support vector machine software is implemented  support vector machines and kernels tend go particularly well together  Whereas  logistic regression and kernels  you know  you can do it  but this would run very slowly  And it won't be able to take advantage of advanced optimization techniques that people have figured out for the particular case of running a support vector machine with a kernel  But all this pertains only to how you actually implement software to minimize the cost function  I will say more about that in the next video  but you really don't need to know about how to write software to minimize this cost function because you can find very good off the shelf software for doing so  And just as  you know  I wouldn't recommend writing code to invert a matrix or to compute a square root  I actually do not recommend writing software to minimize this cost function yourself  but instead to use off the shelf software packages that people have developed and so those software packages already embody these numerical optimization tricks  so you don't really have to worry about them  But one other thing that is worth knowing about is when you're applying a support vector machine  how do you choose the parameters of the support vector machine? And the last thing I want to do in this video is say a little word about the bias and variance trade offs when using a support vector machine  When using an SVM  one of the things you need to choose is the parameter C which was in the optimization objective  and you recall that C played a role similar to 1 over lambda  where lambda was the regularization parameter we had for logistic regression  So  if you have a large value of C  this corresponds to what we have back in logistic regression  of a small value of lambda meaning of not using much regularization  And if you do that  you tend to have a hypothesis with lower bias and higher variance  Whereas if you use a smaller value of C then this corresponds to when we are using logistic regression with a large value of lambda and that corresponds to a hypothesis with higher bias and lower variance  And so  hypothesis with large C has a higher variance  and is more prone to overfitting  whereas hypothesis with small C has higher bias and is thus more prone to underfitting  So this parameter C is one of the parameters we need to choose  The other one is the parameter sigma squared  which appeared in the Gaussian kernel  So if the Gaussian kernel sigma squared is large  then in the similarity function  which was this you know E to the minus x minus landmark varies squared over 2 sigma squared  In this one of the example; If I have only one feature  x1  if I have a landmark there at that location  if sigma squared is large  then  you know  the Gaussian kernel would tend to fall off relatively slowly and so this would be my feature f(i)  and so this would be smoother function that varies more smoothly  and so this will give you a hypothesis with higher bias and lower variance  because the Gaussian kernel that falls off smoothly  you tend to get a hypothesis that varies slowly  or varies smoothly as you change the input x  Whereas in contrast  if sigma squared was small and if that's my landmark given my 1 feature x1  you know  my Gaussian kernel  my similarity function  will vary more abruptly  And in both cases I'd pick out 1  and so if sigma squared is small  then my features vary less smoothly  So if it's just higher slopes or higher derivatives here  And using this  you end up fitting hypotheses of lower bias and you can have higher variance  And if you look at this week's points exercise  you actually get to play around with some of these ideas yourself and see these effects yourself  So  that was the support vector machine with kernels algorithm  And hopefully this discussion of bias and variance will give you some sense of how you can expect this algorithm to behave as well 
zkqH74k1fjY,Using An SVM  So far we've been talking about SVMs in a fairly abstract level  In this video I'd like to talk about what you actually need to do in order to run or to use an SVM  The support vector machine algorithm poses a particular optimization problem  But as I briefly mentioned in an earlier video  I really do not recommend writing your own software to solve for the parameter's theta yourself  So just as today  very few of us  or maybe almost essentially none of us would think of writing code ourselves to invert a matrix or take a square root of a number  and so on  We just  you know  call some library function to do that  In the same way  the software for solving the SVM optimization problem is very complex  and there have been researchers that have been doing essentially numerical optimization research for many years  So you come up with good software libraries and good software packages to do this  And then strongly recommend just using one of the highly optimized software libraries rather than trying to implement something yourself  And there are lots of good software libraries out there  The two that I happen to use the most often are the linear SVM but there are really lots of good software libraries for doing this that you know  you can link to many of the major programming languages that you may be using to code up learning algorithm  Even though you shouldn't be writing your own SVM optimization software  there are a few things you need to do  though  First is to come up with with some choice of the parameter's C  We talked a little bit of the bias/variance properties of this in the earlier video  Second  you also need to choose the kernel or the similarity function that you want to use  So one choice might be if we decide not to use any kernel  And the idea of no kernel is also called a linear kernel  So if someone says  I use an SVM with a linear kernel  what that means is you know  they use an SVM without using without using a kernel and it was a version of the SVM that just uses theta transpose X  right  that predicts 1 theta 0 plus theta 1 X1 plus so on plus theta N  X N is greater than equals 0  This term linear kernel  you can think of this as you know this is the version of the SVM that just gives you a standard linear classifier  So that would be one reasonable choice for some problems  and you know  there would be many software libraries  like linear  was one example  out of many  one example of a software library that can train an SVM without using a kernel  also called a linear kernel  So  why would you want to do this? If you have a large number of features  if N is large  and M the number of training examples is small  then you know you have a huge number of features that if X  this is an X is an Rn  Rn +1  So if you have a huge number of features already  with a small training set  you know  maybe you want to just fit a linear decision boundary and not try to fit a very complicated nonlinear function  because might not have enough data  And you might risk overfitting  if you're trying to fit a very complicated function in a very high dimensional feature space  but if your training set sample is small  So this would be one reasonable setting where you might decide to just not use a kernel  or equivalents to use what's called a linear kernel  A second choice for the kernel that you might make  is this Gaussian kernel  and this is what we had previously  And if you do this  then the other choice you need to make is to choose this parameter sigma squared when we also talk a little bit about the bias variance tradeoffs of how  if sigma squared is large  then you tend to have a higher bias  lower variance classifier  but if sigma squared is small  then you have a higher variance  lower bias classifier  So when would you choose a Gaussian kernel? Well  if your omission of features X  I mean Rn  and if N is small  and  ideally  you know  if n is large  right  so that's if  you know  we have say  a two-dimensional training set  like the example I drew earlier  So n is equal to 2  but we have a pretty large training set  So  you know  I've drawn in a fairly large number of training examples  then maybe you want to use a kernel to fit a more complex nonlinear decision boundary  and the Gaussian kernel would be a fine way to do this  I'll say more towards the end of the video  a little bit more about when you might choose a linear kernel  a Gaussian kernel and so on  But if concretely  if you decide to use a Gaussian kernel  then here's what you need to do  Depending on what support vector machine software package you use  it may ask you to implement a kernel function  or to implement the similarity function  So if you're using an octave or MATLAB implementation of an SVM  it may ask you to provide a function to compute a particular feature of the kernel  So this is really computing f subscript i for one particular value of i  where f here is just a single real number  so maybe I should move this better written f(i)  but what you need to do is to write a kernel function that takes this input  you know  a training example or a test example whatever it takes in some vector X and takes as input one of the landmarks and but only I've come down X1 and X2 here  because the landmarks are really training examples as well  But what you need to do is write software that takes this input  you know  X1  X2 and computes this sort of similarity function between them and return a real number  And so what some support vector machine packages do is expect you to provide this kernel function that take this input you know  X1  X2 and returns a real number  And then it will take it from there and it will automatically generate all the features  and so automatically take X and map it to f1  f2  down to f(m) using this function that you write  and generate all the features and train the support vector machine from there  But sometimes you do need to provide this function yourself  Other if you are using the Gaussian kernel  some SVM implementations will also include the Gaussian kernel and a few other kernels as well  since the Gaussian kernel is probably the most common kernel  Gaussian and linear kernels are really the two most popular kernels by far  Just one implementational note  If you have features of very different scales  it is important to perform feature scaling before using the Gaussian kernel  And here's why  If you imagine the computing the norm between X and l  right  so this term here  and the numerator term over there  What this is doing  the norm between X and l  that's really saying  you know  let's compute the vector V  which is equal to X minus l  And then let's compute the norm does vector V  which is the difference between X  So the norm of V is really equal to V1 squared plus V2 squared plus dot dot dot  plus Vn squared  Because here X is in Rn  or Rn plus 1  but I'm going to ignore  you know  X0  So  let's pretend X is an Rn  square on the left side is what makes this correct  So this is equal to that  right? And so written differently  this is going to be X1 minus l1 squared  plus x2 minus l2 squared  plus dot dot dot plus Xn minus ln squared  And now if your features take on very different ranges of value  So take a housing prediction  for example  if your data is some data about houses  And if X is in the range of thousands of square feet  for the first feature  X1  But if your second feature  X2 is the number of bedrooms  So if this is in the range of one to five bedrooms  then X1 minus l1 is going to be huge  This could be like a thousand squared  whereas X2 minus l2 is going to be much smaller and if that's the case  then in this term  those distances will be almost essentially dominated by the sizes of the houses and the number of bathrooms would be largely ignored  As so as  to avoid this in order to make a machine work well  do perform future scaling  And that will sure that the SVM gives  you know  comparable amount of attention to all of your different features  and not just to in this example to size of houses were big movement here the features  When you try a support vector machines chances are by far the two most common kernels you use will be the linear kernel  meaning no kernel  or the Gaussian kernel that we talked about  And just one note of warning which is that not all similarity functions you might come up with are valid kernels  And the Gaussian kernel and the linear kernel and other kernels that you sometimes others will use  all of them need to satisfy a technical condition  It's called Mercer's Theorem and the reason you need to this is because support vector machine algorithms or implementations of the SVM have lots of clever numerical optimization tricks  In order to solve for the parameter's theta efficiently and in the original design envisaged  those are decision made to restrict our attention only to kernels that satisfy this technical condition called Mercer's Theorem  And what that does is  that makes sure that all of these SVM packages  all of these SVM software packages can use the large class of optimizations and get the parameter theta very quickly  So  what most people end up doing is using either the linear or Gaussian kernel  but there are a few other kernels that also satisfy Mercer's theorem and that you may run across other people using  although I personally end up using other kernels you know  very  very rarely  if at all  Just to mention some of the other kernels that you may run across  One is the polynomial kernel  And for that the similarity between X and l is defined as  there are a lot of options  you can take X transpose l squared  So  here's one measure of how similar X and l are  If X and l are very close with each other  then the inner product will tend to be large  And so  you know  this is a slightly unusual kernel  That is not used that often  but you may run across some people using it  This is one version of a polynomial kernel  Another is X transpose l cubed  These are all examples of the polynomial kernel  X transpose l plus 1 cubed  X transpose l plus maybe a number different then one 5 and  you know  to the power of 4 and so the polynomial kernel actually has two parameters  One is  what number do you add over here? It could be 0  This is really plus 0 over there  as well as what's the degree of the polynomial over there  So the degree power and these numbers  And the more general form of the polynomial kernel is X transpose l  plus some constant and then to some degree in the X1 and so both of these are parameters for the polynomial kernel  So the polynomial kernel almost always or usually performs worse  And the Gaussian kernel does not use that much  but this is just something that you may run across  Usually it is used only for data where X and l are all strictly non negative  and so that ensures that these inner products are never negative  And this captures the intuition that X and l are very similar to each other  then maybe the inter product between them will be large  They have some other properties as well but people tend not to use it much  And then  depending on what you're doing  there are other  sort of more esoteric kernels as well  that you may come across  You know  there's a string kernel  this is sometimes used if your input data is text strings or other types of strings  There are things like the chi-square kernel  the histogram intersection kernel  and so on  There are sort of more esoteric kernels that you can use to measure similarity between different objects  So for example  if you're trying to do some sort of text classification problem  where the input x is a string then maybe we want to find the similarity between two strings using the string kernel  but I personally you know end up very rarely  if at all  using these more esoteric kernels  I think I might have use the chi-square kernel  may be once in my life and the histogram kernel  may be once or twice in my life  I've actually never used the string kernel myself  But in case you've run across this in other applications  You know  if you do a quick web search we do a quick Google search or quick Bing search you should have found definitions that these are the kernels as well  So just two last details I want to talk about in this video  One in multiclass classification  So  you have four classes or more generally 3 classes output some appropriate decision bounday between your multiple classes  Most SVM  many SVM packages already have built-in multiclass classification functionality  So if your using a pattern like that  you just use the both that functionality and that should work fine  Otherwise  one way to do this is to use the one versus all method that we talked about when we are developing logistic regression  So what you do is you trade kSVM's if you have k classes  one to distinguish each of the classes from the rest  And this would give you k parameter vectors  so this will give you  upi lmpw  theta 1  which is trying to distinguish class y equals one from all of the other classes  then you get the second parameter  vector theta 2  which is what you get when you  you know  have y equals 2 as the positive class and all the others as negative class and so on up to a parameter vector theta k  which is the parameter vector for distinguishing the final class key from anything else  and then lastly  this is exactly the same as the one versus all method we have for logistic regression  Where we you just predict the class i with the largest theta transpose X  So let's multiclass classification designate  For the more common cases that there is a good chance that whatever software package you use  you know  there will be a reasonable chance that are already have built in multiclass classification functionality  and so you don't need to worry about this result  Finally  we developed support vector machines starting off with logistic regression and then modifying the cost function a little bit  The last thing we want to do in this video is  just say a little bit about  when you will use one of these two algorithms  so let's say n is the number of features and m is the number of training examples  So  when should we use one algorithm versus the other? Well  if n is larger relative to your training set size  so for example  if you take a business with a number of features this is much larger than m and this might be  for example  if you have a text classification problem  where you know  the dimension of the feature vector is I don't know  maybe  10 thousand  And if your training set size is maybe 10 you know  maybe  up to 1000  So  imagine a spam classification problem  where email spam  where you have 10 000 features corresponding to 10 000 words but you have  you know  maybe 10 training examples or maybe up to 1 000 examples  So if n is large relative to m  then what I would usually do is use logistic regression or use it as the m without a kernel or use it with a linear kernel  Because  if you have so many features with smaller training sets  you know  a linear function will probably do fine  and you don't have really enough data to fit a very complicated nonlinear function  Now if is n is small and m is intermediate what I mean by this is n is maybe anywhere from 1 - 1000  1 would be very small  But maybe up to 1000 features and if the number of training examples is maybe anywhere from 10  you know  10 to maybe up to 10 000 examples  Maybe up to 50 000 examples  If m is pretty big like maybe 10 000 but not a million  Right? So if m is an intermediate size then often an SVM with a linear kernel will work well  We talked about this early as well  with the one concrete example  this would be if you have a two dimensional training set  So  if n is equal to 2 where you have  you know  drawing in a pretty large number of training examples  So Gaussian kernel will do a pretty good job separating positive and negative classes  One third setting that's of interest is if n is small but m is large  So if n is you know  again maybe 1 to 1000  could be larger  But if m was  maybe 50 000 and greater to millions  So  50 000  a 100 000  million  trillion  You have very very large training set sizes  right  So if this is the case  then a SVM of the Gaussian Kernel will be somewhat slow to run  Today's SVM packages  if you're using a Gaussian Kernel  tend to struggle a bit  If you have  you know  maybe 50 thousands okay  but if you have a million training examples  maybe or even a 100 000 with a massive value of m  Today's SVM packages are very good  but they can still struggle a little bit when you have a massive  massive trainings that size when using a Gaussian Kernel  So in that case  what I would usually do is try to just manually create have more features and then use logistic regression or an SVM without the Kernel  And in case you look at this slide and you see logistic regression or SVM without a kernel  In both of these places  I kind of paired them together  There's a reason for that  is that logistic regression and SVM without the kernel  those are really pretty similar algorithms and  you know  either logistic regression or SVM without a kernel will usually do pretty similar things and give pretty similar performance  but depending on your implementational details  one may be more efficient than the other  But  where one of these algorithms applies  logistic regression where SVM without a kernel  the other one is to likely to work pretty well as well  But along with the power of the SVM is when you use different kernels to learn complex nonlinear functions  And this regime  you know  when you have maybe up to 10 000 examples  maybe up to 50 000  And your number of features  this is reasonably large  That's a very common regime and maybe that's a regime where a support vector machine with a kernel kernel will shine  You can do things that are much harder to do that will need logistic regression  And finally  where do neural networks fit in? Well for all of these problems  for all of these different regimes  a well designed neural network is likely to work well as well  The one disadvantage  or the one reason that might not sometimes use the neural network is that  for some of these problems  the neural network might be slow to train  But if you have a very good SVM implementation package  that could run faster  quite a bit faster than your neural network  And  although we didn't show this earlier  it turns out that the optimization problem that the SVM has is a convex optimization problem and so the good SVM optimization software packages will always find the global minimum or something close to it  And so for the SVM you don't need to worry about local optima  In practice local optima aren't a huge problem for neural networks but they all solve  so this is one less thing to worry about if you're using an SVM  And depending on your problem  the neural network may be slower  especially in this sort of regime than the SVM  In case the guidelines they gave here  seem a little bit vague and if you're looking at some problems  you know  the guidelines are a bit vague  I'm still not entirely sure  should I use this algorithm or that algorithm  that's actually okay  When I face a machine learning problem  you know  sometimes its actually just not clear whether that's the best algorithm to use  but as you saw in the earlier videos  really  you know  the algorithm does matter  but what often matters even more is things like  how much data do you have  And how skilled are you  how good are you at doing error analysis and debugging learning algorithms  figuring out how to design new features and figuring out what other features to give you learning algorithms and so on  And often those things will matter more than what you are using logistic regression or an SVM  But having said that  the SVM is still widely perceived as one of the most powerful learning algorithms  and there is this regime of when there's a very effective way to learn complex non linear functions  And so I actually  together with logistic regressions  neural networks  SVM's  using those to speed learning algorithms you're I think very well positioned to build state of the art you know  machine learning systems for a wide region for applications and this is another very powerful tool to have in your arsenal  One that is used all over the place in Silicon Valley  or in industry and in the Academia  to build many high performance machine learning system 
S47aSEqm_0I,Unsupervised Learning  Introduction  In this video  I'd like to start to talk about clustering  This will be exciting  because this is our first unsupervised learning algorithm  where we learn from unlabeled data instead from labelled data  So  what is unsupervised learning? I briefly talked about unsupervised learning at the beginning of the class but it's useful to contrast it with supervised learning  So  here's a typical supervised learning problem where we're given a labeled training set and the goal is to find the decision boundary that separates the positive label examples and the negative label examples  So  the supervised learning problem in this case is given a set of labels to fit a hypothesis to it  In contrast  in the unsupervised learning problem we're given data that does not have any labels associated with it  So  we're given data that looks like this  Here's a set of points add in no labels  and so  our training set is written just x1  x2  and so on up to x m and we don't get any labels y  And that's why the points plotted up on the figure don't have any labels with them  So  in unsupervised learning what we do is we give this sort of unlabeled training set to an algorithm and we just ask the algorithm find some structure in the data for us  Given this data set one type of structure we might have an algorithm find is that it looks like this data set has points grouped into two separate clusters and so an algorithm that finds clusters like the ones I've just circled is called a clustering algorithm  And this would be our first type of unsupervised learning  although there will be other types of unsupervised learning algorithms that we'll talk about later that finds other types of structure or other types of patterns in the data other than clusters  We'll talk about this after we've talked about clustering  So  what is clustering good for? Early in this class I already mentioned a few applications  One is market segmentation where you may have a database of customers and want to group them into different marker segments so you can sell to them separately or serve your different market segments better  Social network analysis  There are actually groups have done this things like looking at a group of people's social networks  So  things like Facebook  Google+  or maybe information about who other people that you email the most frequently and who are the people that they email the most frequently and to find coherence in groups of people  So  this would be another maybe clustering algorithm where you know want to find who are the coherent groups of friends in the social network? Here's something that one of my friends actually worked on which is  use clustering to organize computer clusters or to organize data centers better  Because if you know which computers in the data center in the cluster tend to work together  you can use that to reorganize your resources and how you layout the network and how you design your data center communications  And lastly  something that actually another friend worked on using clustering algorithms to understand galaxy formation and using that to understand astronomical data  So  that's clustering which is our first example of an unsupervised learning algorithm  In the next video we'll start to talk about a specific clustering algorithm 
Ptds1GWxfFM,K-Means Algorithm  In the clustering problem we are given an unlabeled data set and we would like to have an algorithm automatically group the data into coherent subsets or into coherent clusters for us  The K Means algorithm is by far the most popular  by far the most widely used clustering algorithm  and in this video I would like to tell you what the K Means Algorithm is and how it works  The K means clustering algorithm is best illustrated in pictures  Let's say I want to take an unlabeled data set like the one shown here  and I want to group the data into two clusters  If I run the K Means clustering algorithm  here is what I'm going to do  The first step is to randomly initialize two points  called the cluster centroids  So  these two crosses here  these are called the Cluster Centroids and I have two of them because I want to group my data into two clusters  K Means is an iterative algorithm and it does two things  First is a cluster assignment step  and second is a move centroid step  So  let me tell you what those things mean  The first of the two steps in the loop of K means  is this cluster assignment step  What that means is that  it's going through each of the examples  each of these green dots shown here and depending on whether it's closer to the red cluster centroid or the blue cluster centroid  it is going to assign each of the data points to one of the two cluster centroids  Specifically  what I mean by that  is to go through your data set and color each of the points either red or blue  depending on whether it is closer to the red cluster centroid or the blue cluster centroid  and I've done that in this diagram here  So  that was the cluster assignment step  The other part of K means  in the loop of K means  is the move centroid step  and what we are going to do is  we are going to take the two cluster centroids  that is  the red cross and the blue cross  and we are going to move them to the average of the points colored the same colour  So what we are going to do is look at all the red points and compute the average  really the mean of the location of all the red points  and we are going to move the red cluster centroid there  And the same things for the blue cluster centroid  look at all the blue dots and compute their mean  and then move the blue cluster centroid there  So  let me do that now  We're going to move the cluster centroids as follows and I've now moved them to their new means  The red one moved like that and the blue one moved like that and the red one moved like that  And then we go back to another cluster assignment step  so we're again going to look at all of my unlabeled examples and depending on whether it's closer the red or the blue cluster centroid  I'm going to color them either red or blue  I'm going to assign each point to one of the two cluster centroids  so let me do that now  And so the colors of some of the points just changed  And then I'm going to do another move centroid step  So I'm going to compute the average of all the blue points  compute the average of all the red points and move my cluster centroids like this  and so  let's do that again  Let me do one more cluster assignment step  So colour each point red or blue  based on what it's closer to and then do another move centroid step and we're done  And in fact if you keep running additional iterations of K means from here the cluster centroids will not change any further and the colours of the points will not change any further  And so  this is the  at this point  K means has converged and it's done a pretty good job finding the two clusters in this data  Let's write out the K means algorithm more formally  The K means algorithm takes two inputs  One is a parameter K  which is the number of clusters you want to find in the data  I'll later say how we might go about trying to choose k  but for now let's just say that we've decided we want a certain number of clusters and we're going to tell the algorithm how many clusters we think there are in the data set  And then K means also takes as input this sort of unlabeled training set of just the Xs and because this is unsupervised learning  we don't have the labels Y anymore  And for unsupervised learning of the K means I'm going to use the convention that XI is an RN dimensional vector  And that's why my training examples are now N dimensional rather N plus one dimensional vectors  This is what the K means algorithm does  The first step is that it randomly initializes k cluster centroids which we will call mu 1  mu 2  up to mu k  And so in the earlier diagram  the cluster centroids corresponded to the location of the red cross and the location of the blue cross  So there we had two cluster centroids  so maybe the red cross was mu 1 and the blue cross was mu 2  and more generally we would have k cluster centroids rather than just 2  Then the inner loop of k means does the following  we're going to repeatedly do the following  First for each of my training examples  I'm going to set this variable CI to be the index 1 through K of the cluster centroid closest to XI  So this was my cluster assignment step  where we took each of my examples and coloured it either red or blue  depending on which cluster centroid it was closest to  So CI is going to be a number from 1 to K that tells us  you know  is it closer to the red cross or is it closer to the blue cross  and another way of writing this is I'm going to  to compute Ci  I'm going to take my Ith example Xi and and I'm going to measure it's distance to each of my cluster centroids  this is mu and then lower-case k  right  so capital K is the total number centroids and I'm going to use lower case k here to index into the different centroids  But so  Ci is going to  I'm going to minimize over my values of k and find the value of K that minimizes this distance between Xi and the cluster centroid  and then  you know  the value of k that minimizes this  that's what gets set in Ci  So  here's another way of writing out what Ci is  If I write the norm between Xi minus Mu-k  then this is the distance between my ith training example Xi and the cluster centroid Mu subscript K  this is--this here  that's a lowercase K  So uppercase K is going to be used to denote the total number of cluster centroids  and this lowercase K's a number between one and capital K  I'm just using lower case K to index into my different cluster centroids  Next is lower case k  So that's the distance between the example and the cluster centroid and so what I'm going to do is find the value of K  of lower case k that minimizes this  and so the value of k that minimizes you know  that's what I'm going to set as Ci  and by convention here I've written the distance between Xi and the cluster centroid  by convention people actually tend to write this as the squared distance  So we think of Ci as picking the cluster centroid with the smallest squared distance to my training example Xi  But of course minimizing squared distance  and minimizing distance that should give you the same value of Ci  but we usually put in the square there  just as the convention that people use for K means  So that was the cluster assignment step  The other in the loop of K means does the move centroid step  And what that does is for each of my cluster centroids  so for lower case k equals 1 through K  it sets Mu-k equals to the average of the points assigned to cluster  So as a concrete example  let's say that one of my cluster centroids  let's say cluster centroid two  has training examples  you know  1  5  6  and 10 assigned to it  And what this means is  really this means that C1 equals to C5 equals to C6 equals to and similarly well c10 equals  too  right? If we got that from the cluster assignment step  then that means examples 1 5 6 and 10 were assigned to the cluster centroid two  Then in this move centroid step  what I'm going to do is just compute the average of these four things  So X1 plus X5 plus X6 plus X10  And now I'm going to average them so here I have four points assigned to this cluster centroid  just take one quarter of that  And now Mu2 is going to be an n-dimensional vector  Because each of these example x1  x5  x6  x10 each of them were an n-dimensional vector  and I'm going to add up these things and  you know  divide by four because I have four points assigned to this cluster centroid  I end up with my move centroid step  for my cluster centroid mu-2  This has the effect of moving mu-2 to the average of the four points listed here  One thing that I've asked is  well here we said  let's let mu-k be the average of the points assigned to the cluster  But what if there is a cluster centroid no points with zero points assigned to it  In that case the more common thing to do is to just eliminate that cluster centroid  And if you do that  you end up with K minus one clusters instead of k clusters  Sometimes if you really need k clusters  then the other thing you can do if you have a cluster centroid with no points assigned to it is you can just randomly reinitialize that cluster centroid  but it's more common to just eliminate a cluster if somewhere during K means it with no points assigned to that cluster centroid  and that can happen  altthough in practice it happens not that often  So that's the K means Algorithm  Before wrapping up this video I just want to tell you about one other common application of K Means and that's to the problems with non well separated clusters  Here's what I mean  So far we've been picturing K Means and applying it to data sets like that shown here where we have three pretty well separated clusters  and we'd like an algorithm to find maybe the 3 clusters for us  But it turns out that very often K Means is also applied to data sets that look like this where there may not be several very well separated clusters  Here is an example application  to t-shirt sizing  Let's say you are a t-shirt manufacturer you've done is you've gone to the population that you want to sell t-shirts to  and you've collected a number of examples of the height and weight of these people in your population and so  well I guess height and weight tend to be positively highlighted so maybe you end up with a data set like this  you know  with a sample or set of examples of different peoples heights and weight  Let's say you want to size your t shirts  Let's say I want to design and sell t shirts of three sizes  small  medium and large  So how big should I make my small one? How big should I my medium? And how big should I make my large t-shirts  One way to do that would to be to run my k means clustering logarithm on this data set that I have shown on the right and maybe what K Means will do is group all of these points into one cluster and group all of these points into a second cluster and group all of those points into a third cluster  So  even though the data  you know  before hand it didn't seem like we had 3 well separated clusters  K Means will kind of separate out the data into multiple pluses for you  And what you can do is then look at this first population of people and look at them and  you know  look at the height and weight  and try to design a small t-shirt so that it kind of fits this first population of people well and then design a medium t-shirt and design a large t-shirt  And this is in fact kind of an example of market segmentation where you're using K Means to separate your market into 3 different segments  So you can design a product separately that is a small  medium  and large t-shirts  that tries to suit the needs of each of your 3 separate sub-populations well  So that's the K Means algorithm  And by now you should know how to implement the K Means Algorithm and kind of get it to work for some problems  But in the next few videos what I want to do is really get more deeply into the nuts and bolts of K means and to talk a bit about how to actually get this to work really well 
MmCppLiUb-k,Optimization Objective  Most of the supervised learning algorithms we've seen  things like linear regression  logistic regression  and so on  all of those algorithms have an optimization objective or some cost function that the algorithm was trying to minimize  It turns out that k-means also has an optimization objective or a cost function that it's trying to minimize  And in this video I'd like to tell you what that optimization objective is  And the reason I want to do so is because this will be useful to us for two purposes  First  knowing what is the optimization objective of k-means will help us to debug the learning algorithm and just make sure that k-means is running correctly  And second  and perhaps more importantly  in a later video we'll talk about how we can use this to help k-means find better costs for this and avoid the local ultima  But we do that in a later video that follows this one  Just as a quick reminder while k-means is running we're going to be keeping track of two sets of variables  First is the ci's and that keeps track of the index or the number of the cluster  to which an example xi is currently assigned  And then the other set of variables we use is mu subscript k  which is the location of cluster centroid k  Again  for k-means we use capital K to denote the total number of clusters  And here lower case k is going to be an index into the cluster centroids and so  lower case k is going to be a number between one and capital K  Now here's one more bit of notation  which is gonna use mu subscript ci to denote the cluster centroid of the cluster to which example xi has been assigned  right? And to explain that notation a little bit more  lets say that xi has been assigned to cluster number five  What that means is that ci  that is the index of xi  that that is equal to five  Right? Because having ci equals five  if that's what it means for the example xi to be assigned to cluster number five  And so mu subscript ci is going to be equal to mu subscript 5  Because ci is equal to five  And so this mu subscript ci is the cluster centroid of cluster number five  which is the cluster to which my example xi has been assigned  Out with this notation  we're now ready to write out what is the optimization objective of the k-means clustering algorithm and here it is  The cost function that k-means is minimizing is a function J of all of these parameters  c1 through cm and mu 1 through mu K  That k-means is varying as the algorithm runs  And the optimization objective is shown to the right  is the average of 1 over m of sum from i equals 1 through m of this term here  That I've just drawn the red box around  right? The square distance between each example xi and the location of the cluster centroid to which xi has been assigned  So let's draw this and just let me explain this  Right  so here's the location of training example xi and here's the location of the cluster centroid to which example xi has been assigned  So to explain this in pictures  if here's x1  x2  and if a point here is my example xi  so if that is equal to my example xi  and if xi has been assigned to some cluster centroid  I'm gonna denote my cluster centroid with a cross  so if that's the location of mu 5  let's say  If x i has been assigned cluster centroid five as in my example up there  then this square distance  that's the square of the distance between the point xi and this cluster centroid to which xi has been assigned  And what k-means can be shown to be doing is that it is trying to define parameters ci and mu i  Trying to find c and mu to try to minimize this cost function J  This cost function is sometimes also called the distortion cost function  or the distortion of the k-means algorithm  And just to provide a little bit more detail  here's the k-means algorithm  Here's exactly the algorithm as we have written it out on the earlier slide  And what this first step of this algorithm is  this was the cluster assignment step where we assigned each point to the closest centroid  And it's possible to show mathematically that what the cluster assignment step is doing is exactly Minimizing J  with respect to the variables c1  c2 and so on  up to cm  while holding the cluster centroids mu 1 up to mu K  fixed  So what the cluster assignment step does is it doesn't change the cluster centroids  but what it's doing is this is exactly picking the values of c1  c2  up to cm  That minimizes the cost function  or the distortion function J  And it's possible to prove that mathematically  but I won't do so here  But it has a pretty intuitive meaning of just well  let's assign each point to a cluster centroid that is closest to it  because that's what minimizes the square of distance between the points in the cluster centroid  And then the second step of k-means  this second step over here  The second step was the move centroid step  And once again I won't prove it  but it can be shown mathematically that what the move centroid step does is it chooses the values of mu that minimizes J  so it minimizes the cost function J with respect to  wrt is my abbreviation for  with respect to  when it minimizes J with respect to the locations of the cluster centroids mu 1 through mu K  So if is really is doing is this taking the two sets of variables and partitioning them into two halves right here  First the c sets of variables and then you have the mu sets of variables  And what it does is it first minimizes J with respect to the variable c and then it minimizes J with respect to the variables mu and then it keeps on  And  so all that's all that k-means does  And now that we understand k-means as trying to minimize this cost function J  we can also use this to try to debug other any algorithm and just kind of make sure that our implementation of k-means is running correctly  So  we now understand the k-means algorithm as trying to optimize this cost function J  which is also called the distortion function  We can use that to debug k-means and help make sure that k-means is converging and is running properly  And in the next video we'll also see how we can use this to help k-means find better clusters and to help k-means to avoid
cR4rxllyiCs,Random Initialization  In this video  I'd like to talk about how to initialize K-means and more importantly  this will lead into a discussion of how to make K-means avoid local optima as well  Here's the K-means clustering algorithm that we talked about earlier  One step that we never really talked much about was this step of how you randomly initialize the cluster centroids  There are few different ways that one can imagine using to randomly initialize the cluster centroids  But  it turns out that there is one method that is much more recommended than most of the other options one might think about  So  let me tell you about that option since it's what often seems to work best  Here's how I usually initialize my cluster centroids  When running K-means  you should have the number of cluster centroids  K  set to be less than the number of training examples M  It would be really weird to run K-means with a number of cluster centroids that's  you know  equal or greater than the number of examples you have  right? So the way I usually initialize K-means is  I would randomly pick k training examples  So  and  what I do is then set Mu1 of MuK equal to these k examples  Let me show you a concrete example  Lets say that k is equal to 2 and so on this example on the right let's say I want to find two clusters  So  what I'm going to do in order to initialize my cluster centroids is  I'm going to randomly pick a couple examples  And let's say  I pick this one and I pick that one  And the way I'm going to initialize my cluster centroids is  I'm just going to initialize my cluster centroids to be right on top of those examples  So that's my first cluster centroid and that's my second cluster centroid  and that's one random initialization of K-means  The one I drew looks like a particularly good one  And sometimes I might get less lucky and maybe I'll end up picking that as my first random initial example  and that as my second one  And here I'm picking two examples because k equals 2  Some we have randomly picked two training examples and if I chose those two then I'll end up with  may be this as my first cluster centroid and that as my second initial location of the cluster centroid  So  that's how you can randomly initialize the cluster centroids  And so at initialization  your first cluster centroid Mu1 will be equal to x(i) for some randomly value of i and Mu2 will be equal to x(j) for some different randomly chosen value of j and so on  if you have more clusters and more cluster centroid  And sort of the side common  I should say that in the earlier video where I first illustrated K-means with the animation  In that set of slides  Only for the purpose of illustration  I actually used a different method of initialization for my cluster centroids  But the method described on this slide  this is really the recommended way  And the way that you should probably use  when you implement K-means  So  as they suggested perhaps by these two illustrations on the right  You might really guess that K-means can end up converging to different solutions depending on exactly how the clusters were initialized  and so  depending on the random initialization  K-means can end up at different solutions  And  in particular  K-means can actually end up at local optima  If you're given the data sale like this  Well  it looks like  you know  there are three clusters  and so  if you run K-means and if it ends up at a good local optima this might be really the global optima  you might end up with that cluster ring  But if you had a particularly unlucky  random initialization  K-means can also get stuck at different local optima  So  in this example on the left it looks like this blue cluster has captured a lot of points of the left and then the they were on the green clusters each is captioned on the relatively small number of points  And so  this corresponds to a bad local optima because it has basically taken these two clusters and used them into 1 and furthermore  has split the second cluster into two separate sub-clusters like so  and it has also taken the second cluster and split it into two separate sub-clusters like so  and so  both of these examples on the lower right correspond to different local optima of K-means and in fact  in this example here  the cluster  the red cluster has captured only a single optima example  And the term local optima  by the way  refers to local optima of this distortion function J  and what these solutions on the lower left  what these local optima correspond to is really solutions where K-means has gotten stuck to the local optima and it's not doing a very good job minimizing this distortion function J  So  if you're worried about K-means getting stuck in local optima  if you want to increase the odds of K-means finding the best possible clustering  like that shown on top here  what we can do  is try multiple  random initializations  So  instead of just initializing K-means once and hopping that that works  what we can do is  initialize K-means lots of times and run K-means lots of times  and use that to try to make sure we get as good a solution  as good a local or global optima as possible  Concretely  here's how you could go about doing that  Let's say  I decide to run K-meanss a hundred times so I'll execute this loop a hundred times and it's fairly typical a number of times when came to will be something from 50 up to may be 1000  So  let's say you decide to say K-means one hundred times  So what that means is that we would randomnly initialize K-means  And for each of these one hundred random intializations we would run K-means and that would give us a set of clusteringings  and a set of cluster centroids  and then we would then compute the distortion J  that is compute this cause function on the set of cluster assignments and cluster centroids that we got  Finally  having done this whole procedure a hundred times  You will have a hundred different ways of clustering the data and then finally what you do is all of these hundred ways you have found of clustering the data  just pick one  that gives us the lowest cost  That gives us the lowest distortion  And it turns out that if you are running K-means with a fairly small number of clusters   so you know if the number of clusters is anywhere from two up to maybe 10 - then doing multiple random initializations can often  can sometimes make sure that you find a better local optima  Make sure you find the better clustering data  But if K is very large  so  if K is much greater than 10  certainly if K were  you know  if you were trying to find hundreds of clusters  then  having multiple random initializations is less likely to make a huge difference and there is a much higher chance that your first random initialization will give you a pretty decent solution already and doing  doing multiple random initializations will probably give you a slightly better solution but  but maybe not that much  But it's really in the regime of where you have a relatively small number of clusters  especially if you have  maybe 2 or 3 or 4 clusters that random initialization could make a huge difference in terms of making sure you do a good job minimizing the distortion function and giving you a good clustering  So  that's K-means with random initialization  If you're trying to learn a clustering with a relatively small number of clusters  2  3  4  5  maybe  6  7  using multiple random initializations can sometimes  help you find much better clustering of the data  But  even if you are learning a large number of clusters  the initialization  the random initialization method that I describe here  That should give K-means a reasonable starting point to start from for finding a good set of clusters In this video  I'd like to talk about how to initialize K-means and more importantly  this will lead into a discussion of how to make K-means avoid local optima as well  Here's the K-means clustering algorithm that we talked about earlier  One step that we never really talked much about was this step of how you randomly initialize the cluster centroids  There are few different ways that one can imagine using to randomly initialize the cluster centroids  But  it turns out that there is one method that is much more recommended than most of the other options one might think about  So  let me tell you about that option since it's what often seems to work best  Here's how I usually initialize my cluster centroids  When running K-means  you should have the number of cluster centroids  K  set to be less than the number of training examples M  It would be really weird to run K-means with a number of cluster centroids that's  you know  equal or greater than the number of examples you have  right? So the way I usually initialize K-means is  I would randomly pick k training examples  So  and  what I do is then set Mu1 of MuK equal to these k examples  Let me show you a concrete example  Lets say that k is equal to 2 and so on this example on the right let's say I want to find two clusters  So  what I'm going to do in order to initialize my cluster centroids is  I'm going to randomly pick a couple examples  And let's say  I pick this one and I pick that one  And the way I'm going to initialize my cluster centroids is  I'm just going to initialize my cluster centroids to be right on top of those examples  So that's my first cluster centroid and that's my second cluster centroid  and that's one random initialization of K-means  The one I drew looks like a particularly good one  And sometimes I might get less lucky and maybe I'll end up picking that as my first random initial example  and that as my second one  And here I'm picking two examples because k equals 2  Some we have randomly picked two training examples and if I chose those two then I'll end up with  may be this as my first cluster centroid and that as my second initial location of the cluster centroid  So  that's how you can randomly initialize the cluster centroids  And so at initialization  your first cluster centroid Mu1 will be equal to x(i) for some randomly value of i and Mu2 will be equal to x(j) for some different randomly chosen value of j and so on  if you have more clusters and more cluster centroid  And sort of the side common  I should say that in the earlier video where I first illustrated K-means with the animation  In that set of slides  Only for the purpose of illustration  I actually used a different method of initialization for my cluster centroids  But the method described on this slide  this is really the recommended way  And the way that you should probably use  when you implement K-means  So  as they suggested perhaps by these two illustrations on the right  You might really guess that K-means can end up converging to different solutions depending on exactly how the clusters were initialized  and so  depending on the random initialization  K-means can end up at different solutions  And  in particular  K-means can actually end up at local optima  If you're given the data sale like this  Well  it looks like  you know  there are three clusters  and so  if you run K-means and if it ends up at a good local optima this might be really the global optima  you might end up with that cluster ring  But if you had a particularly unlucky  random initialization  K-means can also get stuck at different local optima  So  in this example on the left it looks like this blue cluster has captured a lot of points of the left and then the they were on the green clusters each is captioned on the relatively small number of points  And so  this corresponds to a bad local optima because it has basically taken these two clusters and used them into 1 and furthermore  has split the second cluster into two separate sub-clusters like so  and it has also taken the second cluster and split it into two separate sub-clusters like so  and so  both of these examples on the lower right correspond to different local optima of K-means and in fact  in this example here  the cluster  the red cluster has captured only a single optima example  And the term local optima  by the way  refers to local optima of this distortion function J  and what these solutions on the lower left  what these local optima correspond to is really solutions where K-means has gotten stuck to the local optima and it's not doing a very good job minimizing this distortion function J  So  if you're worried about K-means getting stuck in local optima  if you want to increase the odds of K-means finding the best possible clustering  like that shown on top here  what we can do  is try multiple  random initializations  So  instead of just initializing K-means once and hopping that that works  what we can do is  initialize K-means lots of times and run K-means lots of times  and use that to try to make sure we get as good a solution  as good a local or global optima as possible  Concretely  here's how you could go about doing that  Let's say  I decide to run K-meanss a hundred times so I'll execute this loop a hundred times and it's fairly typical a number of times when came to will be something from 50 up to may be 1000  So  let's say you decide to say K-means one hundred times  So what that means is that we would randomnly initialize K-means  And for each of these one hundred random intializations we would run K-means and that would give us a set of clusteringings  and a set of cluster centroids  and then we would then compute the distortion J  that is compute this cause function on the set of cluster assignments and cluster centroids that we got  Finally  having done this whole procedure a hundred times  You will have a hundred different ways of clustering the data and then finally what you do is all of these hundred ways you have found of clustering the data  just pick one  that gives us the lowest cost  That gives us the lowest distortion  And it turns out that if you are running K-means with a fairly small number of clusters   so you know if the number of clusters is anywhere from two up to maybe 10 - then doing multiple random initializations can often  can sometimes make sure that you find a better local optima  Make sure you find the better clustering data  But if K is very large  so  if K is much greater than 10  certainly if K were  you know  if you were trying to find hundreds of clusters  then  having multiple random initializations is less likely to make a huge difference and there is a much higher chance that your first random initialization will give you a pretty decent solution already and doing  doing multiple random initializations will probably give you a slightly better solution but  but maybe not that much  But it's really in the regime of where you have a relatively small number of clusters  especially if you have  maybe 2 or 3 or 4 clusters that random initialization could make a huge difference in terms of making sure you do a good job minimizing the distortion function and giving you a good clustering  So  that's K-means with random initialization  If you're trying to learn a clustering with a relatively small number of clusters  2  3  4  5  maybe  6  7  using multiple random initializations can sometimes  help you find much better clustering of the data  But  even if you are learning a large number of clusters  the initialization  the random initialization method that I describe here  That should give K-means a reasonable starting point to start from for finding a good set of clusters In this video  I'd like to talk about how to initialize K-means and more importantly  this will lead into a discussion of how to make K-means avoid local optima as well  Here's the K-means clustering algorithm that we talked about earlier  One step that we never really talked much about was this step of how you randomly initialize the cluster centroids  There are few different ways that one can imagine using to randomly initialize the cluster centroids  But  it turns out that there is one method that is much more recommended than most of the other options one might think about  So  let me tell you about that option since it's what often seems to work best  Here's how I usually initialize my cluster centroids  When running K-means  you should have the number of cluster centroids  K  set to be less than the number of training examples M  It would be really weird to run K-means with a number of cluster centroids that's  you know  equal or greater than the number of examples you have  right? So the way I usually initialize K-means is  I would randomly pick k training examples  So  and  what I do is then set Mu1 of MuK equal to these k examples  Let me show you a concrete example  Lets say that k is equal to 2 and so on this example on the right let's say I want to find two clusters  So  what I'm going to do in order to initialize my cluster centroids is  I'm going to randomly pick a couple examples  And let's say  I pick this one and I pick that one  And the way I'm going to initialize my cluster centroids is  I'm just going to initialize my cluster centroids to be right on top of those examples  So that's my first cluster centroid and that's my second cluster centroid  and that's one random initialization of K-means  The one I drew looks like a particularly good one  And sometimes I might get less lucky and maybe I'll end up picking that as my first random initial example  and that as my second one  And here I'm picking two examples because k equals 2  Some we have randomly picked two training examples and if I chose those two then I'll end up with  may be this as my first cluster centroid and that as my second initial location of the cluster centroid  So  that's how you can randomly initialize the cluster centroids  And so at initialization  your first cluster centroid Mu1 will be equal to x(i) for some randomly value of i and Mu2 will be equal to x(j) for some different randomly chosen value of j and so on  if you have more clusters and more cluster centroid  And sort of the side common  I should say that in the earlier video where I first illustrated K-means with the animation  In that set of slides  Only for the purpose of illustration  I actually used a different method of initialization for my cluster centroids  But the method described on this slide  this is really the recommended way  And the way that you should probably use  when you implement K-means  So  as they suggested perhaps by these two illustrations on the right  You might really guess that K-means can end up converging to different solutions depending on exactly how the clusters were initialized  and so  depending on the random initialization  K-means can end up at different solutions  And  in particular  K-means can actually end up at local optima  If you're given the data sale like this  Well  it looks like  you know  there are three clusters  and so  if you run K-means and if it ends up at a good local optima this might be really the global optima  you might end up with that cluster ring  But if you had a particularly unlucky  random initialization  K-means can also get stuck at different local optima  So  in this example on the left it looks like this blue cluster has captured a lot of points of the left and then the they were on the green clusters each is captioned on the relatively small number of points  And so  this corresponds to a bad local optima because it has basically taken these two clusters and used them into 1 and furthermore  has split the second cluster into two separate sub-clusters like so  and it has also taken the second cluster and split it into two separate sub-clusters like so  and so  both of these examples on the lower right correspond to different local optima of K-means and in fact  in this example here  the cluster  the red cluster has captured only a single optima example  And the term local optima  by the way  refers to local optima of this distortion function J  and what these solutions on the lower left  what these local optima correspond to is really solutions where K-means has gotten stuck to the local optima and it's not doing a very good job minimizing this distortion function J  So  if you're worried about K-means getting stuck in local optima  if you want to increase the odds of K-means finding the best possible clustering  like that shown on top here  what we can do  is try multiple  random initializations  So  instead of just initializing K-means once and hopping that that works  what we can do is  initialize K-means lots of times and run K-means lots of times  and use that to try to make sure we get as good a solution  as good a local or global optima as possible  Concretely  here's how you could go about doing that  Let's say  I decide to run K-meanss a hundred times so I'll execute this loop a hundred times and it's fairly typical a number of times when came to will be something from 50 up to may be 1000  So  let's say you decide to say K-means one hundred times  So what that means is that we would randomnly initialize K-means  And for each of these one hundred random intializations we would run K-means and that would give us a set of clusteringings  and a set of cluster centroids  and then we would then compute the distortion J  that is compute this cause function on the set of cluster assignments and cluster centroids that we got  Finally  having done this whole procedure a hundred times  You will have a hundred different ways of clustering the data and then finally what you do is all of these hundred ways you have found of clustering the data  just pick one  that gives us the lowest cost  That gives us the lowest distortion  And it turns out that if you are running K-means with a fairly small number of clusters   so you know if the number of clusters is anywhere from two up to maybe 10 - then doing multiple random initializations can often  can sometimes make sure that you find a better local optima  Make sure you find the better clustering data  But if K is very large  so  if K is much greater than 10  certainly if K were  you know  if you were trying to find hundreds of clusters  then  having multiple random initializations is less likely to make a huge difference and there is a much higher chance that your first random initialization will give you a pretty decent solution already and doing  doing multiple random initializations will probably give you a slightly better solution but  but maybe not that much  But it's really in the regime of where you have a relatively small number of clusters  especially if you have  maybe 2 or 3 or 4 clusters that random initialization could make a huge difference in terms of making sure you do a good job minimizing the distortion function and giving you a good clustering  So  that's K-means with random initialization  If you're trying to learn a clustering with a relatively small number of clusters  2  3  4  5  maybe  6  7  using multiple random initializations can sometimes  help you find much better clustering of the data  But  even if you are learning a large number of clusters  the initialization  the random initialization method that I describe here  That should give K-means a reasonable starting point to start from for finding a good set of clusters 
Zsr8kODwr08,"Choosing the Number of Clusters  In this video I'd like to talk about one last detail of K-means clustering which is how to choose the number of clusters  or how to choose the value of the parameter capsule K  To be honest  there actually isn't a great way of answering this or doing this automatically and by far the most common way of choosing the number of clusters  is still choosing it manually by looking at visualizations or by looking at the output of the clustering algorithm or something else  But I do get asked this question quite a lot of how do you choose the number of clusters  and so I just want to tell you know what are peoples' current thinking on it although  the most common thing is actually to choose the number of clusters by hand  A large part of why it might not always be easy to choose the number of clusters is that it is often generally ambiguous how many clusters there are in the data  Looking at this data set some of you may see four clusters and that would suggest using K equals 4  Or some of you may see two clusters and that will suggest K equals 2 and now this may see three clusters  And so  looking at the data set like this  the true number of clusters  it actually seems genuinely ambiguous to me  and I don't think there is one right answer  And this is part of our supervised learning  We are aren't given labels  and so there isn't always a clear cut answer  And this is one of the things that makes it more difficult to say  have an automatic algorithm for choosing how many clusters to have  When people talk about ways of choosing the number of clusters  one method that people sometimes talk about is something called the Elbow Method  Let me just tell you a little bit about that  and then mention some of its advantages but also shortcomings  So the Elbow Method  what we're going to do is vary K  which is the total number of clusters  So  we're going to run K-means with one cluster  that means really  everything gets grouped into a single cluster and compute the cost function or compute the distortion J and plot that here  And then we're going to run K means with two clusters  maybe with multiple random initial agents  maybe not  But then  you know  with two clusters we should get  hopefully  a smaller distortion  and so plot that there  And then run K-means with three clusters  hopefully  you get even smaller distortion and plot that there  I'm gonna run K-means with four  five and so on  And so we end up with a curve showing how the distortion  you know  goes down as we increase the number of clusters  And so we get a curve that maybe looks like this  And if you look at this curve  what the Elbow Method does it says \""Well  let's look at this plot  Looks like there's a clear elbow there\""  Right  this is  would be by analogy to the human arm where  you know  if you imagine that you reach out your arm  then  this is your shoulder joint  this is your elbow joint and I guess  your hand is at the end over here  And so this is the Elbow Method  Then you find this sort of pattern where the distortion goes down rapidly from 1 to 2  and 2 to 3  and then you reach an elbow at 3  and then the distortion goes down very slowly after that  And then it looks like  you know what  maybe using three clusters is the right number of clusters  because that's the elbow of this curve  right? That it goes down  distortion goes down rapidly until K equals 3  really goes down very slowly after that  So let's pick K equals 3  If you apply the Elbow Method  and if you get a plot that actually looks like this  then  that's pretty good  and this would be a reasonable way of choosing the number of clusters  It turns out the Elbow Method isn't used that often  and one reason is that  if you actually use this on a clustering problem  it turns out that fairly often  you know  you end up with a curve that looks much more ambiguous  maybe something like this  And if you look at this  I don't know  maybe there's no clear elbow  but it looks like distortion continuously goes down  maybe 3 is a good number  maybe 4 is a good number  maybe 5 is also not bad  And so  if you actually do this in a practice  you know  if your plot looks like the one on the left and that's great  It gives you a clear answer  but just as often  you end up with a plot that looks like the one on the right and is not clear where the ready location of the elbow is  It makes it harder to choose a number of clusters using this method  So maybe the quick summary of the Elbow Method is that is worth the shot but I wouldn't necessarily  you know  have a very high expectation of it working for any particular problem  Finally  here's one other way of how  thinking about how you choose the value of K  very often people are running K-means in order you get clusters for some later purpose  or for some sort of downstream purpose  Maybe you want to use K-means in order to do market segmentation  like in the T-shirt sizing example that we talked about  Maybe you want K-means to organize a computer cluster better  or maybe a learning cluster for some different purpose  and so  if that later  downstream purpose  such as market segmentation  If that gives you an evaluation metric  then often  a better way to determine the number of clusters  is to see how well different numbers of clusters serve that later downstream purpose  Let me step through a specific example  Let me go through the T-shirt size example again  and I'm trying to decide  do I want three T-shirt sizes? So  I choose K equals 3  then I might have small  medium and large T-shirts  Or maybe  I want to choose K equals 5  and then I might have  you know  extra small  small  medium  large and extra large T-shirt sizes  So  you can have like 3 T-shirt sizes or four or five T-shirt sizes  We could also have four T-shirt sizes  but I'm just showing three and five here  just to simplify this slide for now  So  if I run K-means with K equals 3  maybe I end up with  that's my small and that's my medium and that's my large  Whereas  if I run K-means with 5 clusters  maybe I end up with  those are my extra small T-shirts  these are my small  these are my medium  these are my large and these are my extra large  And the nice thing about this example is that  this then maybe gives us another way to choose whether we want 3 or 4 or 5 clusters  and in particular  what you can do is  you know  think about this from the perspective of the T-shirt business and ask  \""Well if I have five segments  then how well will my T-shirts fit my customers and so  how many T-shirts can I sell? How happy will my customers be?\"" What really makes sense  from the perspective of the T-shirt business  in terms of whether  I want to have Goer T-shirt sizes so that my T-shirts fit my customers better  Or do I want to have fewer T-shirt sizes so that I make fewer sizes of T-shirts  And I can sell them to the customers more cheaply  And so  the t-shirt selling business  that might give you a way to decide  between three clusters versus five clusters  So  that gives you an example of how a later downstream purpose like the problem of deciding what T-shirts to manufacture  how that can give you an evaluation metric for choosing the number of clusters  For those of you that are doing the program exercises  if you look at this week's program exercise associative K-means  that's an example there of using K-means for image compression  And so if you were trying to choose how many clusters to use for that problem  you could also  again use the evaluation metric of image compression to choose the number of clusters  K? So  how good do you want the image to look versus  how much do you want to compress the file size of the image  and  you know  if you do the programming exercise  what I've just said will make more sense at that time  So  just summarize  for the most part  the number of customers K is still chosen by hand by human input or human insight  One way to try to do so is to use the Elbow Method  but I wouldn't always expect that to work well  but I think the better way to think about how to choose the number of clusters is to ask  for what purpose are you running K-means? And then to think  what is the number of clusters K that serves that  you know  whatever later purpose that you actually run the K-means for "
PGfG-DPosCA,"Motivation I  Data Compression  In this video  I'd like to start talking about a second type of unsupervised learning problem called dimensionality reduction  There are a couple of different reasons why one might want to do dimensionality reduction  One is data compression  and as we'll see later  a few videos later  data compression not only allows us to compress the data and have it therefore use up less computer memory or disk space  but it will also allow us to speed up our learning algorithms  But first  let's start by talking about what is dimensionality reduction  As a motivating example  let's say that we've collected a data set with many  many  many features  and I've plotted just two of them here  And let's say that unknown to us two of the features were actually the length of something in centimeters  and a different feature  x2  is the length of the same thing in inches  So  this gives us a highly redundant representation and maybe instead of having two separate features x1 then x2  both of which basically measure the length  maybe what we want to do is reduce the data to one-dimensional and just have one number measuring this length  In case this example seems a bit contrived  this centimeter and inches example is actually not that unrealistic  and not that different from things that I see happening in industry  If you have hundreds or thousands of features  it is often this easy to lose track of exactly what features you have  And sometimes may have a few different engineering teams  maybe one engineering team gives you two hundred features  a second engineering team gives you another three hundred features  and a third engineering team gives you five hundred features so you have a thousand features all together  and it actually becomes hard to keep track of you know  exactly which features you got from which team  and it's actually not that want to have highly redundant features like these  And so if the length in centimeters were rounded off to the nearest centimeter and lengthened inches was rounded off to the nearest inch  Then  that's why these examples don't lie perfectly on a straight line  because of  you know  round-off error to the nearest centimeter or the nearest inch  And if we can reduce the data to one dimension instead of two dimensions  that reduces the redundancy  For a different example  again maybe when there seems fairly less contrives  For may years I've been working with autonomous helicopter pilots  Or I've been working with pilots that fly helicopters  And so  If you were to measure--if you were to  you know  do a survey or do a test of these different pilots--you might have one feature  x1  which is maybe the skill of these helicopter pilots  and maybe \""x2\"" could be the pilot enjoyment  That is  you know  how much they enjoy flying  and maybe these two features will be highly correlated  And what you really care about might be this sort of this sort of  this direction  a different feature that really measures pilot aptitude  And I'm making up the name aptitude of course  but again  if you highly correlated features  maybe you really want to reduce the dimension  So  let me say a little bit more about what it really means to reduce the dimension of the data from 2 dimensions down from 2D to 1 dimensional or to 1D  Let me color in these examples by using different colors  And in this case by reducing the dimension what I mean is that I would like to find maybe this line  this  you know  direction on which most of the data seems to lie and project all the data onto that line which is true  and by doing so  what I can do is just measure the position of each of the examples on that line  And what I can do is come up with a new feature  z1  and to specify the position on the line I need only one number  so it says z1 is a new feature that specifies the location of each of those points on this green line  And what this means  is that where as previously if i had an example x1  maybe this was my first example  x1  So in order to represent x1 originally x1  I needed a two dimensional number  or a two dimensional feature vector  Instead now I can represent z1  I could use just z1 to represent my first example  and that's going to be a real number  And similarly x2 you know  if x2 is my second example there  then previously  whereas this required two numbers to represent if I instead compute the projection of that black cross onto the line  And now I only need one real number which is z2 to represent the location of this point z2 on the line  And so on through my M examples  So  just to summarize  if we allow ourselves to approximate the original data set by projecting all of my original examples onto this green line over here  then I need only one number  I need only real number to specify the position of a point on the line  and so what I can do is therefore use just one number to represent the location of each of my training examples after they've been projected onto that green line  So this is an approximation to the original training self because I have projected all of my training examples onto a line  But now  I need to keep around only one number for each of my examples  And so this halves the memory requirement  or a space requirement  or what have you  for how to store my data  And perhaps more interestingly  more importantly  what we'll see later  in the later video as well is that this will allow us to make our learning algorithms run more quickly as well  And that is actually  perhaps  even the more interesting application of this data compression rather than reducing the memory or disk space requirement for storing the data  On the previous slide we showed an example of reducing data from 2D to 1D  On this slide  I'm going to show another example of reducing data from three dimensional 3D to two dimensional 2D  By the way  in the more typical example of dimensionality reduction we might have a thousand dimensional data or 1000D data that we might want to reduce to let's say a hundred dimensional or 100D  but because of the limitations of what I can plot on the slide  I'm going to use examples of 3D to 2D  or 2D to 1D  So  let's have a data set like that shown here  And so  I would have a set of examples x(i) which are points in r3  So  I have three dimension examples  I know it might be a little bit hard to see this on the slide  but I'll show a 3D point cloud in a little bit  And it might be hard to see here  but all of this data maybe lies roughly on the plane  like so  And so what we can do with dimensionality reduction  is take all of this data and project the data down onto a two dimensional plane  So  here what I've done is  I've taken all the data and I've projected all of the data  so that it all lies on the plane  Now  finally  in order to specify the location of a point within a plane  we need two numbers  right? We need to  maybe  specify the location of a point along this axis  and then also specify it's location along that axis  So  we need two numbers  maybe called z1 and z2 to specify the location of a point within a plane  And so  what that means  is that we can now represent each example  each training example  using two numbers that I've drawn here  z1  and z2  So  our data can be represented using vector z which are in r2  And these subscript  z subscript 1  z subscript 2  what I just mean by that is that my vectors here  z  you know  are two dimensional vectors  z1  z2  And so if I have some particular examples  z(i)  or that's the two dimensional vector  z(i)1  z(i)2  And on the previous slide when I was reducing data to one dimensional data then I had only z1  right? And that is what a z1 subscript 1 on the previous slide was  but here I have two dimensional data  so I have z1 and z2 as the two components of the data  Now  let me just make sure that these figures make sense  So let me just reshow these exact three figures again but with 3D plots  So the process we went through was that shown in the lab is the optimal data set  in the middle the data set projects on the 2D  and on the right the 2D data sets with z1 and z2 as the axis  Let's look at them a little bit further  Here's my original data set  shown on the left  and so I had started off with a 3D point cloud like so  where the axis are labeled x1  x2  x3  and so there's a 3D point but most of the data  maybe roughly lies on some  you know  not too far from some 2D plain  So  what we can do is take this data and here's my middle figure  I'm going to project it onto 2D  So  I've projected this data so that all of it now lies on this 2D surface  As you can see all the data lies on a plane  'cause we've projected everything onto a plane  and so what this means is that now I need only two numbers  z1 and z2  to represent the location of point on the plane  And so that's the process that we can go through to reduce our data from three dimensional to two dimensional  So that's dimensionality reduction and how we can use it to compress our data  And as we'll see later this will allow us to make some of our learning algorithms run much later as well  but we'll get to that only in a later video "
dGqsH-1EWmg,Motivation II  Visualization  In the last video  we talked about dimensionality reduction for the purpose of compressing the data  In this video  I'd like to tell you about a second application of dimensionality reduction and that is to visualize the data  For a lot of machine learning applications  it really helps us to develop effective learning algorithms  if we can understand our data better  If there is some way of visualizing the data better  and so  dimensionality reduction offers us  often  another useful tool to do so  Let's start with an example  Let's say we've collected a large data set of many statistics and facts about different countries around the world  So  maybe the first feature  X1 is the country's GDP  or the Gross Domestic Product  and X2 is a per capita  meaning the per person GDP  X3 human development index  life expectancy  X5  X6 and so on  And we may have a huge data set like this  where  you know  maybe 50 features for every country  and we have a huge set of countries  So is there something we can do to try to understand our data better? I've given this huge table of numbers  How do you visualize this data? If you have 50 features  it's very difficult to plot 50-dimensional data  What is a good way to examine this data? Using dimensionality reduction  what we can do is  instead of having each country represented by this featured vector  xi  which is 50-dimensional  so instead of  say  having a country like Canada  instead of having 50 numbers to represent the features of Canada  let's say we can come up with a different feature representation that is these z vectors  that is in R2  If that's the case  if we can have just a pair of numbers  z1 and z2 that somehow  summarizes my 50 numbers  maybe what we can do [xx] is to plot these countries in R2 and use that to try to understand the space in [xx] of features of different countries [xx] the better and so  here  what you can do is reduce the data from 50 D  from 50 dimensions to 2D  so you can plot this as a 2 dimensional plot  and  when you do that  it turns out that  if you look at the output of the Dimensionality Reduction algorithms  It usually doesn't astride a physical meaning to these new features you want [xx] to  It's often up to us to figure out you know  roughly what these features means  But  And if you plot those features  here is what you might find  So  here  every country is represented by a point ZI  which is an R2 and so each of those  Dots  and this figure represents a country  and so  here's Z1 and here's Z2  and [xx] [xx] of these  So  you might find  for example  That the horizontial axis the Z1 axis corresponds roughly to the overall country size  or the overall economic activity of a country  So the overall GDP  overall economic size of a country  Whereas the vertical axis in our data might correspond to the per person GDP  Or the per person well being  or the per person economic activity  and  you might find that  given these 50 features  you know  these are really the 2 main dimensions of the deviation  and so  out here you may have a country like the U S A   which is a relatively large GDP  you know  is a very large GDP and a relatively high per-person GDP as well  Whereas here you might have a country like Singapore  which actually has a very high per person GDP as well  but because Singapore is a much smaller country the overall economy size of Singapore is much smaller than the US  And  over here  you would have countries where individuals are unfortunately some are less well off  maybe shorter life expectancy  less health care  less economic maturity that's why smaller countries  whereas a point like this will correspond to a country that has a fair  has a substantial amount of economic activity  but where individuals tend to be somewhat less well off  So you might find that the axes Z1 and Z2 can help you to most succinctly capture really what are the two main dimensions of the variations amongst different countries  Such as the overall economic activity of the country projected by the size of the country's overall economy as well as the per-person individual well-being  measured by per-person GDP  per-person healthcare  and things like that  So that's how you can use dimensionality reduction  in order to reduce data from 50 dimensions or whatever  down to two dimensions  or maybe down to three dimensions  so that you can plot it and understand your data better  In the next video  we'll start to develop a specific algorithm  called PCA  or Principal Component Analysis  which will allow us to do this and also do the earlier application I talked about of compressing the data 
z2oM0ct9kuc,Principal Component Analysis Problem Formulation  For the problem of dimensionality reduction  by far the most popular  by far the most commonly used algorithm is something called principle components analysis  or PCA  In this video  I'd like to start talking about the problem formulation for PCA  In other words  let's try to formulate  precisely  exactly what we would like PCA to do  Let's say we have a data set like this  So  this is a data set of examples x and R2 and let's say I want to reduce the dimension of the data from two-dimensional to one-dimensional  In other words  I would like to find a line onto which to project the data  So what seems like a good line onto which to project the data  it's a line like this  might be a pretty good choice  And the reason we think this might be a good choice is that if you look at where the projected versions of the point scales  so I take this point and project it down here  Get that  this point gets projected here  to here  to here  to here  What we find is that the distance between each point and the projected version is pretty small  That is  these blue line segments are pretty short  So what PCA does formally is it tries to find a lower dimensional surface  really a line in this case  onto which to project the data so that the sum of squares of these little blue line segments is minimized  The length of those blue line segments  that's sometimes also called the projection error  And so what PCA does is it tries to find a surface onto which to project the data so as to minimize that  As an aside  before applying PCA  it's standard practice to first perform mean normalization at feature scaling so that the features x1 and x2 should have zero mean  and should have comparable ranges of values  I've already done this for this example  but I'll come back to this later and talk more about feature scaling and the normalization in the context of PCA later  But coming back to this example  in contrast to the red line that I just drew  here's a different line onto which I could project my data  which is this magenta line  And  as we'll see  this magenta line is a much worse direction onto which to project my data  right? So if I were to project my data onto the magenta line  we'd get a set of points like that  And the projection errors  that is these blue line segments  will be huge  So these points have to move a huge distance in order to get projected onto the magenta line  And so that's why PCA  principal components analysis  will choose something like the red line rather than the magenta line down here  Let's write out the PCA problem a little more formally  The goal of PCA  if we want to reduce data from two-dimensional to one-dimensional is  we're going to try find a vector that is a vector u1  which is going to be an Rn  so that would be an R2 in this case  I'm gonna find the direction onto which to project the data  so it's to minimize the projection error  So  in this example I'm hoping that PCA will find this vector  which l wanna call u(1)  so that when I project the data onto the line that I define by extending out this vector  I end up with pretty small reconstruction errors  And that reference of data that looks like this  And by the way  I should mention that where the PCA gives me u(1) or -u(1)  doesn't matter  So if it gives me a positive vector in this direction  that's fine  If it gives me the opposite vector facing in the opposite direction  so that would be like minus u(1)  Let's draw that in blue instead  right? But it gives a positive u(1) or negative u(1)  it doesn't matter because each of these vectors defines the same red line onto which I'm projecting my data  So this is a case of reducing data from two-dimensional to one-dimensional  In the more general case we have n-dimensional data and we'll want to reduce it to k-dimensions  In that case we want to find not just a single vector onto which to project the data but we want to find k-dimensions onto which to project the data  So as to minimize this projection error  So here's the example  If I have a 3D point cloud like this  then maybe what I want to do is find vectors  So find a pair of vectors  And I'm gonna call these vectors  Let's draw these in red  I'm going to find a pair of vectors  sustained from the origin  Here's u(1)  and here's my second vector  u(2)  And together  these two vectors define a plane  or they define a 2D surface  right? Like this with a 2D surface onto which I am going to project my data  For those of you that are familiar with linear algebra  for this year they're really experts in linear algebra  the formal definition of this is that we are going to find the set of vectors u(1)  u(2)  maybe up to u(k)  And what we're going to do is project the data onto the linear subspace spanned by this set of k vectors  But if you're not familiar with linear algebra  just think of it as finding k directions instead of just one direction onto which to project the data  So finding a k-dimensional surface is really finding a 2D plane in this case  shown in this figure  where we can define the position of the points in a plane using k directions  And that's why for PCA we want to find k vectors onto which to project the data  And so more formally in PCA  what we want to do is find this way to project the data so as to minimize the sort of projection distance  which is the distance between the points and the projections  And so in this 3D example too  Given a point we would take the point and project it onto this 2D surface  We are done with that  And so the projection error would be  the distance between the point and where it gets projected down to my 2D surface  And so what PCA does is I try to find the line  or a plane  or whatever  onto which to project the data  to try to minimize that square projection  that 90 degree or that orthogonal projection error  Finally  one question I sometimes get asked is how does PCA relate to linear regression? Because when explaining PCA  I sometimes end up drawing diagrams like these and that looks a little bit like linear regression  It turns out PCA is not linear regression  and despite some cosmetic similarity  these are actually totally different algorithms  If we were doing linear regression  what we would do would be  on the left we would be trying to predict the value of some variable y given some info features x  And so linear regression  what we're doing is we're fitting a straight line so as to minimize the square error between point and this straight line  And so what we're minimizing would be the squared magnitude of these blue lines  And notice that I'm drawing these blue lines vertically  That these blue lines are the vertical distance between the point and the value predicted by the hypothesis  Whereas in contrast  in PCA  what it does is it tries to minimize the magnitude of these blue lines  which are drawn at an angle  These are really the shortest orthogonal distances  The shortest distance between the point x and this red line  And this gives very different effects depending on the dataset  And more generally  when you're doing linear regression  there is this distinguished variable y they we're trying to predict  All that linear regression as well as taking all the values of x and try to use that to predict y  Whereas in PCA  there is no distinguish  or there is no special variable y that we're trying to predict  And instead  we have a list of features  x1  x2  and so on  up to xn  and all of these features are treated equally  so no one of them is special  As one last example  if I have three-dimensional data and I want to reduce data from 3D to 2D  so maybe I wanna find two directions  u(1) and u(2)  onto which to project my data  Then what I have is I have three features  x1  x2  x3  and all of these are treated alike  All of these are treated symmetrically and there's no special variable y that I'm trying to predict  And so PCA is not a linear regression  and even though at some cosmetic level they might look related  these are actually very different algorithms  So hopefully you now understand what PCA is doing  It's trying to find a lower dimensional surface onto which to project the data  so as to minimize this squared projection error  To minimize the square distance between each point and the location of where it gets projected  In the next video  we'll start to talk about how to actually find this lower dimensional surface onto which to project the data 
d39tTuUbDVw,"Principal Component Analysis Algorithm  In this video I'd like to tell you about the principle components analysis algorithm  And by the end of this video you know to implement PCA for yourself  And use it reduce the dimension of your data  Before applying PCA  there is a data pre-processing step which you should always do  Given the trading sets of the examples is important to always perform mean normalization  and then depending on your data  maybe perform feature scaling as well  this is very similar to the mean normalization and feature scaling process that we have for supervised learning  In fact it's exactly the same procedure except that we're doing it now to our unlabeled data  X1 through Xm  So for mean normalization we first compute the mean of each feature and then we replace each feature  X  with X minus its mean  and so this makes each feature now have exactly zero mean The different features have very different scales  So for example  if x1 is the size of a house  and x2 is the number of bedrooms  to use our earlier example  we then also scale each feature to have a comparable range of values  And so  similar to what we had with supervised learning  we would take x  i substitute j  that's the j feature and so we would subtract of the mean  now that's what we have on top  and then divide by sj  Here  sj is some measure of the beta values of feature j  So  it could be the max minus min value  or more commonly  it is the standard deviation of feature j  Having done this sort of data pre-processing  here's what the PCA algorithm does  We saw from the previous video that what PCA does is  it tries to find a lower dimensional sub-space onto which to project the data  so as to minimize the squared projection errors  sum of the squared projection errors  as the square of the length of those blue lines that and so what we wanted to do specifically is find a vector  u1  which specifies that direction or in the 2D case we want to find two vectors  u1 and u2  to define this surface onto which to project the data  So  just as a quick reminder of what reducing the dimension of the data means  for this example on the left we were given the examples xI  which are in r2  And what we like to do is find a set of numbers zI in r push to represent our data  So that's what from reduction from 2D to 1D means  So specifically by projecting data onto this red line there  We need only one number to specify the position of the points on the line  So i'm going to call that number z or z1  Z here [xx] real number  so that's like a one dimensional vector  So z1 just refers to the first component of this  you know  one by one matrix  or this one dimensional vector  And so we need only one number to specify the position of a point  So if this example here was my example X1  then maybe that gets mapped here  And if this example was X2 maybe that example gets mapped And so this point here will be Z1 and this point here will be Z2  and similarly we would have those other points for These  maybe X3  X4  X5 get mapped to Z1  Z2  Z3  So What PCA has to do is we need to come up with a way to compute two things  One is to compute these vectors  u1  and in this case u1 and u2  And the other is how do we compute these numbers  Z  So on the example on the left we're reducing the data from 2D to 1D  In the example on the right  we would be reducing data from 3 dimensional as in r3  to zi  which is now two dimensional  So these z vectors would now be two dimensional  So it would be z1 z2 like so  and so we need to give away to compute these new representations  the z1 and z2 of the data as well  So how do you compute all of these quantities? It turns out that a mathematical derivation  also the mathematical proof  for what is the right value U1  U2  Z1  Z2  and so on  That mathematical proof is very complicated and beyond the scope of the course  But once you've done [xx] it turns out that the procedure to actually find the value of u1 that you want is not that hard  even though so that the mathematical proof that this value is the correct value is someone more involved and more than i want to get into  But let me just describe the specific procedure that you have to implement in order to compute all of these things  the vectors  u1  u2  the vector z  Here's the procedure  Let's say we want to reduce the data to n dimensions to k dimension What we're going to do is first compute something called the covariance matrix  and the covariance matrix is commonly denoted by this Greek alphabet which is the capital Greek alphabet sigma  It's a bit unfortunate that the Greek alphabet sigma looks exactly like the summation symbols  So this is the Greek alphabet Sigma is used to denote a matrix and this here is a summation symbol  So hopefully in these slides there won't be ambiguity about which is Sigma Matrix  the matrix  which is a summation symbol  and hopefully it will be clear from context when I'm using each one  How do you compute this matrix let's say we want to store it in an octave variable called sigma  What we need to do is compute something called the eigenvectors of the matrix sigma  And an octave  the way you do that is you use this command  u s v equals s v d of sigma  SVD  by the way  stands for singular value decomposition  This is a Much more advanced single value composition  It is much more advanced linear algebra than you actually need to know but now It turns out that when sigma is equal to matrix there is a few ways to compute these are high in vectors and If you are an expert in linear algebra and if you've heard of high in vectors before you may know that there is another octet function called I  which can also be used to compute the same thing  and It turns out that the SVD function and the I function it will give you the same vectors  although SVD is a little more numerically stable  So I tend to use SVD  although I have a few friends that use the I function to do this as wellbut when you apply this to a covariance matrix sigma it gives you the same thing  This is because the covariance matrix always satisfies a mathematical Property called symmetric positive definite You really don't need to know what that means  but the SVD and I-functions are different functions but when they are applied to a covariance matrix which can be proved to always satisfy this mathematical property  they'll always give you the same thing  Okay  that was probably much more linear algebra than you needed to know  In case none of that made sense  don't worry about it  All you need to know is that this system command you should implement in Octave  And if you're implementing this in a different language than Octave or MATLAB  what you should do is find the numerical linear algebra library that can compute the SVD or singular value decomposition  and there are many such libraries for probably all of the major programming languages  People can use that to compute the matrices u  s  and d of the covariance matrix sigma  So just to fill in some more details  this covariance matrix sigma will be an n by n matrix  And one way to see that is if you look at the definition this is an n by 1 vector and this here I transpose is 1 by N so the product of these two things is going to be an N by N matrix  1xN transfers  1xN  so there's an NxN matrix and when we add up all of these you still have an NxN matrix  And what the SVD outputs three matrices  u  s  and v  The thing you really need out of the SVD is the u matrix  The u matrix will also be a NxN matrix  And if we look at the columns of the U matrix it turns out that the columns of the U matrix will be exactly those vectors  u1  u2 and so on  So u  will be matrix  And if we want to reduce the data from n dimensions down to k dimensions  then what we need to do is take the first k vectors  that gives us u1 up to uK which gives us the K direction onto which we want to project the data  the rest of the procedure from this SVD numerical linear algebra routine we get this matrix u  We'll call these columns u1-uN  So  just to wrap up the description of the rest of the procedure  from the SVD numerical linear algebra routine we get these matrices u  s  and d  we're going to use the first K columns of this matrix to get u1-uK  Now the other thing we need to is take my original data set  X which is an RN And find a lower dimensional representation Z  which is a R K for this data  So the way we're going to do that is take the first K Columns of the U matrix  Construct this matrix  Stack up U1  U2 and so on up to U K in columns  It's really basically taking  you know  this part of the matrix  the first K columns of this matrix  And so this is going to be an N by K matrix  I'm going to give this matrix a name  I'm going to call this matrix U  subscript \""reduce \"" sort of a reduced version of the U matrix maybe  I'm going to use it to reduce the dimension of my data  And the way I'm going to compute Z is going to let Z be equal to this U reduce matrix transpose times X  Or alternatively  you know  to write down what this transpose means  When I take this transpose of this U matrix  what I'm going to end up with is these vectors now in rows  I have U1 transpose down to UK transpose  Then take that times X  and that's how I get my vector Z  Just to make sure that these dimensions make sense  this matrix here is going to be k by n and x here is going to be n by 1 and so the product here will be k by 1  And so z is k dimensional  is a k dimensional vector  which is exactly what we wanted  And of course these x's here right  can be Examples in our training set can be examples in our cross validation set  can be examples in our test set  and for example if you know  I wanted to take training example i  I can write this as xi XI and that's what will give me ZI over there  So  to summarize  here's the PCA algorithm on one slide  After mean normalization  to ensure that every feature is zero mean and optional feature scaling whichYou really should do feature scaling if your features take on very different ranges of values  After this pre-processing we compute the carrier matrix Sigma like so by the way if your data is given as a matrix like hits if you have your data Given in rows like this  If you have a matrix X which is your time trading sets written in rows where x1 transpose down to x1 transpose  this covariance matrix sigma actually has a nice vectorizing implementation  You can implement in octave  you can even run sigma equals 1 over m  times x  which is this matrix up here  transpose times x and this simple expression  that's the vectorize implementation of how to compute the matrix sigma  I'm not going to prove that today  This is the correct vectorization whether you want  you can either numerically test this on yourself by trying out an octave and making sure that both this and this implementations give the same answers or you Can try to prove it yourself mathematically  Either way but this is the correct vectorizing implementation  without compusingnext we can apply the SVD routine to get u  s  and d  And then we grab the first k columns of the u matrix you reduce and finally this defines how we go from a feature vector x to this reduce dimension representation z  And similar to k Means if you're apply PCA  they way you'd apply this is with vectors X and RN  So  this is not done with X-0 1  So that was the PCA algorithm  One thing I didn't do is give a mathematical proof that this There it actually give the projection of the data onto the K dimensional subspace onto the K dimensional surface that actually minimizes the square projection error Proof of that is beyond the scope of this course  Fortunately the PCA algorithm can be implemented in not too many lines of code  and if you implement this in octave or algorithm  you actually get a very effective dimensionality reduction algorithm  So  that was the PCA algorithm  One thing I didn't do was give a mathematical proof that the U1 and U2 and so on and the Z and so on you get out of this procedure is really the choices that would minimize these squared projection error  Right  remember we said What PCA tries to do is try to find a surface or line onto which to project the data so as to minimize to square projection error  So I didn't prove that this that  and the mathematical proof of that is beyond the scope of this course  But fortunately the PCA algorithm can be implemented in not too many lines of octave code  And if you implement this  this is actually what will work  or this will work well  and if you implement this algorithm  you get a very effective dimensionality reduction algorithm  That does do the right thing of minimizing this square projection error "
7aQgQGeZ_qo,Reconstruction from Compressed Representation  In some of the earlier videos  I was talking about PCA as a compression algorithm where you may have say  1 000-dimensional data and compress it to 100-dimensional feature vector  Or have three-dimensional data and compress it to a two-dimensional representation  So  if this is a compression algorithm  there should be a way to go back from this compressed representation back to an approximation of your original high-dimensional data  So given zi  which may be 100-dimensional  how do you go back to your original representation  xi which was maybe a 1000-dimensional  In this video  I'd like to describe how to do that  In the PCA algorithm  we may have an example like this  so maybe that's my example x1  and maybe that's my example x2  And what we do is we take these examples  and we project them onto this one dimensional surface  And then now we need to use a real number  say z1  to specify the location of these points after they've been projected onto this one dimensional surface  So  given the point z1  how can we go back to this original two dimensional space? In particular  given the point z  which is R  can we map this back to some approximate representation x and R2 of whatever the original value of the data was? So whereas z equals U reduce transpose x  if you want to go in the opposite direction  the equation for that is  we're going to write x approx equals U reduce  times z  And again  just to check the dimensions  here U reduce is going to be an n by k dimensional vector  z is going to be k by one dimensional vector  So you multiply these out that's going to be n by one  so x approx is going to be an n dimensional vector  And so the intent of PCA  that is if the square projection error is not too big  is that this x approx will be close to whatever was the original value of x that you have used to derive z in the first place  To show a picture of what this looks like  this is what it looks like  What you get back of this procedure are points that lie on the projection of that  onto the green line  So to take our early example  if we started off with this value of x1  and we got this value of z1  if you plug z1 through this formula to get x1 approx  then this point here  that would be x1 approx  which is going to be in R2  And similarly  if you do the same procedure  this would be x2 approx  And that's a pretty decent approximation to the original data  So that's how you go back from your low dimensional representation z  back to an uncompressed representation of the data  We get back an approximation to your original data x  And we also call this process reconstruction of the original data where we think of trying to reconstruct the original value of x from the compressed representation  So  given an unlabeled data set  you now know how to apply PCA and take your high dimensional features x and map that to this lower-dimensional representation z  And from this video hopefully you now also know how to take these low-representation z and map it back up to an approximation of your original high-dimensional data  Now that you know how to implement and apply PCA  what I'd like to do next is talk about some of the mechanics of how to actually use PCA well  And in particular in the next video  I'd like to talk about how to choose k  which is how to choose the dimension of the reduced representation vector z 
Y3z_ywZlPkg,"Choosing the Number of Principal Components  In the PCA algorithm we take N dimensional features and reduce them to some K dimensional feature representation  This number K is a parameter of the PCA algorithm  This number K is also called the number of principle components or the number of principle components that we've retained  And in this video I'd like to give you some guidelines  tell you about how people tend to think about how to choose this parameter K for PCA  In order to choose k  that is to choose the number of principal components  here are a couple of useful concepts  What PCA tries to do is it tries to minimize the average squared projection error  So it tries to minimize this quantity  which I'm writing down  which is the difference between the original data X and the projected version  X-approx-i  which was defined last video  so it tries to minimize the squared distance between x and it's projection onto that lower dimensional surface  So that's the average square projection error  Also let me define the total variation in the data to be the average length squared of these examples Xi so the total variation in the data is the average of my training sets of the length of each of my training examples  And this one says  \""On average  how far are my training examples from the vector  from just being all zeros?\"" How far is  how far on average are my training examples from the origin? When we're trying to choose k  a pretty common rule of thumb for choosing k is to choose the smaller values so that the ratio between these is less than 0 01  So in other words  a pretty common way to think about how we choose k is we want the average squared projection error  That is the average distance between x and it's projections divided by the total variation of the data  That is how much the data varies  We want this ratio to be less than  let's say  0 01  Or to be less than 1%  which is another way of thinking about it  And the way most people think about choosing K is rather than choosing K directly the way most people talk about it is as what this number is  whether it is 0 01 or some other number  And if it is 0 01  another way to say this to use the language of PCA is that 99% of the variance is retained  I don't really want to  don't worry about what this phrase really means technically but this phrase \""99% of variance is retained\"" just means that this quantity on the left is less than 0 01  And so  if you are using PCA and if you want to tell someone  you know  how many principle components you've retained it would be more common to say well  I chose k so that 99% of the variance was retained  And that's kind of a useful thing to know  it means that you know  the average squared projection error divided by the total variation that was at most 1%  That's kind of an insightful thing to think about  whereas if you tell someone that  \""Well I had to 100 principle components\"" or \""k was equal to 100 in a thousand dimensional data\"" it's a little hard for people to interpret that  So this number 0 01 is what people often use  Other common values is 0 05  and so this would be 5%  and if you do that then you go and say well 95% of the variance is retained and  you know other numbers maybe 90% of the variance is retained  maybe as low as 85%  So 90% would correspond to say 0 10  kinda 10%  And so range of values from  you know  90  95  99  maybe as low as 85% of the variables contained would be a fairly typical range in values  Maybe 95 to 99 is really the most common range of values that people use  For many data sets you'd be surprised  in order to retain 99% of the variance  you can often reduce the dimension of the data significantly and still retain most of the variance  Because for most real life data says many features are just highly correlated  and so it turns out to be possible to compress the data a lot and still retain you know 99% of the variance or 95% of the variance  So how do you implement this? Well  here's one algorithm that you might use  You may start off  if you want to choose the value of k  we might start off with k equals 1  And then we run through PCA  You know  so we compute  you reduce  compute z1  z2  up to zm  Compute all of those x1 approx and so on up to xm approx and then we check if 99% of the variance is retained  Then we're good and we use k equals 1  But if it isn't then what we'll do we'll next try K equals 2  And then we'll again run through this entire procedure and check  you know is this expression satisfied  Is this less than 0 01  And if not then we do this again  Let's try k equals 3  then try k equals 4  and so on until maybe we get up to k equals 17 and we find 99% of the data have is retained and then we use k equals 17  right? That is one way to choose the smallest value of k  so that and 99% of the variance is retained  But as you can imagine  this procedure seems horribly inefficient we're trying k equals one  k equals two  we're doing all these calculations  Fortunately when you implement PCA it actually  in this step  it actually gives us a quantity that makes it much easier to compute these things as well  Specifically when you're calling SVD to get these matrices u  s  and d  when you're calling usvd on the covariance matrix sigma  it also gives us back this matrix S and what S is  is going to be a square matrix an N by N matrix in fact  that is diagonal  So is diagonal entries s one one  s two two  s three three down to s n n are going to be the only non-zero elements of this matrix  and everything off the diagonals is going to be zero  Okay? So those big O's that I'm drawing  by that what I mean is that everything off the diagonal of this matrix all of those entries there are going to be zeros  And so  what is possible to show  and I won't prove this here  and it turns out that for a given value of k  this quantity over here can be computed much more simply  And that quantity can be computed as one minus sum from i equals 1 through K of Sii divided by sum from I equals 1 through N of Sii  So just to say that it words  or just to take another view of how to explain that  if K equals 3 let's say  What we're going to do to compute the numerator is sum from one-- I equals 1 through 3 of of Sii  so just compute the sum of these first three elements  So that's the numerator  And then for the denominator  well that's the sum of all of these diagonal entries  And one minus the ratio of that  that gives me this quantity over here  that I've circled in blue  And so  what we can do is just test if this is less than or equal to 0 01  Or equivalently  we can test if the sum from i equals 1 through k  s-i-i divided by sum from i equals 1 through n  s-i-i if this is greater than or equal to 4 99  if you want to be sure that 99% of the variance is retained  And so what you can do is just slowly increase k  set k equals one  set k equals two  set k equals three and so on  and just test this quantity to see what is the smallest value of k that ensures that 99% of the variance is retained  And if you do this  then you need to call the SVD function only once  Because that gives you the S matrix and once you have the S matrix  you can then just keep on doing this calculation by increasing the value of K in the numerator and so you don't need keep to calling SVD over and over again to test out the different values of K  So this procedure is much more efficient  and this can allow you to select the value of K without needing to run PCA from scratch over and over  You just run SVD once  this gives you all of these diagonal numbers  all of these numbers S11  S22 down to SNN  and then you can just you know  vary K in this expression to find the smallest value of K  so that 99% of the variance is retained  So to summarize  the way that I often use  the way that I often choose K when I am using PCA for compression is I would call SVD once in the covariance matrix  and then I would use this formula and pick the smallest value of K for which this expression is satisfied  And by the way  even if you were to pick some different value of K  even if you were to pick the value of K manually  you know maybe you have a thousand dimensional data and I just want to choose K equals one hundred  Then  if you want to explain to others what you just did  a good way to explain the performance of your implementation of PCA to them  is actually to take this quantity and compute what this is  and that will tell you what was the percentage of variance retained  And if you report that number  then  you know  people that are familiar with PCA  and people can use this to get a good understanding of how well your hundred dimensional representation is approximating your original data set  because there's 99% of variance retained  That's really a measure of your square of construction error  that ratio being 0 01  just gives people a good intuitive sense of whether your implementation of PCA is finding a good approximation of your original data set  So hopefully  that gives you an efficient procedure for choosing the number K  For choosing what dimension to reduce your data to  and if you apply PCA to very high dimensional data sets  you know  to like a thousand dimensional data  very often  just because data sets tend to have highly correlated features  this is just a property of most of the data sets you see  you often find that PCA will be able to retain ninety nine per cent of the variance or say  ninety five ninety nine  some high fraction of the variance  even while compressing the data by a very large factor "
qN3g4f2_Xas,Advice for Applying PCA  In an earlier video  I had said that PCA can be sometimes used to speed up the running time of a learning algorithm  In this video  I'd like to explain how to actually do that  and also say some  just try to give some advice about how to apply PCA  Here's how you can use PCA to speed up a learning algorithm  and this supervised learning algorithm speed up is actually the most common use that I personally make of PCA  Let's say you have a supervised learning problem  note this is a supervised learning problem with inputs X and labels Y  and let's say that your examples xi are very high dimensional  So  lets say that your examples  xi are 10 000 dimensional feature vectors  One example of that  would be  if you were doing some computer vision problem  where you have a 100x100 images  and so if you have 100x100  that's 10000 pixels  and so if xi are  you know  feature vectors that contain your 10000 pixel intensity values  then you have 10000 dimensional feature vectors  So with very high-dimensional feature vectors like this  running a learning algorithm can be slow  right? Just  if you feed 10 000 dimensional feature vectors into logistic regression  or a new network  or support vector machine or what have you  just because that's a lot of data  that's 10 000 numbers  it can make your learning algorithm run more slowly  Fortunately with PCA we'll be able to reduce the dimension of this data and so make our algorithms run more efficiently  Here's how you do that  We are going first check our labeled training set and extract just the inputs  we're just going to extract the X's and temporarily put aside the Y's  So this will now give us an unlabelled training set x1 through xm which are maybe there's a ten thousand dimensional data  ten thousand dimensional examples we have  So just extract the input vectors x1 through xm  Then we're going to apply PCA and this will give me a reduced dimension representation of the data  so instead of 10 000 dimensional feature vectors I now have maybe one thousand dimensional feature vectors  So that's like a 10x savings  So this gives me  if you will  a new training set  So whereas previously I might have had an example x1  y1  my first training input  is now represented by z1  And so we'll have a new sort of training example  which is Z1 paired with y1  And similarly Z2  Y2  and so on  up to ZM  YM  Because my training examples are now represented with this much lower dimensional representation Z1  Z2  up to ZM  Finally  I can take this reduced dimension training set and feed it to a learning algorithm maybe a neural network  maybe logistic regression  and I can learn the hypothesis H  that takes this input  these low-dimensional representations Z and tries to make predictions  So if I were using logistic regression for example  I would train a hypothesis that outputs  you know  one over one plus E to the negative-theta transpose Z  that takes this input to one these z vectors  and tries to make a prediction  And finally  if you have a new example  maybe a new test example X  What you do is you would take your test example x  map it through the same mapping that was found by PCA to get you your corresponding z  And that z then gets fed to this hypothesis  and this hypothesis then makes a prediction on your input x  One final note  what PCA does is it defines a mapping from x to z and this mapping from x to z should be defined by running PCA only on the training sets  And in particular  this mapping that PCA is learning  right  this mapping  what that does is it computes the set of parameters  That's the feature scaling and mean normalization  And there's also computing this matrix U reduced  But all of these things that U reduce  that's like a parameter that is learned by PCA and we should be fitting our parameters only to our training sets and not to our cross validation or test sets and so these things the U reduced so on  that should be obtained by running PCA only on your training set  And then having found U reduced  or having found the parameters for feature scaling where the mean normalization and scaling the scale that you divide the features by to get them on to comparable scales  Having found all those parameters on the training set  you can then apply the same mapping to other examples that may be In your cross-validation sets or in your test sets  OK? Just to summarize  when you're running PCA  run your PCA only on the training set portion of the data not the cross-validation set or the test set portion of your data  And that defines the mapping from x to z and you can then apply that mapping to your cross-validation set and your test set and by the way in this example I talked about reducing the data from ten thousand dimensional to one thousand dimensional  this is actually not that unrealistic  For many problems we actually reduce the dimensional data  You know by 5x maybe by 10x and still retain most of the variance and we can do this barely effecting the performance  in terms of classification accuracy  let's say  barely affecting the classification accuracy of the learning algorithm  And by working with lower dimensional data our learning algorithm can often run much much faster  To summarize  we've so far talked about the following applications of PCA  First is the compression application where we might do so to reduce the memory or the disk space needed to store data and we just talked about how to use this to speed up a learning algorithm  In these applications  in order to choose K  often we'll do so according to  figuring out what is the percentage of variance retained  and so for this learning algorithm  speed up application often will retain 99% of the variance  That would be a very typical choice for how to choose k  So that's how you choose k for these compression applications  Whereas for visualization applications while usually we know how to plot only two dimensional data or three dimensional data  and so for visualization applications  we'll usually choose k equals 2 or k equals 3  because we can plot only 2D and 3D data sets  So that summarizes the main applications of PCA  as well as how to choose the value of k for these different applications  I should mention that there is often one frequent misuse of PCA and you sometimes hear about others doing this hopefully not too often  I just want to mention this so that you know not to do it  And there is one bad use of PCA  which iss to try to use it to prevent over-fitting  Here's the reasoning  This is not a great way to use PCA  but here's the reasoning behind this method  which is you know if we have Xi  then maybe we'll have n features  but if we compress the data  and use Zi instead and that reduces the number of features to k  which could be much lower dimensional  And so if we have a much smaller number of features  if k is 1 000 and n is 10 000  then if we have only 1 000 dimensional data  maybe we're less likely to over-fit than if we were using 10 000-dimensional data with like a thousand features  So some people think of PCA as a way to prevent over-fitting  But just to emphasize this is a bad application of PCA and I do not recommend doing this  And it's not that this method works badly  If you want to use this method to reduce the dimensional data  to try to prevent over-fitting  it might actually work OK  But this just is not a good way to address over-fitting and instead  if you're worried about over-fitting  there is a much better way to address it  to use regularization instead of using PCA to reduce the dimension of the data  And the reason is  if you think about how PCA works  it does not use the labels y  You are just looking at your inputs xi  and you're using that to find a lower-dimensional approximation to your data  So what PCA does  is it throws away some information  It throws away or reduces the dimension of your data without knowing what the values of y is  so this is probably okay using PCA this way is probably okay if  say 99 percent of the variance is retained  if you're keeping most of the variance  but it might also throw away some valuable information  And it turns out that if you're retaining 99% of the variance or 95% of the variance or whatever  it turns out that just using regularization will often give you at least as good a method for preventing over-fitting and regularization will often just work better  because when you are applying linear regression or logistic regression or some other method with regularization  well  this minimization problem actually knows what the values of y are  and so is less likely to throw away some valuable information  whereas PCA doesn't make use of the labels and is more likely to throw away valuable information  So  to summarize  it is a good use of PCA  if your main motivation to speed up your learning algorithm  but using PCA to prevent over-fitting  that is not a good use of PCA  and using regularization instead is really what many people would recommend doing instead  Finally  one last misuse of PCA  And so I should say PCA is a very useful algorithm  I often use it for the compression on the visualization purposes  But  what I sometimes see  is also people sometimes use PCA where it shouldn't be  So  here's a pretty common thing that I see  which is if someone is designing a machine-learning system  they may write down the plan like this: let's design a learning system  Get a training set and then  you know  what I'm going to do is run PCA  then train logistic regression and then test on my test data  So often at the very start of a project  someone will just write out a project plan than says lets do these four steps with PCA inside  Before writing down a project plan the incorporates PCA like this  one very good question to ask is  well  what if we were to just do the whole without using PCA  And often people do not consider this step before coming up with a complicated project plan and implementing PCA and so on  And sometime  and so specifically  what I often advise people is  before you implement PCA  I would first suggest that  you know  do whatever it is  take whatever it is you want to do and first consider doing it with your original raw data xi  and only if that doesn't do what you want  then implement PCA before using Zi  So  before using PCA you know  instead of reducing the dimension of the data  I would consider well  let's ditch this PCA step  and I would consider  let's just train my learning algorithm on my original data  Let's just use my original raw inputs xi  and I would recommend  instead of putting PCA into the algorithm  just try doing whatever it is you're doing with the xi first  And only if you have a reason to believe that doesn't work  so that only if your learning algorithm ends up running too slowly  or only if the memory requirement or the disk space requirement is too large  so you want to compress your representation  but if only using the xi doesn't work  only if you have evidence or strong reason to believe that using the xi won't work  then implement PCA and consider using the compressed representation  Because what I do see  is sometimes people start off with a project plan that incorporates PCA inside  and sometimes they  whatever they're doing will work just fine  even with out using PCA instead  So  just consider that as an alternative as well  before you go to spend a lot of time to get PCA in  figure out what k is and so on  So  that's it for PCA  Despite these last sets of comments  PCA is an incredibly useful algorithm  when you use it for the appropriate applications and I've actually used PCA pretty often and for me  I use it mostly to speed up the running time of my learning algorithms  But I think  just as common an application of PCA  is to use it to compress data  to reduce the memory or disk space requirements  or to use it to visualize data  And PCA is one of the most commonly used and one of the most powerful unsupervised learning algorithms  And with what you've learned in these videos  I think hopefully you'll be able to implement PCA and use them through all of these purposes as well 
nRu270DVdTY,Problem Motivation  In this next set of videos  I'd like to tell you about a problem called Anomaly Detection  This is a reasonably commonly use you type machine learning  And one of the interesting aspects is that it's mainly for unsupervised problem  that there's some aspects of it that are also very similar to sort of the supervised learning problem  So  what is anomaly detection? To explain it  Let me use the motivating example of  Imagine that you're a manufacturer of aircraft engines  and let's say that as your aircraft engines roll off the assembly line  you're doing  you know  QA or quality assurance testing  and as part of that testing you measure features of your aircraft engine  like maybe  you measure the heat generated  things like the vibrations and so on  I share some friends that worked on this problem a long time ago  and these were actually the sorts of features that they were collecting off actual aircraft engines so you now have a data set of X1 through Xm  if you have manufactured m aircraft engines  and if you plot your data  maybe it looks like this  So  each point here  each cross here as one of your unlabeled examples  So  the anomaly detection problem is the following  Let's say that on  you know  the next day  you have a new aircraft engine that rolls off the assembly line and your new aircraft engine has some set of features x-test  What the anomaly detection problem is  we want to know if this aircraft engine is anomalous in any way  in other words  we want to know if  maybe  this engine should undergo further testing because  or if it looks like an okay engine  and so it's okay to just ship it to a customer without further testing  So  if your new aircraft engine looks like a point over there  well  you know  that looks a lot like the aircraft engines we've seen before  and so maybe we'll say that it looks okay  Whereas  if your new aircraft engine  if x-test  you know  were a point that were out here  so that if X1 and X2 are the features of this new example  If x-tests were all the way out there  then we would call that an anomaly  and maybe send that aircraft engine for further testing before we ship it to a customer  since it looks very different than the rest of the aircraft engines we've seen before  More formally in the anomaly detection problem  we're give some data sets  x1 through Xm of examples  and we usually assume that these end examples are normal or non-anomalous examples  and we want an algorithm to tell us if some new example x-test is anomalous  The approach that we're going to take is that given this training set  given the unlabeled training set  we're going to build a model for p of x  In other words  we're going to build a model for the probability of x  where x are these features of  say  aircraft engines  And so  having built a model of the probability of x we're then going to say that for the new aircraft engine  if p of x-test is less than some epsilon then we flag this as an anomaly  So we see a new engine that  you know  has very low probability under a model p of x that we estimate from the data  then we flag this anomaly  whereas if p of x-test is  say  greater than or equal to some small threshold  Then we say that  you know  okay  it looks okay  And so  given the training set  like that plotted here  if you build a model  hopefully you will find that aircraft engines  or hopefully the model p of x will say that points that lie  you know  somewhere in the middle  that's pretty high probability  whereas points a little bit further out have lower probability  Points that are even further out have somewhat lower probability  and the point that's way out here  the point that's way out there  would be an anomaly  Whereas the point that's way in there  right in the middle  this would be okay because p of x right in the middle of that would be very high cause we've seen a lot of points in that region  Here are some examples of applications of anomaly detection  Perhaps the most common application of anomaly detection is actually for detection if you have many users  and if each of your users take different activities  you know maybe on your website or in the physical plant or something  you can compute features of the different users activities  And what you can do is build a model to say  you know  what is the probability of different users behaving different ways  What is the probability of a particular vector of features of a users behavior so you know examples of features of a users activity may be on the website it'd be things like  maybe x1 is how often does this user log in  x2  you know  maybe the number of what pages visited  or the number of transactions  maybe x3 is  you know  the number of posts of the users on the forum  feature x4 could be what is the typing speed of the user and some websites can actually track that was the typing speed of this user in characters per second  And so you can model p of x based on this sort of data  And finally having your model p of x  you can try to identify users that are behaving very strangely on your website by checking which ones have probably effects less than epsilon and maybe send the profiles of those users for further review  Or demand additional identification from those users  or some such to guard against you know  strange behavior or fraudulent behavior on your website  This sort of technique will tend of flag the users that are behaving unusually  not just users that maybe behaving fraudulently  So not just constantly having stolen or users that are trying to do funny things  or just find unusual users  But this is actually the technique that is used by many online websites that sell things to try identify users behaving strangely that might be indicative of either fraudulent behavior or of computer accounts that have been stolen  Another example of anomaly detection is manufacturing  So  already talked about the aircraft engine thing where you can find unusual  say  aircraft engines and send those for further review  A third application would be monitoring computers in a data center  I actually have some friends who work on this too  So if you have a lot of machines in a computer cluster or in a data center  we can do things like compute features at each machine  So maybe some features capturing you know  how much memory used  number of disc accesses  CPU load  As well as more complex features like what is the CPU load on this machine divided by the amount of network traffic on this machine? Then given the dataset of how your computers in your data center usually behave  you can model the probability of x  so you can model the probability of these machines having different amounts of memory use or probability of these machines having different numbers of disc accesses or different CPU loads and so on  And if you ever have a machine whose probability of x  p of x  is very small then you know that machine is behaving unusually and maybe that machine is about to go down  and you can flag that for review by a system administrator  And this is actually being used today by various data centers to watch out for unusual things happening on their machines  So  that's anomaly detection  In the next video  I'll talk a bit about the Gaussian distribution and review properties of the Gaussian probability distribution  and in videos after that  we will apply it to develop an anomaly detection algorithm 
H8KqNxET-XA,Gaussian Distribution  In this video  I'd like to talk about the Gaussian distribution which is also called the normal distribution  In case you're already intimately familiar with the Gaussian distribution  it's probably okay to skip this video  but if you're not sure or if it has been a while since you've worked with the Gaussian distribution or normal distribution then please do watch this video all the way to the end  And in the video after this we'll start applying the Gaussian distribution to developing an anomaly detection algorithm  Let's say x is a row value's random variable  so x is a row number  If the probability distribution of x is Gaussian with mean mu and variance sigma squared  Then  we'll write this as x  the random variable  Tilde  this little tilde  this is distributed as  And then to denote a Gaussian distribution  sometimes I'm going to write script N parentheses mu comma sigma script  So this script N stands for normal since Gaussian and normal they mean the thing are synonyms  And the Gaussian distribution is parametarized by two parameters  by a mean parameter which we denote mu and a variance parameter which we denote via sigma squared  If we plot the Gaussian distribution or Gaussian probability density  It'll look like the bell shaped curve which you may have seen before  And so this bell shaped curve is paramafied by those two parameters  mu and sequel  And the location of the center of this bell shaped curve is the mean mu  And the width of this bell shaped curve  roughly that  is this parameter  sigma  is also called one standard deviation  and so this specifies the probability of x taking on different values  So  x taking on values here in the middle here it's pretty high  since the Gaussian density here is pretty high  whereas x taking on values further  and further away will be diminishing in probability  Finally just for completeness let me write out the formula for the Gaussian distribution  So the probability of x  and I'll sometimes write this as the p (x) when we write this as P ( x   mu  sigma squared)  and so this denotes that the probability of X is parameterized by the two parameters mu and sigma squared  And the formula for the Gaussian density is this 1/ root 2 pi  sigma e (-(x-mu/g) squared/2 sigma squared  So there's no need to memorize this formula  This is just the formula for the bell-shaped curve over here on the left  There's no need to memorize it  and if you ever need to use this  you can always look this up  And so that figure on the left  that is what you get if you take a fixed value of mu and take a fixed value of sigma  and you plot P(x) so this curve here  This is really p(x) plotted as a function of X for a fixed value of Mu and of sigma squared  And by the way sometimes it's easier to think in terms of sigma squared that's called the variance  And sometimes is easier to think in terms of sigma  So sigma is called the standard deviation  and so it specifies the width of this Gaussian probability density  where as the square sigma  or sigma squared  is called the variance  Let's look at some examples of what the Gaussian distribution looks like  If mu equals zero  sigma equals one  Then we have a Gaussian distribution that's centered around zero  because that's mu and the width of this Gaussian  so that's one standard deviation is sigma over there  Let's look at some examples of Gaussians  If mu is equal to zero and sigma equals one  then that corresponds to a Gaussian distribution that is centered at zero  since mu is zero  and the width of this Gaussian is is controlled by sigma by that variance parameter sigma  Here's another example  That same mu is equal to 0 and sigma is equal to  5 so the standard deviation is  5 and the variance sigma squared would therefore be the square of 0 5 would be 0 25 and in that case the Gaussian distribution  the Gaussian probability density goes like this  Is also sent as zero  But now the width of this is much smaller because the smaller the area is  the width of this Gaussian density is roughly half as wide  But because this is a probability distribution  the area under the curve  that's the shaded area there  That area must integrate to one this is a property of probability distributing  So this is a much taller Gaussian density because this half is Y but half the standard deviation but it twice as tall  Another example is sigma is equal to 2 then you get a much fatter a much wider Gaussian density and so here the sigma parameter controls that Gaussian distribution has a wider width  And once again  the area under the curve  that is the shaded area  will always integrate to one  that's the property of probability distributions and because it's wider it's also half as tall in order to still integrate to the same thing  And finally one last example would be if we now change the mu parameters as well  Then instead of being centered at 0 we now have a Gaussian distribution that's centered at 3 because this shifts over the entire Gaussian distribution  Next  let's talk about the Parameter estimation problem  So what's the parameter estimation problem? Let's say we have a dataset of m examples so exponents x m and lets say each of this example is a row number  Here in the figure I've plotted an example of the dataset so the horizontal axis is the x axis and either will have a range of examples of x  and I've just plotted them on this figure here  And the parameter estimation problem is  let's say I suspect that these examples came from a Gaussian distribution  So let's say I suspect that each of my examples  x i  was distributed  That's what this tilde thing means  Let's not suspect that each of these examples were distributed according to a normal distribution  or Gaussian distribution  with some parameter mu and some parameter sigma square  But I don't know what the values of these parameters are  The problem of parameter estimation is  given my data set  I want to try to figure out  well I want to estimate what are the values of mu and sigma squared  So if you're given a data set like this  it looks like maybe if I estimate what Gaussian distribution the data came from  maybe that might be roughly the Gaussian distribution it came from  With mu being the center of the distribution  sigma standing for the deviation controlling the width of this Gaussian distribution  Seems like a reasonable fit to the data  Because  you know  looks like the data has a very high probability of being in the central region  and a low probability of being further out  even though probability of being further out  and so on  So maybe this is a reasonable estimate of mu and sigma squared  That is  if it corresponds to a Gaussian distribution function that looks like this  So what I'm going to do is just write out the formula the standard formulas for estimating the parameters Mu and sigma squared  Our estimate or the way we're going to estimate mu is going to be just the average of my example  So mu is the mean parameter  Just take my training set  take my m examples and average them  And that just means the center of this distribution  How about sigma squared? Well  the variance  I'll just write out the standard formula again  I'm going to estimate as sum over one through m of x i minus mu squared  And so this mu here is actually the mu that I compute over here using this formula  And what the variance is  or one interpretation of the variance is that if you look at this term  that's the square difference between the value I got in my example minus the mean  Minus the center  minus the mean of the distribution  And so in the variance I'm gonna estimate as just the average of the square differences between my examples  minus the mean  And as a side comment  only for those of you that are experts in statistics  If you're an expert in statistics  and if you've heard of maximum likelihood estimation  then these parameters  these estimates  are actually the maximum likelihood estimates of the primes of mu and sigma squared but if you haven't heard of that before don't worry about it  all you need to know is that these are the two standard formulas for how to figure out what are mu and Sigma squared given the data set  Finally one last side comment again only for those of you that have maybe taken the statistics class before but if you've taken statistics This class before  Some of you may have seen the formula here where this is M-1 instead of M so this first term becomes 1/M-1 instead of 1/M  In machine learning people tend to learn 1/M formula but in practice whether it is 1/M or 1/M-1 it makes essentially no difference assuming M is reasonably large  a reasonably large training set size  So just in case you've seen this other version before  In either version it works just about equally well but in machine learning most people tend to use 1/M in this formula And the two versions have slightly different theoretical properties like these are different math properties  Bit of practice it really makes makes very little difference  if any  So  hopefully you now have a good sense of what the Gaussian distribution looks like  as well as how to estimate the parameters mu and sigma squared of Gaussian distribution if you're given a training set  that is if you're given a set of data that you suspect comes from a Gaussian distribution with unknown parameters  mu and sigma squared  In the next video  we'll start to take this and apply it to develop an anomaly detection algorithm 
KTn2LVhQmf0,Algorithm  In the last video  we talked about the Gaussian distribution  In this video lets apply that to develop an anomaly detection algorithm  Let's say that we have an unlabeled training set of M examples  and each of these examples is going to be a feature in Rn so your training set could be  feature vectors from the last M aircraft engines being manufactured  Or it could be features from m users or something else  The way we are going to address anomaly detection  is we are going to model p of x from the data sets  We're going to try to figure out what are high probability features  what are lower probability types of features  So  x is a vector and what we are going to do is model p of x  as probability of x1  that is of the first component of x  times the probability of x2  that is the probability of the second feature  times the probability of the third feature  and so on up to the probability of the final feature of Xn  Now I'm leaving space here cause I'll fill in something in a minute  So  how do we model each of these terms  p of X1  p of X2  and so on  What we're going to do  is assume that the feature  X1  is distributed according to a Gaussian distribution  with some mean  which you want to write as mu1 and some variance  which I'm going to write as sigma squared 1  and so p of X1 is going to be a Gaussian probability distribution  with mean mu1 and variance sigma squared 1  And similarly I'm going to assume that X2 is distributed  Gaussian  that's what this little tilda stands for  that means distributed Gaussian with mean mu2 and Sigma squared 2  so it's distributed according to a different Gaussian  which has a different set of parameters  mu2 sigma square 2  And similarly  you know  X3 is yet another Gaussian  so this can have a different mean and a different standard deviation than the other features  and so on  up to XN  And so that's my model  Just as a side comment for those of you that are experts in statistics  it turns out that this equation that I just wrote out actually corresponds to an independence assumption on the values of the features x1 through xn  But in practice it turns out that the algorithm of this fragment  it works just fine  whether or not these features are anywhere close to independent and even if independence assumption doesn't hold true this algorithm works just fine  But in case you don't know those terms I just used independence assumptions and so on  don't worry about it  You'll be able to understand it and implement this algorithm just fine and that comment was really meant only for the experts in statistics  Finally  in order to wrap this up  let me take this expression and write it a little bit more compactly  So  we're going to write this is a product from J equals one through N  of P of XJ parameterized by mu j comma sigma squared j  So this funny symbol here  there is capital Greek alphabet pi  that funny symbol there corresponds to taking the product of a set of values  And so  you're familiar with the summation notation  so the sum from i equals one through n  of i  This means 1 + 2 + 3 plus dot dot dot  up to n  Where as this funny symbol here  this product symbol  right product from i equals 1 through n of i  Then this means that  it's just like summation except that we're now multiplying  This becomes 1 times 2 times 3 times up to N  And so using this product notation  this product from j equals 1 through n of this expression  It's just more compact  it's just shorter way for writing out this product of of all of these terms up there  Since we're are taking these p of x j given mu j comma sigma squared j terms and multiplying them together  And  by the way the problem of estimating this distribution p of x  they're sometimes called the problem of density estimation  Hence the title of the slide  So putting everything together  here is our anomaly detection algorithm  The first step is to choose features  or come up with features xi that we think might be indicative of anomalous examples  So what I mean by that  is  try to come up with features  so that when there's an unusual user in your system that may be doing fraudulent things  or when the aircraft engine examples  you know there's something funny  something strange about one of the aircraft engines  Choose features X I  that you think might take on unusually large values  or unusually small values  for what an anomalous example might look like  But more generally  just try to choose features that describe general properties of the things that you're collecting data on  Next  given a training set  of M  unlabled examples  X1 through X M  we then fit the parameters  mu 1 through mu n  and sigma squared 1 through sigma squared n  and so these were the formulas similar to the formulas we have in the previous video  that we're going to use the estimate each of these parameters  and just to give some interpretation  mu J  that's my average value of the j feature  Mu j goes in this term p of xj  which is parametrized by mu J and sigma squared J  And so this says for the mu J just take the mean over my training set of the values of the j feature  And  just to mention  that you do this  you compute these formulas for j equals one through n  So use these formulas to estimate mu 1  to estimate mu 2  and so on up to mu n  and similarly for sigma squared  and it's also possible to come up with vectorized versions of these  So if you think of mu as a vector  so mu if is a vector there's mu 1  mu 2  down to mu n  then a vectorized version of that set of parameters can be written like so sum from 1 equals one through n xi  So  this formula that I just wrote out estimates this xi as the feature vectors that estimates mu for all the values of n simultaneously  And it's also possible to come up with a vectorized formula for estimating sigma squared j  Finally  when you're given a new example  so when you have a new aircraft engine and you want to know is this aircraft engine anomalous  What we need to do is then compute p of x  what's the probability of this new example? So  p of x is equal to this product  and what you implement  what you compute  is this formula and where over here  this thing here this is just the formula for the Gaussian probability  so you compute this thing  and finally if this probability is very small  then you flag this thing as an anomaly  Here's an example of an application of this method  Let's say we have this data set plotted on the upper left of this slide  if you look at this  well  lets look the feature of x1  If you look at this data set  it looks like on average  the features x1 has a mean of about 5 and the standard deviation  if you only look at just the x1 values of this data set has the standard deviation of maybe 2  So that sigma 1 and looks like x2 the values of the features as measured on the vertical axis  looks like it has an average value of about 3  and a standard deviation of about 1  So if you take this data set and if you estimate mu1  mu2  sigma1  sigma2  this is what you get  And again  I'm writing sigma here  I'm think about standard deviations  but the formula on the previous 5 actually gave the estimates of the squares of theses things  so sigma squared 1 and sigma squared 2  So  just be careful whether you are using sigma 1  sigma 2  or sigma squared 1 or sigma squared 2  So  sigma squared 1 of course would be equal to 4  for example  as the square of 2  And in pictures what p of x1 parametrized by mu1 and sigma squared 1 and p of x2  parametrized by mu 2 and sigma squared 2  that would look like these two distributions over here  And  turns out that if were to plot of p of x  right  which is the product of these two things  you can actually get a surface plot that looks like this  This is a plot of p of x  where the height above of this  where the height of this surface at a particular point  so given a particular x1 x2 values of x2 if x1 equals 2  x equal 2  that's this point  And the height of this 3-D surface here  that's p of x  So p of x  that is the height of this plot  is literally just p of x1 parametrized by mu 1 sigma squared 1  times p of x2 parametrized by mu 2 sigma squared 2  Now  so this is how we fit the parameters to this data  Let's see if we have a couple of new examples  Maybe I have a new example there  Is this an anomaly or not? Or  maybe I have a different example  maybe I have a different second example over there  So  is that an anomaly or not? They way we do that is  we would set some value for Epsilon  let's say I've chosen Epsilon equals 0 02  I'll say later how we choose Epsilon  But let's take this first example  let me call this example X1 test  And let me call the second example X2 test  What we do is  we then compute p of X1 test  so we use this formula to compute it and this looks like a pretty large value  In particular  this is greater than  or greater than or equal to epsilon  And so this is a pretty high probability at least bigger than epsilon  so we'll say that X1 test is not an anomaly  Whereas  if you compute p of X2 test  well that is just a much smaller value  So this is less than epsilon and so we'll say that that is indeed an anomaly  because it is much smaller than that epsilon that we then chose  And in fact  I'd improve it here  What this is really saying is that  you look through the 3d surface plot  It's saying that all the values of x1 and x2 that have a high height above the surface  corresponds to an a non-anomalous example of an OK or normal example  Whereas all the points far out here  all the points out here  all of those points have very low probability  so we are going to flag those points as anomalous  and so it's gonna define some region  that maybe looks like this  so that everything outside this  it flags as anomalous  whereas the things inside this ellipse I just drew  if it considers okay  or non-anomalous  not anomalous examples  And so this example x2 test lies outside that region  and so it has very small probability  and so we consider it an anomalous example  In this video we talked about how to estimate p of x  the probability of x  for the purpose of developing an anomaly detection algorithm  And in this video  we also stepped through an entire process of giving data set  we have  fitting the parameters  doing parameter estimations  We get mu and sigma parameters  and then taking new examples and deciding if the new examples are anomalous or not  In the next few videos we will delve deeper into this algorithm  and talk a bit more about how to actually get this to work well 
DNaJV4wqeWI,Developing and Evaluating an Anomaly Detection System  In the last video  we developed an anomaly detection algorithm  In this video  I like to talk about the process of how to go about developing a specific application of anomaly detection to a problem and in particular this will focus on the problem of how to evaluate an anomaly detection algorithm  In previous videos  we've already talked about the importance of real number evaluation and this captures the idea that when you're trying to develop a learning algorithm for a specific application  you need to often make a lot of choices like  you know  choosing what features to use and then so on  And making decisions about all of these choices is often much easier  and if you have a way to evaluate your learning algorithm that just gives you back a number  So if you're trying to decide  you know  I have an idea for one extra feature  do I include this feature or not  If you can run the algorithm with the feature  and run the algorithm without the feature  and just get back a number that tells you  you know  did it improve or worsen performance to add this feature? Then it gives you a much better way  a much simpler way  with which to decide whether or not to include that feature  So in order to be able to develop an anomaly detection system quickly  it would be a really helpful to have a way of evaluating an anomaly detection system  In order to do this  in order to evaluate an anomaly detection system  we're actually going to assume have some labeled data  So  so far  we'll be treating anomaly detection as an unsupervised learning problem  using unlabeled data  But if you have some labeled data that specifies what are some anomalous examples  and what are some non-anomalous examples  then this is how we actually think of as the standard way of evaluating an anomaly detection algorithm  So taking the aircraft engine example again  Let's say that  you know  we have some label data of just a few anomalous examples of some aircraft engines that were manufactured in the past that turns out to be anomalous  Turned out to be flawed or strange in some way  Let's say we use we also have some non-anomalous examples  so some perfectly okay examples  I'm going to use y equals 0 to denote the normal or the non-anomalous example and y equals 1 to denote the anomalous examples  The process of developing and evaluating an anomaly detection algorithm is as follows  We're going to think of it as a training set and talk about the cross validation in test sets later  but the training set we usually think of this as still the unlabeled training set  And so this is our large collection of normal  non-anomalous or not anomalous examples  And usually we think of this as being as non-anomalous  but it's actually okay even if a few anomalies slip into your unlabeled training set  And next we are going to define a cross validation set and a test set  with which to evaluate a particular anomaly detection algorithm  So  specifically  for both the cross validation test sets we're going to assume that  you know  we can include a few examples in the cross validation set and the test set that contain examples that are known to be anomalous  So the test sets say we have a few examples with y equals 1 that correspond to anomalous aircraft engines  So here's a specific example  Let's say that  altogether  this is the data that we have  We have manufactured 10 000 examples of engines that  as far as we know we're perfectly normal  perfectly good aircraft engines  And again  it turns out to be okay even if a few flawed engine slips into the set of 10 000 is actually okay  but we kind of assumed that the vast majority of these 10 000 examples are  you know  good and normal non-anomalous engines  And let's say that  you know  historically  however long we've been running on manufacturing plant  let's say that we end up getting features  getting 24 to 28 anomalous engines as well  And for a pretty typical application of anomaly detection  you know  the number non-anomalous examples  that is with y equals 1  we may have anywhere from  you know  20 to 50  It would be a pretty typical range of examples  number of examples that we have with y equals 1  And usually we will have a much larger number of good examples  So  given this data set  a fairly typical way to split it into the training set  cross validation set and test set would be as follows  Let's take 10 000 good aircraft engines and put 6 000 of that into the unlabeled training set  So  I'm calling this an unlabeled training set but all of these examples are really ones that correspond to y equals 0  as far as we know  And so  we will use this to fit p of x  right  So  we will use these 6000 engines to fit p of x  which is that p of x one parametrized by Mu 1  sigma squared 1  up to p of Xn parametrized by Mu N sigma squared n  And so it would be these 6 000 examples that we would use to estimate the parameters Mu 1  sigma squared 1  up to Mu N  sigma squared N  And so that's our training set of all  you know  good  or the vast majority of good examples  Next we will take our good aircraft engines and put some number of them in a cross validation set plus some number of them in the test sets  So 6 000 plus 2 000 plus 2 000  that's how we split up our 10 000 good aircraft engines  And then we also have 20 flawed aircraft engines  and we'll take that and maybe split it up  you know  put ten of them in the cross validation set and put ten of them in the test sets  And in the next slide we will talk about how to actually use this to evaluate the anomaly detection algorithm  So what I have just described here is a you know probably the recommend a good way of splitting the labeled and unlabeled example  The good and the flawed aircraft engines  Where we use like a 60  20  20% split for the good engines and we take the flawed engines  and we put them just in the cross validation set  and just in the test set  then we'll see in the next slide why that's the case  Just as an aside  if you look at how people apply anomaly detection algorithms  sometimes you see other peoples' split the data differently as well  So  another alternative  this is really not a recommended alternative  but some people want to take off your 10 000 good engines  maybe put 6000 of them in your training set and then put the same 4000 in the cross validation set and the test set  And so  you know  we like to think of the cross validation set and the test set as being completely different data sets to each other  But you know  in anomaly detection  you know  for sometimes you see people  sort of  use the same set of good engines in the cross validation sets  and the test sets  and sometimes you see people use exactly the same sets of anomalous engines in the cross validation set and the test set  And so  all of these are considered  you know  less good practices and definitely less recommended  Certainly using the same data in the cross validation set and the test set  that is not considered a good machine learning practice  But  sometimes you see people do this too  So  given the training cross validation and test sets  here's how you evaluate or here is how you develop and evaluate an algorithm  First  we take the training sets and we fit the model p of x  So  we fit  you know  all these Gaussians to my m unlabeled examples of aircraft engines  and these  I am calling them unlabeled examples  but these are really examples that we're assuming our goods are the normal aircraft engines  Then imagine that your anomaly detection algorithm is actually making prediction  So  on the cross validation of the test set  given that  say  test example X  think of the algorithm as predicting that y is equal to 1  p of x is less than epsilon  we must be taking zero  if p of x is greater than or equal to epsilon  So  given x  it's trying to predict  what is the label  given y equals 1 corresponding to an anomaly or is it y equals 0 corresponding to a normal example? So given the training  cross validation  and test sets  How do you develop an algorithm? And more specifically  how do you evaluate an anomaly detection algorithm? Well  to this whole  the first step is to take the unlabeled training set  and to fit the model p of x lead training data  So you take this  you know on I'm coming  unlabeled training set  but really  these are examples that we are assuming  vast majority of which are normal aircraft engines  not because they're not anomalies and it will fit the model p of x  It will fit all those parameters for all the Gaussians on this data  Next on the cross validation of the test set  we're going to think of the anomaly detention algorithm as trying to predict the value of y  So in each of like say test examples  We have these X-I tests  Y-I test  where y is going to be equal to 1 or 0 depending on whether this was an anomalous example  So given input x in my test set  my anomaly detection algorithm think of it as predicting the y as 1 if p of x is less than epsilon  So predicting that it is an anomaly  it is probably is very low  And we think of the algorithm is predicting that y is equal to 0  If p of x is greater then or equals epsilon  So predicting those normal example if the p of x is reasonably large  And so we can now think of the anomaly detection algorithm as making predictions for what are the values of these y labels in the test sets or on the cross validation set  And this puts us somewhat more similar to the supervised learning setting  right? Where we have label test set and our algorithm is making predictions on these labels and so we can evaluate it you know by seeing how often it gets these labels right  Of course these labels are will be very skewed because y equals zero  that is normal examples  usually be much more common than y equals 1 than anomalous examples  But  you know  this is much closer to the source of evaluation metrics we can use in supervised learning  So what's a good evaluation metric to use  Well  because the data is very skewed  because y equals 0 is much more common  classification accuracy would not be a good the evaluation metrics  So  we talked about this in the earlier video  So  if you have a very skewed data set  then predicting y equals 0 all the time  will have very high classification accuracy  Instead  we should use evaluation metrics  like computing the fraction of true positives  false positives  false negatives  true negatives or compute the position of the v curve of this algorithm or do things like compute the f1 score  right  which is a single real number way of summarizing the position and the recall numbers  And so these would be ways to evaluate an anomaly detection algorithm on your cross validation set or on your test set  Finally  earlier in the anomaly detection algorithm  we also had this parameter epsilon  right? So  epsilon is this threshold that we would use to decide when to flag something as an anomaly  And so  if you have a cross validation set  another way to and to choose this parameter epsilon  would be to try a different  try many different values of epsilon  and then pick the value of epsilon that  let's say  maximizes f1 score  or that otherwise does well on your cross validation set  And more generally  the way to reduce the training  testing  and cross validation sets  is that when we are trying to make decisions  like what features to include  or trying to  you know  tune the parameter epsilon  we would then continually evaluate the algorithm on the cross validation sets and make all those decisions like what features did you use  you know  how to set epsilon  use that  evaluate the algorithm on the cross validation set  and then when we've picked the set of features  when we've found the value of epsilon that we're happy with  we can then take the final model and evaluate it  you know  do the final evaluation of the algorithm on the test sets  So  in this video  we talked about the process of how to evaluate an anomaly detection algorithm  and again  having being able to evaluate an algorithm  you know  with a single real number evaluation  with a number like an F1 score that often allows you to much more efficient use of your time when you are trying to develop an anomaly detection system  And we try to make these sorts of decisions  I have to chose epsilon  what features to include  and so on  In this video  we started to use a bit of labeled data in order to evaluate the anomaly detection algorithm and this takes us a little bit closer to a supervised learning setting  In the next video  I'm going to say a bit more about that  And in particular we'll talk about when should you be using an anomaly detection algorithm and when should we be thinking about using supervised learning instead  and what are the differences between these two formalisms 
WHWEHwcSI8E,Anomaly Detection vs  Supervised Learning  In the last video we talked about the process of evaluating an anomaly detection algorithm  And there we started to use some label data with examples that we knew were either anomalous or not anomalous with Y equals one  or Y equals 0  And so  the question then arises of  and if we have the label data  that we have some examples and know the anomalies  and some of them will not be anomalies  Why don't we just use a supervisor on half of them? So why don't we just use logistic regression  or a neuro network to try to learn directly from our labeled data to predict whether Y equals one or Y equals 0  In this video  I'll try to share with you some of the thinking and some guidelines for when you should probably use an anomaly detection algorithm  and whether it might be more fruitful instead of using a supervisor in the algorithm  This slide shows what are the settings under which you should maybe use anomaly detection versus when supervised learning might be more fruitful  If you have a problem with a very small number of positive examples  and remember the examples of y equals one are the anomaly examples  Then you might consider using an anomaly detection algorithm instead  So  having 0 to 20  it may be up to 50 positive examples  might be pretty typical  And usually we have such a small positive  set of positive examples  we're going to save the positive examples just for the cross validation set in the test set  And in contrast  in a typical normal anomaly detection setting  we will often have a relatively large number of negative examples of the normal examples of normal aircraft engines  And we can then use this very large number of negative examples With which to fit the model p(x)  And so there's this idea that in many anomaly detection applications  you have very few positive examples and lots of negative examples  And when we're doing the process of estimating p(x)  affecting all those Gaussian parameters  we need only negative examples to do that  So if you have a lot negative data  we can still fit p(x) pretty well  In contrast  for supervised learning  more typically we would have a reasonably large number of both positive and negative examples  And so this is one way to look at your problem and decide if you should use an anomaly detection algorithm or a supervised  Here's another way that people often think about anomaly detection  So for anomaly detection applications  often there are very different types of anomalies  So think about so many different ways for go wrong  There are so many things that could go wrong that could the aircraft engine  And so if that's the case  and if you have a pretty small set of positive examples  then it can be hard for an algorithm  difficult for an algorithm to learn from your small set of positive examples what the anomalies look like  And in particular  you know future anomalies may look nothing like the ones you've seen so far  So maybe in your set of positive examples  maybe you've seen 5 or 10 or 20 different ways that an aircraft engine could go wrong  But maybe tomorrow  you need to detect a totally new set  a totally new type of anomaly  A totally new way for an aircraft engine to be broken  that you've just never seen before  And if that's the case  it might be more promising to just model the negative examples with this sort of calcium model p of x instead of try to hard to model the positive examples  Because tomorrow's anomaly may be nothing like the ones you've seen so far  In contrast  in some other problems  you have enough positive examples for an algorithm to get a sense of what the positive examples are like  In particular  if you think that future positive examples are likely to be similar to ones in the training set  then in that setting  it might be more reasonable to have a supervisor in the algorithm that looks at all of the positive examples  looks at all of the negative examples  and uses that to try to distinguish between positives and negatives  Hopefully  this gives you a sense of if you have a specific problem  should you think about using an anomaly detection algorithm  or a supervised learning algorithm  And a key difference really is that in anomaly detection  often we have such a small number of positive examples that it is not possible for a learning algorithm to learn that much from the positive examples  And so what we do instead is take a large set of negative examples and have it just learn a lot  learn p(x) from just the negative examples  Of the normal [INAUDIBLE] and we've reserved the small number of positive examples for evaluating our algorithms to use in the either the transvalidation set or the test set  And just as a side comment about this many different types of easier  In some earlier videos we talked about the email spam examples  In those examples  there are actually many different types of spam email  right? There's spam email that's trying to sell you things  Spam email trying to steal your passwords  this is called phishing emails and many different types of spam emails  But for the spam problem we usually have enough examples of spam email to see most of these different types of spam email because we have a large set of examples of spam  And that's why we usually think of spam as a supervised learning setting even though there are many different types of  If we look at some applications of anomaly detection versus supervised learning we'll find fraud detection  If you have many different types of ways for people to try to commit fraud and a relatively small number of fraudulent users on your website  then I use an anomaly detection algorithm  I should say  if you have  if you're a very major online retailer and if you actually have had a lot of people commit fraud on your website  so you actually have a lot of examples of y=1  then sometimes fraud detection could actually shift over to the supervised learning column  But  if you haven't seen that many examples of users doing strange things on your website  then more frequently fraud detection is actually treated as an anomaly detection algorithm rather than a supervised learning algorithm  Other examples  we've talked about manufacturing already  Hopefully  you see more and more examples are not that many anomalies but if again for some manufacturing processes  if you manufacture in very large volumes and you see a lot of bad examples  maybe manufacturing can shift to the supervised learning column as well  But if you haven't seen that many bad examples of so to do the anomaly detection monitoring machines in a data center [INAUDIBLE] similar source of apply  Whereas  you must have classification  weather prediction  and classifying cancers  If you have equal numbers of positive and negative examples  Your positive and your negative examples  then we would tend to treat all of these as supervisor problems  So hopefully  that gives you a sense of one of the properties of a learning problem that would cause you to treat it as an anomaly detection problem versus a supervisory problem  And for many other problems that are faced by various technology companies and so on  we actually are in the settings where we have very few or sometimes zero positive training examples  There's just so many different types of anomalies that we've never seen them before  And for those sorts of problems  very often the algorithm that is used is an anomaly detection algorithm 
TkCYBNUCpnA,Choosing What Features to Use  By now you've seen the anomaly detection algorithm and we've also talked about how to evaluate an anomaly detection algorithm  It turns out  that when you're applying anomaly detection  one of the things that has a huge effect on how well it does  is what features you use  and what features you choose  to give the anomaly detection algorithm  So in this video  what I'd like to do is say a few words  give some suggestions and guidelines for how to go about designing or selecting features give to an anomaly detection algorithm  In our anomaly detection algorithm  one of the things we did was model the features using this sort of Gaussian distribution  With xi to mu i  sigma squared i  lets say  And so one thing that I often do would be to plot the data or the histogram of the data  to make sure that the data looks vaguely Gaussian before feeding it to my anomaly detection algorithm  And  it'll usually work okay  even if your data isn't Gaussian  but this is sort of a nice sanitary check to run  And by the way  in case your data looks non-Gaussian  the algorithms will often work just find  But  concretely if I plot the data like this  and if it looks like a histogram like this  and the way to plot a histogram is to use the HIST  or the HIST command in Octave  but it looks like this  this looks vaguely Gaussian  so if my features look like this  I would be pretty happy feeding into my algorithm  But if i were to plot a histogram of my data  and it were to look like this well  this doesn't look at all like a bell shaped curve  this is a very asymmetric distribution  it has a peak way off to one side  If this is what my data looks like  what I'll often do is play with different transformations of the data in order to make it look more Gaussian  And again the algorithm will usually work okay  even if you don't  But if you use these transformations to make your data more gaussian  it might work a bit better  So given the data set that looks like this  what I might do is take a log transformation of the data and if i do that and re-plot the histogram  what I end up with in this particular example  is a histogram that looks like this  And this looks much more Gaussian  right? This looks much more like the classic bell shaped curve  that we can fit with some mean and variance paramater sigma  So what I mean by taking a log transform  is really that if I have some feature x1 and then the histogram of x1 looks like this then I might take my feature x1 and replace it with log of x1 and this is my new x1 that I'll plot to the histogram over on the right  and this looks much more Guassian  Rather than just a log transform some other things you can do  might be  let's say I have a different feature x2  maybe I'll replace that will log x plus 1  or more generally with log x with x2 and some constant c and this constant could be something that I play with  to try to make it look as Gaussian as possible  Or for a different feature x3  maybe I'll replace it with x3  I might take the square root  The square root is just x3 to the power of one half  right? And this one half is another example of a parameter I can play with  So  I might have x4 and maybe I might instead replace that with x4 to the power of something else  maybe to the power of 1/3  And these  all of these  this one  this exponent parameter  or the C parameter  all of these are examples of parameters that you can play with in order to make your data look a little bit more Gaussian  So  let me show you a live demo of how I actually go about playing with my data to make it look more Gaussian  So  I have already loaded in to octave here a set of features x I have a thousand examples loaded over there  So let's pull up the histogram of my data  Use the hist x command  So there's my histogram  By default  I think this uses 10 bins of histograms  but I want to see a more fine grid histogram  So we do hist to the x  50  so  this plots it in 50 different bins  Okay  that looks better  Now  this doesn't look very Gaussian  does it? So  lets start playing around with the data  Lets try a hist of x to the 0 5  So we take the square root of the data  and plot that histogram  And  okay  it looks a little bit more Gaussian  but not quite there  so let's play at the 0 5 parameter  Let's see  Set this to 0 2  Looks a little bit more Gaussian  Let's reduce a little bit more 0 1  Yeah  that looks pretty good  I could actually just use 0 1  Well  let's reduce it to 0 05  And  you know? Okay  this looks pretty Gaussian  so I can define a new feature which is x mu equals x to the 0 05  and now my new feature x Mu looks more Gaussian than my previous one and then I might instead use this new feature to feed into my anomaly detection algorithm  And of course  there is more than one way to do this  You could also have hist of log of x  that's another example of a transformation you can use  And  you know  that also look pretty Gaussian  So  I can also define x mu equals log of x  and that would be another pretty good choice of a feature to use  So to summarize  if you plot a histogram with the data  and find that it looks pretty non-Gaussian  it's worth playing around a little bit with different transformations like these  to see if you can make your data look a little bit more Gaussian  before you feed it to your learning algorithm  although even if you don't  it might work okay  But I usually do take this step  Now  the second thing I want to talk about is  how do you come up with features for an anomaly detection algorithm  And the way I often do so  is via an error analysis procedure  So what I mean by that  is that this is really similar to the error analysis procedure that we have for supervised learning  where we would train a complete algorithm  and run the algorithm on a cross validation set  and look at the examples it gets wrong  and see if we can come up with extra features to help the algorithm do better on the examples that it got wrong in the cross-validation set  So lets try to reason through an example of this process  In anomaly detection  we are hoping that p of x will be large for the normal examples and it will be small for the anomalous examples  And so a pretty common problem would be if p of x is comparable  maybe both are large for both the normal and the anomalous examples  Lets look at a specific example of that  Let's say that this is my unlabeled data  So  here I have just one feature  x1 and so I'm gonna fit a Gaussian to this  And maybe my Gaussian that I fit to my data looks like that  And now let's say I have an anomalous example  and let's say that my anomalous example takes on an x value of 2 5  So I plot my anomalous example there  And you know  it's kind of buried in the middle of a bunch of normal examples  and so  just this anomalous example that I've drawn in green  it gets a pretty high probability  where it's the height of the blue curve  and the algorithm fails to flag this as an anomalous example  Now  if this were maybe aircraft engine manufacturing or something  what I would do is  I would actually look at my training examples and look at what went wrong with that particular aircraft engine  and see  if looking at that example can inspire me to come up with a new feature x2  that helps to distinguish between this bad example  compared to the rest of my red examples  compared to all of my normal aircraft engines  And if I managed to do so  the hope would be then  that  if I can create a new feature  X2  so that when I re-plot my data  if I take all my normal examples of my training set  hopefully I find that all my training examples are these red crosses here  And hopefully  if I find that for my anomalous example  the feature x2 takes on the the unusual value  So for my green example here  this anomaly  right  my X1 value  is still 2 5  Then maybe my X2 value  hopefully it takes on a very large value like 3 5 over there  or a very small value  But now  if I model my data  I'll find that my anomaly detection algorithm gives high probability to data in the central regions  slightly lower probability to that  sightly lower probability to that  An example that's all the way out there  my algorithm will now give very low probability to  And so  the process of this is  really look at the mistakes that it is making  Look at the anomaly that the algorithm is failing to flag  and see if that inspires you to create some new feature  So find something unusual about that aircraft engine and use that to create a new feature  so that with this new feature it becomes easier to distinguish the anomalies from your good examples  And so that's the process of error analysis and using that to create new features for anomaly detection  Finally  let me share with you my thinking on how I usually go about choosing features for anomaly detection  So  usually  the way I think about choosing features is I want to choose features that will take on either very  very large values  or very  very small values  for examples that I think might turn out to be anomalies  So let's use our example again of monitoring the computers in a data center  And so you have lots of machines  maybe thousands  or tens of thousands of machines in a data center  And we want to know if one of the machines  one of our computers is acting up  so doing something strange  So here are examples of features you may choose  maybe memory used  number of disc accesses  CPU load  network traffic  But now  lets say that I suspect one of the failure cases  let's say that in my data set I think that CPU load the network traffic tend to grow linearly with each other  Maybe I'm running a bunch of web servers  and so  here if one of my servers is serving a lot of users  I have a very high CPU load  and have a very high network traffic  But let's say  I think  let's say I have a suspicion  that one of the failure cases is if one of my computers has a job that gets stuck in some infinite loop  So if I think one of the failure cases  is one of my machines  one of my web servers--server code-- gets stuck in some infinite loop  and so the CPU load grows  but the network traffic doesn't because it's just spinning it's wheels and doing a lot of CPU work  you know  stuck in some infinite loop  In that case  to detect that type of anomaly  I might create a new feature  X5  which might be CPU load divided by network traffic  And so here X5 will take on a unusually large value if one of the machines has a very large CPU load but not that much network traffic and so this will be a feature that will help your anomaly detection capture  a certain type of anomaly  And you can also get creative and come up with other features as well  Like maybe I have a feature x6 thats CPU load squared divided by network traffic  And this would be another variant of a feature like x5 to try to capture anomalies where one of your machines has a very high CPU load  that maybe doesn't have a commensurately large network traffic  And by creating features like these  you can start to capture anomalies that correspond to unusual combinations of values of the features  So in this video we talked about how to and take a feature  and maybe transform it a little bit  so that it becomes a bit more Gaussian  before feeding into an anomaly detection algorithm  And also the error analysis in this process of creating features to try to capture different types of anomalies  And with these sorts of guidelines hopefully that will help you to choose good features  to give to your anomaly detection algorithm  to help it capture all sorts of anomalies 
a5n8TMNbyrg,Multivariate Gaussian Distribution  In this and the next video  I'd like to tell you about one possible extension to the anomaly detection algorithm that we've developed so far  This extension uses something called the multivariate Gaussian distribution  and it has some advantages  and some disadvantages  and it can sometimes catch some anomalies that the earlier algorithm didn't  To motivate this  let's start with an example  Let's say that so our unlabeled data looks like what I have plotted here  And I'm going to use the example of monitoring machines in the data center  monitoring computers in the data center  So my two features are x1 which is the CPU load and x2 which is maybe the memory use  So if I take my two features  x1 and x2  and I model them as Gaussians then here's a plot of my X1 features  here's a plot of my X2 features  and so if I fit a Gaussian to that  maybe I'll get a Gaussian like this  so here's P of X 1  which depends on the parameters mu 1  and sigma squared 1  and here's my memory used  and  you know  maybe I'll get a Gaussian that looks like this  and this is my P of X 2  which depends on mu 2 and sigma squared 2  And so this is how the anomaly detection algorithm models X1 and X2  Now let's say that in the test sets I have an example that looks like this  The location of that green cross  so the value of X 1 is about 0 4  and the value of X 2 is about 1 5  Now  if you look at the data  it looks like  yeah  most of the data data lies in this region  and so that green cross is pretty far away from any of the data I've seen  It looks like that should be raised as an anomaly  So  in my data  in my  in the data of my good examples  it looks like  you know  the CPU load  and the memory use  they sort of grow linearly with each other  So if I have a machine using lots of CPU  you know memory use will also be high  whereas this example  this green example it looks like here  the CPU load is very low  but the memory use is very high  and I just have not seen that before in my training set  It looks like that should be an anomaly  But let's see what the anomaly detection algorithm will do  Well  for the CPU load  it puts it at around there 0 5 and this reasonably high probability is not that far from other examples we've seen  maybe  whereas  for the memory use  this appointment  0 5  whereas for the memory use  it's about 1 5  which is there  Again  you know  it's all to us  it's not terribly Gaussian  but the value here and the value here is not that different from many other examples we've seen  and so P of X 1  will be pretty high  reasonably high  P of X 2 reasonably high  I mean  if you look at this plot right  this point here  it doesn't look that bad  and if you look at this plot  you know across here  doesn't look that bad  I mean  I have had examples with even greater memory used  or with even less CPU use  and so this example doesn't look that anomalous  And so  an anomaly detection algorithm will fail to flag this point as an anomaly  And it turns out what our anomaly detection algorithm is doing is that it is not realizing that this blue ellipse shows the high probability region  is that  one of the thing is that  examples here  a high probability  and the examples  the next circle of from a lower probably  and examples here are even lower probability  and somehow  here are things that are  green cross there  it's pretty high probability  and in particular  it tends to think that  you know  everything in this region  everything on the line that I'm circling over  has  you know  about equal probability  and it doesn't realize that something out here actually has much lower probability than something over there  So  in order to fix this  we can  we're going to develop a modified version of the anomaly detection algorithm  using something called the multivariate Gaussian distribution also called the multivariate normal distribution  So here's what we're going to do  We have features x which are in Rn and instead of P of X 1  P of X 2  separately  we're going to model P of X  all in one go  so model P of X  you know  all at the same time  So the parameters of the multivariate Gaussian distribution are mu  which is a vector  and sigma  which is an n by n matrix  called a covariance matrix  and this is similar to the covariance matrix that we saw when we were working with the PCA  with the principal components analysis algorithm  For the second complete is  let me just write out the formula for the multivariate Gaussian distribution  So we say that probability of X  and this is parameterized by my parameters mu and sigma that the probability of x is equal to once again there's absolutely no need to memorize this formula  You know  you can look it up whenever you need to use it  but this is what the probability of X looks like  Transverse  2nd inverse  X minus mu  And this thing here  the absolute value of sigma  this thing here when you write this symbol  this is called the determent of sigma and this is a mathematical function of a matrix and you really don't need to know what the determinant of a matrix is  but really all you need to know is that you can compute it in octave by using the octave command DET of sigma  Okay  and again  just be clear  alright? In this expression  these sigmas here  these are just n by n matrix  This is not a summation and you know  the sigma there is an n by n matrix  So that's the formula for P of X  but it's more interestingly  or more importantly  what does P of X actually looks like? Lets look at some examples of multivariate Gaussian distributions  So let's take a two dimensional example  say if I have N equals 2  I have two features  X 1 and X 2  Lets say I set MU to be equal to 0 and sigma to be equal to this matrix here  With 1s on the diagonals and 0s on the off-diagonals  this matrix is sometimes also called the identity matrix  In that case  p of x will look like this  and what I'm showing in this figure is  you know  for a specific value of X1 and for a specific value of X2  the height of this surface the value of p of x  And so with this setting the parameters p of x is highest when X1 and X2 equal zero 0  so that's the peak of this Gaussian distribution  and the probability falls off with this sort of two dimensional Gaussian or this bell shaped two dimensional bell-shaped surface  Down below is the same thing but plotted using a contour plot instead  or using different colors  and so this heavy intense red in the middle  corresponds to the highest values  and then the values decrease with the yellow being slightly lower values the cyan being lower values and this deep blue being the lowest values so this is really the same figure but plotted viewed from the top instead  using colors instead  And so  with this distribution  you see that it faces most of the probability near 0 0 and then as you go out from 0 0 the probability of X1 and X2 goes down  Now lets try varying some of the parameters and see what happens  So let's take sigma and change it so let's say sigma shrinks a little bit  Sigma is a covariance matrix and so it measures the variance or the variability of the features X1 X2  So if the shrink sigma then what you get is what you get is that the width of this bump diminishes and the height also increases a bit  because the area under the surface is equal to 1  So the integral of the volume under the surface is equal to 1  because probability distribution must integrate to one  But  if you shrink the variance  it's kinda like shrinking sigma squared  you end up with a narrower distribution  and one that's a little bit taller  And so you see here also the concentric ellipsis has shrunk a little bit  Whereas in contrast if you were to increase sigma to 2 2 on the diagonals  so it is now two times the identity then you end up with a much wider and much flatter Gaussian  And so the width of this is much wider  This is hard to see but this is still a bell shaped bump  it's just flattened down a lot  it has become much wider and so the variance or the variability of X1 and X2 just becomes wider  Here are a few more examples  Now lets try varying one of the elements of sigma at the time  Let's say I send sigma to 0 6 there  and 1 over there  What this does  is this reduces the variance of the first feature  X 1  while keeping the variance of the second feature X 2  the same  And so with this setting of parameters  you can model things like that  X 1 has smaller variance  and X 2 has larger variance  Whereas if I do this  if I set this matrix to 2  1 then you can also model examples where you know here we'll say X1 can have take on a large range of values whereas X2 takes on a relatively narrower range of values  And that's reflected in this figure as well  you know where  the distribution falls off more slowly as X 1 moves away from 0  and falls off very rapidly as X 2 moves away from 0  And similarly if we were to modify this element of the matrix instead  then similar to the previous slide  except that here where you know playing around here saying that X2 can take on a very small range of values and so here if this is 0 6  we notice now X2 tends to take on a much smaller range of values than the original example  whereas if we were to set sigma to be equal to 2 then that's like saying X2 you know  has a much larger range of values  Now  one of the cool things about the multivariate Gaussian distribution is that you can also use it to model correlations between the data  That is we can use it to model the fact that X1 and X2 tend to be highly correlated with each other for example  So specifically if you start to change the off diagonal entries of this covariance matrix you can get a different type of Gaussian distribution  And so as I increase the off-diagonal entries from  5 to  8  what I get is this distribution that is more and more thinly peaked along this sort of x equals y line  And so here the contour says that x and y tend to grow together and the things that are with large probability are if either X1 is large and Y2 is large or X1 is small and Y2 is small  Or somewhere in between  And as this entry  0 8 gets large  you get a Gaussian distribution  that's sort of where all the probability lies on this sort of narrow region  where x is approximately equal to y  This is a very tall  thin distribution you know line mostly along this line central region where x is close to y  So this is if we set these entries to be positive entries  In contrast if we set these to negative values  as I decreases it to - 5 down to - 8  then what we get is a model where we put most of the probability in this sort of negative X one in the next 2 correlation region  and so  most of the probability now lies in this region  where X 1 is about equal to -X 2  rather than X 1 equals X 2  And so this captures a sort of negative correlation between x1 and x2  And so this is a hopefully this gives you a sense of the different distributions that the multivariate Gaussian distribution can capture  So follow up in varying  the covariance matrix sigma  the other thing you can do is also  vary the mean parameter mu  and so operationally  we have mu equal 0 0  and so the distribution was centered around X 1 equals 0  X2 equals 0  so the peak of the distribution is here  whereas  if we vary the values of mu  then that varies the peak of the distribution and so  if mu equals 0  0 5  the peak is at  you know  X1 equals zero  and X2 equals 0 5  and so the peak or the center of this distribution has shifted  and if mu was 1 5 minus 0 5 then OK  and similarly the peak of the distribution has now shifted to a different location  corresponding to where  you know  X1 is 1 5 and X2 is -0 5  and so varying the mu parameter  just shifts around the center of this whole distribution  So  hopefully  looking at all these different pictures gives you a sense of the sort of probability distributions that the Multivariate Gaussian Distribution allows you to capture  And the key advantage of it is it allows you to capture  when you'd expect two different features to be positively correlated  or maybe negatively correlated  In the next video  we'll take this multivariate Gaussian distribution and apply it to anomaly detection 
a6NJQ3AMcdo,Anomaly Detection using the Multivariate Gaussian Distribution  In the last video we talked about the Multivariate Gaussian Distribution and saw some examples of the sorts of distributions you can model  as you vary the parameters  mu and sigma  In this video  let's take those ideas  and apply them to develop a different anomaly detection algorithm  To recap the multivariate Gaussian distribution and the multivariate normal distribution has two parameters  mu and sigma  Where mu this an n dimensional vector and sigma  the covariance matrix  is an n by n matrix  And here's the formula for the probability of X  as parameterized by mu and sigma  and as you vary mu and sigma  you can get a range of different distributions  like  you know  these are three examples of the ones that we saw in the previous video  So let's talk about the parameter fitting or the parameter estimation problem  The question  as usual  is if I have a set of examples X1 through XM and here each of these examples is an n dimensional vector and I think my examples come from a multivariate Gaussian distribution  How do I try to estimate my parameters mu and sigma? Well the standard formulas for estimating them is you set mu to be just the average of your training examples  And you set sigma to be equal to this  And this is actually just like the sigma that we had written out  when we were using the PCA or the Principal Components Analysis algorithm  So you just plug in these two formulas and this would give you your estimated parameter mu and your estimated parameter sigma  So given the data set here is how you estimate mu and sigma  Let's take this method and just plug it into an anomaly detection algorithm  So how do we put all of this together to develop an anomaly detection algorithm? Here 's what we do  First we take our training set  and we fit the model  we fit P of X  by  you know  setting mu and sigma as described on the previous slide  Next when you are given a new example X  So if you are given a test example  lets take an earlier example to have a new example out here  And that is my test example  Given the new example X  what we are going to do is compute P of X  using this formula for the multivariate Gaussian distribution  And then  if P of X is very small  then we flagged it as an anomaly  whereas  if P of X is greater than that parameter epsilon  then we don't flag it as an anomaly  So it turns out  if we were to fit a multivariate Gaussian distribution to this data set  so just the red crosses  not the green example  you end up with a Gaussian distribution that places lots of probability in the central region  slightly less probability here  slightly less probability here  slightly less probability here  and very low probability at the point that is way out here  And so  if you apply the multivariate Gaussian distribution to this example  it will actually correctly flag that example  as an anomaly  Finally it's worth saying a few words about what is the relationship between the multivariate Gaussian distribution model  and the original model  where we were modeling P of X as a product of this P of X1  P of X2  up to P of Xn  It turns out that you can prove mathematically  I'm not going to do the proof here  but you can prove mathematically that this relationship  between the multivariate Gaussian model and this original one  And in particular  it turns out that the original model corresponds to multivariate Gaussians  where the contours of the Gaussian are always axis aligned  So all three of these are examples of Gaussian distributions that you can fit using the original model  It turns out that that corresponds to multivariate Gaussian  where  you know  the ellipsis here  the contours of this distribution--it turns out that this model actually corresponds to a special case of a multivariate Gaussian distribution  And in particular  this special case is defined by constraining the distribution of p of x  the multivariate a Gaussian distribution of p of x  so that the contours of the probability density function  of the probability distribution function  are axis aligned  And so you can get a p of x with a multivariate Gaussian that looks like this  or like this  or like this  And you notice  that in all 3 of these examples  these ellipses  or these ovals that I'm drawing  have their axes aligned with the X1 X2 axes  And what we do not have  is a set of contours that are at an angle  right? And this corresponded to examples where sigma is equal to 1 1  0 8  0 8  Let's say  with non-0 elements on the off diagonals  So  it turns out that it's possible to show mathematically that this model actually is the same as a multivariate Gaussian distribution but with a constraint  And the constraint is that the covariance matrix sigma must have 0's on the off diagonal elements  In particular  the covariance matrix sigma  this thing here  it would be sigma squared 1  sigma squared 2  down to sigma squared n  and then everything on the off diagonal entries  all of these elements above and below the diagonal of the matrix  all of those are going to be zero  And in fact if you take these values of sigma  sigma squared 1  sigma squared 2  down to sigma squared n  and plug them into here  and you know  plug them into this covariance matrix  then the two models are actually identical  That is  this new model  using a multivariate Gaussian distribution  corresponds exactly to the old model  if the covariance matrix sigma  has only 0 elements off the diagonals  and in pictures that corresponds to having Gaussian distributions  where the contours of this distribution function are axis aligned  So you aren't allowed to model the correlations between the diffrent features  So in that sense the original model is actually a special case of this multivariate Gaussian model  So when would you use each of these two models? So when would you the original model and when would you use the multivariate Gaussian model? The original model is probably used somewhat more often  and whereas the multivariate Gaussian distribution is used somewhat less but it has the advantage of being able to capture correlations between features  So suppose you want to capture anomalies where you have different features say where features x1  x2 take on unusual combinations of values so in the earlier example  we had that example where the anomaly was with the CPU load and the memory use taking on unusual combinations of values  if you want to use the original model to capture that  then what you need to do is create an extra feature  such as X3 equals X1/X2  you know equals maybe the CPU load divided by the memory used  or something  and you need to create extra features if there's unusual combinations of values where X1 and X2 take on an unusual combination of values even though X1 by itself and X2 by itself looks like it's taking a perfectly normal value  But if you're willing to spend the time to manually create an extra feature like this  then the original model will work fine  Whereas in contrast  the multivariate Gaussian model can automatically capture correlations between different features  But the original model has some other more significant advantages  too  and one huge advantage of the original model is that it is computationally cheaper  and another view on this is that is scales better to very large values of n and very large numbers of features  and so even if n were ten thousand  or even if n were equal to a hundred thousand  the original model will usually work just fine  Whereas in contrast for the multivariate Gaussian model notice here  for example  that we need to compute the inverse of the matrix sigma where sigma is an n by n matrix and so computing sigma if sigma is a hundred thousand by a hundred thousand matrix that is going to be very computationally expensive  And so the multivariate Gaussian model scales less well to large values of N  And finally for the original model  it turns out to work out ok even if you have a relatively small training set this is the small unlabeled examples that we use to model p of x of course  and this works fine  even if M is  you know  maybe 50  100  works fine  Whereas for the multivariate Gaussian  it is sort of a mathematical property of the algorithm that you must have m greater than n  so that the number of examples is greater than the number of features you have  And there's a mathematical property of the way we estimate the parameters that if this is not true  so if m is less than or equal to n  then this matrix isn't even invertible  that is this matrix is singular  and so you can't even use the multivariate Gaussian model unless you make some changes to it  But a typical rule of thumb that I use is  I will use the multivariate Gaussian model only if m is much greater than n  so this is sort of the narrow mathematical requirement  but in practice  I would use the multivariate Gaussian model  only if m were quite a bit bigger than n  So if m were greater than or equal to 10 times n  let's say  might be a reasonable rule of thumb  and if it doesn't satisfy this  then the multivariate Gaussian model has a lot of parameters  right  so this covariance matrix sigma is an n by n matrix  so it has  you know  roughly n squared parameters  because it's a symmetric matrix  it's actually closer to n squared over 2 parameters  but this is a lot of parameters  so you need make sure you have a fairly large value for m  make sure you have enough data to fit all these parameters  And m greater than or equal to 10 n would be a reasonable rule of thumb to make sure that you can estimate this covariance matrix sigma reasonably well  So in practice the original model shown on the left that is used more often  And if you suspect that you need to capture correlations between features what people will often do is just manually design extra features like these to capture specific unusual combinations of values  But in problems where you have a very large training set or m is very large and n is not too large  then the multivariate Gaussian model is well worth considering and may work better as well  and can save you from having to spend your time to manually create extra features in case the anomalies turn out to be captured by unusual combinations of values of the features  Finally I just want to briefly mention one somewhat technical property  but if you're fitting multivariate Gaussian model  and if you find that the covariance matrix sigma is singular  or you find it's non-invertible  they're usually 2 cases for this  One is if it's failing to satisfy this m greater than n condition  and the second case is if you have redundant features  So by redundant features  I mean  if you have 2 features that are the same  Somehow you accidentally made two copies of the feature  so your x1 is just equal to x2  Or if you have redundant features like maybe your features X3 is equal to feature X4  plus feature X5  Okay  so if you have highly redundant features like these  you know  where if X3 is equal to X4 plus X5  well X3 doesn't contain any extra information  right? You just take these 2 other features  and add them together  And if you have this sort of redundant features  duplicated features  or this sort of features  than sigma may be non-invertible  And so there's a debugging set-- this should very rarely happen  so you probably won't run into this  it is very unlikely that you have to worry about this-- but in case you implement a multivariate Gaussian model you find that sigma is non-invertible  What I would do is first make sure that M is quite a bit bigger than N  and if it is then  the second thing I do  is just check for redundant features  And so if there are 2 features that are equal  just get rid of one of them  or if you have redundant if these   X3 equals X4 plus X5  just get rid of the redundant feature  and then it should work fine again  As an aside for those of you who are experts in linear algebra  by redundant features  what I mean is the formal term is features that are linearly dependent  But in practice what that really means is one of these problems tripping up the algorithm if you just make you features non-redundant   that should solve the problem of sigma being non-invertable  But once again the odds of your running into this at all are pretty low so chances are  you can just apply the multivariate Gaussian model  without having to worry about sigma being non-invertible  so long as m is greater than or equal to n  So that's it for anomaly detection  with the multivariate Gaussian distribution  And if you apply this method you would be able to have an anomaly detection algorithm that automatically captures positive and negative correlations between your different features and flags an anomaly if it sees is unusual combination of the values of the features 
UiZE1-wlb4o,Problem Formulation  In this next set of videos  I would like to tell you about recommender systems  There are two reasons  I had two motivations for why I wanted to talk about recommender systems  The first is just that it is an important application of machine learning  Over the last few years  occasionally I visit different  you know  technology companies here in Silicon Valley and I often talk to people working on machine learning applications there and so I've asked people what are the most important applications of machine learning or what are the machine learning applications that you would most like to get an improvement in the performance of  And one of the most frequent answers I heard was that there are many groups out in Silicon Valley now  trying to build better recommender systems  So  if you think about what the websites are like Amazon  or what Netflix or what eBay  or what iTunes Genius  made by Apple does  there are many websites or systems that try to recommend new products to use  So  Amazon recommends new books to you  Netflix try to recommend new movies to you  and so on  And these sorts of recommender systems  that look at what books you may have purchased in the past  or what movies you have rated in the past  but these are the systems that are responsible for today  a substantial fraction of Amazon's revenue and for a company like Netflix  the recommendations that they make to the users is also responsible for a substantial fraction of the movies watched by their users  And so an improvement in performance of a recommender system can have a substantial and immediate impact on the bottom line of many of these companies  Recommender systems is kind of a funny problem  within academic machine learning so that we could go to an academic machine learning conference  the problem of recommender systems  actually receives relatively little attention  or at least it's sort of a smaller fraction of what goes on within Academia  But if you look at what's happening  many technology companies  the ability to build these systems seems to be a high priority for many companies  And that's one of the reasons why I want to talk about them in this class  The second reason that I want to talk about recommender systems is that as we approach the last few sets of videos of this class I wanted to talk about a few of the big ideas in machine learning and share with you  you know  some of the big ideas in machine learning  And we've already seen in this class that features are important for machine learning  the features you choose will have a big effect on the performance of your learning algorithm  So there's this big idea in machine learning  which is that for some problems  maybe not all problems  but some problems  there are algorithms that can try to automatically learn a good set of features for you  So rather than trying to hand design  or hand code the features  which is mostly what we've been doing so far  there are a few settings where you might be able to have an algorithm  just to learn what feature to use  and the recommender systems is just one example of that sort of setting  There are many others  but engraved through recommender systems  will be able to go a little bit into this idea of learning the features and you'll be able to see at least one example of this  I think  big idea in machine learning as well  So  without further ado  let's get started  and talk about the recommender system problem formulation  As my running example  I'm going to use the modern problem of predicting movie ratings  So  here's a problem  Imagine that you're a website or a company that sells or rents out movies  or what have you  And so  you know  Amazon  and Netflix  and I think iTunes are all examples of companies that do this  and let's say you let your users rate different movies  using a 1 to 5 star rating  So  users may  you know  something one  two  three  four or five stars  In order to make this example just a little bit nicer  I'm going to allow 0 to 5 stars as well  because that just makes some of the math come out just nicer  Although most of these websites use the 1 to 5 star scale  So here  I have 5 movies  You know  Love That Lasts  Romance Forever  Cute Puppies of Love  Nonstop Car Chases  and Swords vs  Karate  And we have 4 users  which  calling  you know  Alice  Bob  Carol  and Dave  with initials A  B  C  and D  we'll call them users 1  2  3  and 4  So  let's say Alice really likes Love That Lasts and rates that 5 stars  likes Romance Forever  rates it 5 stars  She did not watch Cute Puppies of Love  and did rate it  so we don't have a rating for that  and Alice really did not like Nonstop Car Chases or Swords vs  Karate  And a different user Bob  user two  maybe rated a different set of movies  maybe she likes to Love at Last  did not to watch Romance Forever  just have a rating of 4  a 0  a 0  and maybe our 3rd user  rates this 0  did not watch that one  0  5  5  and  you know  let's just fill in some of the numbers  And so just to introduce a bit of notation  this notation that we'll be using throughout  I'm going to use NU to denote the number of users  So in this example  NU will be equal to 4  So the u-subscript stands for users and Nm  going to use to denote the number of movies  so here I have five movies so Nm equals equals 5  And you know for this example  I have for this example  I have loosely 3 maybe romantic or romantic comedy movies and 2 action movies and you know  if you look at this small example  it looks like Alice and Bob are giving high ratings to these romantic comedies or movies about love  and giving very low ratings about the action movies  and for Carol and Dave  it's the opposite  right? Carol and Dave  users three and four  really like the action movies and give them high ratings  but don't like the romance and love- type movies as much  Specifically  in the recommender system problem  we are given the following data  Our data comprises the following  we have these values r(i  j)  and r(i  j) is 1 if user J has rated movie I  So our users rate only some of the movies  and so  you know  we don't have ratings for those movies  And whenever r(i  j) is equal to 1  whenever user j has rated movie i  we also get this number y(i  j)  which is the rating given by user j to movie i  And so  y(i  j) would be a number from zero to five  depending on the star rating  zero to five stars that user gave that particular movie  So  the recommender system problem is given this data that has give these r(i  j)'s and the y(i  j)'s to look through the data and look at all the movie ratings that are missing and to try to predict what these values of the question marks should be  In the particular example  I have a very small number of movies and a very small number of users and so most users have rated most movies but in the realistic settings your users each of your users may have rated only a minuscule fraction of your movies but looking at this data  you know  if Alice and Bob both like the romantic movies maybe we think that Alice would have given this a five  Maybe we think Bob would have given this a 4 5 or some high value  as we think maybe Carol and Dave were doing these very low ratings  And Dave  well  if Dave really likes action movies  maybe he would have given Swords and Karate a 4 rating or maybe a 5 rating  okay? And so  our job in developing a recommender system is to come up with a learning algorithm that can automatically go fill in these missing values for us so that we can look at  say  the movies that the user has not yet watched  and recommend new movies to that user to watch  You try to predict what else might be interesting to a user  So that's the formalism of the recommender system problem  In the next video we'll start to develop a learning algorithm to address this problem 
GpK36dizqEM,Content Based Recommendations  In the last video  we talked about the recommender systems problem where for example you might have a set of movies and you may have a set of users  each who have rated some subset of the movies  They've rated the movies one to five stars or zero to five stars  And what we would like to do is look at these users and predict how they would have rated other movies that they have not yet rated  In this video I'd like to talk about our first approach to building a recommender system  This approach is called content based recommendations  Here's our data set from before and just to remind you of a bit of notation  I was using nu to denote the number of users and so that's equal to 4  and nm to denote the number of movies  I have 5 movies  So  how do I predict what these missing values would be? Let's suppose that for each of these movies I have a set of features for them  In particular  let's say that for each of the movies have two features which I'm going to denote x1 and x2  Where x1 measures the degree to which a movie is a romantic movie and x2 measures the degree to which a movie is an action movie  So  if you take a movie  Love at last  you know it's 0 9 rating on the romance scale  This is a highly romantic movie  but zero on the action scale  So  almost no action in that movie  Romance forever is a 1 0  lot of romance and 0 01 action  I don't know  maybe there's a minor car crash in that movie or something  So there's a little bit of action  Skipping one  let's do Swords vs karate  maybe that has a 0 romance rating and no romance at all in that but plenty of action  And Nonstop car chases  maybe again there's a tiny bit of romance in that movie but mainly action  And Cute puppies of love mainly a romance movie with no action at all  So if we have features like these  then each movie can be represented with a feature vector  Let's take movie one  So let's call these movies 1  2  3  4  and 5  But my first movie  Love at last  I have my two features  0 9 and 0  And so these are features x1 and x2  And let's add an extra feature as usual  which is my interceptor feature x0 = 1  And so putting these together I would then have a feature x1  The superscript 1 denotes it's the feature vector for my first movie  and this feature vector is equal to 1  The first 1 there is this interceptor  And then my two feature is 0 90 like so  So for Love at last I would have a feature vector x1  for the movie Romance forever I may have a software feature of vector x2  and so on  and for Swords vs karate I would have a different feature vector x superscript 5  Also  consistence with our earlier node notation that we were using  we're going to set n to be the number of features not counting this x0 interceptor  So n is equal to 2 because it's we have two features x1 and x2 capturing the degree of romance and the degree of action in each movie  Now in order to make predictions here's one thing that we do which is that we could treat predicting the ratings of each user as a separate linear regression problem  So specifically  let's say that for each user j  we're going to learn the parameter vector theta j  which would be an R3 in this case  More generally  theta (j) would be an R (n+1)  where n is the number of features not counting the set term  And we're going to predict user j as rating movie i with just the inner product between parameters vectors theta and the features xi  So let's take a specific example  Let's take user 1  so that would be Alice  And associated with Alice would be some parameter vector theta 1  And our second user  Bob  will be associated a different parameter vector theta 2  Carol will be associated with a different parameter vector theta 3 and Dave a different parameter vector theta 4  So let's say you want to make a prediction for what Alice will think of the movie Cute puppies of love  Well that movie is going to have some parameter vector x3 where we have that x3 is going to be equal to 1  which is my intercept term and then 0 99 and then 0  And let's say  for this example  let's say that we've somehow already gotten a parameter vector theta 1 for Alice  We'll say it later exactly how we come up with this parameter vector  But let's just say for now that some unspecified learning algorithm has learned the parameter vector theta 1 and is equal to this 0 5 0  So our prediction for this entry is going to be equal to theta 1  that is Alice's parameter vector  transpose x3  that is the feature vector for the Cute puppies of love movie  number 3  And so the inner product between these two vectors is gonna be 5 times 0 99  which is equal to 4 95  And so my prediction for this value over here is going to be 4 95  And maybe that seems like a reasonable value if indeed this is my parameter vector theta 1  So  all we're doing here is we're applying a different copy of this linear regression for each user  and we're saying that what Alice does is Alice has some parameter vector theta 1 that she uses  that we use to predict her ratings as a function of how romantic and how action packed a movie is  And Bob and Carol and Dave  each of them have a different linear function of the romanticness and actionness  or degree of romance and degree of action in a movie and that that's how we're gonna predict that their star ratings  More formally  here's how we can write down the problem  Our notation is that r(i j) is equal to 1 if user j has rated movie i and y(i j) is the rating of that movie  if that rating exists  That is  if that user has actually rated that movie  And  on the previous slide we also defined these  theta j  which is a parameter for the user xi  which is a feature vector for a specific movie  And for each user and each movie  we predict that rating as follows  So let me introduce just temporarily introduce one extra bit of notation mj  We're gonna use mj to denote the number of users rated by movie j  We don't need this notation only for this line  Now in order to learn the parameter vector for theta j  well how do we do so  This is basically a linear regression problem  So what we can do is just choose a parameter vector theta j so that the predicted values here are as close as possible to the values that we observed in our training sets and the values we observed in our data  So let's write that down  In order to learn the parameter vector theta j  let's minimize over the parameter vector theta j of sum  and I want to sum over all movies that user j has rated  So we write it as sum over all values of i  That's a  r(i j) equals 1  So the way to read this summation syntax is this is summation over all the values of i  so the r(i j) is equal to 1  So you'll be summing over all the movies that user j has rated  And then I'm going to compute theta j  transpose x i  So that's the prediction of using j's rating on movie i - y (i j)  So that's the actual observed rating squared  And then  let me just divide by the number of movies that user j has actually rated  So let's just divide by 1 over 2m j  And so this is just like the least squares regressions  It's just like linear regression  where we want to choose the parameter vector theta j to minimize this type of squared error term  And if you want  you can also add in irregularization terms so plus lambda over 2m and this is really 2mj because we have mj examples  User j has rated that many movies  it's not like we have that many data points with which to fit the parameters of theta j  And then let me add in my usual regularization term here of theta j k squared  As usual  this sum is from k equals 1 through n  so here  theta j is going to be an n plus 1 dimensional vector  where in our early example n was equal to 2  But more broadly  more generally n is the number of features we have per movie  And so as usual we don't regularize over theta 0  We don't regularize over the bias terms  The sum is from k equals 1 through n  So if you minimize this as a function of theta j you get a good solution  you get a pretty good estimate of a parameter vector theta j with which to make predictions for user j's movie ratings  For recommender systems  I'm gonna change this notation a little bit  So to simplify the subsequent math  I with to get rid of this term mj  So that's just a constant  right? So I can delete it without changing the value of theta j that I get out of this optimization  So if you imagine taking this whole equation  taking this whole expression and multiplying it by mj  get rid of that constant  And when I minimize this  I should still get the same value of theta j as before  So just to repeat what we wrote on the previous slide  here's our optimization objective  In order to learn theta j which is the parameter for user j  we're going to minimize over theta j of this optimization objectives  So this is our usual squared error term and then this is our regularizations term  Now of course in building a recommender system  we don't just want to learn parameters for a single user  We want to learn parameters for all of our users  I have n subscript u users  so I want to learn all of these parameters  And so  what I'm going to do is take this optimization objective and just add the mixture summation there  So this expression here with the one half on top of this is exactly the same as what we had on top  Except that now instead of just doing this for a specific user theta j  I'm going to sum my objective over all of my users and then minimize this overall optimization objective  minimize this overall cost on  And when I minimize this as a function of theta 1  theta 2  up to theta nu  I will get a separate parameter vector for each user  And I can then use that to make predictions for all of my users  for all of my n subscript users  So putting everything together  this was our optimization objective on top  And to give this thing a name  I'll just call this J(theta1       theta nu)  So j as usual is my optimization objective  which I'm trying to minimize  Next  in order to actually do the minimization  if you were to derive the gradient descent update  these are the equations that you would get  So you take theta j  k  and subtract from an alpha  which is the learning rate  times these terms over here on the right  So there's slightly different cases when k equals 0 and when k does not equal 0  Because our regularization term here regularizes only the values of theta jk for k not equal to 0  so we don't regularize theta 0  so with slightly different updates when k equals 0 and k is not equal to 0  And this term over here  for example  is just the partial derivative with respect to your parameter  that of your optimization objective  Right and so this is just gradient descent and I've already computed the derivatives and plugged them into here  And if this gradient descent update look a lot like what we have here for linear regression  That's because these are essentially the same as linear regression  The only minor difference is that for linear regression we have these 1 over m terms  this really would've been 1 over mj  But because earlier when we are deriving the optimization objective  we got rid of this  that's why we don't have this 1 over m term  But otherwise  it's really some of my training examples of the ever times xk plus that regularization term  plus that term of regularization contributes to the derivative  And so if you're using gradient descent here's how you can minimize the cost function j to learn all the parameters  And using these formulas for the derivative if you want  you can also plug them into a more advanced optimization algorithm  like conjugate gradient or LBFGS or what have you  And use that to try to minimize the cost function j as well  So hopefully you now know how you can apply essentially a deviation on linear regression in order to predict different movie ratings by different users  This particular algorithm is called a content based recommendations  or a content based approach  because we assume that we have available to us features for the different movies  And so where features that capture what is the content of these movies  of how romantic is this movie  how much action is in this movie  And we're really using features of a content of the movies to make our predictions  But for many movies  we don't actually have such features  Or maybe very difficult to get such features for all of our movies  for all of whatever items we're trying to sell  And so  in the next video  we'll start to talk about an approach to recommender systems that isn't content based and does not assume that we have someone else giving us all of these features for all of the movies in our data set 
cO5KrmSwhl4,Collaborative Filtering  In this video we'll talk about an approach to building a recommender system that's called collaborative filtering  The algorithm that we're talking about has a very interesting property that it does what is called feature learning and by that I mean that this will be an algorithm that can start to learn for itself what features to use  Here was the data set that we had and we had assumed that for each movie  someone had come and told us how romantic that movie was and how much action there was in that movie  But as you can imagine it can be very difficult and time consuming and expensive to actually try to get someone to  you know  watch each movie and tell you how romantic each movie and how action packed is each movie  and often you'll want even more features than just these two  So where do you get these features from? So let's change the problem a bit and suppose that we have a data set where we do not know the values of these features  So we're given the data set of movies and of how the users rated them  but we have no idea how romantic each movie is and we have no idea how action packed each movie is so I've replaced all of these things with question marks  But now let's make a slightly different assumption  Let's say we've gone to each of our users  and each of our users has told has told us how much they like the romantic movies and how much they like action packed movies  So Alice has associated a current of theta 1  Bob theta 2  Carol theta 3  Dave theta 4  And let's say we also use this and that Alice tells us that she really likes romantic movies and so there's a five there which is the multiplier associated with X1 and lets say that Alice tells us she really doesn't like action movies and so there's a 0 there  And Bob tells us something similar so we have theta 2 over here  Whereas Carol tells us that she really likes action movies which is why there's a 5 there  that's the multiplier associated with X2  and remember there's also X0 equals 1 and let's say that Carol tells us she doesn't like romantic movies and so on  similarly for Dave  So let's assume that somehow we can go to users and each user J just tells us what is the value of theta J for them  And so basically specifies to us of how much they like different types of movies  If we can get these parameters theta from our users then it turns out that it becomes possible to try to infer what are the values of x1 and x2 for each movie  Let's look at an example  Let's look at movie 1  So that movie 1 has associated with it a feature vector x1  And you know this movie is called Love at last but let's ignore that  Let's pretend we don't know what this movie is about  so let's ignore the title of this movie  All we know is that Alice loved this move  Bob loved this movie  Carol and Dave hated this movie  So what can we infer? Well  we know from the feature vectors that Alice and Bob love romantic movies because they told us that there's a 5 here  Whereas Carol and Dave  we know that they hate romantic movies and that they love action movies  So because those are the parameter vectors that you know  uses 3 and 4  Carol and Dave  gave us  And so based on the fact that movie 1 is loved by Alice and Bob and hated by Carol and Dave  we might reasonably conclude that this is probably a romantic movie  it is probably not much of an action movie  this example is a little bit mathematically simplified but what we're really asking is what feature vector should X1 be so that theta 1 transpose x1 is approximately equal to 5  that's Alice's rating  and theta 2 transpose x1 is also approximately equal to 5  and theta 3 transpose x1 is approximately equal to 0  so this would be Carol's rating  and theta 4 transpose X1 is approximately equal to 0  And from this it looks like  you know  X1 equals one that's the intercept term  and then 1 0  0 0  that makes sense given what we know of Alice  Bob  Carol  and Dave's preferences for movies and the way they rated this movie  And so more generally  we can go down this list and try to figure out what might be reasonable features for these other movies as well  Let's formalize this problem of learning the features XI  Let's say that our users have given us their preferences  So let's say that our users have come and  you know  told us these values for theta 1 through theta of NU and we want to learn the feature vector XI for movie number I  What we can do is therefore pose the following optimization problem  So we want to sum over all the indices J for which we have a rating for movie I because we're trying to learn the features for movie I that is this feature vector XI  So and then what we want to do is minimize this squared error  so we want to choose features XI  so that  you know  the predictive value of how user J rates movie I will be similar  will be not too far in the squared error sense of the actual value YIJ that we actually observe in the rating of user j on movie I  So  just to summarize what this term does is it tries to choose features XI so that for all the users J that have rated that movie  the algorithm also predicts a value for how that user would have rated that movie that is not too far  in the squared error sense  from the actual value that the user had rated that movie  So that's the squared error term  As usual  we can also add this sort of regularization term to prevent the features from becoming too big  So this is how we would learn the features for one specific movie but what we want to do is learn all the features for all the movies and so what I'm going to do is add this extra summation here so I'm going to sum over all Nm movies  N subscript m movies  and minimize this objective on top that sums of all movies  And if you do that  you end up with the following optimization problem  And if you minimize this  you have hopefully a reasonable set of features for all of your movies  So putting everything together  what we  the algorithm we talked about in the previous video and the algorithm that we just talked about in this video  In the previous video  what we showed was that you know  if you have a set of movie ratings  so if you have the data the rij's and then you have the yij's that will be the movie ratings  Then given features for your different movies we can learn these parameters theta  So if you knew the features  you can learn the parameters theta for your different users  And what we showed earlier in this video is that if your users are willing to give you parameters  then you can estimate features for the different movies  So this is kind of a chicken and egg problem  Which comes first? You know  do we want if we can get the thetas  we can know the Xs  If we have the Xs  we can learn the thetas  And what you can do is  and then this actually works  what you can do is in fact randomly guess some value of the thetas  Now based on your initial random guess for the thetas  you can then go ahead and use the procedure that we just talked about in order to learn features for your different movies  Now given some initial set of features for your movies you can then use this first method that we talked about in the previous video to try to get an even better estimate for your parameters theta  Now that you have a better setting of the parameters theta for your users  we can use that to maybe even get a better set of features and so on  We can sort of keep iterating  going back and forth and optimizing theta  x theta  x theta  nd this actually works and if you do this  this will actually cause your album to converge to a reasonable set of features for you movies and a reasonable set of parameters for your different users  So this is a basic collaborative filtering algorithm  This isn't actually the final algorithm that we're going to use  In the next video we are going to be able to improve on this algorithm and make it quite a bit more computationally efficient  But  hopefully this gives you a sense of how you can formulate a problem where you can simultaneously learn the parameters and simultaneously learn the features from the different movies  And for this problem  for the recommender system problem  this is possible only because each user rates multiple movies and hopefully each movie is rated by multiple users  And so you can do this back and forth process to estimate theta and x  So to summarize  in this video we've seen an initial collaborative filtering algorithm  The term collaborative filtering refers to the observation that when you run this algorithm with a large set of users  what all of these users are effectively doing are sort of collaboratively--or collaborating to get better movie ratings for everyone because with every user rating some subset with the movies  every user is helping the algorithm a little bit to learn better features  and then by helping-- by rating a few movies myself  I will be helping the system learn better features and then these features can be used by the system to make better movie predictions for everyone else  And so there is a sense of collaboration where every user is helping the system learn better features for the common good  This is this collaborative filtering  And  in the next video what we going to do is take the ideas that have worked out  and try to develop a better an even better algorithm  a slightly better technique for collaborative filtering 
eCAHcfA-2c8,Collaborative Filtering Algorithm  In the last couple videos  we talked about the ideas of how  first  if you're given features for movies  you can use that to learn parameters data for users  And second  if you're given parameters for the users  you can use that to learn features for the movies  In this video we're going to take those ideas and put them together to come up with a collaborative filtering algorithm  So one of the things we worked out earlier is that if you have features for the movies then you can solve this minimization problem to find the parameters theta for your users  And then we also worked that out  if you are given the parameters theta  you can also use that to estimate the features x  and you can do that by solving this minimization problem  So one thing you could do is actually go back and forth  Maybe randomly initialize the parameters and then solve for theta  solve for x  solve for theta  solve for x  But  it turns out that there is a more efficient algorithm that doesn't need to go back and forth between the x's and the thetas  but that can solve for theta and x simultaneously  And here it is  What we are going to do  is basically take both of these optimization objectives  and put them into the same objective  So I'm going to define the new optimization objective j  which is a cost function  that is a function of my features x and a function of my parameters theta  And  it's basically the two optimization objectives I had on top  but I put together  So  in order to explain this  first  I want to point out that this term over here  this squared error term  is the same as this squared error term and the summations look a little bit different  but let's see what the summations are really doing  The first summation is sum over all users J and then sum over all movies rated by that user  So  this is really summing over all pairs IJ  that correspond to a movie that was rated by a user  Sum over J says  for every user  the sum of all the movies rated by that user  This summation down here  just does things in the opposite order  This says for every movie I  sum over all the users J that have rated that movie and so  you know these summations  both of these are just summations over all pairs ij for which r of i J is equal to 1  It's just something over all the user movie pairs for which you have a rating  and so those two terms up there is just exactly this first term  and I've just written the summation here explicitly  where I'm just saying the sum of all pairs IJ  such that RIJ is equal to 1  So what we're going to do is define a combined optimization objective that we want to minimize in order to solve simultaneously for x and theta  And then the other terms in the optimization objective are this  which is a regularization in terms of theta  So that came down here and the final piece is this term which is my optimization objective for the x's and that became this  And this optimization objective j actually has an interesting property that if you were to hold the x's constant and just minimize with respect to the thetas then you'd be solving exactly this problem  whereas if you were to do the opposite  if you were to hold the thetas constant  and minimize j only with respect to the x's  then it becomes equivalent to this  Because either this term or this term is constant if you're minimizing only the respective x's or only respective thetas  So here's an optimization objective that puts together my cost functions in terms of x and in terms of theta  And in order to come up with just one optimization problem  what we're going to do  is treat this cost function  as a function of my features x and of my user pro user parameters data and just minimize this whole thing  as a function of both the Xs and a function of the thetas  And really the only difference between this and the older algorithm is that  instead of going back and forth  previously we talked about minimizing with respect to theta then minimizing with respect to x  whereas minimizing with respect to theta  minimizing with respect to x and so on  In this new version instead of sequentially going between the 2 sets of parameters x and theta  what we are going to do is just minimize with respect to both sets of parameters simultaneously  Finally one last detail is that when we're learning the features this way  Previously we have been using this convention that we have a feature x0 equals one that corresponds to an interceptor  When we are using this sort of formalism where we're are actually learning the features  we are actually going to do away with this convention  And so the features we are going to learn x  will be in Rn  Whereas previously we had features x and Rn + 1 including the intercept term  By getting rid of x0 we now just have x in Rn  And so similarly  because the parameters theta is in the same dimension  we now also have theta in RN because if there's no x0  then there's no need parameter theta 0 as well  And the reason we do away with this convention is because we're now learning all the features  right? So there is no need to hard code the feature that is always equal to one  Because if the algorithm really wants a feature that is always equal to 1  it can choose to learn one for itself  So if the algorithm chooses  it can set the feature X1 equals 1  So there's no need to hard code the feature of 001  the algorithm now has the flexibility to just learn it by itself  So  putting everything together  here is our collaborative filtering algorithm  first we are going to initialize x and theta to small random values  And this is a little bit like neural network training  where there we were also initializing all the parameters of a neural network to small random values  Next we're then going to minimize the cost function using great intercepts or one of the advance optimization algorithms  So  if you take derivatives you find that the great intercept like these and so this term here is the partial derivative of the cost function  I'm not going to write that out  with respect to the feature value Xik and similarly this term here is also a partial derivative value of the cost function with respect to the parameter theta that we're minimizing  And just as a reminder  in this formula that we no longer have this X0 equals 1 and so we have that x is in Rn and theta is a Rn  In this new formalism  we're regularizing every one of our perimeters theta  you know  every one of our parameters Xn  There's no longer the special case theta zero  which was regularized differently  or which was not regularized compared to the parameters theta 1 down to theta  So there is now no longer a theta 0  which is why in these updates  I did not break out a special case for k equals 0  So we then use gradient descent to minimize the cost function j with respect to the features x and with respect to the parameters theta  And finally  given a user  if a user has some parameters  theta  and if there's a movie with some sort of learned features x  we would then predict that that movie would be given a star rating by that user of theta transpose j  Or just to fill those in  then we're saying that if user J has not yet rated movie I  then what we do is predict that user J is going to rate movie I according to theta J transpose Xi  So that's the collaborative filtering algorithm and if you implement this algorithm you actually get a pretty decent algorithm that will simultaneously learn good features for hopefully all the movies as well as learn parameters for all the users and hopefully give pretty good predictions for how different users will rate different movies that they have not yet rated
mzfVimVbguQ,Vectorization  Low Rank Matrix Factorization  In the last few videos  we talked about a collaborative filtering algorithm  In this video I'm going to say a little bit about the vectorization implementation of this algorithm  And also talk a little bit about other things you can do with this algorithm  For example  one of the things you can do is  given one product can you find other products that are related to this so that for example  a user has recently been looking at one product  Are there other related products that you could recommend to this user? So let's see what we could do about that  What I'd like to do is work out an alternative way of writing out the predictions of the collaborative filtering algorithm  To start  here is our data set with our five movies and what I'm going to do is take all the ratings by all the users and group them into a matrix  So  here we have five movies and four users  and so this matrix y is going to be a 5 by 4 matrix  It's just you know  taking all of the elements  all of this data  Including question marks  and grouping them into this matrix  And of course the elements of this matrix of the (i  j) element of this matrix is really what we were previously writing as y superscript i  j  It's the rating given to movie i by user j  Given this matrix y of all the ratings that we have  there's an alternative way of writing out all the predictive ratings of the algorithm  And  in particular if you look at what a certain user predicts on a certain movie  what user j predicts on movie i is given by this formula  And so  if you have a matrix of the predicted ratings  what you would have is the following matrix where the i  j entry  So this corresponds to the rating that we predict using j will give to movie i is exactly equal to that theta j transpose XI  and so  you know  this is a matrix where this first element the one-one element is a predictive rating of user one or movie one and this element  this is the one-two element is the predicted rating of user two on movie one  and so on  and this is the predicted rating of user one on the last movie and if you want  you know  this rating is what we would have predicted for this value and this rating is what we would have predicted for that value  and so on  Now  given this matrix of predictive ratings there is then a simpler or vectorized way of writing these out  In particular if I define the matrix x  and this is going to be just like the matrix we had earlier for linear regression to be sort of x1 transpose x2 transpose down to x of nm transpose  So I'm take all the features for my movies and stack them in rows  So if you think of each movie as one example and stack all of the features of the different movies and rows  And if we also to find a matrix capital theta  and what I'm going to do is take each of the per user parameter vectors  and stack them in rows  like so  So that's theta 1  which is the parameter vector for the first user  And  you know  theta 2  and so  you must stack them in rows like this to define a matrix capital theta and so I have nu parameter vectors all stacked in rows like this  Now given this definition for the matrix x and this definition for the matrix theta in order to have a vectorized way of computing the matrix of all the predictions you can just compute x times the matrix theta transpose  and that gives you a vectorized way of computing this matrix over here  To give the collaborative filtering algorithm that you've been using another name  The algorithm that we're using is also called low rank matrix factorization  And so if you hear people talk about low rank matrix factorization that's essentially exactly the algorithm that we have been talking about  And this term comes from the property that this matrix x times theta transpose has a mathematical property in linear algebra called that this is a low rank matrix and so that's what gives rise to this name low rank matrix factorization for these algorithms  because of this low rank property of this matrix x theta transpose  In case you don't know what low rank means or in case you don't know what a low rank matrix is  don't worry about it  You really don't need to know that in order to use this algorithm  But if you're an expert in linear algebra  that's what gives this algorithm  this other name of low rank matrix factorization  Finally  having run the collaborative filtering algorithm here's something else that you can do which is use the learned features in order to find related movies  Specifically for each product i really for each movie i  we've learned a feature vector xi  So  you know  when you learn a certain features without really know that can the advance what the different features are going to be  but if you run the algorithm and perfectly the features will tend to capture what are the important aspects of these different movies or different products or what have you  What are the important aspects that cause some users to like certain movies and cause some users to like different sets of movies  So maybe you end up learning a feature  you know  where x1 equals romance  x2 equals action similar to an earlier video and maybe you learned a different feature x3 which is a degree to which this is a comedy  Then some feature x4 which is  you know  some other thing  And you have N features all together and after you have learned features it's actually often pretty difficult to go in to the learned features and come up with a human understandable interpretation of what these features really are  But in practice  you know  the features even though these features can be hard to visualize  It can be hard to figure out just what these features are  Usually  it will learn features that are very meaningful for capturing whatever are the most important or the most salient properties of a movie that causes you to like or dislike it  And so now let's say we want to address the following problem  Say you have some specific movie i and you want to find other movies j that are related to that movie  And so well  why would you want to do this? Right  maybe you have a user that's browsing movies  and they're currently watching movie j  than what's a reasonable movie to recommend to them to watch after they're done with movie j? Or if someone's recently purchased movie j  well  what's a different movie that would be reasonable to recommend to them for them to consider purchasing  So  now that you have learned these feature vectors  this gives us a very convenient way to measure how similar two movies are  In particular  movie i has a feature vector xi  and so if you can find a different movie  j  so that the distance between xi and xj is small  then this is a pretty strong indication that  you know  movies j and i are somehow similar  At least in the sense that some of them likes movie i  maybe more likely to like movie j as well  So  just to recap  if your user is looking at some movie i and if you want to find the 5 most similar movies to that movie in order to recommend 5 new movies to them  what you do is find the five movies j  with the smallest distance between the features between these different movies  And this could give you a few different movies to recommend to your user  So with that  hopefully  you now know how to use a vectorized implementation to compute all the predicted ratings of all the users and all the movies  and also how to do things like use learned features to find what might be movies and what might be products that aren't related to each other 
hVPDals5HDE,Implementational Detail  Mean Normalization  By now you've seen all of the main pieces of the recommender system algorithm or the collaborative filtering algorithm  In this video I want to just share one last implementational detail  namely mean normalization  which can sometimes just make the algorithm work a little bit better  To motivate the idea of mean normalization  let's consider an example of where there's a user that has not rated any movies  So  in addition to our four users  Alice  Bob  Carol  and Dave  I've added a fifth user  Eve  who hasn't rated any movies  Let's see what our collaborative filtering algorithm will do on this user  Let's say that n is equal to 2 and so we're going to learn two features and we are going to have to learn a parameter vector theta 5  which is going to be in R2  remember this is now vectors in Rn not Rn+1  we'll learn the parameter vector theta 5 for our user number 5  Eve  So if we look in the first term in this optimization objective  well the user Eve hasn't rated any movies  so there are no movies for which Rij is equal to one for the user Eve and so this first term plays no role at all in determining theta 5 because there are no movies that Eve has rated  And so the only term that effects theta 5 is this term  And so we're saying that we want to choose vector theta 5 so that the last regularization term is as small as possible  In other words we want to minimize this lambda over 2 theta 5 subscript 1 squared plus theta 5 subscript 2 squared so that's the component of the regularization term that corresponds to user 5  and of course if your goal is to minimize this term  then what you're going to end up with is just theta 5 equals 0 0  Because a regularization term is encouraging us to set parameters close to 0 and if there is no data to try to pull the parameters away from 0  because this first term doesn't effect theta 5  we just end up with theta 5 equals the vector of all zeros  And so when we go to predict how user 5 would rate any movie  we have that theta 5 transpose xi  for any i  that's just going to be equal to zero  Because theta 5 is 0 for any value of x  this inner product is going to be equal to 0  And what we're going to have therefore  is that we're going to predict that Eve is going to rate every single movie with zero stars  But this doesn't seem very useful does it? I mean if you look at the different movies  Love at Last  this first movie  a couple people rated it 5 stars  And for even the Swords vs  Karate  someone rated it 5 stars  So some people do like some movies  It seems not useful to just predict that Eve is going to rate everything 0 stars  And in fact if we're predicting that eve is going to rate everything 0 stars  we also don't have any good way of recommending any movies to her  because you know all of these movies are getting exactly the same predicted rating for Eve so there's no one movie with a higher predicted rating that we could recommend to her  so  that's not very good  The idea of mean normalization will let us fix this problem  So here's how it works  As before let me group all of my movie ratings into this matrix Y  so just take all of these ratings and group them into matrix Y  And this column over here of all question marks corresponds to Eve's not having rated any movies  Now to perform mean normalization what I'm going to do is compute the average rating that each movie obtained  And I'm going to store that in a vector that we'll call mu  So the first movie got two 5-star and two 0-star ratings  so the average of that is a 2 5-star rating  The second movie had an average of 2 5-stars and so on  And the final movie that has 0  0  5  0  And the average of 0  0  5  0  that averages out to an average of 1 25 rating  And what I'm going to do is look at all the movie ratings and I'm going to subtract off the mean rating  So this first element 5 I'm going to subtract off 2 5 and that gives me 2 5  And the second element 5 subtract off of 2 5  get a 2 5  And then the 0  0  subtract off 2 5 and you get -2 5  -2 5  In other words  what I'm going to do is take my matrix of movie ratings  take this wide matrix  and subtract form each row the average rating for that movie  So  what I'm doing is just normalizing each movie to have an average rating of zero  And so just one last example  If you look at this last row  0 0 5 0  We're going to subtract 1 25  and so I end up with these values over here  So now and of course the question marks stay a question mark  So each movie in this new matrix Y has an average rating of 0  What I'm going to do then  is take this set of ratings and use it with my collaborative filtering algorithm  So I'm going to pretend that this was the data that I had gotten from my users  or pretend that these are the actual ratings I had gotten from the users  and I'm going to use this as my data set with which to learn my parameters theta J and my features XI - from these mean normalized movie ratings  When I want to make predictions of movie ratings  what I'm going to do is the following  for user J on movie I  I'm gonna predict theta J transpose XI  where X and theta are the parameters that I've learned from this mean normalized data set  But  because on the data set  I had subtracted off the means in order to make a prediction on movie i  I'm going to need to add back in the mean  and so i'm going to add back in mu i  And so that's going to be my prediction where in my training data subtracted off all the means and so when we make predictions and we need to add back in these means mu i for movie i  And so specifically if you user 5 which is Eve  the same argument as the previous slide still applies in the sense that Eve had not rated any movies and so the learned parameter for user 5 is still going to be equal to 0  0  And so what we're going to get then is that on a particular movie i we're going to predict for Eve theta 5  transpose xi plus add back in mu i and so this first component is going to be equal to zero  if theta five is equal to zero  And so on movie i  we are going to end a predicting mu i  And  this actually makes sense  It means that on movie 1 we're going to predict Eve rates it 2 5  On movie 2 we're gonna predict Eve rates it 2 5  On movie 3 we're gonna predict Eve rates it at 2 and so on  This actually makes sense  because it says that if Eve hasn't rated any movies and we just don't know anything about this new user Eve  what we're going to do is just predict for each of the movies  what are the average rating that those movies got  Finally  as an aside  in this video we talked about mean normalization  where we normalized each row of the matrix y  to have mean 0  In case you have some movies with no ratings  so it is analogous to a user who hasn't rated anything  but in case you have some movies with no ratings  you can also play with versions of the algorithm  where you normalize the different columns to have means zero  instead of normalizing the rows to have mean zero  although that's maybe less important  because if you really have a movie with no rating  maybe you just shouldn't recommend that movie to anyone  anyway  And so  taking care of the case of a user who hasn't rated anything might be more important than taking care of the case of a movie that hasn't gotten a single rating  So to summarize  that's how you can do mean normalization as a sort of pre-processing step for collaborative filtering  Depending on your data set  this might some times make your implementation work just a little bit better 
_45M-dO99-M,Learning With Large Datasets  In the next few videos  we'll talk about large scale machine learning  That is  algorithms but viewing with big data sets  If you look back at a recent 5 or 10-year history of machine learning  One of the reasons that learning algorithms work so much better now than even say  5-years ago  is just the sheer amount of data that we have now and that we can train our algorithms on  In these next few videos  we'll talk about algorithms for dealing when we have such massive data sets  So why do we want to use such large data sets? We've already seen that one of the best ways to get a high performance machine learning system  is if you take a low-bias learning algorithm  and train that on a lot of data  And so  one early example we have already seen was this example of classifying between confusable words  So  for breakfast  I ate two (TWO) eggs and we saw in this example  these sorts of results  where  you know  so long as you feed the algorithm a lot of data  it seems to do very well  And so it's results like these that has led to the saying in machine learning that often it's not who has the best algorithm that wins  It's who has the most data  So you want to learn from large data sets  at least when we can get such large data sets  But learning with large data sets comes with its own unique problems  specifically  computational problems  Let's say your training set size is M equals 100 000 000  And this is actually pretty realistic for many modern data sets  If you look at the US Census data set  if there are  you know  300 million people in the US  you can usually get hundreds of millions of records  If you look at the amount of traffic that popular websites get  you easily get training sets that are much larger than hundreds of millions of examples  And let's say you want to train a linear regression model  or maybe a logistic regression model  in which case this is the gradient descent rule  And if you look at what you need to do to compute the gradient  which is this term over here  then when M is a hundred million  you need to carry out a summation over a hundred million terms  in order to compute these derivatives terms and to perform a single step of decent  Because of the computational expense of summing over a hundred million entries in order to compute just one step of gradient descent  in the next few videos we've spoken about techniques for either replacing this with something else or to find more efficient ways to compute this derivative  By the end of this sequence of videos on large scale machine learning  you know how to fit models  linear regression  logistic regression  neural networks and so on even today's data sets with  say  a hundred million examples  Of course  before we put in the effort into training a model with a hundred million examples  We should also ask ourselves  well  why not use just a thousand examples  Maybe we can randomly pick the subsets of a thousand examples out of a hundred million examples and train our algorithm on just a thousand examples  So before investing the effort into actually developing and the software needed to train these massive models is often a good sanity check  if training on just a thousand examples might do just as well  The way to sanity check of using a much smaller training set might do just as well  that is if using a much smaller n equals 1000 size training set  that might do just as well  it is the usual method of plotting the learning curves  so if you were to plot the learning curves and if your training objective were to look like this  that's J train theta  And if your cross-validation set objective  Jcv of theta would look like this  then this looks like a high-variance learning algorithm  and we will be more confident that adding extra training examples would improve performance  Whereas in contrast if you were to plot the learning curves  if your training objective were to look like this  and if your cross-validation objective were to look like that  then this looks like the classical high-bias learning algorithm  And in the latter case  you know  if you were to plot this up to  say  m equals 1000 and so that is m equals 500 up to m equals 1000  then it seems unlikely that increasing m to a hundred million will do much better and then you'd be just fine sticking to n equals 1000  rather than investing a lot of effort to figure out how the scale of the algorithm  Of course  if you were in the situation shown by the figure on the right  then one natural thing to do would be to add extra features  or add extra hidden units to your neural network and so on  so that you end up with a situation closer to that on the left  where maybe this is up to n equals 1000  and this then gives you more confidence that trying to add infrastructure to change the algorithm to use much more than a thousand examples that might actually be a good use of your time  So in large-scale machine learning  we like to come up with computationally reasonable ways  or computationally efficient ways  to deal with very big data sets  In the next few videos  we'll see two main ideas  The first is called stochastic gradient descent and the second is called Map Reduce  for viewing with very big data sets  And after you've learned about these methods  hopefully that will allow you to scale up your learning algorithms to big data and allow you to get much better performance on many different applications 
U8C48-lCa8g,Stochastic Gradient Descent  For many learning algorithms  among them linear regression  logistic regression and neural networks  the way we derive the algorithm was by coming up with a cost function or coming up with an optimization objective  And then using an algorithm like gradient descent to minimize that cost function  We have a very large training set gradient descent becomes a computationally very expensive procedure  In this video  we'll talk about a modification to the basic gradient descent algorithm called Stochastic gradient descent  which will allow us to scale these algorithms to much bigger training sets  Suppose you are training a linear regression model using gradient descent  As a quick recap  the hypothesis will look like this  and the cost function will look like this  which is the sum of one half of the average square error of your hypothesis on your m training examples  and the cost function we've already seen looks like this sort of bow-shaped function  So  plotted as function of the parameters theta 0 and theta 1  the cost function J is a sort of a bow-shaped function  And gradient descent looks like this  where in the inner loop of gradient descent you repeatedly update the parameters theta using that expression  Now in the rest of this video  I'm going to keep using linear regression as the running example  But the ideas here  the ideas of Stochastic gradient descent is fully general and also applies to other learning algorithms like logistic regression  neural networks and other algorithms that are based on training gradient descent on a specific training set  So here's a picture of what gradient descent does  if the parameters are initialized to the point there then as you run gradient descent different iterations of gradient descent will take the parameters to the global minimum  So take a trajectory that looks like that and heads pretty directly to the global minimum  Now  the problem with gradient descent is that if m is large  Then computing this derivative term can be very expensive  because the surprise  summing over all m examples  So if m is 300 million  alright  So in the United States  there are about 300 million people  And so the US or United States census data may have on the order of that many records  So you want to fit the linear regression model to that then you need to sum over 300 million records  And that's very expensive  To give the algorithm a name  this particular version of gradient descent is also called Batch gradient descent  And the term Batch refers to the fact that we're looking at all of the training examples at a time  We call it sort of a batch of all of the training examples  And it really isn't the  maybe the best name but this is what machine learning people call this particular version of gradient descent  And if you imagine really that you have 300 million census records stored away on disc  The way this algorithm works is you need to read into your computer memory all 300 million records in order to compute this derivative term  You need to stream all of these records through computer because you can't store all your records in computer memory  So you need to read through them and slowly  you know  accumulate the sum in order to compute the derivative  And then having done all that work  that allows you to take one step of gradient descent  And now you need to do the whole thing again  You know  scan through all 300 million records  accumulate these sums  And having done all that work  you can take another little step using gradient descent  And then do that again  And then you take yet a third step  And so on  And so it's gonna take a long time in order to get the algorithm to converge  In contrast to Batch gradient descent  what we are going to do is come up with a different algorithm that doesn't need to look at all the training examples in every single iteration  but that needs to look at only a single training example in one iteration  Before moving on to the new algorithm  here's just a Batch gradient descent algorithm written out again with that being the cost function and that being the update and of course this term here  that's used in the gradient descent rule  that is the partial derivative with respect to the parameters theta J of our optimization objective  J train of theta  Now  let's look at the more efficient algorithm that scales better to large data sets  In order to work off the algorithms called Stochastic gradient descent  this vectors the cost function in a slightly different way then they define the cost of the parameter theta with respect to a training example x(i)  y(i) to be equal to one half times the squared error that my hypothesis incurs on that example  x(i)  y(i)  So this cost function term really measures how well is my hypothesis doing on a single example x(i)  y(i)  Now you notice that the overall cost function j train can now be written in this equivalent form  So j train is just the average over my m training examples of the cost of my hypothesis on that example x(i)  y(i)  Armed with this view of the cost function for linear regression  let me now write out what Stochastic gradient descent does  The first step of Stochastic gradient descent is to randomly shuffle the data set  So by that I just mean randomly shuffle  or randomly reorder your m training examples  It's sort of a standard pre-processing step  come back to this in a minute  But the main work of Stochastic gradient descent is then done in the following  We're going to repeat for i equals 1 through m  So we'll repeatedly scan through my training examples and perform the following update  Gonna update the parameter theta j as theta j minus alpha times h of x(i) minus y(i) times x(i)j  And we're going to do this update as usual for all values of j  Now  you notice that this term over here is exactly what we had inside the summation for Batch gradient descent  In fact  for those of you that are calculus is possible to show that that term here  that's this term here  is equal to the partial derivative with respect to my parameter theta j of the cost of the parameters theta on x(i)  y(i)  Where cost is of course this thing that was defined previously  And just the wrap of the algorithm  let me close my curly braces over there  So what Stochastic gradient descent is doing is it is actually scanning through the training examples  And first it's gonna look at my first training example x(1)  y(1)  And then looking at only this first example  it's gonna take like a basically a little gradient descent step with respect to the cost of just this first training example  So in other words  we're going to look at the first example and modify the parameters a little bit to fit just the first training example a little bit better  Having done this inside this inner for-loop is then going to go on to the second training example  And what it's going to do there is take another little step in parameter space  so modify the parameters just a little bit to try to fit just a second training example a little bit better  Having done that  is then going to go onto my third training example  And modify the parameters to try to fit just the third training example a little bit better  and so on until you know  you get through the entire training set  And then this ultra repeat loop may cause it to take multiple passes over the entire training set  This view of Stochastic gradient descent also motivates why we wanted to start by randomly shuffling the data set  This doesn't show us that when we scan through the training site here  that we end up visiting the training examples in some sort of randomly sorted order  Depending on whether your data already came randomly sorted or whether it came originally sorted in some strange order  in practice this would just speed up the conversions to Stochastic gradient descent just a little bit  So in the interest of safety  it's usually better to randomly shuffle the data set if you aren't sure if it came to you in randomly sorted order  But more importantly another view of Stochastic gradient descent is that it's a lot like descent but rather than wait to sum up these gradient terms over all m training examples  what we're doing is we're taking this gradient term using just one single training example and we're starting to make progress in improving the parameters already  So rather than  you know  waiting 'till taking a path through all 300 000 United States Census records  say  rather than needing to scan through all of the training examples before we can modify the parameters a little bit and make progress towards a global minimum  For Stochastic gradient descent instead we just need to look at a single training example and we're already starting to make progress in this case of parameters towards  moving the parameters towards the global minimum  So  here's the algorithm written out again where the first step is to randomly shuffle the data and the second step is where the real work is done  where that's the update with respect to a single training example x(i)  y(i)  So  let's see what this algorithm does to the parameters  Previously  we saw that when we are using Batch gradient descent  that is the algorithm that looks at all the training examples in time  Batch gradient descent will tend to  you know  take a reasonably straight line trajectory to get to the global minimum like that  In contrast with Stochastic gradient descent every iteration is going to be much faster because we don't need to sum up over all the training examples  But every iteration is just trying to fit single training example better  So  if we were to start stochastic gradient descent  oh  let's start stochastic gradient descent at a point like that  The first iteration  you know  may take the parameters in that direction and maybe the second iteration looking at just the second example maybe just by chance  we get more unlucky and actually head in a bad direction with the parameters like that  In the third iteration where we tried to modify the parameters to fit just the third training examples better  maybe we'll end up heading in that direction  And then we'll look at the fourth training example and we will do that  The fifth example  sixth example  7th and so on  And as you run Stochastic gradient descent  what you find is that it will generally move the parameters in the direction of the global minimum  but not always  And so take some more random-looking  circuitous path to watch the global minimum  And in fact as you run Stochastic gradient descent it doesn't actually converge in the same same sense as Batch gradient descent does and what it ends up doing is wandering around continuously in some region that's in some region close to the global minimum  but it doesn't just get to the global minimum and stay there  But in practice this isn't a problem because  you know  so long as the parameters end up in some region there maybe it is pretty close to the global minimum  So  as parameters end up pretty close to the global minimum  that will be a pretty good hypothesis and so usually running Stochastic gradient descent we get a parameter near the global minimum and that's good enough for  you know  essentially any  most practical purposes  Just one final detail  In Stochastic gradient descent  we had this outer loop repeat which says to do this inner loop multiple times  So  how many times do we repeat this outer loop? Depending on the size of the training set  doing this loop just a single time may be enough  And up to  you know  maybe 10 times may be typical so we may end up repeating this inner loop anywhere from once to ten times  So if we have a you know  truly massive data set like the this US census gave us that example that I've been talking about with 300 million examples  it is possible that by the time you've taken just a single pass through your training set  So  this is for i equals 1 through 300 million  It's possible that by the time you've taken a single pass through your data set you might already have a perfectly good hypothesis  In which case  you know  this inner loop you might need to do only once if m is very  very large  But in general taking anywhere from 1 through 10 passes through your data set  you know  maybe fairly common  But really it depends on the size of your training set  And if you contrast this to Batch gradient descent  With Batch gradient descent  after taking a pass through your entire training set  you would have taken just one single gradient descent steps  So one of these little baby steps of gradient descent where you just take one small gradient descent step and this is why Stochastic gradient descent can be much faster  So  that was the Stochastic gradient descent algorithm  And if you implement it  hopefully that will allow you to scale up many of your learning algorithms to much bigger data sets and get much more performance that way 
dgyPeFemyL8,"Mini-Batch Gradient Descent  In the previous video  we talked about Stochastic gradient descent  and how that can be much faster than Batch gradient descent  In this video  let's talk about another variation on these ideas is called Mini-batch gradient descent they can work sometimes even a bit faster than stochastic gradient descent  To summarize the algorithms we talked about so far  In Batch gradient descent we will use all m examples in each generation  Whereas in Stochastic gradient descent we will use a single example in each generation  What Mini-batch gradient descent does is somewhere in between  Specifically  with this algorithm we're going to use b examples in each iteration where b is a parameter called the \""mini batch size\"" so the idea is that this is somewhat in-between Batch gradient descent and Stochastic gradient descent  This is just like batch gradient descent  except that I'm going to use a much smaller batch size  A typical choice for the value of b might be b equals 10  lets say  and a typical range really might be anywhere from b equals 2 up to b equals 100  So that will be a pretty typical range of values for the Mini-batch size  And the idea is that rather than using one example at a time or m examples at a time we will use b examples at a time  So let me just write this out informally  we're going to get  let's say  b  For this example  let's say b equals 10  So we're going to get  the next 10 examples from my training set so that may be some set of examples xi  yi  If it's 10 examples then the indexing will be up to x (i+9)  y (i+9) so that's 10 examples altogether and then we'll perform essentially a gradient descent update using these 10 examples  So  that's any rate times one tenth times sum over k equals i through i+9 of h subscript theta of x(k) minus y(k) times x(k)j  And so in this expression  where summing the gradient terms over my ten examples  So  that's number ten  that's  you know  my mini batch size and just i+9 again  the 9 comes from the choice of the parameter b  and then after this we will then increase  you know  i by tenth  we will go on to the next ten examples and then keep moving like this  So just to write out the entire algorithm in full  In order to simplify the indexing for this one at the right top  I'm going to assume we have a mini-batch size of ten and a training set size of a thousand  what we're going to do is have this sort of form  for i equals 1 and that in 21's the stepping  in steps of 10 because we look at 10 examples at a time  And then we perform this sort of gradient descent update using ten examples at a time so this 10 and this i+9 those are consequence of having chosen my mini-batch to be ten  And you know  this ultimate four-loop  this ends at 991 here because if I have 1000 training samples then I need 100 steps of size 10 in order to get through my training set  So this is mini-batch gradient descent  Compared to batch gradient descent  this also allows us to make progress much faster  So we have again our running example of  you know  U S  Census data with 300 million training examples  then what we're saying is after looking at just the first 10 examples we can start to make progress in improving the parameters theta so we don't need to scan through the entire training set  We just need to look at the first 10 examples and this will start letting us make progress and then we can look at the second ten examples and modify the parameters a little bit again and so on  So  that is why Mini-batch gradient descent can be faster than batch gradient descent  Namely  you can start making progress in modifying the parameters after looking at just ten examples rather than needing to wait 'till you've scan through every single training example of 300 million of them  So  how about Mini-batch gradient descent versus Stochastic gradient descent  So  why do we want to look at b examples at a time rather than look at just a single example at a time as the Stochastic gradient descent? The answer is in vectorization  In particular  Mini-batch gradient descent is likely to outperform Stochastic gradient descent only if you have a good vectorized implementation  In that case  the sum over 10 examples can be performed in a more vectorized way which will allow you to partially parallelize your computation over the ten examples  So  in other words  by using appropriate vectorization to compute the rest of the terms  you can sometimes partially use the good numerical algebra libraries and parallelize your gradient computations over the b examples  whereas if you were looking at just a single example of time with Stochastic gradient descent then  you know  just looking at one example at a time their isn't much to parallelize over  At least there is less to parallelize over  One disadvantage of Mini-batch gradient descent is that there is now this extra parameter b  the Mini-batch size which you may have to fiddle with  and which may therefore take time  But if you have a good vectorized implementation this can sometimes run even faster that Stochastic gradient descent  So that was Mini-batch gradient descent which is an algorithm that in some sense does something that's somewhat in between what Stochastic gradient descent does and what Batch gradient descent does  And if you choose their reasonable value of b  I usually use b equals 10  but  you know  other values  anywhere from say 2 to 100  would be reasonably common  So we choose value of b and if you use a good vectorized implementation  sometimes it can be faster than both Stochastic gradient descent and faster than Batch gradient descent "
OU3jI8j7Ozw,Stochastic Gradient Descent Convergence  You now know about the stochastic gradient descent algorithm  But when you're running the algorithm  how do you make sure that it's completely debugged and is converging okay? Equally important  how do you tune the learning rate alpha with Stochastic Gradient Descent  In this video we'll talk about some techniques for doing these things  for making sure it's converging and for picking the learning rate alpha  Back when we were using batch gradient descent  our standard way for making sure that gradient descent was converging was we would plot the optimization cost function as a function of the number of iterations  So that was the cost function and we would make sure that this cost function is decreasing on every iteration  When the training set sizes were small  we could do that because we could compute the sum pretty efficiently  But when you have a massive training set size then you don't want to have to pause your algorithm periodically  You don't want to have to pause stochastic gradient descent periodically in order to compute this cost function since it requires a sum of your entire training set size  And the whole point of stochastic gradient was that you wanted to start to make progress after looking at just a single example without needing to occasionally scan through your entire training set right in the middle of the algorithm  just to compute things like the cost function of the entire training set  So for stochastic gradient descent  in order to check the algorithm is converging  here's what we can do instead  Let's take the definition of the cost that we had previously  So the cost of the parameters theta with respect to a single training example is just one half of the square error on that training example  Then  while stochastic gradient descent is learning  right before we train on a specific example  So  in stochastic gradient descent we're going to look at the examples xi  yi  in order  and then sort of take a little update with respect to this example  And we go on to the next example  xi plus 1  yi plus 1  and so on  right? That's what stochastic gradient descent does  So  while the algorithm is looking at the example xi  yi  but before it has updated the parameters theta using that an example  let's compute the cost of that example  Just to say the same thing again  but using slightly different words  A stochastic gradient descent is scanning through our training set right before we have updated theta using a specific training example x(i) comma y(i) let's compute how well our hypothesis is doing on that training example  And we want to do this before updating theta because if we've just updated theta using example  you know  that it might be doing better on that example than what would be representative  Finally  in order to check for the convergence of stochastic gradient descent  what we can do is every  say  every thousand iterations  we can plot these costs that we've been computing in the previous step  We can plot those costs average over  say  the last thousand examples processed by the algorithm  And if you do this  it kind of gives you a running estimate of how well the algorithm is doing  on  you know  the last 1000 training examples that your algorithm has seen  So  in contrast to computing J<u>train periodically which needed to scan through the entire training set </u> With this other procedure  well  as part of stochastic gradient descent  it doesn't cost much to compute these costs as well right before updating to parameter theta  And all we're doing is every thousand integrations or so  we just average the last 1 000 costs that we computed and plot that  And by looking at those plots  this will allow us to check if stochastic gradient descent is converging  So here are a few examples of what these plots might look like  Suppose you have plotted the cost average over the last thousand examples  because these are averaged over just a thousand examples  they are going to be a little bit noisy and so  it may not decrease on every single iteration  Then if you get a figure that looks like this  So the plot is noisy because it's average over  you know  just a small subset  say a thousand training examples  If you get a figure that looks like this  you know that would be a pretty decent run with the algorithm  maybe  where it looks like the cost has gone down and then this plateau that looks kind of flattened out  you know  starting from around that point  look like  this is what your cost looks like then maybe your learning algorithm has converged  If you want to try using a smaller learning rate  something you might see is that the algorithm may initially learn more slowly so the cost goes down more slowly  But then eventually you have a smaller learning rate is actually possible for the algorithm to end up at a  maybe very slightly better solution  So the red line may represent the behavior of stochastic gradient descent using a slower  using a smaller leaning rate  And the reason this is the case is because  you remember  stochastic gradient descent doesn't just converge to the global minimum  is that what it does is the parameters will oscillate a bit around the global minimum  And so by using a smaller learning rate  you'll end up with smaller oscillations  And sometimes this little difference will be negligible and sometimes with a smaller than you can get a slightly better value for the parameters  Here are some other things that might happen  Let's say you run stochastic gradient descent and you average over a thousand examples when plotting these costs  So  you know  here might be the result of another one of these plots  Then again  it kind of looks like it's converged  If you were to take this number  a thousand  and increase to averaging over 5 thousand examples  Then it's possible that you might get a smoother curve that looks more like this  And by averaging over  say 5 000 examples instead of 1 000  you might be able to get a smoother curve like this  And so that's the effect of increasing the number of examples you average over  The disadvantage of making this too big of course is that now you get one date point only every 5 000 examples  And so the feedback you get on how well your learning learning algorithm is doing is  sort of  maybe it's more delayed because you get one data point on your plot only every 5 000 examples rather than every 1 000 examples  Along a similar vein some times you may run a gradient descent and end up with a plot that looks like this  And with a plot that looks like this  you know  it looks like the cost just is not decreasing at all  It looks like the algorithm is just not learning  It's just  looks like this here a flat curve and the cost is just not decreasing  But again if you were to increase this to averaging over a larger number of examples it is possible that you see something like this red line it looks like the cost actually is decreasing  it's just that the blue line averaging over 2  3 examples  the blue line was too noisy so you couldn't see the actual trend in the cost actually decreasing and possibly averaging over 5 000 examples instead of 1 000 may help  Of course we averaged over a larger number examples that we've averaged here over 5 000 examples  I'm just using a different color  it is also possible that you that see a learning curve ends up looking like this  That it's still flat even when you average over a larger number of examples  And as you get that  then that's maybe just a more firm verification that unfortunately the algorithm just isn't learning much for whatever reason  And you need to either change the learning rate or change the features or change something else about the algorithm  Finally  one last thing that you might see would be if you were to plot these curves and you see a curve that looks like this  where it actually looks like it's increasing  And if that's the case then this is a sign that the algorithm is diverging  And what you really should do is use a smaller value of the learning rate alpha  So hopefully this gives you a sense of the range of phenomena you might see when you plot these cost average over some range of examples as well as suggests the sorts of things you might try to do in response to seeing different plots  So if the plots looks too noisy  or if it wiggles up and down too much  then try increasing the number of examples you're averaging over so you can see the overall trend in the plot better  And if you see that the errors are actually increasing  the costs are actually increasing  try using a smaller value of alpha  Finally  it's worth examining the issue of the learning rate just a little bit more  We saw that when we run stochastic gradient descent  the algorithm will start here and sort of meander towards the minimum And then it won't really converge  and instead it'll wander around the minimum forever  And so you end up with a parameter value that is hopefully close to the global minimum that won't be exact at the global minimum  In most typical implementations of stochastic gradient descent  the learning rate alpha is typically held constant  And so what you we end up is exactly a picture like this  If you want stochastic gradient descent to actually converge to the global minimum  there's one thing which you can do which is you can slowly decrease the learning rate alpha over time  So  a pretty typical way of doing that would be to set alpha equals some constant 1 divided by iteration number plus constant 2  So  iteration number is the number of iterations you've run of stochastic gradient descent  so it's really the number of training examples you've seen And const 1 and const 2 are additional parameters of the algorithm that you might have to play with a bit in order to get good performance  One of the reasons people tend not to do this is because you end up needing to spend time playing with these 2 extra parameters  constant 1 and constant 2  and so this makes the algorithm more finicky  You know  it's just more parameters able to fiddle with in order to make the algorithm work well  But if you manage to tune the parameters well  then the picture you can get is that the algorithm will actually around towards the minimum  but as it gets closer because you're decreasing the learning rate the meanderings will get smaller and smaller until it pretty much just to the global minimum  I hope this makes sense  right? And the reason this formula makes sense is because as the algorithm runs  the iteration number becomes large So alpha will slowly become small  and so you take smaller and smaller steps until it hopefully converges to the global minimum  So If you do slowly decrease alpha to zero you can end up with a slightly better hypothesis  But because of the extra work needed to fiddle with the constants and because frankly usually we're pretty happy with any parameter value that is  you know  pretty close to the global minimum  Typically this process of decreasing alpha slowly is usually not done and keeping the learning rate alpha constant is the more common application of stochastic gradient descent although you will see people use either version  To summarize in this video we talk about a way for approximately monitoring how the stochastic gradient descent is doing in terms for optimizing the cost function  And this is a method that does not require scanning over the entire training set periodically to compute the cost function on the entire training set  But instead it looks at say only the last thousand examples or so  And you can use this method both to make sure the stochastic gradient descent is okay and is converging or to use it to tune the learning rate alpha 
lNkhfuLjK3s,"Online Learning  In this video  I'd like to talk about a new large-scale machine learning setting called the online learning setting  The online learning setting allows us to model problems where we have a continuous flood or a continuous stream of data coming in and we would like an algorithm to learn from that  Today  many of the largest websites  or many of the largest website companies use different versions of online learning algorithms to learn from the flood of users that keep on coming to  back to the website  Specifically  if you have a continuous stream of data generated by a continuous stream of users coming to your website  what you can do is sometimes use an online learning algorithm to learn user preferences from the stream of data and use that to optimize some of the decisions on your website  Suppose you run a shipping service  so  you know  users come and ask you to help ship their package from location A to location B and suppose you run a website  where users repeatedly come and they tell you where they want to send the package from  and where they want to send it to (so the origin and destination) and your website offers to ship the package for some asking price  so I'll ship your package for $50  I'll ship it for $20  And based on the price that you offer to the users  the users sometimes chose to use a shipping service  that's a positive example and sometimes they go away and they do not choose to purchase your shipping service  So let's say that we want a learning algorithm to help us to optimize what is the asking price that we want to offer to our users  And specifically  let's say we come up with some sort of features that capture properties of the users  If we know anything about the demographics  they capture  you know  the origin and destination of the package  where they want to ship the package  And what is the price that we offer to them for shipping the package  and what we want to do is learn what is the probability that they will elect to ship the package  using our shipping service given these features  and again just as a reminder these features X also captures the price that we're asking for  And so if we could estimate the chance that they'll agree to use our service for any given price  then we can try to pick a price so that they have a pretty high probability of choosing our website while simultaneously hopefully offering us a fair return  offering us a fair profit for shipping their package  So if we can learn this property of y equals 1 given any price and given the other features we could really use this to choose appropriate prices as new users come to us  So in order to model the probability of y equals 1  what we can do is use logistic regression or neural network or some other algorithm like that  But let's start with logistic regression  Now if you have a website that just runs continuously  here's what an online learning algorithm would do  I'm gonna write repeat forever  This just means that our website is going to  you know  keep on staying up  What happens on the website is occasionally a user will come and for the user that comes we'll get some x y pair corresponding to a customer or to a user on the website  So the features x are  you know  the origin and destination specified by this user and the price that we happened to offer to them this time around  and y is either one or zero depending one whether or not they chose to use our shipping service  Now once we get this {x y} pair  what an online learning algorithm does is then update the parameters theta using just this example x y  and in particular we would update my parameters theta as Theta j get updated as Theta j minus the learning rate alpha times my usual gradient descent rule for logistic regression  So we do this for j equals zero up to n  and that's my close curly brace  So  for other learning algorithms instead of writing X-Y  right  I was writing things like Xi  Yi but in this online learning setting where actually discarding the notion of there being a fixed training set instead we have an algorithm  Now what happens as we get an example and then we learn using that example like so and then we throw that example away  We discard that example and we never use it again and so that's why we just look at one example at a time  We learn from that example  We discard it  Which is why  you know  we're also doing away with this notion of there being this sort of fixed training set indexed by i  And  if you really run a major website where you really have a continuous stream of users coming  then this sort of online learning algorithm is actually a pretty reasonable algorithm  Because of data is essentially free if you have so much data  that data is essentially unlimited then there is really may be no need to look at a training example more than once  Of course if we had only a small number of users then rather than using an online learning algorithm like this  you might be better off saving away all your data in a fixed training set and then running some algorithm over that training set  But if you really have a continuous stream of data  then an online learning algorithm can be very effective  I should mention also that one interesting effect of this sort of online learning algorithm is that it can adapt to changing user preferences  And in particular  if over time because of changes in the economy maybe users start to become more price sensitive and willing to pay  you know  less willing to pay high prices  Or if they become less price sensitive and they're willing to pay higher prices  Or if different things become more important to users  if you start to have new types of users coming to your website  This sort of online learning algorithm can also adapt to changing user preferences and kind of keep track of what your changing population of users may be willing to pay for  And it does that because if your pool of users changes  then these updates to your parameters theta will just slowly adapt your parameters to whatever your latest pool of users looks like  Here's another example of a sort of application to which you might apply online learning  this is an application in product search in which we want to apply learning algorithm to learn to give good search listings to a user  Let's say you run an online store that sells phones - that sells mobile phones or sells cell phones  And you have a user interface where a user can come to your website and type in the query like \""Android phone 1080p camera\""  So 1080p is a type of a specification for a video camera that you might have on a phone  a cell phone  a mobile phone  Suppose  suppose we have a hundred phones in our store  And because of the way our website is laid out  when a user types in a query  if it was a search query  we would like to find a choice of ten different phones to show what to offer to the user  What we'd like to do is have a learning algorithm help us figure out what are the ten phones out of the 100 we should return the user in response to a user-search query like the one here  Here's how we can go about the problem  For each phone and given a specific user query  we can construct a feature vector X  So the feature vector X might capture different properties of the phone  It might capture things like  how similar the user search query is in the phones  We capture things like how many words in the user search query match the name of the phone  how many words in the user search query match the description of the phone and so on  So the features x capture properties of the phone and it captures things about how similar or how well the phone matches the user query along different dimensions  What we like to do is estimate the probability that a user will click on the link for a specific phone  because we want to show the user phones that they are likely to want to buy  want to show the user phones that they have high probability of clicking on in the web browser  So I'm going to define y equals one if the user clicks on the link for a phone and y equals zero otherwise and what I would like to do is learn the probability the user will click on a specific phone given  you know  the features x  which capture properties of the phone and how well the query matches the phone  To give this problem a name in the language of people that run websites like this  the problem of learning this is actually called the problem of learning the predicted click-through rate  the predicted CTR  It just means learning the probability that the user will click on the specific link that you offer them  so CTR is an abbreviation for click through rate  And if you can estimate the predicted click-through rate for any particular phone  what we can do is use this to show the user the ten phones that are most likely to click on  because out of the hundred phones  we can compute this for each of the 100 phones and just select the 10 phones that the user is most likely to click on  and this will be a pretty reasonable way to decide what ten results to show to the user  Just to be clear  suppose that every time a user does a search  we return ten results what that will do is it will actually give us ten x y pairs  this actually gives us ten training examples every time a user comes to our website because  because for the ten phone that we chose to show the user  for each of those 10 phones we get a feature vector X  and for each of those 10 phones we show the user we will also get a value for y  we will also observe the value of y  depending on whether or not we clicked on that url or not and so  one way to run a website like this would be to continuously show the user  you know  your ten best guesses for what other phones they might like and so  each time a user comes you would get ten examples  ten x y pairs  and then use an online learning algorithm to update the parameters using essentially 10 steps of gradient descent on these 10 examples  and then you can throw the data away  and if you really have a continuous stream of users coming to your website  this would be a pretty reasonable way to learn parameters for your algorithm so as to show the ten phones to your users that may be most promising and the most likely to click on  So  this is a product search problem or learning to rank phones  learning to search for phones example  So  I'll quickly mention a few others  One is  if you have a website and you're trying to decide  you know  what special offer to show the user  this is very similar to phones  or if you have a website and you show different users different news articles  So  if you're a news aggregator website  then you can again use a similar system to select  to show to the user  you know  what are the news articles that they are most likely to be interested in and what are the news articles that they are most likely to click on  Closely related to special offers  will we profit from recommendations  And in fact  if you have a collaborative filtering system  you can even imagine a collaborative filtering system giving you additional features to feed into a logistic regression classifier to try to predict the click through rate for different products that you might recommend to a user  Of course  I should say that any of these problems could also have been formulated as a standard machine learning problem  where you have a fixed training set  Maybe  you can run your website for a few days and then save away a training set  a fixed training set  and run a learning algorithm on that  But these are the actual sorts of problems  where you do see large companies get so much data  that there's really maybe no need to save away a fixed training set  but instead you can use an online learning algorithm to just learn continuously  from the data that users are generating on your website  So  that was the online learning setting and as we saw  the algorithm that we apply to it is really very similar to this schotastic gradient descent algorithm  only instead of scanning through a fixed training set  we're instead getting one example from a user  learning from that example  then discarding it and moving on  And if you have a continuous stream of data for some application  this sort of algorithm may be well worth considering for your application  And of course  one advantage of online learning is also that if you have a changing pool of users  or if the things you're trying to predict are slowly changing like your user taste is slowly changing  the online learning algorithm can slowly adapt your learned hypothesis to whatever the latest sets of user behaviors are like as well "
nKGjVQWPf4w,Map Reduce and Data Parallelism  In the last few videos  we talked about stochastic gradient descent  and  you know  other variations of the stochastic gradient descent algorithm  including those adaptations to online learning  but all of those algorithms could be run on one machine  or could be run on one computer  And some machine learning problems are just too big to run on one machine  sometimes maybe you just so much data you just don't ever want to run all that data through a single computer  no matter what algorithm you would use on that computer  So in this video I'd like to talk about different approach to large scale machine learning  called the map reduce approach  And even though we have quite a few videos on stochastic gradient descent and we're going to spend relative less time on map reduce--don't judge the relative importance of map reduce versus the gradient descent based on the amount amount of time I spend on these ideas in particular  Many people will say that map reduce is at least an equally important  and some would say an even more important idea compared to gradient descent  only it's relatively simpler to explain  which is why I'm going to spend less time on it  but using these ideas you might be able to scale learning algorithms to even far larger problems than is possible using stochastic gradient descent  Here's the idea  Let's say we want to fit a linear regression model or a logistic regression model or some such  and let's start again with batch gradient descent  so that's our batch gradient descent learning rule  And to keep the writing on this slide tractable  I'm going to assume throughout that we have m equals 400 examples  Of course  by our standards  in terms of large scale machine learning  you know m might be pretty small and so  this might be more commonly applied to problems  where you have maybe closer to 400 million examples  or some such  but just to make the writing on the slide simpler  I'm going to pretend we have 400 examples  So in that case  the batch gradient descent learning rule has this 400 and the sum from i equals 1 through 400 through my 400 examples here  and if m is large  then this is a computationally expensive step  So  what the MapReduce idea does is the following  and I should say the map reduce idea is due to two researchers  Jeff Dean and Sanjay Gimawat  Jeff Dean  by the way  is one of the most legendary engineers in all of Silicon Valley and he kind of built a large fraction of the architectural infrastructure that all of Google runs on today  But here's the map reduce idea  So  let's say I have some training set  if we want to denote by this box here of X Y pairs  where it's X1  Y1  down to my 400 examples  Xm  Ym  So  that's my training set with 400 training examples  In the MapReduce idea  one way to do  is split this training set in to different subsets  I'm going to  assume for this example that I have 4 computers  or 4 machines to run in parallel on my training set  which is why I'm splitting this into 4 machines  If you have 10 machines or 100 machines  then you would split your training set into 10 pieces or 100 pieces or what have you  And what the first of my 4 machines is to do  say  is use just the first one quarter of my training set--so use just the first 100 training examples  And in particular  what it's going to do is look at this summation  and compute that summation for just the first 100 training examples  So let me write that up I'm going to compute a variable temp 1 to superscript 1 the first machine J equals sum from equals 1 through 100  and then I'm going to plug in exactly that term there--so I have X-theta  Xi  minus Yi times Xij  right? So that's just that gradient descent term up there  And then similarly  I'm going to take the second quarter of my data and send it to my second machine  and my second machine will use training examples 101 through 200 and you will compute similar variables of a temp to j which is the same sum for index from examples 101 through 200  And similarly machines 3 and 4 will use the third quarter and the fourth quarter of my training set  So now each machine has to sum over 100 instead of over 400 examples and so has to do only a quarter of the work and thus presumably it could do it about four times as fast  Finally  after all these machines have done this work  I am going to take these temp variables and put them back together  So I take these variables and send them all to a You know centralized master server and what the master will do is combine these results together  and in particular  it will update my parameters theta j according to theta j gets updated as theta j minus Of the learning rate alpha times one over 400 times temp  1  J  plus temp 2j plus temp 3j plus temp 4j and of course we have to do this separately for J equals 0  You know  up to and within this number of features  So operating this equation into I hope it's clear  So what this equation is doing is exactly the same is that when you have a centralized master server that takes the results  the ten one j the ten two j ten three j and ten four j and adds them up and so of course the sum of these four things  Right  that's just the sum of this  plus the sum of this  plus the sum of this  plus the sum of that  and those four things just add up to be equal to this sum that we're originally computing a batch stream descent  And then we have the alpha times 1 of 400  alpha times 1 of 100  and this is exactly equivalent to the batch gradient descent algorithm  only  instead of needing to sum over all four hundred training examples on just one machine  we can instead divide up the work load on four machines  So  here's what the general picture of the MapReduce technique looks like  We have some training sets  and if we want to paralyze across four machines  we are going to take the training set and split it  you know  equally  Split it as evenly as we can into four subsets  Then we are going to take the 4 subsets of the training data and send them to 4 different computers  And each of the 4 computers can compute a summation over just one quarter of the training set  and then finally take each of the computers takes the results  sends them to a centralized server  which then combines the results together  So  on the previous line in that example  the bulk of the work in gradient descent  was computing the sum from i equals 1 to 400 of something  So more generally  sum from i equals 1 to m of that formula for gradient descent  And now  because each of the four computers can do just a quarter of the work  potentially you can get up to a 4x speed up  In particular  if there were no network latencies and no costs of the network communications to send the data back and forth  you can potentially get up to a 4x speed up  Of course  in practice  because of network latencies  the overhead of combining the results afterwards and other factors  in practice you get slightly less than a 4x speedup  But  none the less  this sort of macro juice approach does offer us a way to process much larger data sets than is possible using a single computer  If you are thinking of applying Map Reduce to some learning algorithm  in order to speed this up  By paralleling the computation over different computers  the key question to ask yourself is  can your learning algorithm be expressed as a summation over the training set? And it turns out that many learning algorithms can actually be expressed as computing sums of functions over the training set and the computational expense of running them on large data sets is because they need to sum over a very large training set  So  whenever your learning algorithm can be expressed as a sum of the training set and whenever the bulk of the work of the learning algorithm can be expressed as the sum of the training set  then map reviews might a good candidate for scaling your learning algorithms through very  very good data sets  Lets just look at one more example  Let's say that we want to use one of the advanced optimization algorithm  So  things like  you know  l  b  f  g  s constant gradient and so on  and let's say we want to train a logistic regression of the algorithm  For that  we need to compute two main quantities  One is for the advanced optimization algorithms like  you know  LPF and constant gradient  We need to provide it a routine to compute the cost function of the optimization objective  And so for logistic regression  you remember that a cost function has this sort of sum over the training set  and so if youre paralizing over ten machines  you would split up the training set onto ten machines and have each of the ten machines compute the sum of this quantity over just one tenth of the training data  Then  the other thing that the advanced optimization algorithms need  is a routine to compute these partial derivative terms  Once again  these derivative terms  for which it's a logistic regression  can be expressed as a sum over the training set  and so once again  similar to our earlier example  you would have each machine compute that summation over just some small fraction of your training data  And finally  having computed all of these things  they could then send their results to a centralized server  which can then add up the partial sums  This corresponds to adding up those tenth i or tenth ij variables  which were computed locally on machine number i  and so the centralized server can sum these things up and get the overall cost function and get the overall partial derivative  which you can then pass through the advanced optimization algorithm  So  more broadly  by taking other learning algorithms and expressing them in sort of summation form or by expressing them in terms of computing sums of functions over the training set  you can use the MapReduce technique to parallelize other learning algorithms as well  and scale them to very large training sets  Finally  as one last comment  so far we have been discussing MapReduce algorithms as allowing you to parallelize over multiple computers  maybe multiple computers in a computer cluster or over multiple computers in the data center  It turns out that sometimes even if you have just a single computer  MapReduce can also be applicable  In particular  on many single computers now  you can have multiple processing cores  You can have multiple CPUs  and within each CPU you can have multiple proc cores  If you have a large training set  what you can do if  say  you have a computer with 4 computing cores  what you can do is  even on a single computer you can split the training sets into pieces and send the training set to different cores within a single box  like within a single desktop computer or a single server and use MapReduce this way to divvy up work load  Each of the cores can then carry out the sum over  say  one quarter of your training set  and then they can take the partial sums and combine them  in order to get the summation over the entire training set  The advantage of thinking about MapReduce this way  as paralyzing over cause within a single machine  rather than parallelizing over multiple machines is that  this way you don't have to worry about network latency  because all the communication  all the sending of the [xx] back and forth  all that happens within a single machine  And so network latency becomes much less of an issue compared to if you were using this to over different computers within the data sensor  Finally  one last caveat on parallelizing within a multi-core machine  Depending on the details of your implementation  if you have a multi-core machine and if you have certain numerical linear algebra libraries  It turns out that the sum numerical linear algebra libraries that can automatically parallelize their linear algebra operations across multiple cores within the machine  So if you're fortunate enough to be using one of those numerical linear algebra libraries and certainly this does not apply to every single library  If you're using one of those libraries and  If you have a very good vectorizing implementation of the learning algorithm  Sometimes you can just implement you standard learning algorithm in a vectorized fashion and not worry about parallelization and numerical linear algebra libararies could take care of some of it for you  So you don't need to implement [xx] but  for other any problems  taking advantage of this sort of map reducing commentation  finding and using this MapReduce formulation and to paralelize a cross coarse except yourself might be a good idea as well and could let you speed up your learning algorithm  In this video  we talked about the MapReduce approach to parallelizing machine learning by taking a data and spreading them across many computers in the data center  Although these ideas are critical to paralysing across multiple cores within a single computer as well  Today there are some good open source implementations of MapReduce  so there are many users in open source system called Hadoop and using either your own implementation or using someone else's open source implementation  you can use these ideas to parallelize learning algorithms and get them to run on much larger data sets than is possible using just a single machine 
Nf08BQbZHMQ,"Problem Description and Pipeline  In this and the next few videos  I want to tell you about a machine learning application example  or a machine learning application history centered around an application called Photo OCR   There are three reasons why I want to do this  first I wanted to show you an example of how a complex machine learning system can be put together  Second  once told the concepts of a machine learning a type line and how to allocate resources when you're trying to decide what to do next  And this can either be in the context of you working by yourself on the big application Or it can be the context of a team of developers trying to build a complex application together  And then finally  the Photo OCR problem also gives me an excuse to tell you about just a couple more interesting ideas for machine learning  One is some ideas of how to apply machine learning to computer vision problems  and second is the idea of artificial data synthesis  which we'll see in a couple of videos  So  let's start by talking about what is the Photo OCR problem  Photo OCR stands for Photo Optical Character Recognition  With the growth of digital photography and more recently the growth of camera in our cell phones we now have tons of visual pictures that we take all over the place  And one of the things that has interested many developers is how to get our computers to understand the content of these pictures a little bit better  The photo OCR problem focuses on how to get computers to read the text to the purest in images that we take  Given an image like this it might be nice if a computer can read the text in this image so that if you're trying to look for this picture again you type in the words  lulu bees and and have it automatically pull up this picture  so that you're not spending lots of time digging through your photo collection Maybe hundreds of thousands of pictures in  The Photo OCR problem does exactly this  and it does so in several steps  First  given the picture it has to look through the image and detect where there is text in the picture  And after it has done that or if it successfully does that it then has to look at these text regions and actually read the text in those regions  and hopefully if it reads it correctly  it'll come up with these transcriptions of what is the text that appears in the image  Whereas OCR  or optical character recognition of scanned documents is relatively easier problem  doing OCR from photographs today is still a very difficult machine learning problem  and you can do this  Not only can this help our computers to understand the content of our though images better  there are also applications like helping blind people  for example  if you could provide to a blind person a camera that can look at what's in front of them  and just tell them the words that my be on the street sign in front of them  With car navigation systems  For example  imagine if your car could read the street signs and help you navigate to your destination  In order to perform photo OCR  here's what we can do  First we can go through the image and find the regions where there's text and image  So  shown here is one example of text and image that the photo OCR system may find  Second  given the rectangle around that text region  we can then do character segmentation  where we might take this text box that says \""Antique Mall\"" and try to segment it out into the locations of the individual characters  And finally  having segmented out into individual characters  we can then run a crossfire  which looks at the images of the visual characters  and tries to figure out the first character's an A  the second character's an N  the third character is a T  and so on  so that up by doing all this how that hopefully you can then figure out that this phrase is Rulegee's antique mall and similarly for some of the other words that appear in that image  I should say that there are some photo OCR systems that do even more complex things  like a bit of spelling correction at the end  So if  for example  your character segmentation and character classification system tells you that it sees the word c 1 e a n i n g  Then  you know  a sort of spelling correction system might tell you that this is probably the word 'cleaning'  and your character classification algorithm had just mistaken the l for a 1  But for the purpose of what we want to do in this video  let's ignore this last step and just focus on the system that does these three steps of text detection  character segmentation  and character classification  A system like this is what we call a machine learning pipeline  In particular  here's a picture showing the photo OCR pipeline  We have an image  which then fed to the text detection system text regions  we then segment out the characters--the individual characters in the text--and then finally we recognize the individual characters  In many complex machine learning systems  these sorts of pipelines are common  where you can have multiple modules--in this example  the text detection  character segmentation  character recognition modules--each of which may be machine learning component  or sometimes it may not be a machine learning component but to have a set of modules that act one after another on some piece of data in order to produce the output you want  which in the photo OCR example is to find the transcription of the text that appeared in the image  If you're designing a machine learning system one of the most important decisions will often be what exactly is the pipeline that you want to put together  In other words  given the photo OCR problem  how do you break this problem down into a sequence of different modules  And you design the pipeline and each the performance of each of the modules in your pipeline  will often have a big impact on the final performance of your algorithm  If you have a team of engineers working on a problem like this is also very common to have different individuals work on different modules  So I could easily imagine tech easily being the of anywhere from 1 to 5 engineers  character segmentation maybe another 1-5 engineers  and character recognition being another 1-5 engineers  and so having a pipeline like often offers a natural way to divide up the workload amongst different members of an engineering team  as well  Although  or course  all of this work could also be done by just one person if that's how you want to do it  In complex machine learning systems the idea of a pipeline  of a machine of a pipeline  is pretty pervasive  And what you just saw is a specific example of how a Photo OCR pipeline might work  In the next few videos I'll tell you a little bit more about this pipeline  and we'll continue to use this as an example to illustrate--I think--a few more key concepts of machine learning "
63M6vaCXQ3c,"Sliding Windows  In the previous video  we talked about the photo OCR pipeline and how that worked  In which we would take an image and pass the Through a sequence of machine learning components in order to try to read the text that appears in an image  In this video I like to  A little bit more about how the individual components of the pipeline works  In particular most of this video will center around the discussion  of whats called a sliding windows  The first stage of the filter was the Text detection where we look at an image like this and try to find the regions of text that appear in this image  Text detection is an unusual problem in computer vision  Because depending on the length of the text you're trying to find  these rectangles that you're trying to find can have different aspect  So in order to talk about detecting things in images let's start with a simpler example of pedestrian detection and we'll then later go back to  Ideas that were developed in pedestrian detection and apply them to text detection  So in pedestrian detection you want to take an image that looks like this and the whole idea is the individual pedestrians that appear in the image  So there's one pedestrian that we found  there's a second one  a third one a fourth one  a fifth one  And a one  This problem is maybe slightly simpler than text detection just for the reason that the aspect ratio of most pedestrians are pretty similar  Just using a fixed aspect ratio for these rectangles that we're trying to find  So by aspect ratio I mean the ratio between the height and the width of these rectangles  They're all the same  for different pedestrians but for text detection the height and width ratio is different for different lines of text Although for pedestrian detection  the pedestrians can be different distances away from the camera and so the height of these rectangles can be different depending on how far away they are  but the aspect ratio is the same  In order to build a pedestrian detection system here's how you can go about it  Let's say that we decide to standardize on this aspect ratio of 82 by 36 and we could have chosen some rounded number like 80 by 40 or something  but 82 by 36 seems alright  What we would do is then go out and collect large training sets of positive and negative examples  Here are examples of 82 X 36 image patches that do contain pedestrians and here are examples of images that do not  On this slide I show 12 positive examples of y1 and 12 examples of y0  In a more typical pedestrian detection application  we may have anywhere from a 1 000 training examples up to maybe 10 000 training examples  or even more if you can get even larger training sets  And what you can do  is then train in your network or some other learning algorithm to take this input  an MS patch of dimension 82 by 36  and to classify 'y' and to classify that image patch as either containing a pedestrian or not  So this gives you a way of applying supervised learning in order to take an image patch can determine whether or not a pedestrian appears in that image capture  Now  lets say we get a new image  a test set image like this and we want to try to find a pedestrian's picture image  What we would do is start by taking a rectangular patch of this image  Like that shown up here  so that's maybe a 82 X 36 patch of this image  and run that image patch through our classifier to determine whether or not there is a pedestrian in that image patch  and hopefully our classifier will return y equals 0 for that patch  since there is no pedestrian  Next  we then take that green rectangle and we slide it over a bit and then run that new image patch through our classifier to decide if there's a pedestrian there  And having done that  we then slide the window further to the right and run that patch through the classifier again  The amount by which you shift the rectangle over each time is a parameter  that's sometimes called the step size of the parameter  sometimes also called the slide parameter  and if you step this one pixel at a time  So you can use the step size or stride of 1  that usually performs best  that is more cost effective  and so using a step size of maybe 4 pixels at a time  or eight pixels at a time or some large number of pixels might be more common  since you're then moving the rectangle a little bit more each time  So  using this process  you continue stepping the rectangle over to the right a bit at a time and running each of these patches through a classifier  until eventually  as you slide this window over the different locations in the image  first starting with the first row and then we go further rows in the image  you would then run all of these different image patches at some step size or some stride through your classifier  Now  that was a pretty small rectangle  that would only detect pedestrians of one specific size  What we do next is start to look at larger image patches  So now let's take larger images patches  like those shown here and run those through the crossfire as well  And by the way when I say take a larger image patch  what I really mean is when you take an image patch like this  what you're really doing is taking that image patch  and resizing it down to 82 X 36  say  So you take this larger patch and re-size it to be smaller image and then it would be the smaller size image that is what you would pass through your classifier to try and decide if there is a pedestrian in that patch  And finally you can do this at an even larger scales and run that side of Windows to the end And after this whole process hopefully your algorithm will detect whether theres pedestrian appears in the image  so thats how you train a the classifier  and then use a sliding windows classifier  or use a sliding windows detector in order to find pedestrians in the image  Let's have a turn to the text detection example and talk about that stage in our photo OCR pipeline  where our goal is to find the text regions in unit  similar to pedestrian detection you can come up with a label training set with positive examples and negative examples with examples corresponding to regions where text appears  So instead of trying to detect pedestrians  we're now trying to detect texts  And so positive examples are going to be patches of images where there is text  And negative examples is going to be patches of images where there isn't text  Having trained this we can now apply it to a new image  into a test set image  So here's the image that we've been using as example  Now  last time we run  for this example we are going to run a sliding windows at just one fixed scale just for purpose of illustration  meaning that I'm going to use just one rectangle size  But lets say I run my little sliding windows classifier on lots of little image patches like this if I do that  what Ill end up with is a result like this where the white region show where my text detection system has found text and so the axis' of these two figures are the same  So there is a region up here  of course also a region up here  so the fact that this black up here represents that the classifier does not think it's found any texts up there  whereas the fact that there's a lot of white stuff here  that reflects that classifier thinks that it's found a bunch of texts  over there on the image  What i have done on this image on the lower left is actually use white to show where the classifier thinks it has found text  And different shades of grey correspond to the probability that was output by the classifier  so like the shades of grey corresponds to where it thinks it might have found text but has lower confidence the bright white response to whether the classifier  up with a very high probability  estimated probability of there being pedestrians in that location  We aren't quite done yet because what we actually want to do is draw rectangles around all the region where this text in the image  so were going to take one more step which is we take the output of the classifier and apply to it what is called an expansion operator  So what that does is  it take the image here  and it takes each of the white blobs  it takes each of the white regions and it expands that white region  Mathematically  the way you implement that is  if you look at the image on the right  what we're doing to create the image on the right is  for every pixel we are going to ask  is it withing some distance of a white pixel in the left image  And so  if a specific pixel is within  say  five pixels or ten pixels of a white pixel in the leftmost image  then we'll also color that pixel white in the rightmost image  And so  the effect of this is  we'll take each of the white blobs in the leftmost image and expand them a bit  grow them a little bit  by seeing whether the nearby pixels  the white pixels  and then coloring those nearby pixels in white as well  Finally  we are just about done  We can now look at this right most image and just look at the connecting components and look at the as white regions and draw bounding boxes around them  And in particular  if we look at all the white regions  like this one  this one  this one  and so on  and if we use a simple heuristic to rule out rectangles whose aspect ratios look funny because we know that boxes around text should be much wider than they are tall  And so if we ignore the thin  tall blobs like this one and this one  and we discard these ones because they are too tall and thin  and we then draw a the rectangles around the ones whose aspect ratio thats a height to what ratio looks like for text regions  then we can draw rectangles  the bounding boxes around this text region  this text region  and that text region  corresponding to the Lula B's antique mall logo  the Lula B's  and this little open sign  Of over there  This example by the actually misses one piece of text  This is very hard to read  but there is actually one piece of text there  That says [xx] are corresponding to this but the aspect ratio looks wrong so we discarded that one  So you know it's ok on this image  but in this particular example the classifier actually missed one piece of text  It's very hard to read because there's a piece of text written against a transparent window  So that's text detection using sliding windows  And having found these rectangles with the text in it  we can now just cut out these image regions and then use later stages of pipeline to try to meet the texts  Now  you recall that the second stage of pipeline was character segmentation  so given an image like that shown on top  how do we segment out the individual characters in this image? So what we can do is again use a supervised learning algorithm with some set of positive and some set of negative examples  what were going to do is look in the image patch and try to decide if there is split between two characters right in the middle of that image match  So for initial positive examples  This first cross example  this image patch looks like the middle of it is indeed the middle has splits between two characters and the second example again this looks like a positive example  because if I split two characters by putting a line right down the middle  that's the right thing to do  So  these are positive examples  where the middle of the image represents a gap or a split between two distinct characters  whereas the negative examples  well  you know  you don't want to split two characters right in the middle  and so these are negative examples because they don't represent the midpoint between two characters  So what we will do is  we will train a classifier  maybe using new network  maybe using a different learning algorithm  to try to classify between the positive and negative examples  Having trained such a classifier  we can then run this on this sort of text that our text detection system has pulled out  As we start by looking at that rectangle  and we ask  \""Gee  does it look like the middle of that green rectangle  does it look like the midpoint between two characters?\""  And hopefully  the classifier will say no  then we slide the window over and this is a one dimensional sliding window classifier  because were going to slide the window only in one straight line from left to right  theres no different rows here  There's only one row here  But now  with the classifier in this position  we ask  well  should we split those two characters or should we put a split right down the middle of this rectangle  And hopefully  the classifier will output y equals one  in which case we will decide to draw a line down there  to try to split two characters  Then we slide the window over again  optic process  don't close the gap  slide over again  optic says yes  do split there and so on  and we slowly slide the classifier over to the right and hopefully it will classify this as another positive example and so on  And we will slide this window over to the right  running the classifier at every step  and hopefully it will tell us  you know  what are the right locations to split these characters up into  just split this image up into individual characters  And so thats 1D sliding windows for character segmentation  So  here's the overall photo OCR pipe line again  In this video we've talked about the text detection step  where we use sliding windows to detect text  And we also use a one-dimensional sliding windows to do character segmentation to segment out  you know  this text image in division of characters  The final step through the pipeline is the character qualification step and that step you might already be much more familiar with the early videos on supervised learning where you can apply a standard supervised learning within maybe on your network or maybe something else in order to take it's input  an image like that and classify which alphabet or which 26 characters A to Z  or maybe we should have 36 characters if you have the numerical digits as well  the multi class classification problem where you take it's input and image contained a character and decide what is the character that appears in that image? So that was the photo OCR pipeline and how you can use ideas like sliding windows classifiers in order to put these different components to develop a photo OCR system  In the next few videos we keep on using the problem of photo OCR to explore somewhat interesting issues surrounding building an application like this "
IqdSdbxrTsY,Getting Lots of Data and Artificial Data  I've seen over and over that one of the most reliable ways to get a high performance machine learning system is to take a low bias learning algorithm and to train it on a massive training set  But where did you get so much training data from? Turns out that the machine earnings there's a fascinating idea called artificial data synthesis  this doesn't apply to every single problem  and to apply to a specific problem  often takes some thought and innovation and insight  But if this idea applies to your machine  only problem  it can sometimes be a an easy way to get a huge training set to give to your learning algorithm  The idea of artificial data synthesis comprises of two variations  main the first is if we are essentially creating data from [xx]  creating new data from scratch  And the second is if we already have it's small label training set and we somehow have amplify that training set or use a small training set to turn that into a larger training set and in this video we'll go over both those ideas  To talk about the artificial data synthesis idea  let's use the character portion of the photo OCR pipeline  we want to take it's input image and recognize what character it is  If we go out and collect a large label data set  here's what it is and what it look like  For this particular example  I've chosen a square aspect ratio  So we're taking square image patches  And the goal is to take an image patch and recognize the character in the middle of that image patch  And for the sake of simplicity  I'm going to treat these images as grey scale images  rather than color images  It turns out that using color doesn't seem to help that much for this particular problem  So given this image patch  we'd like to recognize that that's a T  Given this image patch  we'd like to recognize that it's an 'S'  Given that image patch we would like to recognize that as an 'I' and so on  So all of these  our examples of row images  how can we come up with a much larger training set? Modern computers often have a huge font library and if you use a word processing software  depending on what word processor you use  you might have all of these fonts and many  many more Already stored inside  And  in fact  if you go different websites  there are  again  huge  free font libraries on the internet we can download many  many different types of fonts  hundreds or perhaps thousands of different fonts  So if you want more training examples  one thing you can do is just take characters from different fonts and paste these characters against different random backgrounds  So you might take this ---- and paste that c against a random background  If you do that you now have a training example of an image of the character C  So after some amount of work  you know this  and it is a little bit of work to synthisize realistic looking data  But after some amount of work  you can get a synthetic training set like that  Every image shown on the right was actually a synthesized image  Where you take a font  maybe a random font downloaded off the web and you paste an image of one character or a few characters from that font against this other random background image  And then apply maybe a little blurring operators -----of app finder  distortions that app finder  meaning just the sharing and scaling and little rotation operations and if you do that you get a synthetic training set  on what the one shown here  And this is work  grade  it is  it takes thought at work  in order to make the synthetic data look realistic  and if you do a sloppy job in terms of how you create the synthetic data then it actually won't work well  But if you look at the synthetic data looks remarkably similar to the real data  And so by using synthetic data you have essentially an unlimited supply of training examples for artificial training synthesis And so  if you use this source synthetic data  you have essentially unlimited supply of label data to create a improvised learning algorithm for the character recognition problem  So this is an example of artificial data synthesis where youre basically creating new data from scratch  you just generating brand new images from scratch  The other main approach to artificial data synthesis is where you take a examples that you currently have  that we take a real example  maybe from real image  and you create additional data  so as to amplify your training set  So here is an image of a compared to a from a real image  not a synthesized image  and I have overlayed this with the grid lines just for the purpose of illustration  Actually have these ----  So what you can do is then take this alphabet here  take this image and introduce artificial warpings[sp?] or artificial distortions into the image so they can take the image a and turn that into 16 new examples  So in this way you can take a small label training set and amplify your training set to suddenly get a lot more examples  all of it  Again  in order to do this for application  it does take thought and it does take insight to figure out what our reasonable sets of distortions  or whether these are ways that amplify and multiply your training set  and for the specific example of character recognition  introducing these warping seems like a natural choice  but for a different learning machine application  there may be different the distortions that might make more sense  Let me just show one example from the totally different domain of speech recognition  So the speech recognition  let's say you have audio clips and you want to learn from the audio clip to recognize what were the words spoken in that clip  So let's see how one labeled training example  So let's say you have one labeled training example  of someone saying a few specific words  So let's play that audio clip here  0 -1-2-3-4-5  Alright  so someone counting from 0 to 5  and so you want to try to apply a learning algorithm to try to recognize the words said in that  So  how can we amplify the data set? Well  one thing we do is introduce additional audio distortions into the data set  So here I'm going to add background sounds to simulate a bad cell phone connection  When you hear beeping sounds  that's actually part of the audio track  that's nothing wrong with the speakers  I'm going to play this now  0-1-2-3-4-5  Right  so you can listen to that sort of audio clip and recognize the sounds  that seems like another useful training example to have  here's another example  noisy background  Zero  one  two  three four five you know of cars driving past  people walking in the background  here's another one  so taking the original clean audio clip so taking the clean audio of someone saying 0 1 2 3 4 5 we can then automatically synthesize these additional training examples and thus amplify one training example into maybe four different training examples  So let me play this final example  as well  0-1 3-4-5 So by taking just one labelled example  we have to go through the effort to collect just one labelled example fall of the 01205  and by synthesizing additional distortions  by introducing different background sounds  we've now multiplied this one example into many more examples  Much work by just automatically adding these different background sounds to the clean audio Just one word of warning about synthesizing data by introducing distortions  if you try to do this yourself  the distortions you introduce should be representative the source of noises  or distortions  that you might see in the test set  So  for the character recognition example  you know  the working things begin introduced are actually kind of reasonable  because an image A that looks like that  that's  could be an image that we could actually see in a test set Reflect a fact And  you know  that image on the upper-right  that could be an image that we could imagine seeing  And for audio  well  we do wanna recognize speech  even against a bad self internal connection  against different types of background noise  and so for the audio  we're again synthesizing examples are actually representative of the sorts of examples that we want to classify  that we want to recognize correctly  In contrast  usually it does not help perhaps you actually a meaning as noise to your data  I'm not sure you can see this  but what we've done here is taken the image  and for each pixel  in each of these 4 images  has just added some random Gaussian noise to each pixel  To each pixel  is the pixel brightness  it would just add some  you know  maybe Gaussian random noise to each pixel  So it's just a totally meaningless noise  right? And so  unless you're expecting to see these sorts of pixel wise noise in your test set  this sort of purely random meaningless noise is less likely to be useful  But the process of artificial data synthesis it is you know a little bit of an art as well and sometimes you just have to try it and see if it works  But if you're trying to decide what sorts of distortions to add  you know  do think about what other meaningful distortions you might add that will cause you to generate additional training examples that are at least somewhat representative of the sorts of images you expect to see in your test sets  Finally  to wrap up this video  I just wanna say a couple of words  more about this idea of getting loss of data via artificial data synthesis  As always  before expending a lot of effort  you know  figuring out how to create artificial training examples  it's often a good practice is to make sure that you really have a low biased crossfire  and having a lot more training data will be of help  And standard way to do this is to plot the learning curves  and make sure that you only have a low as well  high variance falsifier  Or if you don't have a low bias falsifier  you know  one other thing that's worth trying is to keep increasing the number of features that your classifier has  increasing the number of hidden units in your network  saying  until you actually have a low bias falsifier  and only then  should you put the effort into creating a large  artificial training set  so what you really want to avoid is to  you know  spend a whole week or spend a few months figuring out how to get a great artificially synthesized data set  Only to realize afterward  that  you know  your learning algorithm  performance doesn't improve that much  even when you're given a huge training set  So that's about my usual advice about of a testing that you really can make use of a large training set before spending a lot of effort going out to get that large training set  Second is  when i'm working on machine learning problems  one question I often ask the team I'm working with  often ask my students  which is  how much work would it be to get 10 times as much date as we currently had  When I face a new machine learning application very often I will sit down with a team and ask exactly this question  I've asked this question over and over and over and I've been very surprised how often this answer has been that  You know  it's really not that hard  maybe a few days of work at most  to get ten times as much data as we currently have for a machine running application and very often if you can get ten times as much data there will be a way to make your algorithm do much better  So  you know  if you ever join the product team working on some machine learning application product this is a very good questions ask yourself ask the team don't be too surprised if after a few minutes of brainstorming if your team comes up with a way to get literally ten times this much data  in which case  I think you would be a hero to that team  because with 10 times as much data  I think you'll really get much better performance  just from learning from so much data  So there are several waysand that comprised both the ideas of generating data from scratch using random fonts and so on  As well as the second idea of taking an existing example and and introducing distortions that amplify to enlarge the training set A couple of other examples of ways to get a lot more data are to collect the data or to label them yourself  So one useful calculation that I often do is  you know  how many minutes  how many hours does it take to get a certain number of examples  so actually sit down and figure out  you know  suppose it takes me ten seconds to label one example then and  suppose that  for our application  currently we have 1000 labeled examples examples so ten times as much of that would be if n were equal to ten thousand  A second way to get a lot of data is to just collect the data and you label it yourself  So what I mean by this is I will often set down and do a calculation to figure out how much time  you know just like how many hours will it take  how many hours or how many days will it take for me or for someone else to just sit down and collect ten times as much data  as we have currently  by collecting the data ourselves and labeling them ourselves  So  for example  that  for our machine learning application  currently we have 1 000 examples  so M 1 000  That what we do is sit down and ask  how long does it take me really to collect and label one example  And sometimes maybe it will take you  you know ten seconds to label one new example  and so if I want 10 X as many examples  I'd do a calculation  If it takes me 10 seconds to get one training example  If I wanted to get 10 times as much data  then I need 10 000 examples  So I do the calculation  how long is it gonna take to label  to manually label 10 000 examples  if it takes me 10 seconds to label 1 example  So when you do this calculation  often I've seen many you would be surprised  you know  how little  or sometimes a few days at work  sometimes a small number of days of work  well I've seen many teams be very surprised that sometimes how little work it could be  to just get a lot more data  and let that be a way to give your learning app to give you a huge boost in performance  and necessarily  you know  sometimes when you've just managed to do this  you will be a hero and whatever product development  whatever team you're working on  because this can be a great way to get much better performance  Third and finally  one sometimes good way to get a lot of data is to use what's now called crowd sourcing  So today  there are a few websites or a few services that allow you to hire people on the web to  you know  fairly inexpensively label large training sets for you  So this idea of crowd sourcing  or crowd sourced data labeling  is something that has  is obviously  like an entire academic literature  has some of it's own complications and so on  pertaining to labeler reliability  Maybe  you know  hundreds of thousands of labelers  around the world  working fairly inexpensively to help label data for you  and that I've just had mentioned  there's this one alternative as well  And probably Amazon Mechanical Turk systems is probably the most popular crowd sourcing option right now  This is often quite a bit of work to get to work  if you want to get very high quality labels  but is sometimes an option worth considering as well  If you want to try to hire many people  fairly inexpensively on the web  our labels launch miles of data for you  So this video  we talked about the idea of artificial data synthesis of either creating new data from scratch  looking  using the ramming funds as an example  or by amplifying an existing training set  by taking existing label examples and introducing distortions to it  to sort of create extra label examples  And finally  one thing that I hope you remember from this video this idea of if you are facing a machine learning problem  it is often worth doing two things  One just a sanity check  with learning curves  that having more data would help  And second  assuming that that's the case  I will often seat down and ask yourself seriously  what would it take to get ten times as much creative data as you currently have  and not always  but sometimes  you may be surprised by how easy that turns out to be  maybe a few days  a few weeks at work  and that can be a great way to give your learning algorithm a huge boost in performance
qC2hHLlxqPY,Ceiling Analysis  What Part of the Pipeline to Work on Next  In earlier videos  I've said over and over that  when you're developing a machine learning system  one of the most valuable resources is your time as the developer  in terms of picking what to work on next  Or  if you have a team of developers or a team of engineers working together on a machine learning system  Again  one of the most valuable resources is the time of the engineers or the developers working on the system  And what you really want to avoid is that you or your colleagues your friends spend a lot of time working on some component  Only to realize after weeks or months of time spent  that all that worked just doesn't make a huge difference on the performance of the final system  In this video what I'd like to do is something called ceiling analysis  When you're the team working on the pipeline machine on your system  this can sometimes give you a very strong signal  a very strong guidance on what parts of the pipeline might be the best use of your time to work on  To talk about ceiling analysis I'm going to keep on using the example of the photo OCR pipeline  And see right here each of these boxes  text detection  character segmentation  character recognition  each of these boxes can have even a small engineering team working on it  Or maybe the entire system is just built by you  either way  But the question is where should you allocate resources? Which of these boxes is most worth your effort of trying to improve the performance of  In order to explain the idea of ceiling analysis  I'm going to keep using the example of our photo OCR pipeline  As I mentioned earlier  each of these boxes here  each of these machines and components could be the work of a small team of engineers  or the whole system could be built by just one person  But the question is  where should you allocate scarce resources? That is  which of these components  which one or two or maybe all three of these components is most worth your time  to try to improve the performance of  So here's the idea of ceiling analysis  As in the development process for other machine learning systems as well  in order to make decisions on what to do for developing the system is going to be very helpful to have a single rolled number evaluation metric for this learning system  So let's say we pick character level accuracy  So if you're given a test set image  what is the fraction of alphabets or characters in a test image that we recognize correctly? Or you can pick some other single road number evaluation that you could  if you want  But let's say for whatever evaluation measure we pick  we find that the overall system currently has 72% accuracy  So in other words  we have some set of test set images  And from each test set images  we run it through text detection  then character segmentation  then character recognition  And we find that on our test set the overall accuracy of the entire system was 72% on whatever metric you chose  Now here's the idea behind ceiling analysis  which is that we're going to go through  let's say the first module of our machinery pipeline  say text detection  And what we're going to do  is we're going to monkey around with the test set  We're gonna go to the test set  For every test example  which is going to provide it the correct text detection outputs  so in other words  we're going to go to the test set and just manually tell the algorithm where the text is in each of the test examples  So in other words gonna simulate what happens if you have a text detection system with a hundred percent accuracy  for the purpose of detecting text in an image  And really the way you do that's pretty simple  right? Instead of letting your learning algorhtim detect the text in the images  You wouldn't say go to the images and just manually label what is the location of the text in my test set image  And you would then let these correct or let these ground truth labels of where is the text be part of your test set  And just use these ground truth labels as what you feed in to the next stage of the pipeline  so the character segmentation pipeline  Okay? So just to say that again  By putting a checkmark over here  what I mean is I'm going to go to my test set and just give it the correct answers  Give it the correct labels for the text detection part of the pipeline  So that as if I have a perfect test detection system on my test set  What we need to do then is run this data through the rest of the pipeline  Through character segmentation and character recognition  And then use the same evaluation metric as before  to measure what was the overall accuracy of the entire system  And with perfect text detection  hopefully the performance will go up  And in this example  it goes up by by 89%  And then we're gonna keep going  let's got o the next stage of the pipeline  so character segmentation  So again  I'm gonna go to my test set  and now I'm going to give it the correct text detection output and give it the correct character segmentation output  So go to the test set and manually label the correct segmentations of the text into individual characters  and see how much that helps  And let's say it goes up to 90% accuracy for the overall system  Right? So as always the accuracy of the overall system  So is whatever the final output of the character recognition system is  Whatever the final output of the overall pipeline  is going to measure the accuracy of that  And finally I'm going to build a character recognition system and give that correct labels as well  and if I do that too then no surprise I should get 100% accuracy  Now the nice thing about having done this analysis is  we can now understand what is the upside potential of improving each of these components? So we see that if we get perfect text detection  our performance went up from 72 to 89%  So that's a 17% performance gain  So this means that if we take our current system we spend a lot of time improving text detection  that means that we could potentially improve our system's performance by 17%  It seems like it's well worth our while  Whereas in contrast  when going from text detection when we gave it perfect character segmentation  performance went up only by 1%  so that's a more sobering message  It means that no matter how much time you spend on character segmentation  Maybe the upside potential is going to be pretty small  and maybe you do not want to have a large team of engineers working on character segmentation  This sort of analysis shows that even when you give it the perfect character segmentation  you performance goes up by only one percent  That really estimates what is the ceiling  or what is an upper bound on how much you can improve the performance of your system and working on one of these components  And finally  going from character  when we get better character recognition with the forms went up by ten percent  So again you can decide is ten percent improvement  how much is worth your while? This tells you that maybe with more effort spent on the last stage of the pipeline  you can improve the performance of the systems as well  Another way of thinking about this  is that by going through these sort of analysis you're trying to think about what is the upside potential of improving each of these components  Or how much could you possibly gain if one of these components became absolutely perfect? And this really places an upper bound on the performance of that system  So the idea of ceiling analysis is pretty important  let me just answer this idea again but with a different example but more complex one  Let's say that you want to do face recognition from images  You want to look at the picture and recognize whether or not the person in this picture is a particular friend of yours  and try to recognize the person Shown in this image  This is a slightly artificial example  this isn't actually how face recognition is done in practice  But we're going to set for an example  what a pipeline might look like to give you another example of how a ceiling analysis process might look  So we have a camera image  and let's say that we design a pipeline as follows  the first thing you wanna do is pre-processing of the image  So let's take this image like we have shown on the upper right  and let's say we want to remove the background  So do pre-processing and the background disappears  Next we want to say detect the face of the person  that's usually done on the learning So we'll run a sliding Windows crossfire to draw a box around a person's face  Having detected the face  it turns out that if you want to recognize people  it turns out that the eyes is a highly useful cue  We actually are  in terms of recognizing your friends the appearance of their eyes is actually one of the most important cues that you use  So lets run another crossfire to detect the eyes of the person  So the segment of the eyes and then since this will give us useful features to recognize the person  And then other parts of the face of physical interest  Maybe segment of the nose  segment of the mouth  And then having found the eyes  the nose  and the mouth  all of these give us useful features to maybe feed into a logistic regression classifier  And there's a job with a cost priority  they'd give us the overall label  to find the label for who we think is the identity of this person  So this is a kind of complicated pipeline  it's actually probably more complicated than you should be using if you actually want to recognize people  but there's an illustrative example that's useful to think about for ceiling analysis  So how do you go through ceiling analysis for this pipeline  Well se step through these pieces one at a time  Let's say your overall system has 85% accuracy  The first thing I do is go to my test set and manually give it the full background segmentation  So manually go to the test set  And use Photoshop or something to just tell it where's the background and just manually remove the graph background  so this is a ground true background  and see how much the accuracy changes  In this example the accuracy goes up by 0 1%  So this is a strong sign that even if you have perfect background segmentation  the form is  even with perfect background removal the performance or your system isn't going to go up that much  So it's maybe not worth a huge effort to work on pre-processing on background removal  Then quickly goes to test set give it the correct face detection images then again step though the eyes nose and mouth segmentation in some order just pick one order  Just give the correct location of the eyes  Correct location in noses  correct location in mouth  and then finally if I just give it the correct overall label I can get 100% accuracy  And so as I go through the system and just give more and more components  the correct labels in the test set  the performance of the overall system goes up and you can look at how much the performance went up on different steps  So from giving it the perfect face detection  it looks like the overall performance of the system went up by 5 9%  So that's a pretty big jump  It means that maybe it's worth quite a bit effort on better face detection  Went up 4% there  it went up 1% there  1% there  and 3% there  So it looks like the components that most work are while are  when I gave it perfect face detection system went up by 5 9 performance when given perfect eyes segmentation went to four percent  And then my final which is cost for well there's another three percent  gap there maybe  And so this tells maybe whether the components are most worthwhile working on  And by the way I want to tell you a true cautionary story  The reason I put this is in this in preprocessing background removal is because I actually know of a true story where there was a research team that actually literally had to people spend about a year and a half  spend 18 months working on better background removal  But actually I'm obscuring the details for obvious reasons  but there was a computer vision application where there's a team of two engineers that literally spent about a year and a half working on better background removal  actually worked out really complicated algorithms and ended up publishing one research paper  But after all that work they found that it just did not make huge difference to the overall performance of the actual application they were working on and if only someone were to do ceiling analysis before hand maybe they could have realized  And one of them said to me afterward  If only you've did this sort of analysis like this maybe they could have realized before their 18 months of work  That they should have spend their effort focusing on some different component then literally spending 18 months working on background removal  So to summarize  pipelines are pretty pervasive in complex machine learning applications  And when you're working on a big machine learning application  your time as developer is so valuable  so just don't waste your time working on something that ultimately isn't going to matter  And in this video we'll talk about this idea of ceiling analysis  which I've often found to be a very good tool for identifying the component of a video as you put focus on that component and make a big difference  Will actually have a huge effect on the overall performance of your final system  So over the years working machine learning  I've actually learned to not trust my own gut feeling about what components to work on  So very often  I've work on machine learning for a long time  but often I look at a machine learning problem  and I may have some gut feeling about oh  let's jump on that component and just spend all the time on that  But over the years  I've come to even trust my own gut feelings and learn not to trust gut feelings that much  And instead  if you have a sort of machine learning problem where it's possible to structure things and do a ceiling analysis  often there's a much better and much more reliable way for deciding where to put a focused effort  to really improve the performance of some component  And be kind of reassured that  when you do that  it won't actually have a huge effect on the final performance of the overall system 
tORLeHHtazM,What Is Machine Learning?  So  now let's try to get a little bit of a sense of how this technology works  So  we're going to give a very simple introduction to the basic concepts that we will then dive into in more depth subsequently  So  one of the key things to Machine Learning is the idea that we're going to teach a machine to learn  So  the way that we do that  is we give the machine examples  and the examples are characterized by data samples and then what we would like the machine to predict of that data  So here is an example where the data is just a set of numbers or a vector of numbers  and then the label is what we would like to assign to that data  Here we're going to say that the label is denoted by y  here for simplicity  we're going to say that the label is either zero or one  but we can generalize that to other labels  So  as an example we might wish to analyze images and determine whether someone has a malignancy or not based upon what is seen in the image  So  in that case  the acts or the data would correspond to the pixellated image and then the label y which here would again be binary  would classify whether the image is malignant or not  So  the way that we teach a machine to learn  and by machine we mean algorithm  is we give it examples  So  we give it many examples here N examples  where an example corresponds to a data sample  and then the true label  So  here we have N examples where we have the data x  and the label y  Then what we would like to do is to take this data which we call training data to teach a machine or an algorithm to do prediction accurately in the sense that it is capable of reproducing the training data  in the sense that if you give it any of these vectors acts  it accurately predicts the associated y  But more importantly we would like it to be able to make predictions for future examples  So we would like to give it a new data sample x and predict the associated y  To make this a little bit concrete so that it's clear what we're talking about  let's consider a simple example  So  let's ask the question whether it is going to rain or not on a given day  So  let's assume that we can measure some characteristics of the environment at a given point in time for example  in the morning  we'd look at the level of cloud cover  the level of humidity  temperature  air pressure  et cetra  These are a set of numbers  those are the data and then on a given day  it either rains or not  we'll say that the y is equal to one if it rained  and y is equal to zero if it did not rain  So  the idea is that we would have a training set which would be composed of multiple examples of the associated data which is represented by the vector x  and the associated label which tells us whether it rained or not that day  We would then like to build a predictive model which takes in the training data  so  the training data is on the top left  it's all of the examples that we have based upon historical data of the associated conditions which are represented by x  and then y whether it rained or not that day  We have a mathematical model which we'll call the machine  that mathematical model has parameters which are denoted by the question mark  those are unknown parameters  What we would like to do is to learn those model parameters such that the algorithm or the mathematical model does an accurate job of representing the training set  Then ideally if we give that mathematical model a new example of conditions represented by the vector x  we would like that model to accurately predict whether it will rain that day  So  then the key task then would be how well the model performs when testing  and so in this case again  we would observe a new set of data x and then our goal is to predict the label y  So  this gives a very early introduction into Machine Learning  The key thing to take away from this is that we have training examples which correspond to data and outcomes that we would like to model to predict  Based upon that training data we then apply that training data to a model  that model is typically characterized by a set of parameters  What our goal is to try to learn the parameters of that model such that they are consistent with the training data  After we do that learning  we then apply that model to new data and make predictions in the future  What we will now do as we move forward is to try to dig into some of these details to provide more clarity on the aspects of Machine Learning 
PpB5D5yI1QY,Logistic Regression  Okay  So  what we would like to now do is to dive in to a little bit more of the details of what machine learning is  So  one of the key aspects of machine learning is that we're going to have a training set which again are examples of data and outcomes  Our goal is to be able to learn a model which is capable of predicting the outcomes given the data  So  to do learning we have an algorithm  that algorithm is characterised by a set of parameters and learning means that we would like to try to infer what parameters of that model are consistent with our training data  To make this a little bit more precise  we're going to start by considering one of the most basic and widely used algorithms  logistic regression  Logistic regression is a very good starting point  As we'll see it is a nice launching or point or building point for subsequent more complicated models  So  in machine learning  the goal is to take a training set and a training set in this case corresponds to N examples where we have a data X and an outcome Y  What we would like to do is to build a predictive model that is capable of predicting Y given X  That model again is characterised by parameters  Let's look at a very simple model which is a linear predictive model  So  here remember that X corresponds to the data  We will say that X sub i corresponds to the ith example  So  remember that we have N training examples shown here  We're going to look at one of them which will be the ith example and then X sub i1 corresponds to the first component of the vector X sub i  X sub i2 corresponds to the second component a vector X sub i down to X sub im which corresponds to the nth component of that vector  A very simple idea here is we're just going to multiply every component of the vector X by a parameter  Where that parameter B1 will be multiplied by the first component of Xi  parameter B2 will be multiplied by the second component of Xi and parameter BM will be multiplied by the nth component  We do all of those multiplications  we add them up and then we add an additional constant or what we call a bias  B sub zero and we're going to call this  this a mapping from the data X sub i to a number Z sub i  a variable Z sub i this is a very simple linear predictive model  So  if we go back to our example  the Y  if you recall corresponded to whether it rained or not on a given day and Yi equal to one  means that on day i it rained  Yi equals zero means that on day i it did not rain and then the features or the data that we're going to feed into our model with which to make a prediction are going to be for example the cloud cover  humidity  temperature  et cetera  So  therefore what we're doing with this model is we're taking the values for cloud cover  humidity and temperature multiplying them by corresponding parameters B1  B2  B3  et cetera  We're doing those multiplications adding that all up and then we constitute a variable Z1 for the first example  Z2 for the second example  Then what we would like to do is to convert our prediction of whether it's going to rain or not to a probabilistic representation so recall that Y sub i equal to one will correspond to it raining on day i  So  whenever we observe the data for example cloud cover  humidity  temperature  et cetera and we want to make a prediction  it's oftentimes better to not absolutely say whether it will rain or not on a given day but rather to assign a probability to say that with 70 percent confidence for example  it's going to rain on a given day  and so the way that we do this is by something called a logistic function which is represented by the symbol Sigma and it's drawn here  So  the idea is that we take that variable Zi  which is manifested as a multiplication of the components of the data X with corresponding parameters B1  B2 through BM  we calculate Z  we then feed it into this function and then out comes a number between zero and one  So  the key thing to notice about this function Sigma which is called the Sigmoid function  is that it always lives between zero and one  So  when Zi is large positive for example  around say five or six when Zi is around five or six  the Sigmoid function then converts that into a number which is close to one which would mean that we have high confidence that based upon that value of Zi it is going to rain on that day  If Zi is small  say negative two  negative three  negative four  then the output of the Sigmoid function is rather small close to zero which means that based upon that data  we say that there is a low probability that it will rain on a given day  So  this Sigmoid function is a way for us to convert predictions about the outcome on a given day into a probabilistic perspective  So  as was said as if Z is large  we have a high probability of outcome one  if Z is negative then we have a high probability of outcome Y equal to zero  So  this is again called logistic regression  To again link this back to the example  So  we might have data  the data in our objective  is based upon observed information about the climate we want to make a prediction about whether it is going to rain today  We're looking at for example cloud cover  humidity  we multiply those by corresponding parameter values B1  B2  et cetera  Those parameters B1  B2  B3  tell us how important those variables  those data variables are to the prediction and then finally that Z is sent into the Sigmoid function  This model  which is a very simple model just a linear combination of multiplying the observed data variables by associated parameters just summing them up  mapping that to a variable Zi and then running that through a Sigmoid function  This is a very classic model is called logistic regression  So  this really encapsulates what machine learning is all about  So  at the top center what machine learning  the heart of machine learning is we have an algorithmic model which is a parametric model that is characterised in terms of a set of parameters which we would like to learn  The way we do the learning on the left we have a set of data and for that data we have observations X and we have outcomes Y  We would like to learn the parameters of our model such that the predictions of the model are consistent with the training data  So  what we mean by learning is to infer the parameters B0 through BM which provide outputs of providing mapping from X to Y  that is consistent with the data  So  this concept of logistic regression is one of the most simple concept from standpoint of a model that one can look at  Although it is widely used and it is quite effective model  it captures really the heart and in some sense the simplest way of what machine learning is about  What we will see as we move forward is that the basic concepts of logistic regression are used repeatedly and in many aspects of machine learning in particular  deep learning 
RR6Ben6qVnE,Interpretation of Logistic Regression  So logistic regression is one of the simplest algorithms that one can consider in machine learning  But in many ways it's not overly simple it's widely used and it provides a nice introduction into machine learning  It's therefore worthwhile to look into logistic regression a little bit more deeply and to try to use it as a model to interpret what machine learning is doing  So to do that  let's look at a particular problem  So what is shown here are some examples of handwritten digits from 0 to 9 and this is a very classic data set  it's called the MNIST Data Set  What we would like to do is to build a predictive model that could look at a picture of one of these images and then can map it  map that picture to a digit from 0 to 9  So this as one could imagine would be very applicable for analysis of human writing  the use of 0 to 9 is just an example of course we can use this basic construct for the analysis of any hand written language  So the question is  how do we do this and how do we do this within the context of the tools we have at our disposal  particularly logistic regression  So again the goal is take a picture  here is a picture of 8 on the left  and what we would like to do is to map that to a prediction and we would like the machine to look at that picture and say that that is an 8  The way that we do this is that we take the image it is pixelized into digits  so that's what you see on the right and so each one of those pixels is the strength of the image at that point in space  We then take those pixels and those pixels now are going to correspond to the vector x that we talked about previously and that we'll talk about as well  Now to make this concrete  remember that the tools we currently have at our disposal is logistic regression which assumes that the output of our model is binary  So what we're going to do is we're going to consider a binary problem  in particular we're going to consider the problem where a digit is either a 0 or a 1 and we would like the machine to automatically look at the image and correctly say whether it is a 0 or a 1  So this is a very simple binary task  of course we can generalize this to more than binary  The binary is chosen because it's consistent with the assumptions of the logistic regression model that we're currently looking at  So recall that the way that we do this is that we give a set of training data  so on the left you see some examples of 0s and 1s and so the training data is manifested by images and the true label  We have our Logistic Regression Model  so here the pixels of the image are going to correspond to the data x  We weigh each of the pixel values  by multiplying by the parameters b1 through bm  we sum them up  we then get the parameter Z and then Z is sent through the sigmoid function  And then we can say with a certain probability that we are looking at a 1 digit 1  or a digit 0  so what we mean by learning is to infer the set of parameters b0 through bm  So to give an example of what this model is doing  This is what is learned in the context of training a Logistic Regression Model on those 1s and 0s  To try to understand this more closely  let's look at this example  again of a picture that corresponds to a 0  and the parameters of our model which are shown in the right with the blue and the red  So if we do the pointwise multiplication  we do for every pixel on the left  we multiply it by the corresponding pixel on the right  We do that multiplication pixel by pixel and then we sum all that together  notice that the only place where we get a strong contribution is where the white intersects the blue  The white of the picture intersects the blue of our filter and what happens is is that that yields a very negative value  which after being sent through the sigmoid function  yields a very low probability that this corresponds to a 1  By contrast  when we look at the case in which the digit corresponds to a 1  the black and white image corresponds to a 1 and then we multiply  if we look at the bottom example here  If we take the picture of a 1 and we multiply it by the set of parameters b which again is shown by the image of blue and red  if we take every pixel of the image and multiply it by the corresponding pixel of the set of parameters B  The only place where there is a strong contribution is where the positive components of the parameters B overlap with the positive components of the picture  We then get a strong positive value that is then sent through the sigmoid function  which yields a high probability that this is the number 1  And so this concept  I want to provide a little bit of notation which will be useful later  So to provide a little bit more detail  remember  what we're doing is we're taking the vector xi  which corresponds to our data  a set of numbers  and we're multiplying each component of x by the corresponding component of what we call a vector  A vector which is characterized by parameters of our model b1  b2 through bm and so what we're doing is  we can visualize this as the data is a vector xi which is characterized by a set of pixels  The parameters of our model is a vector b which represents parameters b1 through bm  We multiply every pixel of x with the corresponding pixel of b  we then sum those all together and that implements the equation on the top  This expression is called an inner product  it's an inner product between the vector xi and the vector b  This concise notation  xi dot with a circle b  is called inner product  that compact notation will be useful later whenever we look at more sophisticated models  And  so  this model  what we're doing is  we're looking at images  we're showing you examples of cases whenever the true label is yi = 0  Other cases where the true label is y = 1  the filter is shown at the right  What we're doing is we're taking an inner product of the data xi with b and that gives us our z and then that is sent through a logistic function  So the way that we can think of this is that the parameters of our model b are like a filter and you can see that the filter looks a lot like a one  And so what the model is doing what logistic regression is doing  is it's learning a filter of parameters b and then it multiplies that filter parameters on the data  If there is a strong match between the filter and the data then we say  there's a high probability of outcome yi equal to 1  If there is a low match between the filter and the data  we say that there is a low probability of a match between the filter and the data  And so again this is ultimately sent through a logistic function  the sigmoid function which then converts that into a probability  So this concept of data in a product with a filter and then sent through a logistic function or the sigmoid function is at the heart of logistic regression  What we'll do as we move forward is to is to use this concept from logistic regression to move to more sophisticated models that we'll find in deep learning 
ng1oliu806U,Motivation for Multilayer Perceptron  What we would like to do now is to move beyond logistic regression  which is a simple machine learning model  and move to more sophisticated model  in particular  something called a multilayer perceptron  This is one of the most basic forms of deep learning  it's one of the most basic forms of neural networks  So  in logistic regression  recall that  what we do is we have data Xi  we take an inner product of the data Xi with a set of parameters b  which we call a filter  and that inner product Xi with b is then added to what we call a bias or just a number B nought  which is an additional parameter that we need to learn  That ultimately live gives us Zi  which is then sent through this logistic function or the sigmoid function which converts Zi to a probability  In particular  the probability that the label is equal to one  Now  this is a very powerful model  the logistic regression  But  we would like to move beyond it  So  the question is  what's wrong with logistic regression? So  it turns out that logistic regression is a very good model in situation in which the delineation between class one and class zero in our binary problem can be separated by a line  So here  a line or a plane  A line in two dimensions  a plane in three dimensions  and what's called a hyperplane and higher dimensional problems  Here  we're looking at a situation in which the data are two-dimensions  So  we're looking at X sub one and X sub two  which are the two dimensions of our data  Every point here corresponds to a data point  So  every point corresponds to an example of X1  X2  a particular data  So  recall that that logistic regression  is solving a binary problem  So  here red corresponds to one class and blue corresponds to another class  In this case  a line does an effective job in separating or distinguishing between the red and the blue classes  So  for the data of this sort  logistic regression is appropriate  because it designs a linear classifier  However  there are situations in which the data are not well separated by a linear classifier  So  here is an example where again  we have the blue dots corresponding to one class  and red dots corresponding to another class  You can see that the decision boundary between the red and the blue dots is much more complicated than a line  In other words  simple line will not separate these  So  what we would like to do now  is to build a model that is capable of building decision boundaries between the class one and class zero that is more sophisticated than what a linear classifier can do  This is our motivation to go into more sophisticated models and in particular  the multilayer perceptron  The key thing to take away from this is that the logistic regression  which is a very nice model  it has a limitation that it only works effectively whenever a linear classifier is effective at distinguishing class one from class zero  Any other case such as the one you're looking at here in which the decision boundary is more complicated for which a linear classifier will not be effective  We want to use more sophisticated model  this motivates the multilayer perceptron  which is a natural extension of the logistic regression 
yxnbvS2t_QA,Multilayer Perceptron Concepts  Multilayer perceptron  which we're going to introduce now  is actually a rather direct or natural extension from logistic regression  So remember that x sub i  corresponds to the ith data example  and it has m components  x sub i-1 through x sub i- m  Recall that in logistic regression  what we did was we took each component of the data  multiplied it by the parameter  added that up and mapped it to a variable z  and then that variable z was sent through a sigmoid function which is represented by the symbol sigma  What we're going to do here is the same concept as in logistic regression  but instead of doing it one time  we're going to do it k times  and so now  what we're doing is we're doing effectively K implementations of what we did with logistic regression  which are going to map the data x to a k dimensional vector  which is represented by the probability from zero to one of what we call k latent processes or features associated with the data  These are called latent because these are representative of things that in the data characteristics of the data  we cannot in principle observe  Then those k latent features are sent through a logistic regression model to ultimately yield a binary probability for the classification of the data  and so this is very much like logistic regression but what we have done is introduced instead of doing logistic regression directly from the data to the binary outcome  what we're doing is we're doing logistic regression  we're first going from the data to k latent processes or the probabilities on those k latent processes and then that corresponds to a k dimensional feature vector and then that k dimensional feature vector is used to apply logistic regression  which is then ultimately yields a outcome  the probability that the data corresponds to class y equal to one  So  what this can be viewed as  as logistic regression being performed on the k features  instead of being performed directly on the raw in the initial data  Now  just to give some motivation for why doing the simple logistic regression may not be as effective and to motivate why the newer model  this model that we've introduced here may be more effective  So let's go back to the problem of digit recognition and let's examine the situation for which our goal is to perhaps identify handwritten examples of the number four  So what you're seeing here are four examples of handwritten representations of the number four  and so you can see that people have many different ways of writing the number four  If we were to use logistic regression  which as you recall  takes a single filter B  and does an inner product with the data xi  what would typically happen is you might find that that filter here represented by the values of the pixels in the image to the right  might look like an average four  So this average four doesn't look like any of the fours  So the idea of building a classifier based upon a single filter of this type seems undesirable  So alternatively  we might consider a situation in which we have three intermediate filters  so if we go back to our multilayer perceptron  we can take our data  which here corresponds to the pixels of the image  and instead of multiplying them by filters that correspond to one average four  we might consider k equal to three filters  which correspond to four different ways in which one might draw the number four  So  what we see on the right are examples of what three filters might look like  and then those  the latent features that are inferred will then be used to ultimately make a final prediction about whether the number four is present  so the key idea in going to this new model is that we're no longer going to limit ourselves to a single filter  B to examine the data  we're going to introduce k filters 
BA1rnEMBjjQ,Multilayer Perceptron Math Model  So  now what we would like to do is take a look at the multilayer perceptron  and add some of the math that drives that model  The aspects of the mathematics are worth going into because they are at the heart of most neural models that we'll be interested in  So  while we're talking specifically about the multilayer perceptron  these ideas extend to other types of models  So  the key thing that we would like to try to communicate is the idea that the multilayer perceptron is a natural extension of logistic regression  So  you recall that in logistic regression  we had data which was represented here by x sub i  x sub i one through x sub i  m  which corresponds to the m components of the ith data samples  So  x sub i is the ith data sample  In the logistic regression  what we did was we mapped that input data xi to a variable zi  through an inner product  or a dot product between the data xi and a vector b  and then  so there was a single if you will filter  that was applied to to the data  The idea in the multilayer perceptron is that instead of just projecting the data xi with one template vector as we did in logistic regression  what we're going to do is consider k filters or reference factors b1 through bk  and we're going to take xi  and we're going to take the inner product of xi with b1  xi with b2  and all the way to xi with bk  inner product with bk  So  the idea is that instead of taking the data  and taking an inner product with the single vector as we did in logistic regression  we're going to do it with k  vectors b1 through bk  where going to then add biases in the same way that we did with logistic regression  and then we will have outputs of each of those operations which will be zi1 through zik  So  instead of realizing as we did with logistic regression just a single output  here we have k outputs which are k features from layer one  Those features are then sent through a logistic function which is shown on the top right  which is a mapping of a real number  a number that can be anywhere from negative infinity to positive infinity  a real number  we squash it or map it to a number between zero and one  and the reason we do that is is that now we're going to be introducing probability on k latent features  So  the way to think about this is that we have a data xi  and then there are k latent features or processes that are responsible  or at least from a modeling perspective are represented as being responsible for the data  After this  the inner product  and then going to the logistic function we then have to the sigma  that's called a sigmoid function  we then have a mapping for each of those k-features to a probability from 0-1  So  that basically represents the degree  or the degree to which each of those latent features a is represented in the data  Then  those k-latent features or  those k-latent probabilities are then sent through a logistic regression type model in the same way that we did previously with logistic regression  and now we have a single template filter C with which we take the inner product of those latent features  we get a variable Zeta sub i  that that Greek symbol is called Zeta  Zeta sub i is then again sent through a sigmoid function which tells us the probability of the data being associated with a particular binary label  So  the key thing to notice about what we're doing with the multilayer perceptron is that instead of taking the data xi  which has m components xi1 through xim  and then directly using logistic regression to map that to a probability of a binary outcome  We have an intermediate step where we introduce k-latent features  and then those k-latent features are then subsequently sent through a logistic regression type model to ultimately give us a probability of a binary outcome  This added model sophistication through that intermediate or middle layer adds significant modeling flexibility  In particular  it allows us to consider non-linear decision boundaries in feature space  Therefore  oftentimes  has been shown to be more effective from the standpoint of yielding more effective results 
E7RMU8kqyd4,Deep Learning  So  at the beginning I explained that deep learning is now a very important area of machine learning  We're now going to get a sense of what we mean by deep learning  So now  what we're going to do here is the same thing we did before but we're going to do it twice  So  starting at the bottom  X is our feature vector  It has M components  So X_i  which is the ith data sample has M features which are represented X_i1  X_i2  through X_iM  Those are the circles at the bottom  Those are the features of our data  Each of the features of the data is multiplied by a parameter vector B  and summed together  We do this K times for K different templates  B_1 through B_K  Then after sending that through a logistic function  we get the probability of each of those K latent features  and that probability is a number between zero one is binary  Then what we're going to do is instead of going directly into the logistic regression to make a binary classification  what we're going to do now is predict what we call layer two latent processes  So now  you can start to get a concept of what we mean by deep because now we have two layers of latent processes  So  at the bottom  we have our data  those data are then mapped to probabilities on latent processes  Then we repeat this process  and then we get a layer two latent processes again represented by probabilities  Then at the top of this model  we then do logistic regression  which is a way for us to do binary classification  Then at the top  we have a probability that the label is Y equal one or Y equals zero  So now  when we look at this  the model is certainly more complicated and we'll talk about that complication and what it implies later  But we at least now can get a sense of what we mean by deep  So  deep learning is a form of machine learning where our model is deep in the sense that it has multiple layers of latent processes  Then in a subsequent example  will explain given in some intuition as to what these latent processes mean  Always at the top  we have a logistic regression classifier  which is giving us the probability of the binary output Y equal one or Y equals zero  But then before we get to that  the intermediate layers correspond here too K latent features at layer one and J latent features at layer two  Now as you might imagine you can go to even deeper models  Here we have a two-layer model  We can go to a three-layer model  a four-layer model  where we have three or four layers of latent processes  So  the key thing to notice is that this model  which is a very classic model  which is called a multilayer perceptron it's also called a neural network  is basically built up by a sequence of repeated application of the logistic regression  So  we now start to understand why the logistic regression is such a fundamental model  Because every component of the multilayer perceptron  which is a fundamental form of a neural network  can be viewed as a repeated application of logistic regression  Now  one of the key things that this multilayer perceptron does  and so the question that you might ask yourself is why would we go to such a complicated model? This is certainly more complicated than the logistic regression  But recall that the logistic regression had limitations  One of those limitations was that the decision boundary between the two the types of data  the Y equal one and Y equals zero was required to be linear  a line  So  with this more complicated multilayer perceptron  it turns out that we can learn decision boundaries which are far more sophisticated  So  here is another example  where again the red dots correspond to data of one label and the blue dots correspond to data of another label  You notice in this case  in this two-dimensional feature space  so in this case  the vectors axis have to components  presented X_1  X_2  and you notice that the blue and the red dots cannot be separated by a line  So therefore  logistic regression would be ill-suited for data of this form  It turns out that often time  data is of this form  So therefore  the logistic regression in its simple form is often inappropriate  So  this motivates going to a more sophisticated model  this multilayer perceptron  In this figure  what we're showing is the probability of the data and so red means high probability of the red data and blue represents high probability of the blue data  This is the output of a multilayer perceptron 
XZ8vB81-tV8,Example  Document Analysis  In the previous lesson  we introduced the concept of the multilayered perceptron  which in many ways is the most fundamental example of a neural network  Now  it is a relatively complicated and sophisticated model  it's a very powerful model  but there may be some confusion about how it works? Why it works? Why is this a good model? So  what we would like to do in this lesson  is to spend a little bit of time working through or talking through an example to try to give some intuition as to what we mean by these latent features and these latent processes  what does that mean? So  let's consider a relatively simple but important example  So  let's assume that our job is to analyze documents  So  here a document is just a set of words  So remember that x_i is a vector which represents a set of features for the ith data of interest  For us now in this example  what we mean is that x_i represents the features of document i  here in the context of document analysis  The simplest way that we can do this is just to count the words  So  assume that we're given a document  and let's assume that we have a vocabulary that is composed of V words  so the vocabulary size is V  What we're going to do to characterize a given document  here document i  is simply count the number of times each word appears in the document  Then what we're going to do is we're going to make a prediction based upon that feature vector  So  here prediction we're going to still limit ourselves to the binary case for simplicity  So  what we would like to do is given the document  and given the words in the document  we want to make a prediction about whether a given person will like the document or find it interesting or whether they will dislike it  So  in the binary case  what we'll say is  if they like it  the label will be  Y will be one  If they dislike it  Y will be equal to zero  So  we're given data for training  remember when we do learning  the first thing we have to assume is that we have data with which to learn  So  that data is used to teach the model or to teach the algorithm  and when we talk about learning  what we mean by learning  is learning the parameters of our model  So  in this case  we're going to assume that we have N documents represented by X_1  X_2 through X_N  These are feature vectors which simply represent the number of times in each case that each of the V words is manifested in the document and then we're also given labels Y_1 through Y_N  which represent whether the individual of interest liked or disliked the document  Our goal is to build a model such that if we're given a new vector x  for a new document that the person has never seen before  we want to make a prediction will they like it or not like it 
sG6WGvzmVaw,Interpretation of Multilayer Perceptron  Now  the way that we're going to do this  is using a multilayer perceptron  So  just to remind you of how this works  so the multilayer perceptron is shown on the left  This is represented at the first layer  so we start at the bottom and work our way up  So  let's look at what's going on for the first layer  So  remember what we're going to do is logistic regression essentially K times  So  the data xi are the counts of words for the ith document and b1  b2  bK are templates with which we take the inner product between xi in each of those templates b1  b2  through bK  So  the question is  in the context of this example  what might b1 through bK represent? What might those templates mean? So  there might be some latent topics that are characteristic of the documents  So  remember  we get to see the words  and then there are latent processes in the data or here in the document that we don't explicitly get to see  But we understand documents  and the documents are usually about some subject  So  the way that we might think about this is that b1  which is the first filter  might be representative of words that are characteristic of a particular topic  for example  sports  So  therefore  in our vocabulary of v words  there might be a subset of words that have high likelihood of appearing when the subject is sports  So  what we would expect is that the template b1 or the filter b1  with which we take the inner product of the data  if that filter is characteristic of the latent topic  sports  We would expect that that template b1 will have high values for words that are characteristic of sports and would basically have zero values for components which correspond to words that have nothing to do with sports  As another example  b2  which corresponds to the second latent process may correspond to a topic corresponding to history  So  then in that case  we would expect that the template b2 would have high values for words that are associated with history and essentially zero values for words that have nothing to do with history  Then this can be done for each of the k latent topics and for example  the kth topic might be associated with politics  So  a way to think about this is  we're given a feature vector xi  which represents the counts of words in the document  We're going to take an inner product of xi with k filters or k templates  We can think of these as representing k latent processes  In the context of documents  we might think of these as topics  topics of sports  history  politics  So  the features that come out of the first layer of our model  represent given the words that we see in our feature vector  what is the probability that this document is about sports? What is the probability from zero to one  that this document is about history? What is the probability from zero to one that this document is about politics? Those are the k latent or can be thought of as the k latent processes or features that come out of the first layer of our model  So  now we go to the second layer  Now  we're going to do the same thing  We're going to take the outputs of the first layer which are represented as Sigma Zi  which represent the features that come out of the first layer Zi that then go through the Sigmoid function which turn it into a probability from zero to one  We play exactly the same game  we take those features and now multiply them by layer two filters  which we'll represent as C1  C2 through CJ  So  in this case  J filters that were manifesting  So  what do these filters represent? Well  these filters might emphasize certain features from layer one  and remember the features from layer one correspond to topics  So therefore  we could think of the filters C1 through CJ representing meta-topics  which means combinations of topics from layer one  So remember  the layer one topics correspond to things like sports  history  politics  The layer two filters  Z1 through CJ  are basically providing waiting on the layer one features  So  for example  the template C1 may have high strength for the features at layer one that correspond to the topics from sports and the topic from history  So therefore  if the output of the template or filter C1 is large  that implies that the document is about a combination of sports and history  in other words the history of sports  If C2 for example  might emphasize the features from layer one that are associated with politics and also sports  So  these meta-topics  which means combination of topics C2 for example  might represent how politics is connected to sports  So therefore  if the document is about politics and sports  we would expect that when we take the inner product at layer two with C2  the output will be high  So  this is done for each of the filters C1 through CJ  So  again to think about this  the data x  which in this case is a count of words at layer one  the filters b1 through bK are looking for topics  in other words  sets of words that are associated with certain topics  At layer two the filters C1 through CJ are looking for meta-topics  which means combinations of topics  Then finally  when we get to the top  this logistic regression at the top  what we're doing is we're making a decision and we're asking the question  given the meta-topics that seemed to be apparent characteristic of the latent features at layer two  what is the probability that the person of interest is going to like the document or not? So  again if we look at what this layer two meta-topics represent  if a person is interested in the history of sports  then we would expect that if the filters Z1  when we take the inner product of the features Sigma Zi with filters C1  if C1 is characteristic of sports and history  and if the person of interests likes the history of sports  then we would expect that the filter at layer three D  that filter D would look for the situation in which meta-topics associated with the history of sports was high  So  in this way  we manifest a deep architecture which has an intuitive meaning of what these latent features represent  Now  this is a relatively simple example chosen specifically to try to give some intuition  It's not always the case that the latent features are easily interpreted  but this hopefully gives you some intuition as to why a deep learning model might work well and captures aspects  latent aspects of the data that we're interested in
TS2odp6rQHU,Transfer Learning  Another question that I try to give a sense again is  why is deep learning so powerful? What can deep learning do that is not done easily by other model? So  if we go back to this model  this is a model that allows us to make prediction about whether a person of interest will like or dislike certain documents  and the like or dislike of those documents in this example are controlled by whether that document is characterized by certain topic and meta topic and whether the person of interests likes those particular topic and meta topic  Now  we can now think of how we could use this multi-layer perceptron to analyze multiple people  So oftentimes  we're interested not in predicting how just one person is going to like documents  we're interested in how many people might like documents  So  a way that we can think about this  is the first and second layers of our model recall are looking for the presence of topics and meta-topics in our document  So  therefore the first two layers of our model are actually characterizing the documents  They don't really characterize the person they characterize the documents themselves  So  consequently what we could do and they analyze if we wanted to build models for many different people  what we can do is the first two layers of this model or the parameters associated with the first two layers of this model can be shared or reuse in models of multiple people  So  therefore from the standpoint of model building  so remember when we do model learning we have to use training data to learn  So  if I can reuse the first two layers of this model across many different people  in other words  across data from many different people  then that means that I have the opportunity to leverage much more data than I otherwise could to learn the parameters of the first two layers of our model  Then  the top layer  which characterizes whether a given particular person likes particular meta-topics  this has to be person dependent  So  the thing that I want to try to communicate here  is that as we look at this deep architecture  the lower layers or the layers in the model that are closest to the data at the bottom are characteristic of the data  The layer at the top is characteristic of the person  So  we call this transfer learning  So  if I have data from many different people  I can transfer data from one person to the other when learning the parameters of the bottom of the model  Then I only use data from the person of interests to learn the parameters at the top  So  this transfer learning allows a much more efficient use of data  So  in addition to the other advantages of deep learning that we talked about earlier such as the ability to learn a more general decision boundary  a more sophisticated decision boundary between the features of data characteristic of label y equal one and y equals zero  The other significant advantage of deep learning and here the multilayer perception  is it offers us the ability to do transfer learning  which allows us to use our data in a much more efficient way instead of using data from one person to build one model for one person and then use data from another person to build a model for that person  what we can do is we can take the data across all of our people and therefore have much more data to share that data or transfer that data across the different subjects and then use all of it to learn the bottom parameters of our model and then only use the data associated with a given person to learn the relatively small set of parameters at the top  So  now thinking back to the logistic regression  which was our starting point and our building block algorithm  this is a very simple model and it is therefore a nice place to start  However  it doesn't have the capacity like this multi-layer perceptron to do transfer learning  So  with the logistic regression  we have separate parameters B1 through BM  If we want to build a predictive model of whether a person likes a document or not  We really do not have the capacity to do transfer learning in the same way 
8vRZi05VE3Q,Model Selection  Oftentimes  in machine learning  we have a question of model selection  There may be multiple different models that one might use to represent the data  You have to make choices  The way to make those choices oftentimes  is gained through experience  So  this is with the tools that we have developed thus far  We can look at a particular example of model selection that we might consider with the tools that we have  So  recall that x sub i is the ith data sample  It has M components  so we represent the data as xi1 through xiM  the M components of the data  In the logistic regression  we take the data  we take an inner product with the weights b1 through bM  So  we basically multiply each component of the data xi  with a respective component of the vector b  We sum those products up  we then get a zi  we typically then add a bias  Then finally  this is sent through a sigmoid function denoted by Sigma from which we get a probability of an outcome  That's a very simple model  it may be very effective in some situations  Another model that we now have at our disposal  is the multi-layer perceptron  which we see at the right  So again  at the bottom we have the data xi1 through xiM  the M components of the data  Instead of directly going through with logistic regression to make a classification decision  in this case  we show two intermediate layers of latent features  Then at the top  the latent features at the top are again sent through a logistic regression to make a classification  So  here through this picture  we can see the connection between the logistic regression and the multilayer perceptron  The multilayer perceptron has logistic regression in it at the top of the model  but there are intermediate features that are manifested through this neural network or multilayer perceptrons  So  the question that one often what we'll have is  which model to use? So  this is something that in machine learning is called the Bias Variance Trade-Off  The way to think about this is that  if you look at the logistic regression model  it's a relatively simple model  The number of parameters in the model is relatively small  The multilayer perceptron  by contrast  is a rather complicated model  or rather sophisticated model  But in consequently  it also has a lot more parameters  So  the variance  what we think about when we talk about variances is that  if I were to take different examples of data  If I were to look at a situation where I have N different data samples  and I were to train my model using those data samples  If I were to look at different instantiations of those N training examples  how much variation would there be in the model? So  with the simple model  because of the fact that it is biased towards simplicity  the amount of variance that we would expect to see in different training scenarios  with different training of N samples of data  We would expect in the logistic regression case that the amount of variance that we would see that between different training scenarios would be small  On the multilayer perceptron case  given the added complexity  we might anticipate that it has greater variation from training on one example of data  to training on another example of data  So  this is called the Bias Variance Trade-Off  We oftentimes  will bias a model towards simplicity such that there is not a lot of variants  However  that bias or that simplicity  might undermine performance  So  to show a simple example of this MNIST  is a very widely used dataset for digit recognition  So here  we're looking at 10 digits  zero through nine  and the objective is to take one of those little pictures of a digit  to send it and the pixel that correspond to that little picture  are going to be the elements of our data xi  So  a simple model that we might consider to do is the logistic regression  In that case  the pixels of the image are directly weighted by b1 through bm  and then sent through a sigmoid  and then a classification decision is made  If you do this  If you try to solve the MNIST problem with the logistic regression  you would  for example  see an accuracy of around 91%  If we use by contrast the multilayer perceptron  we can see that the performance oftentimes can improve to 96% accuracy  So  this is an example where the added sophistication of the more complicated multilayer perceptron  Here actually  there's only one additional layer relative to the logistic regression  yields a significant improvement in performance  The reason that this is true  we've talked about this elsewhere  is that the logistic regression is restricted to a linear classifier where the multilayer perceptron allows a nonlinear classifier  That nonlinearity in the classification decision yields improve performance  The other thing that I should briefly note  is that this is a 10 class problem  So  we have digits from zero to nine  So therefore  this is not a binary problem  this is a 10 class problem  For simplicity  I'm using the same binary logistic regression and multi-layered perceptron pictures  But  in the actual implementation at the top of the network  we have a 10 class classification problem or algorithm  which is manifested as a generalization of the logistic or sigmoid link function and it's called the softmax  We'll talk more about the softmax  elsewhere in our class 
mTQ6pZB62Lg,Early History of Neural Networks  Consequently  I think it's a good idea to take a look at the history of the multilayer perceptron  This is meant somewhat as tongue-in-cheek  Other words  not meant to be too precise on things like dates  But it is accurate in the sense that it tells us the history of multilayer perceptrons in neural networks  In that history  I think is a cautionary tale  which will will come to towards the end of this lesson  So  the multilayer perceptron  which we've introduced here  which is at the heart in many ways of deep learning  it consequently seems like very new  It looks interesting  So  one might think that it is something that was just invented  But actually  the multilayer perceptron was invented in around 1960 or 1959  So  it actually has been around a long time  So  this baby is to connote the idea of the birth of the multilayer perceptrons  So  let's look at the history  So  1960 was a long time ago  Computing power was not what it is today  So  while the multilayer perceptron was developed  its use was limited  The other aspect of that is the amount of data that one had in 1960 was significantly less than we have today  So  from that early birth in 1960  the next big innovation occurred in 1986  which is a technique called back propagation  Back propagation is a method that allows us to learn the parameters of the model in a very efficient way  So  from the back propagation is a very important model from the standpoint of computational  making the model computationally relevant  The other thing to note is in 1986  we had far more significant computational power  So again  what I want to reflect here are what I'm calling the seasons of neural networks  Will become clear why we're calling it the seasons of neural networks as we move move through this  So  in 1986  the back propagation model was developed for the multilayer perceptron  We're having increasing computational power  increasing access to data  this seems like a very bright future  In 1989  a model called the convolutional neural network was developed  We'll talk about the convolutional neural network elsewhere in this module  As we'll be seeing in later part of the module  the convolutional neural network was a key technology for analyzing images  Again 1989  so that was shortly after the back propagation  this was a significant period of optimism  So  as we look at the seasons of neural networks  summer  very hot  very promising period of time  So  around that time  because of the enthusiasm that neural nets had engendered  the people started to apply them in what I'll call the wild  which means applying them to datasets in the real world  and that occurred in the early 1990's  So  now  we've gone from summer into a rather cold winter  The reality is at that time  the models did not perform well  The enthusiasm that existed previously was proven to be unwarranted from the standpoint of how they actually performed in the Wild  There were many reasons for this  but a lot of it was that  I recall that the neural network is much more complicated model than the logistic regression  and the amount of data that one might need to train such a model well is significantly increased  In the early 90's  people really didn't fully understand that these models did not perform well  Around 1995  another key development was something called long short-term memory  It's a bit of a weird name  We will talk about that as well later in this module  Long short-term memory is a very key technology for analyzing data that varies as a function of time  So for example  music or audio signals vary as a function of time  Video varies as a function of time  Long short-term memory is very well suited for that  The key thing I want to reflect in this slide is again that in the mid 90's  there was again some enthusiasm excitement about the potential of neural networks  The other thing to note that I want to highlight here  is that the convolutional neural network  which is at the heart of the modern revolution in neural networks and deep learning  was developed in 1989  a long time ago  The long short-term memory which is now used widely for natural language processing or for analyzing text as we'll see later in the module  was developed in 1995  So  the key technologies which are driving the current revolution in deep learning actually existed at or before 1995  So  this tells us that there were some things that were missing at that time that led to problems as we'll see  and that the underlying technology really hasn't changed  What's changed are some other factors which we'll discuss in a moment  So  after the introduction of long-term  short-term memory  and we had back propagation and we had the convolutional neural network  again  people tried to use neural networks in the wild  You can see this picture denotes fall  Not exactly winter  but it is getting chilly  During this time  again  neural networks did not perform as well as has often advertised  Other methods in machine learning came to the forefront  From roughly around 2005 to 2010  in machine learning  there was  and again this is tongue-and-cheek but fairly accurate  a period of banishment  In other words  in the main machine learning forums  people did not do neural networks or at least did not speak about neural networks at all  The reason why is that the technology really had not performed well enough up to that point  and other methods that were simpler  perhaps easier to understand or to interpret  worked much better  So literally  up until the end of around 2010 or there abouts  In machine learning community  one did not even use the term neural networks for fear that they  if somebody were to use that word  they might be banished or even laughed at  Then because of the fact that neural networks  the term neural networks had come to mean such a negative thing based upon its history and based upon how it performed in the wild with real data  There was a need actually to change the name because if you use  as I said  if you use the word neural network around 2010  people might even laugh at you  So  some people decided to rebrand the technology with the idea that there was now a new moment where this technology really could work effectively but nobody would pay attention to it just because of the name  So  this brought to the fore a new name roughly around 2010 Deep Learning  So  deep learning recall the multilayer perceptron it has a multi-layer architecture remember that that was invented in 1960  So  there's actually not very much new about Deep Learning except the name  it's kind of a cool name  and it's not neural networks  and people were willing to give it a second look  So  we've gone from the banishment of winter  the depths of winter to a spring  Maybe this is the time  Around 2013  a key moment occurred and this  and notice we've now moved to a time of spring in hope  So  the CNN which was developed in 1989 by Yann LaCun and his colleagues  Which dates back to 1989 was applied in the context of two things that did not exist in 1989 and did not exist up until very recently  So  the GPU is a graphical processor unit  This is a computing platform that was originally developed for the gaming community  It turns out to be a very effective tool for parallel or a form of parallel computation  So  the GPU provided a computational platform that we did not have previously  The other piece of this is ImageNet  ImageNet was a dataset or is a data set of images over a million images  So  what we see happen in 2013 is the CNN  the deep convolutional neural network which we will talk about in more detail later in the module  which dates back to 1989 which went through many seasons of despair  was combined with modern computing  the graphical processor unit and was combined with massive quantities of images over a million labeled images  So  these are images for which we had the labels and recall that to do learning we often need labels  In 2013  it was demonstrated that using this technology one could demonstrate a market improvement in performance relative to other approaches  and so this was a fundamental advance  But the thing to notice is that the underlying deep learning technology which really was the convolutional neural network  hadn't changed  Then around 2015 AlphaGo occurred  AlphaGo also was based upon the convolutional neural network and something called reinforcement learning  AlphaGo demonstrated the capacity to play the game Go at a level that exceeded the performance of humans  Now in addition  in the context of CNN in image analysis  we've also exceeded the performance of humans  So  the story that this history tells us is that the underlying technology of Deep Learning which is really a rebranding of neural networks  dates back to 1960 with the multilayer perceptron  1989 with a convolutional neural net and long-term short-term bond short-term memory  roughly 1995  That technology is decades old  What's changed to constitute this really exciting moment for Machine Learning is that we have brought extraordinary computational power in the form of the GPU and massive quantities of data ImageNet and others  When you bring these together  the performance of these Deep Neural Networks or these deep learning models can oftentimes exceed the performance of humans  So  the thing that I just want to conclude this little history is to remember that we should be humble because at every moment in that history  where we saw spring or summer  only to be followed by winter  there were moments of tremendous excitement that and then disappointment  So  we have learned a lot over those decades and we know a lot more  We now know the importance of computational power  We know the importance of massive quantities of data  but the lesson that I think we need to take from this is that if we do not understand this history  we may be doomed to repeat it  So  there are situations for which this simple logistic regression model on the left may actually be more appropriate than the Deep Neural Network  So  one of the keys as we move forward with this technology is to understand where the Deep Learning is effective  What type of Deep Learning is effective and also where simpler models might be appropriate  So  there's a very famous idea called Ockham's razor and what it basically says is that if you have two ways of describing data in other words  if you have two models of data and they are equally good at describing the data  you should always choose the simpler model  One should always look for simplicity  and so when we look at the simple but often effective logistic regression and the more complicated Deep Neural Network  one must always recognize that it's probably a good idea to look at data from both perspectives and where the simple model works  the simple model should be used  But there are often situations as history has shown us for which these deep architectures do indeed provide performance which is well beyond that of simpler methods  So  as we move forward in this module  we'll start to study in greater detail why it is that these more sophisticated deep architectures yield performance that has generated so much recent excitement in Deep Learning and Deep Neural Networks 
pW1kbWljsAk,Hierarchical Structure of Images  So  we have discussed in some detail the multilayer perceptron  and we have tried to give some intuition into why it works effectively  However  the multilayer perceptron is not appropriate for all data and in our history of deep learning and neural networks  we briefly discussed the convolutional neural network  which is one of the most significant pieces of technology from the standpoint of analyzing images and many types of data that we're interested in are in the form of images  So  what I would like to do  in this lesson  is to try to provide an intuitive understanding of what the convolutional neural network is doing and why it works  Through that discussion it is hoped that one will also get a greater understanding of why deep learning works and why deep architectures are effective  So  in this lesson  we're going to focus on images and we're going to focus on how one analyzes images using neural networks  As we've discussed elsewhere in this module  the technology that we're going to discuss here  the convolutional neural network  in many settings is demonstrating performance for analyzing images that exceeds the performance of humans  So this is truly remarkable technology and it's worthwhile to try to understand it from an intuitive perspective  So  to do that  what we're going to do is consider what I'll call toy images  So toy means these are cartoon representative images to try to provide an intuition  these are of course are not real images  But the idea is that we're going to develop here are indeed transferable to real images  So  recall that to do learning  we need multiple data samples  here the data samples will be in the form of images  So the data we're going to analyze are images and so what I'm showing in this slide are three images and then an additional piece of an image  where each of these squares represents an image in the shapes that you see within the image are the fundamental building blocks that constitute the image  So  what we're going to do is we're going to look at the analysis of these simple toy images  with the goal of trying to gain some understanding of the underlying motivation for the deep architecture  So  the thing that I want you to notice about these images  is that each of them is composed of examples of these high  what I call high-level motifs  that are shifted to different locations within the image  So  not any one image is composed of all of the motifs  but each image is characterized by a subset of the motifs and the location of those motifs  changes from image to image  So  if you think about images that you might see in the real world  you notice that  they have certain structures that are repeated such as edges  corners  textures  shapes  this is characteristic of all virtually all natural images  So this characteristic is something that we want to leverage or exploit in the context of the deep convolutional neural network  These toy images will allow us to get a sense of how this works  So again  each image is composed of a subset of these motifs  each of which has shifted to a different location within the image  Now  this look at these motifs a little bit more closely  What we notice is that each of the motifs is composed of often repeated sub-structure  So  what I'm doing here is highlighting that within some of the motifs  we see a repeated substructure  and then another repeated substructure  and another repeated substructure  and another repeated substructure  So  the thing that I want to try to communicate through these slides  is that  the overall image is composed of a subset of shifted motifs  Each of the motifs are themselves composed of shifted  what I call sub-motifs  that repeat within the image  So the way that I would like you to think about the data if we start at the top  each image is composed of a subset of these motifs  these are what I'll call layer three of the model  Each of the motifs at layer three is composed of a subset of shifted versions of the sub-motifs and then each of the sub-motifs is composed of basic  what I'll call atomic elements which are these fundamental shapes  So  the key thing to take away from this slide  is that we believe that data can be represented in a hierarchy  So you also can get a sense of what we mean by deep or multi-layered structure  So what we want to do  is to try to build a model that captures this structure  that captures this representation of images  So  the question is  how can we build a model that can capture the intuition in the way that images might be constituted through a hierarchy of this form?
jqhQx9U9kok,Convolution Filters  So  this is to recall the image  just to remind you of the day  the quote unquote data  These are toy demonstration images  to denote this concept  So  the question is  how can we build a model to actually capture this phenomenon? So  what I'm showing at the bottom left is an example image  So  this again is our toy  an example of a toy image  What we would like to do  is to start at the bottom of our hierarchy  and look for one of those atomic elements  So  that triangle that you see on the bottom right is one of our atomic elements  What we're going to do is  we're going to shift that triangle to every location in the image  So  we're going to shift the atomic element  here in the triangle to every possible location in the two-dimensional image  We're going to calculate a correlation  A correlation which measures how similar the local region in the images to our atomic element  which we'll call a filter  So  on the top  what we're showing is the output of that process  So  if you look carefully at the image on the bottom left  you'll notice that there are two locations within that image  where that triangular atomic element on the bottom right is manifested  So  when we shift that atomic element to every possible position in two-dimensional space of the image  when the atomic element lines up with a point in the image where that triangle is actually manifested  we get a high correlation or a high match  So  what we're looking at on the top is a map  we call it a feature map  and what it is meant to depict is the degree of match or the degree of correlation between the atomic filter at bottom right  with every location in the image  So  red in the top image  red means strong or high  So  at those two circular points  those are the locations where that atomic triangle is manifested in the image  Everywhere else where the triangle is not manifested  the match or the correlation between the filter and the image at that location is very small  The process by which you shift that atomic element the triangle  to every location in the image that process has a name is called convolution  So  this is why the name convolutional neural network  The convolution is manifested by shifting that filter to every location in the image  Now  we have multiple atomic elements  not just one  Remember at the bottom of our hierarchy we had multiple atomic elements  So  now what I wish to depict here  is at the left we have again one of our images  and then at the right we have each of the atomic elements  What we're going to do is shift each of those atomic elements  each of those fundamental filters  one by one to every location in the image through two-dimensional Convolutional Process that I talked about  What we're going to get is a feature map for each atomic element  So  remember a feature map denotes how strongly that atomic element matches the local region in the image  So  we call that a map  Its a map that reflects the degree of match between the atomic element in the image  We do that for each of the atomic elements  So  therefore  we get a stack or multiple feature maps  So  that depiction of a stack of rectangles or squares is meant to reflect each of the feature maps for the respective filters  So  using this process  we can see how we can identify each of the atomic elements  the lowest layer in the hierarchy  So  the next question becomes  how can we identify the sub-motifs  Remember the sub-motifs are composed of combinations of shifted versions of the atomic elements  So  now what we're going to do is  we're going to repeat the process  where we're going to convolve or shift filters to every two-dimensional location in the feature map  But here now the filters are many stacks of atomic elements  So  what I'm trying to depict here is one the sub-motifs which is composed of a square  an ellipse  and a triangle  We're trying to find that sub-motif composed of those three fundamental elements  So  it's fundamental atomic elements  What we would expect if that set of three shapes constituting that sub-motif  If that is present in the image  what we would expect is that the respective layers of the feature maps corresponding to the square  the ellipse  and the triangle would be high  strong  We can do this for another of the sub-motif  So  here now is showing a different sub-motif  and so here those three fundamental shapes correspond to three different layers in the stack of feature maps  What we would expect is that  if that sub-motif is present  then the respective feature maps associated with those atomic elements would have strong amplitude nearby each other in space  So  we can do this for each of the sub-motifs  So  what this slide is depicting is that  each of the atomic elements for notational purposes  is assigned a color and the filters at the second layer correspond to the sub-motifs  So  what this is showing via the colors is  those are the layers that we would expect those respective filters at the second layer to have strong amplitude  if they correspond to the respective sub-motif  So  by constituting this filtering process at the second layer  we can now search for each of these sub-motifs  So  then we repeat the process  So  now we're going to convolve this second layer filters which are designed to look for those sub-motifs  We're going to convolve them with the feature maps from layer 1  The process of convolving each of the filters from layer 2 with the feature maps from layer 1 is going to yield a new set of feature maps  what we'll call the layer 2 feature maps  These layer 2 feature maps are going to be telling us where each of the sub-motifs are manifested in the image  Now  we can repeat this process once again  So  now at layer 2 in the middle of the slide on the right  we see each of the filters corresponding to the sub-motifs  Now  what we're looking for is the top layer motif  That top layer motif is composed of four of the sub-motifs  So  each of those four of motifs is assigned a color  and what we're seeing is that  when we look at the layer 3 filters to manifest the motif  we have strong excitation nearby physically of four of the corresponding sub-motifs  So  then we can do this for each of the motifs at the top layer of the model  So  now we're going to have filters which are designed for each of the motifs at layer 3  Then  finally after this we're going to convolve or shift to all two-dimensional spacial locations  the layer 3 filters with the layer 2 feature maps  and then finally constitute a layer 3 feature map 
oPVj17gB7gY,Convolutional Neural Network  Then finally  we're going to make a classification decision  The classification decision will be based upon the features at the top of the model  or at layer three  So the thing that is really important to notice about this deep architecture  is that it's manifested by repeated application of the same process  So beginning at the bottom  we take the original image on the bottom left  We convolve that image with a set of atomic or fundamental building block images  So  convolve means that we take each of those atomic elements  we shift it to every location in the image  After we're done with that  we get a map  what we call a feature map  which is a reflection of the degree to which each of the atomic elements is manifested within the image  We do that for each atomic element and we get what we call a stack of features  a stack of feature maps  That gives us the map by which the basic fundamental building blocks are manifested in the image  We then constitute layer two filters  Those layer two filters are looking for combinations of elements from the first layer which are simultaneously excited nearby each other  So  if we have simultaneous first layer atomic elements  which are excited near each other  they constitute a sub-motif  So  the layer two filters are searching for those sub-motifs  We convolve the layer two filters with the layer one feature maps and then we get a layer two feature maps  Then we constitute layer three filters which are now designed to look for motifs  convolve those filters with the layer two feature maps and then finally we get a layer three feature map  That layer three feature map is our final feature map and then based upon those features  we're going to make a classification decision  We're going to decide the classification of the image  So  for example if we're doing radiological studies in medicine  the classification that we might do is to determine whether the image  the medical image corresponds to a benign or a malignant tumor for example  That might be a classification decision we would make  That classification decision is based upon the features at the top of the network  So this architecture is called a convolutional neural network  The convolution is manifested through the shifting process  The neural network is manifested for some other technical details which I'm leaving out here for simplicity  The deep nature of this architecture is evident through the deep construction  So  now the next thing that we have to ask ourselves is now that we have some intuition on how we might design in architecture of this form  In the real world  we don't deal with toy images and we don't deal with these simple shapes  So the question is  how do we design a model that allows us to learn how to build a model like this for real images? That will be the next topic that we will take up in our next lesson 
3LqwxAbOXpM,CNN Math Model  Now  the interesting thing is that everything that we learned in our illustrative example essentially directly carries over  So  let's again start by studying the convolutional neural network which is designed to study and analyze images  So  i sub n is meant to represent the nth image  which is meant to emphasize that we're not going to do this for just one image  we're going to do this for thousands  and millions  tens of millions of images  So  to do learning  we're going to use many  many images  and by studying these in analyzing these images  we'll be able to learn a model  So  within the context of that learning  the model is given access to each of those images  and so just for depiction  we again show one of those toy images  but of course it will be a real image  and i sub n is the nth image  Now  remember that the first part of this model was to convolve atomic elements  remember in the form of shapes in the toy example  convolve those atomic elements to every location in the image  every two dimensional location in the image  and then to constitute a feature map  where the feature map reflects how strongly that atomic element is manifested in the image  So  what we're going to do is the same thing  but now I do not know the atomic elements  So  what I'm going to do is  I'm going to represent them by parameters that we're going to learn  So  phi 1  phi 2  through phi k correspond to k filters  Instead of them being shapes  as they were in our toy example  these are just going to be the parameters of these filters are just the pixel values  So  what we're going to try to do is learn the pixel values associated with filter phi 1  filter phi 2  and filter phi k  Now  I want to learn what those parameters are  I want to learn the values of the pixels for each of those filters  Now  let's assume we knew what they were  At this moment  let's just assume we knew phi 1 through phi k  If we know those pixel values  we can shift them to every location in the image and constitute a feature map exactly in the way we did for our toy example  So  the way that this is depicted in the second line  is that In is the Inth image  This process of filtering which manifests these feature maps is a function of the filters phi 1 through phi k  Then  this yields a feature map  which I'll call m sub n  where m sub n is the feature map  the stack of feature map at layer one for the nth image  Then  we can repeat this process  So  now  the stack of feature maps from layer one is convolved with a layer two set of filters which I'll represent as psi 1  psi 2  through psi k  Again  I don't know what those are  I don't know what the values of those filters are  What I mean by the values is  each one of those filters  psi 1  psi 2  through psi k is a digital mini stack of images  stacked to match the feature map from the layer below  I don't know what the values are  but if I didn't know what the values were  I can convolve each of those filters  psi 1 to psi k  with the layer one feature maps  and then manifest layer two feature maps  which are a function of the feature maps from the layer below  m sub n and the filters psi 1 through psi k from the second layer  If I knew what those were  if I knew what psi 1 to psi k are  I can do that operation  What we're going to do is  we're going to learn those parameters  We're going to learn psi 1 through psi k  Then  this process can be repeated   We go to the layer three  and we have filters omega 1 through omega k  These are again stacks of filters  They have values  We no longer have those simple shapes  but they have values in the same way  We can convolve those layer three filters  with the layer two feature maps  and then finally  get a feature map at the top layer  which we call g sub n  which is the top layer feature map  which is a function of the feature map at the layer below and the filters at the layer below  Then  finally based upon the features at the top  we can build a classifier which will provide a label for that image  So  script L is meant to represent the label  The label is a function of the features at the top  g sub n and parameters w  which reflect the parameters of our classifier  So  now if we look at this  first of all  the structure of this model is exactly the same  as in our toy example  The only thing that distinguishes this from the toy example is that we're no longer assuming those basic atomic shapes  Instead  we have these parameters  The parameters are the pixel values at every location within those filters  In the process of convolutional filtering is exactly the same  and then at the top of this model  we have the top layer feature maps  which we call g sub n  Those are sent into a classifier which is characterized by parameters w  That classifier could take a form that we're familiar with  For example  that classifier could be a multilayer perceptron or it could be a logistic regression  But in any case  it's characterized by some parameters which we denote by w 
CJQ9qrFo5ko,How the Model Learns  So  if we look at this model  what we now have is a model characterized by parameters  The parameters of the model are the filters at the bottom  the filters at the second layer  and the filters at the third layer  Phi  Psi  and Omega  and then finally  the parameters of our classifier W  So  on the right  the last bullet is to emphasize that the parameters of this model  which are unknown and which we wish to learn  are Phi  Psi  Omega  and W  So  now  the assumption  the way we do learning  is we assume that we have access to a large set of data  a large set of labeled data  Now  recall that I said that one of the key things that has made this technology  which was developed in 1989  one of the key things that has made it so powerful in recent years  is access to massive quantities of data  So  here  that data is represented by the pair I_n is the nth image  and y_n is the label of the image  Recall that from the very beginning of this module  we always talked about the concept that we needed labeled data to do learning  Here  we assume we have capital N examples of images and labels  We look at the second bullet  where for simplicity  much as we did when we studied the logistic regression in the multilayer perceptron  we're going to assume that the labels are binary  either plus one or minus one  We're doing that here purely for simplicity  In other situations  the number of labels could be very large  hundreds  thousands  But for simplicity  let's just leave the labels to be plus one or minus one  binary  to be consistent with our discussion of logistic regression in our prior discussion of the multilayer perceptron  So  now  looking at the third bullet on the right  what we're going do is we're going to constitute  what we call  an energy function  what I represent by E  which is going to compute  what we call  the loss between the true label  y_n  and our model label  l_n  So  if we look at the figure on the left  we see that at the top of the network  we take the feature maps at the top of the network and using our model or our classifier with parameters W  we make a prediction of the label script l  So  what we could do is we can calculate a loss  The loss is a measure of the difference between the true label  y_n  and our predicted label  l_n  What we would like to do is to make that loss as small as possible  and there are many ways that one can choose that loss  The loss is just simply a measure of the difference between y_n and l_n  Again  there are many different ways that one can compute that  So  our goal here  looking at the simple equation after the third bullet  is to try to learn  or identify  or compute  the parameters  Phi  Psi  Omega  and W  that minimize that sum over loss  The loss  again  is essentially the difference between our accuracy of our predicted labels and the true labels  This is what we mean by learning  So  when we talk about machine learning  this figure  this slide  really brings it together  The left figure  our convolutional neural network  is the machine  That's what we're trying to build to try to characterize images  The learning part is manifested by taking data here and examples of image I_n and the label y_n  and trying to learn the parameters  So  what we mean by learning is to learn the parameters of our machine  of our model  such that the difference between the prediction of the model  l_n  and the true label  y_n  is small  So  in the last bullet  we say that we put  what's called  a hat sign on the top of those symbols  which is the  anyway  a small little symbol like a hat  Those are our predictions  So  our goal is to try to design  or predict  or estimate  those model parameters  Phi  Psi  Omega  and W  such that the total loss manifested in the middle equation  between our predicted labels and the true labels is small  This is what we mean by learning  Now  why is this hard? It turns out that this is a very challenging problem  which is part of the reason that we've had that long history of machine learning  So  when I talked about the seasons of machine learning  and we went through springs and winters  and more springs and summers and winters  the reason that we went through those periods of enthusiasm and then feelings of desperation was because there were many challenges to doing this learning  to estimating these parameters  One of the key aspects of that is that this search  the estimation of those optimal parameters to  try to estimate Phi  Psi  Omega  and W  the number of parameters that we have to estimate is massive  very  very large  So  consequently  trying to estimate those parameters is very difficult  So  just to give you an example of what that might mean  here is a situation in which we're only looking at two parameters  So  along the two-dimensional bottom axes  we might be looking at parameter 1 and parameter 2  Along the vertical axis  we're looking at the value of that function E  which is a measure of the quality of the fit of the parameters to the data  So  what you notice here is that there are multiple valleys  multiple areas where the parameters might lead to relatively small error  E  So  what we find is that there are many different sets of parameters which give what we call local optimal solutions for the model  So  it is very difficult to move in this parameter space to actually optimize for the parameters  Here in this simple example  I'm only showing it for two parameters  Again  the vertical axis is the quality that the error  E  and the other two axes represent the values of the parameters  So  in two dimensions  in this simple example  I can actually draw a picture and look at it  But for the real problem  the number of parameters in Phi  Psi  Omega  and W  could be millions  millions of parameters  So  consequently  we can't even draw a picture like this  and it is extremely difficult to search in that massively high-dimensional parameter space to find the parameters that yield the best match between the predictions of the model and the truth of the labels  So  it has taken literally decades of research and study to learn how to do that in an effective way  That research and the developments that have been made during in that period have played a key role in the success of modern deep learning  in addition to the computational power and the access to data 
0HUU612UuXI,"Advantages of Hierarchical Features  What is the advantage of this deep architecture? So  we'll go back to where we started  So  remember  that the motivation here was we had those toy examples  and we had those  fundamental atomic elements composed of the shapes  Here  at layer one we have the simplest atomic elements which are the basic shapes  At layer two  we have what I call the sub motifs which are basic combinations of those fundamental shapes and then finally at the layer three  we have the motifs which are combinations of some motifs  So  the thing that one might ask is  \""If in this model  at the end  at the top of the architecture  the learning is based upon the features at the top of the architecture  which are ultimately used to make a classification decision  Why can't we just build a model based upon the top layer motifs? Without having to worry about the sub motifs at layer two  and the atomic elements at layer one\""  What's the advantage of this deep architecture? So  if we were to build an architecture  which was only based upon the filters at layer three  then each of those layer three filters  would be learned independently  There would be no understanding in the learning of those filters  that they share structure  So  I'm reminding you by showing examples of repeated structure at layer three and layer two  That indeed it's that layered structure that motivates the deep architecture  So  any time a particular sub motif is manifested in any of the motifs at layer three  the learning of that motif provides information about the other motifs through the shared information  So  the idea is that by sharing structure  between the different elements of the filters at the various layers  be manifested by the fact that they are all made up of shifted versions of more fundamental building blocks  through that shared structure  and by this deep architecture  We can use our data more effectively  and share knowledge between each of the motifs  such that knowledge of one motif can improve knowledge of another motif  through the shared substructure that they possess  So now  we have a little bit of a sense of how machine learning is manifested  We have a sense of how we build a machine  Here the deep convolutional neural network  We have our first taste of how learning occurs  Where learning is manifested by taking large quantities of data  typically labelled data  which means that we have the data and we have the associated label  We have seen what we mean by learning  learning is the concept of trying to estimate the model parameters  such that the predictions of our model are consistent with the labels  the true labels of the data  We also have gotten some understanding at least some intuition  that this could be quite challenging  that learning can be challenging  particularly when the number of parameters is large  So  as we conclude this module on the introduction to machine learning  in the next lesson  we'll provide some examples of how modern machine learning is transforming life for people around the world  Those examples  and how this technology is providing breakthroughs in many areas  will help to motivate us as we dig deeper  and try to learn more about modern machine learning"
1vvOgSqDKpE,CNN on Real Images  So we have gained some intuition into how the convolutional neural network works  We've also gained some early indications on how machine learning is manifested from the standpoint of learning the model parameters  based upon the data  So as we move forward in this module  we're going to learn more of the complexity and details of how this model learning is actually implemented in practice  Some of these details are challenging  And so as we move through those challenges  I think that it might be nice for us to remind ourselves from time to time about the significant advances that have been manifested by modern machine learning and deep learning  And through that motivation  to push through some of the technical challenges and details to actually make this work  So in this lesson  we're going to talk a little bit more about these models and also show how they're used in practice  Recall that in the previous lesson  we introduced the concept of how one learns these filters based upon data  So now we're moving away from those toy examples of atomic elements and we're now moving towards real data  So when we analyze real data  what do these filters look like? What do the filters at the different layers look like? So just to give an example of what these look like  and to also provide some intuition  and motivation  we'll look at these filters for a particular data set which was composed of images of faces  cars  motorcycles and things like that  And so at the top  what were depicting are each of the filters that were learned at the first layer of the model  at the bottom layer of the model  And so each of those small squares represent each  an example  of one of the learned filters through the convolutional neural network  And recall that in our original discussion  we were talking about basic shapes  but now we're talking about filters  which are actually learned based upon data  In these small images at the top  you notice that there are dark colors and bright colors  white colors  The dark colors correspond to negative values at those pixels and the white corresponds to positive values at those pixels  And so the thing that you notice  if you look at those sub images closely  is that they are characterized by rotated versions of a fundamental shape which is manifested by a dark set of pixels  and a bright set of pixels which are near each other  are aligned with each other  What this corresponds to is like one cycle of a Sine Wave  where one cycle goes down  the other cycle goes up  These are manifested in two-dimensional shape and that single cycle of a sine wave is rotated to a different rotational angle for each one of these layer one filters  So each of the layer one filters are by and large manifested by a single cycle of a sine wave rotated to a different angle within that particular atomic element or that particular filter  These shapes  it turns out  are have been widely studied in mathematics  They have very fundamental characteristics in mathematics  And they're called Gabor functions  Perhaps  more interesting is that people have done studies on mammals  like rabbits and rats and mice  And what they've done is they've shined light onto the retina of a mammal and have asked the question  what shape should one shine on the light of a retina of a mammal  such that a signal neuron in the visual cortex of that animal is turned on? And so if you turn on a single neuron in the visual cortex of a mammal  arguably  that would indicate that that is the most fundamental shape that the visual cortex can recognize  So again  what has been done is that very fundamental shapes have been shined on the retina of a mammal with the goal of trying to identify which shape shine on to the eye of a mammal will trigger a single neuron  Which is reflective of the fact that this is the most fundamental shape that the visual cortex can recognize  It has been determined that those shapes look almost exactly like the atoms that the convolutional neural network has learned at the first layer of this model  So it turns out that the shapes  and the shapes that you see on the top of this figure  each of those little squares which are the basic atomic elements that have been learned by the convolutional neural network  correspond very closely to what biological studies indicate are the most fundamental elements that a visual system of a mammal can recognize  This was not required of the convolutional neural network  but it's  many people feel  very interesting to see that this computational engine  the convolutional neural network  at least at the first layer of this architecture  seems to behave in a way that's consistent with the visual system of mammals  So now  we go to the second layer of our deep architecture  And that corresponds to the images at the bottom left and at the bottom left what we're reflecting each of those squares represents one of the filters at the second layer of our deep architecture  And remember that I called those sub-motifs  previously  If you look at these closely  you'll notice that each of them corresponds to a fundamental shape or structure which is more sophisticated than our atomic elements at layer one  If you look closely at some of those layer to images or filters  you'll notice things like eyeballs  And remember that these are images of faces that we're analyzing and other such things  And then if we look at the bottom right  these correspond to filters at the third layer of the architecture  at the top layer of the architecture  And if you look closely  some of those filters correspond to sketches of faces  So the interesting thing that these filters  which are learned entirely based upon the data  is that at the first layer  the most fundamental layer  the layer one filters  correspond to building block filters which are consistent with what seems to be the fundamental means of exciting the neuro architecture of the visual system of mammals  At the layer two  which is at the bottom left  we see more sophisticated structure  If you look closely  you'll see things like eyeballs  Those are manifested of substructure in the image  And then  finally  at the higher level where we now see more sophisticated structure  we can actually see sketches of faces  So this gives us some further intuition into what these filters  at the multiple layers of the model  are actually learning whenever they're given real images 
NpDIRrrXd-Y,Applications in Use and Practice  So  now let's based upon using that even further intuition  let's ask the question  how well does this technology perform on real data at massive scale? So  recall that one of the significant developments in deep learning occurred around 2013  which was the bringing together of the convolutional neural network which was our fundamental model  which actually was developed around 1989  computational power  the graphical processor unit  and massive data at scale  So  what you see here are many many images  reflective of what's called the ImageNet Dataset  This corresponds to classifying a thousand different types of images  a thousand classes  So  remember when we were talking about the labels of the images  we were talking about binary labels  plus one  minus one  We introduced that purely for simplicity  We are now using with this deep convolutional neural network architecture  At the top of the network  we're making a classification to 1 000 different classes  So  significantly more than binary  For the training data  and remember that one of the key things that made Neuro Networks work so well  was massive quantities of data  So  there was roughly 1 000 example training images for each of the 1 000 image classes  and consequently about one million or actually 1 2 million training images  To give you an example of what some of these images look like  three of them are shown at the bottom left  So  the thing to take away for this figure is  we look at the bottom left  These are real images  These are sophisticated images  and we're going to use these deep convolutional neural networks to classify them  The image to the right is simply to depict massive quantities of data  So  now how well does this technology work? So  the technology was developed as I said  around 2013  This is the performance of these models as a function of time  So  these are the best performing algorithms as a function of time  So  the 2010 and 2011  that is technology that preceded the convolutional neural net  Then  around 2012 to 2013 is when the convolutional neural network came to the forefront  You could see that there was a significant improvement in performance  This is the classification accuracy  the vertical axis represents classification accuracy  and the horizontal line at five percent is representative of performance of humans  So  we could see that as the years have moved from 2012 to 2013 to 2017  we are now at a point where these deep convolutional neural network architectures are providing performance that is better than that of humans  So  now to give you a sense of other things that we could do with this architecture  So  these are eight example images  and this is an example of how the model works  So  recall that there are a 1 000 different labels  and the model provides a probability of which label is associated with the image under test  So  what you're seeing here is  in each case you see the image under test  and then beneath that you see the probabilities from the model of what the model thinks the image is representative of  So  you could see that in each example  the bar represents probability  So  what we're listing is  for each image on the top  beneath it we're depicting the five most probable labels that correspond to that image  So  if you look at each of these  and these are representative examples  the performance is quite impressive  So  the takeaway from this is that  first the images are of significant realism  and complexity  Secondly  the performance of the model is truly extraordinary  When we look at this relative to the performance of humans  the ability of humans to do this labeling process is actually and currently inferior to the performance of these deep neural network architectures  Another key breakthrough that I talked about in the history of the Deep Architecture occurred recently  was in the context of Go  So  Go is a very famous game that is played largely in Asia  The Go board which is depicted here is significantly more complicated than a game such as chess  This game Go is almost a religious game in places like South Korea  There are people in South Korea who are essentially professional Go players in their schools for learning how to play Go  The game Go  which is a two-player game  was believed to be so sophisticated and so challenging that there was no way that a machine or an algorithm of the sort that we have looked at could defeat a human  So  recently  DeepMind who is based in the UK  they developed an algorithm which at its heart  was based upon the deep convolutional neural network technology that we have discussed at length  They built an algorithm that could take a picture of the Go board  send that picture into a deep convolutional neural network  Then  based upon what the algorithm saw in the image of the Go board  it would recommend the best next move for the machine to take  So  DeepMind developed this technology  it worked well  They ultimately did a match between the machine  which essentially is based upon a deep convolutional neural network  and some additional technology which is put on the top of that  which is called reinforcement learning  But the heart of it was actually the convolutional neural network  That algorithm or that machine  played some of the finest Go players in the world  Many of them from Asia  and as I mentioned  these are essentially professional Go players  Something occurred that previously people thought was inconceivable  That was that this deep machine learning technology  beat the very finest Go players in the world  This was a very very significant milestone  It represented the fact that this deep learning technology had truly matured to the point where it could do things that previously people thought were impossible  Other things that have been done  digit recognition  These deep architectures have been used to do other things such as recognizing images that correspond to human  writing a significant challenge  Another thing that has been done just to give again a sense of the sophistication of the technology  So  here are six images  each of which was analyzed by the convolutional neural network architecture that we've discussed  deep convolutional network architecture  So  the algorithm took the image in and then send that through a deep neural network architecture  The features at the top of the architecture which were previously used for classification are here used to do something arguably even more challenging  So  given the features at the top of the neural network  those features were sent in to a natural language processing algorithm  which then synthesized text to describe the image  So  each of the captions that you see written here in English was manifested by a machine  So  what is going on here is the image goes in  That image is sent through a deep convolutional neural network  The features at the top of that deep convolutional network are then sent into a different Neuro Network  which is then used to synthesize text  So  if you look at the text  you notice that it's a very high quality  highly realistic  grammatically correct  also very well representative of what is happening in the image  So  this is an example of how modern machine learning is generating so much excitement  This is an integration of image analysis and text synthesis  If we recall back to a prior lesson  we talked about something called Long Short-Term Memory  which we will discuss in further detail elsewhere in this module  The text analysis  or the text synthesis that you see here is based upon the long short-term memory architecture 
86gQZEJIjHw,Deep Learning and Transfer Learning  Now to close this lesson playing the go game is interesting  Synthesizing text to describe images is also interesting  But there are many other things that one might be concerned with that perhaps have more value in human life  And so another area where image analysis is very important is in fields like  radiology  opthamology  and dermatology in medicine  And so  medical doctors are very busy people  And some medical doctors spend a tremendous amount of time looking at images  Instead of talking and counseling patients  So one area where deep learning seems to offer significant opportunity is in the context of image analysis for medical imaging  And so  areas in which deep learning has shown tremendous promise is in ophthalmology  for example  in diabetic retinopathy  it's been demonstrated that convolutional neural networks can perform as well if not better than opthamologists  Similar performance has been manifested in dermatology and in radiology  So  one of the things as we close this lesson that is interesting  so recall that in the deep architecture  so what we're showing here is deep learning and in one of our previous lessons we talked about transfer learning  I talked about ImageNet in the fact that we have millions of labelled real images  And so on the top portion of this slide  I'm showing a set of images from ImageNet  And then on the top to the right of them  I'm showing a deep convolutional neural network  And so the question that you might ask is  as we look at the deep convolutional neural network  which is a sequence of convolutional filters acting upon an image at multiple layers at multiple scales  Is there something unique about medical images that would require us to only train on medical images? Or can we re-use the parameters of the model from the ImageNet and apply them to medical imaging? So this is actually very important because recall that one of the key aspects that has made deep learning work so effectively is the access to massive quantities of labeled data  So the ImageNet is an example  Millions of examples of images and corresponding labels  However  in medicine  this is actually very challenging  so if you think about  for example  dermatology  for us to build a deep learning algorithm for which we had millions of images  dermatological images  that were labelled  That would imply that we would have to have millions of images of various aspects of skin disease  And we would need medical doctors to label them  That would be a very expensive and time consuming process  So the question that one might ask is if we look at the architecture  that was designed for the ImageNet based upon natural images  can we transfer those filters to a model applied to dermatology or to ophthalmology  or to radiology? If the answer to that is yes  that would imply that the ability to apply this technology to other fields can be significantly advanced by leveraging the massive quantities of data that we have for natural scenes in the world  again based upon digital hand held cameras for example  And so the key thing I want to reflect by this slide is that it has been found that one can take a deep neural network designed for ImageNet  you can take that deep learning architecture and the parameters of that  and almost entirely transfer them to applications  for example  in for the analysis of diabetic retinopathy and ophthalmology  The rectangle is meant to show that we can take the weight of the model that were learned based upon ImageNet and transfer them to images that look ostensibly very different here from a confocal images  And then transfer those images  sorry  transfer those parameters in the rectangle and then only learned parameters at the top of a network  which are directly applicable to the confocal images  And so  consequently the number of parameters that we have to learn is significantly reduced  because instead of having to learn all of the parameters  we only learn for the medical images  the new parameters at the top of the network  and then we transfer all of the parameters from the ImageNet  This concept of transferring parameters from a model trained on ImageNet to a different images for example  in medical applications  this is a significant positive aspect of deep neural networks  Which has been proven to be very valuable in applying these technologies to new and different fields for which we may not have quite as much labeled data such as in medical imaging  So I close this lesson by showing some examples of digital pathology data  These are images of cells used in  for example  cancer diagnosis  This is another area where deep learning and deep convolutional neural networks are expected to have very very significant application  So as we close this lesson  the key thing to notice Is that this deep learning architecture  this convolutional neural network  is now being used across a wide array of different types of applications and different types of images  and is now producing performance which is often exceeding the performance of humans  The key thing is through this concept of transfer learning  many of the parameters across these arguably very different applications can be transferred almost directly  This concept of transfer is very powerful and will play an increasingly important role  As deep learning is applied to other fields of image analysis 
A5ts_-xuxnA,Introduction to PyTorch  Hello  everyone  Welcome to the hands-on portion of the course  My name is Kevin Liang  I'm a PhD student advised by Professor Lawrence Carin here at Duke  and I'll be your instructor for these sections  In these parts  we'll walk through how to build the basic components of the neural networks we learned about during previous video lectures in Python  These sections are considered optional  but we strongly believe that working through these exercises is the best way to develop a thorough understanding just like in any other math or computer science course  We could actually build a neural networks in pure Python or with Python's numerical computation package  NumPy  However  we'd find these implementations to be slow both to write and to run  Although it's a great exercise if you want full mastery  in practice  we typically build deep learning models with one of a number of machine learning libraries  For this class  we'll be using PyTorch  the framework built primarily by Facebook AI Research  There are many advantages to using a library like PyTorch  First  many deep learning models have certain operations in common  Using something like PyTorch means we don't have to write all these functions to ourselves  Importantly though  a key difference between these libraries and the way we would probably implement them is that they are also written to be computationally efficient with the capability of utilizing graphical processing units or GPUs  Deep learning models can take a long time to train  and a GPU implementation can be the difference between a model training in hours and one that takes days or weeks  Additionally  as we'll see in the next module  almost all neural networks are learned through backpropagation  which requires taking gradients with respect to your network  These can be derived by hand and indeed  if you're using Python and NumPy  you would probably have to do that yourself  On the other hand  PyTorch's Autograd engine can do this differentiation for us automatically  which makes building models much easier  Finally  it's a really exciting time to be in machine learning right now with a very large community out there  researchers  professionals  and students  PyTorch has one of the fastest growing communities of users  and it's very common to find open-source implementations of state to art models  step-by-step tutorials with code examples  and active forums teaming with debugging advice or recent trends  Most of the materials for the hands-on sections are organized in Jupyter IPython notebooks with tax explanations and figures embedded between the cells of code  Before I direct you over to the first notebooks on setting up your environment  a couple brief comments  First  we're going to assume some basic familiarity with Python and NumPy  Check out the notebook  Python Prerequisites  and make sure you understand most of the sampler code  If you're a little rusty or you've never seen Python before  we recommend brushing up before attempting any of the hands-on sessions of this course  Second  while GPUs greatly accelerate the training process  not everyone has access to one's capable of handling the computation for deep learning models  As such  we'll be sticking to simpler problems that can be run with just CPUs  You'll find that writing for CPU computation is very similar to writing for GPUs  All right  that wraps up our introduction  Now  let's go to coding environment setup 
K6SqhPqL6fY,How Do We Define Learning? Previously  we've gone through and we've defined different types of networks  we started introducing what a deep network actually is  what a deep neural network actually is and we've set up and talked about what they can do  But we haven't yet really talked about how we can actually learn a deep network  In this video today  we're going to be talking about how we actually can define learning  and this is going to mean how can we define learning mathematically  and this will allow us to set this up as a problem and allow us to actually learn and network  So  if you recall  we had this previous schematic  where we are showing that we wanted to combine our training set or training data  which is shown here on the left  where we have an different training examples  these are independent data examples  where we have features or co-variates and we have a particular outcome that shown here in the y's and what we want to be able to do  is create a network that can actually predict these outcomes given the features  So  on the right we're showing here the logistic regression model or network  where we're just showing we have all these features and they're combining linearly to construct the zi and we give the equation there  this is the exact same equation that you've seen several times already and then we're going to convert it into a probability sigma zi  We previously have stated that  we're just going to combine our training set and with this network to learn the parameters and we're going to have our learn parameters here  But we need to talk about how we actually do this  how do we actually learn the parameters  given our training data and our network that we want to learn  So  this fundamental question that we're going to talk about today  is how do we actually do this? In particular  given a large amount of data and a model or network that we want to fit  how can we both efficiently and effectively learn the model parameters  and we want to have these parameters be effective  we want them actually to predict fairly accurately  But we also want them to be learned efficiently  We have a finite amount of computational resources  we want to use these resources efficiently and this is going to be a big deal if you want to use this in something like a cell phone or learn these on the fly  Specifically  let's talk about what it is we're trying to do when you're trying to do learning and so succinctly  we just want to learn parameters that give us the best performance  All this means is  given data  let's find the best parameters b for that data  But  we're going to run into our first problem here  we haven't actually defined performance  So  how can we actually go through and define the performance of this network? So  we have this network on the right and we say it can make predictions and we're going to find this parameters  but how are we actually going to define how well it's actually doing  So  the way this actually happens is we're going to use something called Empirical Risk Minimization  Empirical Risk Minimization is jargon in the machine learning field and we'll talk about what it actually means  But what we're going to have here  is we're going to have a loss function and a loss function is going to take two inputs  It's going to have a true value or true outcome and it's also going to have our prediction and this loss function is going to define a penalty on a poor prediction  If we have really great prediction  we're not going to really pay a loss  but whenever we have poor predictions we're going to pay a big loss and what we want to do is minimize the average loss and so we still need to define what this loss actually is  but if we know that we have this loss  all we're going want do  is just minimize the average loss  if the loss is penalty for doing bad  we just want to minimize it  So  mathematically we can state how we actually want to find our parameters b  So  in this equation here on the bottom  we have that b star  these are optimal parameter  is going to be the argument of the minimum of one over N  the sum over all of our data points and it's going to be for each data point we have our loss function  we have our true label and we have our guess  right? So  this guess the sigma zi here  the sigma zi comes from our logistic regression model and we have the parameters b a little bit hidden here  but zi is dependent on parameters b  so our parameters b are actually going to choose  what we're guessing for each of these  So  what we want to say  is now that we have some sort of loss function  we just want to find the parameters that give us the minimum average loss and this is our mathematical statement of what it is  but we still need to define what our loss function actually is  So  our loss function  so just as a reminder we have sigma zi  it's going to be defined as our predicted probability and yi is our true label or outcome  This is something that we have from our training set  Our network is going to change into this probability by combining with our parameters b  and this was shown in that network model  So we can view this loss function  we can view this as the negative log-likelihood  the log-likelihood is widely used in statistics  generally in statistics  we're trying to do a maximum log-likelihood  When we talk about losses we're trying to minimize a loss  so we just take the negative of the log-likelihood  So  we say that  the loss of our true outcome and our predicted probability is just equivalent to its negative log-likelihood and we're just writing it in there  minus log p of yi given our predicted probability  and the mathematical form here for this negative log-likelihood  when we're dealing with any binary problem is given below here  So  we have this loss function of y and our prediction  the sigma z and it's going to be equal to minus y log of sigma z minus 1 minus y log of 1 minus sigma z  So  this is a little bit of an obtuse mathematical form if you've never seen it before  So  we're going to go through and we're actually going to draw what this equation is actually doing to try to get some understanding about what's going on  So  what we have here is just a visualization of the logistic loss function  so the logistic across entropy loss here  is that equation that we have defined  we defined this on our previous slide  this is just a reminder of what it is  But there's two possible outcomes for what y is  y can either be positive or y can be negative  right? If it's positive it's a one and if it's negative it's a zero  this is just how we've decided to define y  So  we can visualize the two cases here  we can visualize when the case is positive and we're showing this in the dashed blue line  and if we look at the trend in the dashed blue line here  which is when we have a positive outcome  If we're predicting that the probability of a one is happening every single time  we're saying there's a 100 percent chance that there's a one  then we pay no penalty at all if we get it correct  But the problem is  that if we're overconfident we're actually going to pay huge penalty  So what's going to happen  is if we're predicting that it's a one and we're predicting this extremely confidently and it's actually a zero  we're going to pay a penalty that actually increases to infinity and so we want to guess the correct answer but we actually don't want to be overconfident  and so this is a nice loss function that's going to penalize overconfidence  but still encourage us to get a correct and it comes from deep properties of statistics  We can talk about what this goal actually looks like for binary classification  right? So  if we have our optimization goal  we're going to minimize the average loss  So  we have this b star here  this argument minimum over all possible b of our average loss function here  and the average is taken over all data points  So  if we have a logistic problem or we're using logistic regression or anything where we're doing a binary problem and we're doing it probabilistically  we can use this logistic loss  or it's also sometimes called the cross-entropy loss and we've just defined it there again  this is the same exact equation you've seen before  But now we're just going to try to find the b star that minimizes this loss and we're defining lost just like this  So  we need to come up with a way of actually saying while now we have a precise mathematical statement on what our parameters should be  we need to come up with an optimization algorithm that can actually find these parameters  So  just to recap what we've talked about in this video  we have our training set  we have our model or network that we're trying to learn and we can define this  however we want and here we're just visualizing the logistic regression model and we have this learn parameters  and in order to learn these parameters we're going to define this loss function and we're going to define an average loss and we're going to set up an optimization algorithm to find these parameters and that's what's going to come up in our next videos 
OPqJv609aIc,"How Do We Evaluate Our Networks? In this video  we're going to talk about how we actually evaluate our networks  We've talked about a lot about different options we have for defining a network  We can make very shallow networks  we can use logistic regression  which is considered to be a fairly simple model  or we can use these deep neural networks  and we're going to talk about more and more complicated networks as this course goes on  But one of the fundamental questions we're going to have is how do we actually evaluate how well these networks are going to work? We're going to start out with a question  We've talked about this logistic regression  this logistic regression  we went through and we talked about in depth what this network is doing  It has all these features  we're going to combine them in a linear fashion  and then we're going to convert it into a probability  So  when we do this  this is a fairly simple model  it's backed by decades of statistical theory on what it's actually doing and when it's going to work  On the right  we're showing this multilayer perceptron  This is a deep neural network  and we showed you this network and it can do a lot more than logistic regression  It can capture a lot more properties than logistic regression ever could  Now  the question is  do we actually always want to do this? So  let's think about what would happen as we increase the complexity  Well  first  creating deep models really helps us learn complex relationships  So  what I'm showing here is a famous toy dataset called the two moons dataset  So  we have our red class on the top and it curls around  and we have our blue class on the bottom and it curls around  If we use something like logistic regression  it can't separate these two classes  it can never predict very well on this because the model is just very limited in what it can capture  If we use a deep neural network  we can actually capture much more complicated relationships  So  what I'm showing here is I'm showing in the color red where a multilayer perceptron has learned to predict  that it's going to be the red class  and in blue where the multilayer perceptron has learned to predict the blue class  A deep neural network  in this case  a multilayer perceptron can represent this very very accurately and it predicts very very well  But we can come up with a lot of cases where this type of complexity is not wanted  So  a deep model can actually give perfect performance on the training dataset  and then when we go into the real world  it can fail completely  and so this is really bad and we actually need to validate the performance  Is this neural network going to work as well as we think it is? So  how can we actually go through and do this validation  and why do we need to do this? Before we jump into how we do the validation  let's think a little bit more about the concept of overfitting  Overfitting is what happens when our learned model is going to increase complexity deferred the observed training data too well  We're going to predict extremely well on our training dataset  But then  when it comes to real data  we're not going to predict well  So  why does this happen? So  let's start out with a really simple example  So  on the right here  I'm showing a collection of a few data points  We have a few observations and we want to come up with a function that's going to predict what our observation will be given the value of x  So  what do we want to use to fit these example datapoints? One strategy we can use from a very classical point of view is to increase our polynomial order  So  on the left here  I'm showing a simple linear regression fit  and we have our observations  and we have our x here and we can say this first order fit  If we get a new value of x  this is what our prediction is going to be  It's this red line right here  So  this does pretty well and a lot of people will be very satisfied with this  But there's this question  can we actually increase the complexity a little bit to actually predict better? So  let's look at what happens when we go to a third order fit  This is a cubic fit  So  in this cubic fit  we have x here  we have f of x  and you can see that we can trace out this line here  If we look at our observed data points  it actually fits a little bit better  Over the range that I'm showing plotted here  it seems pretty feasible that this could be what's happening  You might be a little bit worried about what's happening outside the visible range here  So  outside zero and four  this could be going off in finding things  But inside this range here  it actually seems like it's doing really well  But we can keep increasing the complexity  So  what I'm showing here is an eighth order fit to these data points  I think if you go and show this to anybody that's taking you seriously  they will start taking you seriously because this is not a legitimate prediction  We'd never have enough information to say that this is good  It just looks ridiculous  I don't think anyone would argue that this is the best model that's going to work the best in reality  It's fitting in really well to our training data but it's not fitting very well if we try to predict future points  So  what actually happens in overfitting? So  when we're increasing the depth of our neural network  which is what we've talked about so far  it means we're actually going to increase the number of parameters in the model  So  logistic regression has a finite number of parameters and each time we add an additional wire and a multilayer perceptron  we're getting more and more parameters  But remember  we have to estimate these parameters  So  if we have more parameters to estimate  we're going to get each parameter a little bit more wrong  and all the errors are going to add up and these parameters  The second thing which we really like actually is that when we have these more complex models or networks  we can learn more complex relationships  But maybe these relationships are actually too complex for reality  We can learn any complex relationship we want  Does this complex relationship actually happen? When we're overfitting  this means we're not going to generalize  We want our models and our analysis to generalize  which means that if we go out into the real world and say  \""This is a model that we trained \"" we're now going to make predictions  we wanted these predictions to actually hold  We want this to work when they go out into the real-world  So  we want to know how well is our network and our model fitting actually going to work in the real-world  So  we had our training dataset  we'll use our training dataset  and we talked about how to mathematically setup our goal to get our parameters  and now we want to say  \""Well  how well is this actually going to work in the real-world?\"" I want to now introduce a very standard validation strategy  So  if our goal in the end is to understand how all these network will perform in the real world  a good approach and a really the gold standard approach is actually try it in the real-world So  what we can do? We can take our training dataset  we can estimate are parameters  and then we're going to get new real world data  and say  \""Let's use our network on this  Did we predict well?\"" We can estimate the real world performance in this way  This is really the gold standard  If you want to say that your network is working  this is the best thing you can do  However  this is extremely costly  So  we don't want to do this when we want to just trying to validate whether our model is working  Instead  can we actually use existing data to estimate performance? The answer is yes  We're going to try to use our existing data  and we're going to try to use our existing data intelligently to create ways of validating how well we think this is going to work in the real-world based off of our available data that we've already collected  We don't want to have to go out and collect a new experiment  So  we have all of our available data here and what we're going to do is we're going to split this data into separate groups  So  we're going to define three groups of data  and I'm going to talk through what each of these groups of data is actually doing and what we use them for  But for now  we're just going to have a training dataset  a validation dataset  and a testing dataset  We're showing these three different datasets in different colors  What we're going to do is we're going to take all of our available data  we're going to put some of that into our training dataset  and we're going to put some of that into a validation dataset  we're going to put some of that into a test dataset  We're just going to randomly assign them to these different groups  and we're going to assign them in different proportions  So  our training dataset  we've already talked a lot about what it's used for  We talk about how we use a training dataset to get our model parameters  So  we already know what we're going to use our training dataset for  and we can just show this right here  We can split these data into separate groups  We can take all of our available data  We can make some of our training dataset  We already know how to use this  But we've now created two separate datasets  a testing dataset and a validation dataset  So  why have we done this and why is this a good thing for us to do? First  the test set  Test set is a very standard practice in machine learning  and we want to create the test set prior to any analysis  We don't want to use the data at all before we create the test set  and a test set will never be used to learn or fit any parameters  After learning the network  we can evaluate its performance on the test set  So  the idea here is that this data was not included in the training or fitting  So  what this means is that if we fit our data or we've trained or learned from our training dataset  we now are trying it on this new test set  But we've never seen this test set before  so this is analogous to running a new experiment  It's a synthetic experiment but it's analogous to running a new experiment  A big deal is that the test set is ideally going to be used once  If we reuse the test set it's going to lead to bias  and what this means is that our performance estimates will be optimistic  When we say they're optimistic  this means that when we go out into the real world  we think that our performance is going to be higher than it actually is  and this comes from the fact that we kept reusing our test dataset  To get around this problem that we only want to use a test set once  we're going to create a validation set  So  the idea of the validation set is we want to be able to compare which approach is best  and we can't really do this if we're only using the tests at once  We can't compare all these approaches and just say  this approach is best  because then our tests that we're overestimating performance  So  we want to create the second [inaudible] dataset that we're going to call the validation dataset  So  the validation dataset  we're also not going to use it to learn parameters but it can be used to repeatedly estimate the performance of a model  The idea is that if we're going to try a lot of different structures of networks  we're going to have our logistic regression model  we're going to have our multilayered perceptron  we'll have a number of other different models we want to try  We can learn all of them and we can estimate the performance on the validation dataset  Once we've picked out what model we want to use  we can run the final evaluation on the test data  and we're only going to run on the test data once  To visualize what's happening here  we have our training dataset that we've split out from all of our available data and we're going to put it and we're going to learn our parameters from our training dataset  But then  using our validation dataset  we're going to estimate their performance  and what we're going to do is we're going to take this performance estimate on this particular model and we're going to use it to refine our model or network  We're going to change our architecture  We're going to say  well maybe we need more complexity  maybe we do use less complexity  and we're going to keep doing this until we think we know what model is best for our data  Then once we have that  we're going to break out of this loop here  and we're going to go and combine it and use our test dataset once to estimate our final performance  So  this is the structure that we want to use when are using machine learning  We want to use our validation data to help us figure out what model to use in our test data to estimate real-world performance "
h9CxARIB8gY,"How Do We Learn Our Network? So far  we've talked about how we can define a network  This could be a simple network such as logistic regression or more complicated deep networks  We've also talked about how we can actually mathematically set up our learning goal  How do we actually want to define what our optimal parameters are? But one of the questions we still have left is how do we actually learn the network? We've set up a mathematical description  how do we learn from this mathematical description? So  learning itself is going to turn into an optimization problem  So  as a reminder  we want to find the parameters that minimize the average loss  and we can define our average loss here  We're saying our optimal parameters b are going to be the parameters that minimize our average loss  and this is going to be over all of our training data examples  So  we have our loss over our true values and our predicted values  So  how do we actually go through and find the parameters? We have this nice clear mathematical statement  but we want to understand how can we actually find the parameters that minimize this? How do we actually optimize b in order to minimize this function? So  in order to do this  I want to talk about gradient descent  Gradient descent is an incredibly famous mathematical optimization algorithm that has a long  long history  Instead of talking about it from a mathematical point of view  we want to talk about visually what's happening  So  our goal as a reminder is we have some mathematical function such as our average loss function that we want to minimize  The approach you'll want do is to try to find something that minimizes this function  So  I'm showing here on the right  I'm showing a simple one-dimensional function  and we want to find the parameter b  Here  it's a single parameter that minimizes this function  Now  everyone can see just by looking at this visualization that the minimum is at zero  But because this is a one-dimensional function  and we can just plot out the whole function  When we go to these very high dimensional and very complex functions  we're not going to be able to plot it out and just see where the minimum is  so we have to come up with an algorithm that can help us find the minimum  So  one approach we can take to do this is to start at a particular parameter value  Here we're starting at about 1 5  and we're showing here in this blue circle  and we have current point that's marking where we currently are  Often what we can do is instead of saying where the minimum is  we can say  \""Well  at our current point we can figure out what the slope of our function is \"" So  we're showing here the slope of our function  Now  if we know we want to go to the direction that's going to help us find the minimum  we can just say  \""Well  if we know our slope  what direction is pointing us down the hill?\"" So  at our current point we have our slope  and we say if we go to the right we're going to be going up the hill because that's what our slope is saying is going to happen  and if we go left we're going to go down the hill  So  we can actually do this by saying  \""Well  if this is our current point  we can figure out what our slope is and our slope says  let's take a step to the left  because if we take a step to the left we're going to be able to get to the lower function value and actually start getting closer and closer to this minimum that we want  so our update is going to take a step to the left \"" So if we do that  we just visualize what happens  and we have a new current point  We've taken a step to the left  we have a new current point  we have a new slope  So after one update  we could say  \""Well  we've taken one update now  we have a lower function value  we're not at the minimum yet  but we have a strategy that seems to be getting us closer and closer to the minimum \"" So  we can just repeat the exact same thing we did before  We can say  \""Well  we know our current point  we know the slope of our current point  and it says once again  we want to take a step to the left \"" So  we can take a second update where we again take a step to the left  So after the second update  we can now say we've taken one step to the left and another step to the left  We now are getting closer and closer to the minimum  and we can once again calculate the slope  We can keep doing this  so this is after two updates  after three updates  we can see what's happening  and we get a little bit closer  We can repeat the same thing again  fourth update  and we just keep repeating this until we're satisfied  So  instead of talking about this visually  we can also talk about what this is doing mathematically  So if we have our function here  and we're saying we want to find the parameter that minimizes this function  So  what we're going to do if we're going to run with this gradient descent strategy is we're going to start at initial value  So  we're going to start at what we're going to call b superscript zero  and what we're going to do is we're going to keep running a series of updates  and this update is going to move us from b or kth value of b to a k plus oneth value of b  This can be  for example  for moving us from our zeroth update to our first update  and our zeroth update is where we start  So  we're going to iteratively run the procedure  So one  we're going to calculate the slope at the current point  For one parameter  this is going to be the derivative  This is just the name from calculus  For multiple parameters  this is going to be the gradient  and the gradient here is going to be denoted by this upside down triangle or upside down delta on our function  The gradient here is just going to mean a multi-dimensional slope  It just means we're going to take the slope in each one of our dimensions and we're going to use this to define the gradient of our function  We're going to move in the direction of the negative gradient with step size alpha k  and alpha k is going to be our step size on our kth iteration  We get to choose what it is  During the hands-on session  we'll talk about some strategies for choosing what this value alpha is going to be  but for now  we're just going to consider it a value that saying how much do we want to move in this direction and we're going to run our update  If we have this beta k  and we're going to move a little bit in the direction of the negative gradient  So if we're looking at our slope here  negative is going to be to the left  and so  this is just a mathematical description of this exact update we just did  We're just going to keep running wanted to until we're converged  So far  we've talked about gradient descent  Now  we need to talk about how to make this work when we have truly big data  and I'll come up in a following lecture "
pGSwasYOv0o,How Do We Handle Big Data? So  we have previously talked about Gradient Descent  and Gradient Descent has a fantastic optimization algorithm  But we're going to run into trouble when we hit truly big data  So  in this video  we're going to talk about how we actually can handle big data from an optimization perspective  So  in order to do this  let's actually go back to our optimization goal  So we have this network  we have our training data and we want to find the best parameters for all data points  So we have this b star as the value that minimizes our function on the right  our average loss function  and we have just talked about how we can do this through gradient descent  Calculating the gradient requires looking at every single data point  This is a big drawback because we need to calculate the gradient or multidimensional slope over all data points  this is going to require looking at all of our data  And you can see this mathematically here if you want by just showing that the gradient over all of our data points is just going to be the sum of the gradients of our individual data points  This just happens because a gradient is something known as a linear operator  This can be very problematic in big data  So  MNIST which we've previously talked about this with our dataset of images of numbers  it has only order 60000 images  When you go to these very large data sets  we can actually have millions of examples or billions depending on the application and this is very feasible  So  this is really problematic if we're going to look at every single data point  If we have to run through a million images just to calculate the gradient to take one little update  this is not very scalable  So  let's just go through and approximate the gradient  and we don't want to look at every single data point to update our parameters  So there's a question  do we actually need to do that? So  just think about a strategy and just run it through and see what would happen  So  we're going to take a single example j  I'm going to pick j at random  and i'm going to use it to approximate the gradient  What we're showing here  is that we're just going to say the gradient over average loss  If we take the gradient over every single data point  we're going to approximate it by taking the gradient with respect to a single data point  and it's a randomly chosen data point  So  the reason we want to do this is that this is incredibly faster  So  if we think about this MNIST dataset  this dataset we're looking at just images of numbers of handwritten numbers  this is 60000 times faster  We can run this 60000 times  And dataset with a million images  this is a million times faster literally a million  So  does this work? Right? Is this actually a good idea  and what would this look like? So  let's actually go through a visualization to show what's actually happening here when we run Gradient Descent versus Stochastic Gradient Descent  So  I've just started the simulation here on gradient descent  So  what's happening in gradient descent? We're making very smooth progress  we're taking little steps that are going down the hill  Now  we can see that it's making good progress it keeps going and we're going to get to the bottom eventually and it's going to make very smooth progress  we know we're going to be improving  If we run Stochastic Gradient Descent  we can run it and we get way faster updates  Sometimes we're actually moving in the wrong direction  sometimes our gradient approximation is not very good  But on average we're moving in the correct direction and we get near the minimum very quickly  What we're showing here  is that we can run updates many times faster in Stochastic Gradient Descent  But in reality  this is on a 1000 data points  this is a simple problem  I've actually drastically slowed down Stochastic Gradient Descent so that you can actually see what's happening  It's actually many times faster than this and as we get more data points it's going to be an even bigger difference  We can see the Gradient Descent is still going and we're not going to let it finish because we just don't want to sit here and watch it  So  why does this work? One of the reason this works  is that data is often redundant  If we go back to this MNIST example  where we had this dataset of handwritten digits  we have a bunch of people that have written zeros  we have a bunch of people have written ones  twos  threes  fours and so on  and we have 60000 images  But in reality we only have 10 different types of images  There are a few different ways you can write a zero  a few different types of ways you can write a four and so on but there's only so many different types of digits  If we want to understand about a digit  do we have to look at every single digit to get an approximation to a gradient or do we just need to have a few in order to get an idea of what's going on in this dataset  We can take advantage of this redundancy  And just to talk about mathematically what's happening here  is on the left we're showing this gradient descent strategy with this is the same mathematical description that we had in a previous video  So  we're going to start with an initial b-0  we're starting with an initial location of our parameters and we're going to calculate the gradient over all the data and then we're just going to iteratively update  So  we're going to take a step in the negative direction of the gradient  I was just going to repeat steps two to three until we're good enough  If we want to change this to Stochastic Gradient Descent  we have something that's very similar but much faster  We're going to start with an initial value and we're going to choose a data entry j at random we're just going to pick one data entry and it's going to be chosen completely at random  We're going to estimate the gradient by the single data point  and we have a hat here over our gradient because it's now an estimate and we're going to iteratively update  And I want to point out that the iterative update in stochastic gradient descent is exactly the same as the iterative update in gradient descent  The only thing that's changed is that we have an estimate of what a gradient is rather than our exact gradient  Now  we're just going to repeat steps two to four until the solution is good enough  So let's step back and think about what's actually happening here  Let's start by noting that if we take the expectation over our gradient estimate which is just a gradient of a single data point where we've chosen this datapoint completely at random that it's exactly equivalent in expectation to our true gradient where we've calculated over every single data point  So if we start Gradient Descent and Stochastic Gradient Descent at the same place  how do their updates vary and how do the expectations of their updates actually vary  So if we think about what's happening in these updates  they're going to go to the same place and expectation  So  if we take our Gradient Descent update and we take our Stochastic Gradient Descent update  they're going to be on average going to the exact same spot  So on average there's no difference between them  But the differences that Gradient Descent will go to the same spot every single time because we're calculating our gradient fully  Whereas Stochastic Gradient Descent we're going to get a lot of variants in where we're moving to  On average we're going in the same place as Gradient Descent but we're actually going to have some variance  So sometimes we're going to move a little too far  sometimes we're going to move a little too small  sometimes we're actually going to move in the wrong direction but on average we can do exactly the same thing  We can go back and think about this while we're doing these visualizations again  So we're looking at Gradient descent very smooth  No variance in our updates we run Gradient Descent from the same starting point every single time we are going to get the exact same result  it's going to be very smooth  Stochastic Gradient Descent once again we're moving on average in the same way Gradient Descent is but we're having a lot more variance  Sometimes we move in the wrong direction  but that's okay because on average we're moving in the right direction and we can do this much faster than we ever could of Gradient Descent  So just to recap  we have shown this figure in the schematic several times now about learning the model parameters  We have our training data and we also have some sort of network or model that we want to learn  We said that we're going to combine our train data with this network in order to learn our parameters and before  we just said that we're going to do this through some sort of optimization  we are going to define our mathematical loss  But all we're going to do in practice is just use stochastic gradient descent to the step where we're going to go from our network to our learn parameters  we're going to use Stochastic gradient descent or an alternative stochastic gradient method and this is what we do in practice on almost every single network  So  just some conclusions from this video today  Just to reiterate  Stochastic Gradient Descent can update many more times than gradient descent  If we have 60000 data points  we can run 60000 updates of Stochastic Gradient Descent and about the same amount of time it takes to run a single update of gradient descent  We're going to get near the solution very quickly and often all we need to do is get near the solution  we don't need the exact solution we just need to be very near it  And this is going to allow us to scale to big data  And the reason this is so helpful to scale to big data is that our update time doesn't increase with our data size  If we double the size of our dataset  it's going to take about the same amount of time to run this Optimization Algorithm  Whereas in a lot of Classical Optimization Algorithms such as Gradient Descent  it will take much longer  In practice  we're often going to use a mini-batch  Which means that instead of using a single data example  we're going to run a few data examples to estimate the gradient and this will reduce variance  But overall  Stochastic Gradient Descent is just going to allow us to scale to big data in a very simple way 
gkEJmMFKtzE,Early Stopping  So  we have just talked about Stochastic Gradient Descent  and we have previously talked about validation  and so far  we haven't talked about the combination of how you can do validation with our optimisation algorithms  But in practice we're often going to combine these  and we're going to combine these through a technique called as early stopping  The idea is that we want to run our validation during optimization  and so the idea is that if we want to maximize the generalization of the network  this is actually mismatched with our optimization goal  Our optimization goal says we want to do as well as possible on our training dataset  but in practice  we're not actually trying to maximize our training dataset performance  we're trying to maximize how well we do when we go out into the real world  So  this becomes a bit of an issue because Stochastic Gradient Descent is optimizing a mathematical goal that's mismatched with what we're actually trying to do  and so there's a question where can we actually validate the model while running the optimization loop to address this problem? So  the idea in early stopping is that we can check the validation loss as we go  and the idea is that we're going to run our training loop  where we're going to run Stochastic Gradient Descent  and we're going to estimate what are average losses on our training dataset  and every so often we're going to also estimate what the performance is on our validation dataset  So  we're not going to use the validation data set to do the optimisation  or get our gradients  but we're going to use the validation data set to estimate our real-world performance  The idea is that let's not optimize to convergence on our training goal  let's optimize until the validation loss stops improving  This is going to be good for two reasons  one  this is going to help save us computational cost  this algorithms can take a very long time to train on a computer  and we want to use the minimum amount of computation possible  The second and better reason is that it's actually going to perform better when we go out into the real world  And so what we're showing here on the right is that if you run this optimization algorithm  Stochastic Gradient Descent  we're going to keep improving our training losses as we run more and more iterations of Stochastic Gradient Descent  we're going to keep getting a little bit better over time  But our validation dataset is actually going to get slightly worse as we overfit more and more  So  we want to stop when our validation is minimum  not when our training loss is minimum  and so the best validation and generalization is going to happen in about the middle of this plot  and this is a widely used technique in the field that can help us save computation  and can also help us give the best-performing network possible 
D-c7QCzEYy4,Model Learning with PyTorch  In module 2  we learned about how a neural network learns from data  By defining a loss function that quantifies as a single number how poorly a model does  we can use Stochastic gradient descent to adjust each of the parameters in our network to improve the model's performance  Throughout module 2  logistic regression was used as a running example to illustrate these concepts  In this section  we're going to put these concepts to use and actually code a working model that uses logistic regression to classify the digits of the MNIST data sets  Once you've trained it  we can evaluate our models performance and as an added bonus  visualize what it actually learned  We'll first write our code with basic low-level PyTorch operations  but we'll finish by seeing how we can rearrange our code in a more object-oriented manner using higher level APIs to build our model more efficiently  As an assignment  you'll be extending our logistic regression model to a multilayer perceptron  which was introduced in module 1  You should find that it performs better than our simple logistic regression model 
0O-JGqUa1aM,Motivation  Diabetic Retinopathy  Okay  In this module we'll be diving deeper into deep convolutional neural networks  These networks are powerful tools for doing image analysis  and have been very successful in real-world applications especially in medical image analysis  I want to highlight here  one very successful application of convolution neural networks to Diabetic Retinopathy Classification  Diabetic Retinopathy is a disease that affects the retina of diabetic individuals where in the blood vessels deteriorate over time  leading to blindness if left untreated  But can be caught early  if a trained ophthalmologist can find structure  in the unhealthy retina  that distinguishes the unhealthy retina from a healthy retina  So  here on the left you see a healthy retina  and on the right you see an unhealthy retina in a patient with Diabetic Retinopathy  I hope that you can see the differences between these two retina  So on the right  in the unhealthy retina you see little hemorrhagic spots  on the left side of the retina  you see neovascularization close to the optic nerve that bright spot  on the right side of the image  Together  those are telltale signs of this being an unhealthy retina that if left untreated could eventually lead to blindness  Ophthalmologists are trained  to distinguish this retina from the other  trained with several years of experience in medical school  and during residency  They gain supervisory signals from their colleagues to help them formally distinguish this unhealthy retina  and healthy retina between healthy and unhealthy patients  But because these convolutional neural networks are so good at doing image classification  they can learn the features that distinguish the unhealthy retina from the healthy retina  and thus have a machine performance classification  in a way that circumvents the need for a trained ophthalmologist  So  there is a convolutional neural network that has been applied to this problem  it has worked very well and was published in a study in the Journal of American Medical Association in 2016  and I just want to show you here the money figure from their paper  that's showing the results of a trained neural network during this Diabetic Retinopathy Classification  so trying to find a healthy retina versus an unhealthy retina  To explain the results  I need to unpack the axes here  So sensitivity on the left the y-axis  and one minus specificity on the x-axis on the bottom  What do these figures mean? There are common metrics within the medical community  Sensitivity is a quantity that helps define whether or not a doctor or a machine is doing well at finding the positive examples in a dataset  So  an unhealthy retina if we call that a positive  the name of the game is to try to find as many of the positives as possible out of a large dataset of both positive unhealthy retina examples and negative healthy retina examples  If a doctor comes through  and labels a large set of images healthy and unhealthy  we calculate the sensitivity by looking at the number of true positives  the number of true unhealthy retina in that dataset the doctor was able to find  and dividing that number by the total number of true positives in the dataset  We want this quantity to be as high as possible  However you should be able to see how you can potentially cheat this metric  If you just label all of the examples in the dataset as positive  you'll get a very high sensitivity score  but this will clearly results in a lot of false positives as a doctor is classifying all the healthy retina as unhealthy  So  we need a complimentary metric to help distinguish these false positive rates from false negative rates  and this is specificity  So  this is the complementary metric to sensitivity where  within the labeled datasets  where you're calling a retina healthy or unhealthy  The true positives go into that sensitivity metric and the labeled true negatives go into the specificity metrics  So  you count the number of true negatives  the number of healthy retina the doctor was able to correctly identify  and divide that by the total number of true negatives in the dataset  So  in theory you want specificity to also be as high as possible  together with sensitivity  In this graph here on the left we're plotting one minus specificity  So  because we want specificity to be high  one minus specificity we want to be low or closer to zero  This means that the best value for a classifier of these images is up in the top left corner of this plot  Okay  so now let's revisit this plot  So the colored circles in this plot represent eight individual ophthalmologist  we did this classification task on a large set of healthy and unhealthy retina  and you can see this as a distribution of their ability to do this classification  So the purple doctor dot in the top left hand corner is probably the best ophthalmologists in this group  but there seems to be some variation amiability in this trade off between sensitivity  and specificity so how well these doctors are doing the classification problem  So  those colored dots are the doctors  If you train the convolutional neural network  and we'll get to the details of how you do this later  If you train this network to do the same task in a large set of training images with labels  you can get the machine to perform  according to this black line here  Okay  Because we just said that it's better to be up in that top left hand corner  you can see that anytime that black line is closer to the top left hand corner than those colored dots the machine is beating  the doctors  the ophthalmologists  and in this case you should be able to see that  the machine is beating 80% of the ophthalmologists on this particular task  So  that's a very powerful result  and we think for this particular task machines may be aiding in the diagnosis of this disease  and it also promises  a very promising future for these types of convolution neural networks  and other types of medical image analysis tasks including  radiology  and pathology  Okay  So in addition to image classification  so that was just an example of your classifying the retina as unhealthy or healthy  These convolutional neural networks can also be used to segment out particular features of an image  that might be important for a downstream user  So  imagine you are a TSA screening agent at the airport  looking at images of X-rays through passengers baggage  and the goal is to find prohibited items  in these bags  But you've been sitting in front of these monitors for very long time  you're particularly tired right  it might help to have a machine highlight regions of interests  that correspond to those prohibited items in those bags automatically  and it turns out you can train very well a convolutional neural network to do this task  So on the top here you're seeing  the predicted positions and bounding boxes of guns in multiple different views of a particular bag  So these red boxes are highlighting a toy pistol in this case and some of these bags  Little yellow pixels  if you can see them around the gun are actually this sort of fine grain tracing of that objects in the bag  In the bottom you can see a similar analysis performed on pocket knives  and other type of prohibited item  it's not allowed through the TSA Screening checkpoints  So  the convolutional neural network because it's so good at extracting information from images  it can do this task  and potentially be employed by the TSA to help augment the job of these TSA agents at the screening checkpoints  So this is another powerful application of this tool  So  in addition to those applications  this is maybe a segmentation task taken to the extreme  So this is a network that can read in a video  so in the bottom right hand corner of this video  So this is just a single camera taking a video of a scene  and the goal of the network is to isolate all the humans in these videos  segment them out like kind of like the pistol in the previous example  But instead of just outlining the humans a sign of a dense map of key points that correspond to different anatomical locations on the human to form a mesh  a 3D mesh  that's specific for each of the humans in the video  Okay  and so this is a very challenging task  especially in these single camera views  and typically these meshes are developed using very expensive motion capture technology  and very awkward scenarios  Okay? The particular application of having such a mesh  and why people use the current technology which are these motion capture suits  is that you'd like to develop these message so that you can apply textures in the entertainment industry onto these humans to develop new video games  and new CGI animation for movies  and these meshes may also help interactions with new autonomous vehicles  robots and things like this in the future 
PoqDab1JhZY,Breakdown of the Convolution (1D and 2D)  Okay  So  let's break down the convolution operation itself  So  just to remind you about the general architecture for a convolutional neural network  the idea is that you have some sort of input image  there are filters  convolutional filters that are applied to this input image via a convolution operation  and that operation is repeated as feature maps are built up over and over again to get to a high level representation of an image that can be fed to a classifier  So  what exactly is that convolution operation? The convolution is essentially a sliding of two signals  one over another  that helps search for particular features in that signal of interest  So  I'm going to introduce the convolution to you in one dimension  So  now we have two signals here  in 1D is signal f  a square wave  and a signal g  is sort of a single Sawtooth  and the mathematical definition of the convolution of f with g is given here at the bottom of the slide  If you unpack the right side  you can see that the idea is that you multiply g times f at every position from minus infinity to infinity  and then you sum up the result of that operation  basically taking the area under the curve of that resulting multiplication and then assigning that sum to a single value of the convolution  That operation is performed multiple times for various values of n which end up being various lags or shifts of the signal g relative to f  and I'll just show you a few examples of how that's done in this one-dimensional case  So  I'll add some reference values here  So  we'll add an axis zero  minus five  five  If I plug in minus five for n in that convolution equation  I assign minus five to n on the right side as well  and if I distribute that negative sign  what you'll see is now I have g minus five plus m  If you recall from algebra  having an m plus five  so f of x plus five  shifts the function to the left by five  so I'm showing here another reflection  that triangle relative to the blue by five  and then the negative sign performs a reflection along the midline of that signal  of the read signal  okay? So the result of that shift and then reflection as shown here  If I multiply these two signals together and then sum up the results  I'm looking at the area  sort of the intersectional area between the red and blue curves  and so this area is going to be the value of the convolution which in this particular case is 15  Okay? So  now if I apply a different shift zeros  so I plug in zero for n  actually do not shift the red curve at all relative to the blue  but I do reflect it  okay? Then again I multiply the signals  take the sum of the result  and in this case the area is shown here in yellow  which is slightly bigger than the area before  Okay  so that's why the result is 20  If I continue to shift this red curve over to the right more and more  I get different values of the convolution at different lags or shifts as I move g relative to f  So  in this case  I've moved the red over by 10  I reflected and then I've taken the area and I get a smaller value because the region of overlap is smaller between those two signals  Of course if I move this over substantially such that there's no longer any overlap  all the positive values of both of these functions are zeroed  your multiplication by zero  and I get no resulting signal or area  So  the value of the convolution here is zero once there is no overlap  Okay  So  here's just an animation of a similar process here just using two square waves that are moving g  The red one is moving across the blue one  and as the red curve is shifted from left to right  as it begins to overlap with the blue signal  there's then an area under the result of that multiplication  and that black line is now drawing out the value of that area and thus the value of the convolution for that particular lag  okay? So that's the whole convolution in 1D  What this means is that the convolutional filter g in this case can be used to specifically pull out features in f that match it  okay? So  if you have a matched feature  you're going to get a high value and the convolution  So  here's an example of that here  So  if I filter with g and a shift of zero overlap perfectly with f  we see that the result of that convolution would be area accumulated in those two left lobes at the top  and then a substantial amount of value or area accumulated at the bottom where you have negative values of g multiplied by negative values of f  But if we have this long square wave filter for g  when it's overlapping with f at zero lag  you do get positive values for the area on these two side lobes  but in the middle where previously you had negative times negative equals positive  where a positive times a negative equals negative  and so there's this very large negative area component subtracted from the yellow resulting in a lower value for the convolution with that mismatch filter and then with the matched filter on the left  Okay  So  that's in 1D  and the convolutional neural network we're trying to convolve in 2D image with a 2D filter  and so  you basically do the same exact thing except the filter is a 2D extent  So  here on the top is a three by three filter  it's being applied to a particular region of the image below  there's multiplication just like in the 1D case  and then a sum  and the resulting value of that sum is the value of the convolution at that particular shift  This filter is then moved along the image until you finally have a convolution every single points of the image with that filter  Just to show you what that looks like with a particular pattern  imagine the filter where a cross  and there was a cross-like feature in the resulting image as that filter with the crosses overlap slightly with the feature in the image  you get a positive value for the convolution  but as it overlaps perfectly with the underlying feature  you get a higher value here shown in darker purple and the resulting convolved image or feature map  As it begins to slide off of the feature  you get lower and lower values of the convolved image such that by the time you're done  you have a heatmap in the one corner corresponding to where that feature was in the image that you were searching for with the respective filter  So  just to show you now what number is what this looks like  if you have a filter here  W  you apply it to the image  Okay  this is just for one shift  so you apply it first into the top left hand corner  you do element-wise multiplication of all those values  then you take the sum over the rows and columns of that particular piece where the filters overlapping with the input and then you deposit the results of that sum in the corresponding locations that convolved feature map  in this case  value of four as a result of that convolution  So  this filter here I'm showing you on the left is actually a real filter that people use to extract features from real images  and actually is an edge detector  and just to show you how we can actually form a real meaningful map from running this filter over an image  we're showing here now is stock photo  black and white photo  I run this filter over the image with various shifts right  So  I just move this filter and slide it all over this image  and the result of that convolution operation is now an image in which I've detected the feature that filter was designed to detect  in this case  the edges of the image  Okay  and just to wrap up  just to really hammer home this point  so what happens when I'm doing this convolution in 2D to try to do feature extraction in the convolutional neural network  a filter  in this case  a circle in this example  is moved over the image left  right  top to bottom  and at the points where the filter overlaps with the feature corresponding to it  you get a big high-amplitude hotspots  So  when that red circle was overlapping with the blue circle  there was a lot of signal  and so you see in the resulting heat map on the right  a high value for the convolution that point  whereas when that circle is overlapping with shapes that are not match to it  you get lower values of the convolution operation  right? So  you can see how the circle as it's being passed over this image is highlighting particular features and forming a feature map on the right that that can be used by upstream layers of the network to understand the hierarchical properties and features within the input 
mZr0StOMam4,Core Components of the Convolutional Layer  Okay  now we're going to get into nuts and bolts of various components of the convolutional neural network  the core elements of the system that allow it to extract meaningful features from these images  So  we're going to walk through all of these convolutional layers  activation functions  pooling layers and fully connected layers  And so let's start with what goes on in the convolutional layer? You know about the convolution operation  but there are various features of the convolution that can be chosen by the user when designing the system  And that includes the convolutional filter size  The convolutional filter stride  which we'll define in a minute  And the convolutional filter number  So the filter size as shown here is just what it sounds like  so I have been showing convolutions in a 3 x 3 configurations  So the filter is 3 x 3 and that filter is run over the image to result in a feature map  But you can also change the size of that filter  so on the right there is a filter that is 5 x 5  And that results in a different feature map  because the filter can potentially be bigger and contain more information about the feature that it's searching for  okay? Typically  filters range from 3 x 3 to 7 x 7  And the rule of thumb is typically filters should be just large enough to capture small local features  for instance  edges and space  but not multiple of those  So you don't want to have a filter that can look for a circle and a triangle together  you want to let that correspondence occur in higher layers of the network  You can also define a filter stride  so filter stride  I've been showing you a stride of 1  what that means is that every time the filter moves  it moves down one pixel at a time during the convolution  resulting in a feature map  But  if you increase the stride  you skip more pixels as you're moving the filter over the image  So  for a stride of two  as you see on the right  the filter is moving two pixels at a time as it's being slid over the image  And the result of that is a feature map that has been downsized  All right  so  instead of the large feature map on the left  You get a smaller feature map on the right  and that's because you can't fit the filter as many times within the input image  So the stride helps reduce the computational load by downsampling the input  Typical strides that you would see in a neural network range from one to two  sometimes a little higher  but you really  one to two is very common  Okay  so filter number  we have potentially addressed before  So the idea here is that at each layer in the network  you can define the number of filters with a number of unique features that you're looking for in the input  So in this particular case  I'm showing six elementary building blocks in a toy example  or you might be looking for a hexagon  a rectangle  a circle  a triangle  a square  or a diamond  And to get all of those isolated in their own feature maps here shown as different colors  you need to have a filter for each one of those  right? And you're going to have many more than this  but the idea is the filter number determines the number of unique feature detectors that operate on the inputs  Okay  so those are the core components of the convolution layer  I just want to turn it back on its side and show you how for each of the filters that we use  we develop a feature volume  right  that's passed on to the higher layers of the network  So if we have a single filter here that's pulling on a particular feature  In this case a red feature map  But if we have another filter right here shown in purple that operates on the input and has its own unique feature map  And that continues for all the different filters that you have such as you're building up a volume of feature maps  a stack of feature maps that's fed on as input to the next layer in the network  Recall from the toy example earlier  that we had these layer 2 filters that we're operating on the feature maps  So the filters that then need to operate on the stack of feature maps need to also have an extent in depth as well  Such as they cover all of the feature maps that are being fed into that layers input  Okay  and that's also true of the very first layer  So even though you've been treating the input image  as just a single  potentially grayscale image  images all actually have color  And when they do have color  they have three channels  a red channel  a green channel and a blue channel  That helped define the color in each pixel  In the layer one filters  paying attention to that input image  need to have weights corresponding to each of those color channels  You can essentially think of those filters as having a 2D extent in each of the input channels such that there's a stack for each filter that goes down three for each of those channels  Okay  so in every case  these filters are operating over input volumes  primarily or at every stage  So even when you're feeding in the original image input  but especially after you've done a convolution  such as you've built up a stack of feature maps  Okay  so just to put this back in the context of our original animation  what we're really doing when we're sliding this filter over is that we have a small cube right at 3 x 3 now by 3 with weights corresponding to every single slice of the image input  every channel of the image input which might be a 9 x 9 x 3 image input  where that three corresponds to the three color channels  And the result of that  though  because we're still doing element wise multiplication and then a sum  is that we get a single 2D feature map for that particular filter  Okay  so this animation is now just summarizing everything that I've talked about for the operational and convolutional layer  So on the left you have an input volume  that in this case has three slices  So you can think of this as being an input image that has a red channel  a green channel  and a blue channel  We're calling that input I  and we're using now array notation that you might find in Python to sort of index all of these slices  So at the top you have I[       0]  That just means all of the rows and all of the columns in the zero width  right? Or the zero index IE first slice of that input  which might be the red channel  And then below it we have I[       1]  which might be the green channel  And input and I[     2] which might be the blue channel  Okay  so what I want you to take away from this is that the filter  right? Zero so that's a single filter  W0  has weights that correspond to spacial positions in all the respective channels of the input  Okay so the filter is actually three dimensional  okay  it has a 2D extent and then an extent in depth  okay? But still what you're doing is getting doing an element wise multiplication between all of the values in the filters and their respective values in the inputs  summing those and depositing the results into a single value in the feature map corresponding to filter zero  okay? And for filter one  a second filter  You do exactly the same thing  but the result of that summation of that convolution is deposited at a respective position within a second feature map  feature map one 
2Wm_zTKWw3U,Activation Functions  Okay  So  the second core components that we're going to talk about are now activation functions  So  recall the example of a filter being applied to an image input  You get element-wise multiplication and then a sum  That's the convolution operation  the result of that sum is deposited in a pixel in the resulting feature map  But really what exists at every position in that feature map is a artificial neuron  that's taking that input and potentially transforming it into a different value that's passed on to subsequent layers in the network  So  if we take one of those red circles and blow it up  so that's a single artificial neuron  it's receiving some inputs  and in this case  I'm just showing three input values X1  X2  X3  For each of those inputs  you have a respective weight  so those are weights in the convolution  so W1  W2  W3  The result of convolving with those weights that input is a sum 'a'  which again is just the element-wise multiplication  and then a sum over all of those multiplications  But instead of just taking that 'a' and passing it on to later layers in the network  there's typically a function that's applied to that input to result in a different type of output  What do I mean by that? Here is just a linear activation  and this is what I've been showing you the entire time when I've been showing you these convolution operations  is that the result of taking that sum  the convolution in getting sum value 'a'  then the result of a linear activation function applied to that value 'a' would just be the value 'a' again  So  there would be no transformation of the input to output  and that's a very simple linear activation function  But in reality  what we use when designing these neural networks are non-linear activation functions  and so you may have seen these before when going over logistic regression  or going over the deep multilayer perceptron  these non-linear activation functions  So  when there's an input 'a' to the artificial neuron  rather than just spitting out the same value  you transform that value 'a' in a non-linear way  This increases the functional capacity of the neural network  because it allows it to represent non-linear relationships between features in the input  which can be quite powerful if you're trying to digest those features in a way that can be used to classify or segment in these other real world applications  So  here I'm showing you now on the left  a sigmoid function activation which is commonly used  So  for a given input value 'a'  you squash the results between zero and one  so you can think of this as forming a problem  In the logistic regression case  you can think of that as forming a probabilistic output between zero and one  On the right  you have a commonly-used function called the hyperbolic tangent function  which takes in some value 'a' and does a very similar squashing as the sigmoid function  except it collapses the values between minus one and one  So  it squeezes all the values between minus one and one instead of zero and one  but you can sort of think of it as almost the same thing  So  all these have been very powerful non-linear activation functions for neural networks  They have resulted in some problems  so if you notice that at the either top or bottom of these plots  you see these very flat regions  In those regions  for very high variance in the value of the input 'a'  you get very little change in the resulting output such that there's very little signal for the neural network as values are changing in dramatically different ways  and this results in a sort of stagnation  occasionally when training these networks that's not particularly useful for learning the representations needed to do classification  So  people have begun to use a different type of non-linear activation  in this case  a rectified linear unit which happens to look a lot like the way a real neuron in a brain might respond to an input  So  it's a very simple function where for any negative value of 'a'  the result of the function applied to that input is zero  So  it passes on zero if the total sum of the inputs is less than zero  or returns 'a' if the value of the input is greater than zero  So  the non-linearity is very simple  it's just a maximum value between the value of the input 'a' and zero  But this non-linearity  when introduced to all of the neurons in each layer of the network  increases dramatically the capacity of that network to represent information within the input 
lmXil72qCiQ,Pooling and Fully Connected Layers  Okay  so the next core component of the convolution neural network is called a pooling layer  And this is typically applied after the convolution and after the activation to reduce the size of the input that's passed up to subsequent layers of the network  This pooling reduces the computational complexity of the overall network  it makes it easier to train  It also combats overfitting and encourages some translational invariance  By translational invariance I mean  when you see a particular feature in one corner of an image  the neural network can still classify its identity  if it's in the top left-hand corner or in the middle of the image or in the bottom right hand corner  So no matter where that particular thing is translated within the image  the neural network can still figure out its identity  That's called translational invariance  So how does that occur? Well  the pooling step  the pooling layer has also a filter  sort of like a convolutional filter  except rather than doing convolution  it takes everything within the filter window and collapses it to a single value  So a maximum pool would take in this particular case  if you have a window size of 2 by 2  and a stride of 2  you would move a little 2 by 2 window over from the top left-hand corner to the top right-hand corner to the bottom left-hand corner to the bottom right-hand corner  And then each of those windows you would just take the max value and assign that max value to respect the position in the output  All right  so that should be clear here  So in the top left corner the max is 6  so 6 is moved over to the resulting output location  The max in the top right-hand corner is an 8  you move 8 over  3 and 4 respectively for the bottom corners  You can also take an average to help do this down sampling  but typically the maximum is used  And I mentioned that it provides some degree of translational invariance  And you can see that if I actually add a 6 here in the top left-hand corner  you can see that independent of where the 6 actually sits in that top left hand corner  the 6 is relayed to the output  Such that the neural network doesn't necessarily need to care about whether or not the 6 was in the top left of the top left corner  or the bottom right of the top left hand corner  if that information is potentially not useful for doing the ultimate classification  So the max pooling picks out strong activations with some position independence  Okay  so in typical neural network we build on the things we've just gone over  so you have the convolutions and the convolutional layer to build up feature maps  There is activations  which aren't pictured here  after those convolutions  and then there's a max pooling step  which down samples the feature maps  And then there's subsequent convolutions  and those operations are stacked over and over again to build up high level representations of the features within the image  How are these high level features processed to arrive at a final classification? Well  you just use a multi layer perceptron akin to what you've learned before  and we call these layers fully connected layers  The simplest version of this would be a fully connected readout layer  Where if this was an MNIST task  so a digit classification  you'd have a single neuron for each of the output classes that you wanted to classify  And the way that you would connect them to the last pooling layer in this network would be by first flattening  or vectorizing the pooling layer  so in this case taking a 4 by 4 by 20 volume  okay? The result of convolutions  activations  poolings  convolutions  activations  pooling  but that final representation  vectorizing it  okay? And then having each neuron in the readout layer fully connected  so it has weights now connected to all of the upstream elements in that vectorized representation of the pooling layer  And I'm just showing now connections for a single class of readout neuron for the zero  But the one has also connections to all of the upstream elements of that vectorized  high-level feature map  the result of that pooling layer  Just like in the multi-layer perceptron  you can also have multiple layers of fully connected neurons  So in this case  I'm just showing now an intermediate latent or hidden layer of neurons that are connected to the upstream elements in this pooling layer  And then the fully connected readout  class readout neurons  are then fully connected to that latent layer  And that provides some degree of meta feature representations that can help for these classification problems 
8I5rpAhYk4Y,Training the Network  So  now that we have an idea of what happens at each of these convolutional layers and at these pooling layers  let's review what it looks like to train such a network with gradient descent  So  we assume that we have a set of input images  Each of those input images has a respective ground truth label associated with it  So  for a binary classification task like diabetic retinopathy  you have a label of plus one for an unhealthy retina and a label minus one for a healthy retina  For each of those input images In  so the nth input image  we feed that input image through the convolutional neural network  The first thing that happens is there's a convolution operation that uses the filters of each of the convolutional elements  each of the features that we want to extract from an input image  we have first have layer one filter  so that's Phi one  two  through Phi K  if we have k different features we want to extract from that input image  Those are applied to the input image to result in a stack of feature maps via the convolution function  which is represented here as f  So  that function f takes in as input  the In  the nth input image  the convolutional filters and the output of that operation is Mn  the stack of feature maps  That convolutional operation happens again  but now with the layer two filters  here represented as Psi one  two through K  and the convolutional operation here f takes in as input Mn  the layer one feature maps and also the parameters of the layer two filters to give an output resulting stack of layer two feature maps Ln  That's repeated one more time  There's another convolutional function here  You see at the top  that takes in as input Ln  the layer two feature maps  the layer three filters  omega one  two through K results in the final layer three feature maps Gn  These feature maps are then transformed one more time via these fully connected layers  which we're representing here as a function script L that has a set of fully connected a weight matrix W  Those weights determine how the layer three feature maps  here Gn are transformed to result in the predicted label Ln for that input image In  So  each input image In via these operations results in a label prediction Ln  The whole idea is that we want to find the parameters that help match the predicted label to associate it with that input image to the ground truth label  The way that we do that is we set up some sort of loss function that compares how well the prediction matches the ground truth  that's typically something like the binary cross entropy function  We want to reduce and minimize the average loss across all of the input images In  So  that is that average loss is called the empirical risk function  which you see here on the right  that function e  That function  of course  is a function of all the layer one filters and the final fully connected weights in that classifier  The name of the game is to learn the layer one filter parameters  the layer two filter parameters  layer three filter parameters  and the final readout weights in those fully-connected layers that minimize this loss function  In doing so  you've now developed a system that can take in an input image and then correctly predict whether or not it has diabetic retinopathy  So typically  this empirical risk function can be quite complicated  and it exists in this very high-dimensional space and the way that we find the best fit parameters to reduce this loss function and best predict what's in these images  you need to use this thing called gradient descent  Gradient descent is this strategy in which you make a point estimate of the slope  the multidimensional slope or the gradient of this empirical risk function for a given set of parameters  Here  we're just encapsulating the entire set of parameters  these layer one  layer two  layer three filters in the fully-connected weights as one single theta parameter  So  but imagine you have a value for each of those weights in the filters and then the readout weights  We can get at that point  for those values of parameters in the risk function  We can now get the slope  the multidimensional slope of that risk function  Based on that slope  we can take a step with size alpha here in the gradient descent equation that can bring us closer to the minimum  So  that's what's showing here on the graph on the left  So  imagine for some values of parameters  look at this positive slope points for some initial starting point for our parameters that puts us at that region of the risk function  We calculate the slope  It's positive slope  so then we reverse the sign of that because we want to walk downwards to find the minimum and then we take a step size here denoted by that black arrow  Multiplying now that step size by the negative of the slope  we update our parameters and that moves us down the hill to that next red spot  We continue that process until we reach a green spot  the nearest local minimum in our function  You see on the left another alternative  We had started with the parameter values on the left underneath that word negative slope  we'd have also followed the gradient down to that local minimum  So in practice  to calculate that gradient  we don't actually feed in all the input images which will be required to get the actual slope of the entire risk function  instead we use this thing called Stochastic gradient descent  where we make an estimate of the value of the risk function of the gradient to the risk function by taking a random subset of the data and getting the gradient with respect to the current parameter values for that random subset of data  the corresponding value of that risk function  and then we just update the parameters using those gradients from that random subset rather than the entire set of data  In practice  that leads us to very similar solutions at a much faster rate than trying to calculate gradient descent across the entire input data set 
bLzn6RhSb6I,Transfer Learning and Fine-Tuning  What is the advantage of having this hierarchical representation of image features where the convolutional neural network extracts in sequence? One big advantage is that by learning and sharing statistical similarities within the high level pieces of the image  within these high level motifs  we better leverage all the training data  What does that mean? That means that if you're doing let's say  building an eminence classification system where you want to distinguish nine from seven  because the nine has low-level elements that the seven also has  you can learn a little bit about the building blocks of a nine from looking at the seven  right? So  across the entire training set  you can learn a lot from the other examples to help you classify the example of interests  So  that's very powerful  Related point is that if you're trying to learn some high-level filter on its own  and reinventing the wheel every single time  right? So  having a high-level seven feature  high-level nine feature  you don't want when you bring in a new class that you might want to classify  you don't get to use what you've learned about the low-level features to aid you in building up a representation to help you identify and classify that new class  That's a particularly powerful feature of the convolutional neural network  So  being able to leverage what the network has learned at lower levels  bring that information up to a new particular application of the network  okay? So  what this is called is transfer learning  and so I just want to go over this  So  in transfer learning  you take a network that's already been trained on a previous large database classification task  So  the ImageNet competition for instance  Then  you do additional training in the domain of interests  So  for that diabetic retinopathy case  what happens is  in the training neural network is that you take in an input image of retina  Okay  that retina is image is digested and features are extracted throughout all the layers in the network  and then there's a classification decision at the top  but whether or not the retina is healthy or diseased  It turns out that in this paper and most papers of this kind  they use pre-initialized weights from a network that have been trained on the ImageNet dataset  most drastically increases and improves performance  The reason why is that the top-level features  sometimes just in the classifier  those fully-connected layers  those are the ones that are highly specialized for particular task  right? For a particular domain  but the low-level features  right? These layer with these elementary building blocks are universal to all images  So  once you've learned them from a large data set  you don't need to necessarily learn them again because the retina image has low-level building blocks  are also reused in images of cats and of cars  et cetera  So  you can speed up the entire training process by reusing a network  by transferring one another network has learned on a different image classification task to the classification task of interest  That's called transfer learning  Right  So  just to hammer this point home  so the low-level features are universal to all images  So  on the left  I want to show you layer when filters from a convolutional neural network  So  these are little edge detectors that are extracting edges at different orientations  Okay? You see very similar receptive fields or image features that activates single neurons in a real monkey visual cortex here on the right  Okay  so a real mammalian visual system is also extracting these low-level patterns to build up its own representation to build up a perception of the real world in a biological brain  Okay  so there's something very elemental and fundamental about these particular low-level feature extractors as you can see here  and that is the principle that's exploited by this transfer learning process 
cOpYHlqis3o,CNN with PyTorch  Computer vision is a domain where deep learning has achieved some of its biggest successes with convolutional neural networks  commonly abbreviated as CNN's  constituting a major part of actually every state of the art vision model  In this section  you get to see the major building blocks of a CNN in PyTorch  Finally  we'll take a brief look at PyTorch's vision library  With Torch vision  we can easily load many of the state of the art CNN's and the pre-trained weights to use in your own applications  For the assignment  you'll be building your own CNN  You should see a slight increase in performance for the CNN versus a multi-layer perceptron  Play around with our architecture and see what you can achieve 
ELp9ytLjupE,Introduction to the Concept of Word Vectors  So  as we now start to look at natural language processing particularly with neural class models  one of the key concepts that we're going to repeatedly utilize is this concept of word vectors  So  what this means is that every word in our vocabulary is going to be mapped to a vector  and then we will do our analysis of natural language in the context of these word vectors  This is a fairly novel and perhaps unusual concept  and so before we dive too deep into the natural language processing  I think it would be worthwhile to spend a little time just to get some deeper preparation or understanding for what we mean by word vectors  So to try to understand this  let's just think about a map of the globe  So  what I'm showing on this map are several different symbols  squares  triangles etc  and the thing that I want to communicate through those symbols is the idea that if two points on the globe are on this map are physically near by each other  So if two points on the globe are nearby each other  we would expect that the characteristics of the associated regions are similar  So  for example  you see the triangles in South America are near each other  where the squares and near Asia are near each other  and so  the idea is that the regions characterized by the two triangles since they're physically near each other  we would expect that the characteristics of perhaps the people  the geography  the history etc  of the two triangles would be similar to each other and the squares since they are physically proximate would be similar to each other  but very different from the regions characterized by the triangles  So  the concept here is that we can think about the globe as a breakdown into longitude and latitude  Two numbers  longitude and latitude  So  we can think about this as a two dimensional space  If two points in that space have similar longitude and latitude  we would expect that those regions on the globe are similar  If the associated longitude and latitude of two points is very different  we would expect those regions to be different  So  we have a concept of similarity manifested through proximity  So the way that we could think of this is that we're going to do a similar type of concept with words  So  every word in our vocabulary is going to be mapped to a point in a 2D space  and that the closer two words are in that mapping or in that two-dimensional space the more related or synonymous we would think the words are  The further apart two words are in this 2D space  the more dissimilar we would expect the words to be  So purely conceptually  we can think of a situation where we have a vocabulary of v words  So we have word one  word two  word  three  all the way up to word v  Then we can think about mapping every word to a two-dimensional space in longitude and latitude on long and lat  analogous to the way that we were thinking about the globe  So  the way that we would like to do this is that we would like to learn these two-dimensional vectors of the words in such a way that if two words are similar or synonymous or related to each other  we would want their associated longitude and latitude positions to be near each other  If two words are very dissimilar or unrelated  we would like them to be far apart in longitude and latitude  This is the basic concept in the geography of this word vector or this is called word to vec  So this is very difficult to read  but if you'd look closely at the screen you will see that every point in this two dimensional space corresponds to a word and the words are actually written here  If you look closely  you will see that words that are nearby each other are related and words that are far apart are not related  So the key  and this is a closer look  so this is now zooming in on that prior slide  So  that you can see that there is a relationship or a connection between words based upon their physical proximity  and so this concept of mapping words to vectors is fundamental to how we're going to do the modeling of natural language  So  the key thing to think about is that words are not numbers  they're not in the form of numbers  So  whenever we do modeling of natural language  that modeling is usually done in terms of algorithms and those algorithms like to word in terms of numbers  So  therefore what we need to achieve is a mapping of each word mapped to numbers  and then once we have achieved that we can then do analysis  So  the way that we're going to do this is that we're going to map or relate every word in our vocabulary to a vector which is like a point in space  However into motivate the concept  we thought about two-dimensional space much as we would think about the longitude and latitude of the globe  but that is purely just conceptual to help one understand it  In practice  we do not limit ourselves to two dimensional vectors  So each word is represented by a vector  that vector may be more than two dimensions  So  in fact  typically is larger than two-dimensional and the idea is that when words are similar  they should be nearby each other in this vector space  whenever they are unrelated they should be far apart from each other in this vector space  Within the context of learning  we're going to learn that the mapping of every word to a vector  So  when we talk about natural language processing modeling  one of the key aspects of this is going to be learning the mapping of every word to a vector  and subsequently in our discussions  we will describe many different ways of doing that  There are many ways that that can be achieved  So  I underscore that we will learn how to achieve this word to vector mapping  and as I said  this is not restricted to two-dimensional vectors  So usually  the vectors will be larger than two-dimensional  So  in this discussion  all we wanted to do was to introduce this concept of mapping each word to a vector  So  in the subsequent discussions we're going to describe how that will be achieved actually using many of the tools that we already know  But before we get into those technical details in this lesson  we wanted to first just introduce the concept of mapping words to vectors  So now as we move forward  we'll see how to achieve that mapping and then how we utilize it in natural language processing 
QQHlZGDpVyA,Words to Vectors  So we've introduced the idea of mapping every word to a vector  And I just want to underscore that we really have to do something like this because words are composed of letters or phenoms  they are not numbers  And to do analysis on a computer or with an algorithm  we have to have a way of mapping words to numbers with which we can analyze  And so this concept of mapping every word to a vector is fundamental  So now what we would like to do is to look at this a little bit more deeply and provide some more details on this concept  And so this idea of modeling words or mapping words to vectors is called Word2Vec  so very natural and so very widely term Word2Vec  Each of those vectors that is associated with a given word is often sometimes called an embedding  So this is just purely to introduce jargon or nomenclature that you might see as you read about these concept  So the idea of mapping every word to a vector is related to the concept of mapping every word to an embedding  And what this means is that the idea of embedding is to embed the word in some feature space  So we think back to the globe  the picture of the map that we looked at previously  That's a two-dimensional space  and we're going to  in that case  take every word and embed it in a two-dimensional space  So sometimes this Word2Vec concept is also sometimes called in embedding  So let's let w sub i correspond to a word  So w just represents a particular word  is just a symbol to represent a word  And let's let w i represent the ith word in our vocabulary  And so the idea of this word embedding is that we're going to take that word  the ith word and map it to  in this case an m dimensional vector  So that's m numbers  In the simple two dimensional case  where we were looking at the globe and the map  m was equal to 2  But we do not have to restrict ourselves to that  so typically  m might be 100  or 200 it depends upon the problem that we're trying to solve  So the key idea to this Word2Vec concept  mapping every word to a vector  or achieving a so called embedding is that we're going to map each word to an m dimensional vector  The value of m which mean the dimensionality of that vector is dependent upon the particular problem that we choose and we can choose different values of m  So to think about this conceptually  Imagine that we have a vocabulary which is composed of v words  And so w(1) represents word one  w(2) represents word two  through w(V)  which represents the Vth word  This is our vocabulary  Whenever we do this Word2Vec concept what we're going to achieve is a mapping from every word in our vocabulary  w(1) through w(V) is going to be mapped to a vector which we'll call vector 1 will be C(1)  C1 is the vector associated with word one  C2 is the vector associated with word two  etc  Each of these vectors is m-dimensional  So this is what we're going to achieve with the Word2Vec concept  Now at this point  it's not clear how we would actually achieve this  how we would actually learn this relationship  we would consider that subsequently  But right now we are just trying to introduce the concept  And so what we are going to do then is after we do this learning  we manifest what we call a codebook  which is a set of the V vectors associated with each of the V words in our vocabulary  So each vector C(i) is associated with the word w(i)  And then the thing that we hope to achieve is that if two words are similar to each other  synonymous  or related to each other  We want those words to have vectors that are similar or nearby each other in this m dimensional space  And then we're going to learn those vectors C(1) thru C(V) base upon a large corpus of text  And so the idea through this learning process is to achieve this concept of proximity and this m-dimensional vector space is connected to relatedness of the associated words  So to give a little bit of an example  Consider the situation for which we have six words  So this is an example of text  Typically we consider what we call bos  which is beginning of sentence  we have a token  which represents the beginning of sentence  And then we have another token which represents the end of sentence just so that the natural language mode knows when the sentence starts and when the sentence ends  And then each one  whenever we have a situation in which we have  in this case  six words  those six words  which are members of a vocabulary are now mapped to six codes  or C(w1) corresponds to the code for the first word  And C(w6) corresponds to the code for the sixth word in this example  And so once we have achieved this mapping  where we can take the words  the actual words of a vocabulary and map them into numbers  which are represented in this case by six m-dimensional vectors  which  of course  are composed of numbers  Once we have achieved this mapping  we're now prepared to do analysis on a computer  because now it's ready for mathematics  So the question now that we need to think about is  how would we actually learn these word vectors? How do we learn these word vectors such that they allow us to do the natural language processing which is our goal  So as we move forward we'll start to develop multiple different ways to do this  So in fact  there's not one way to learn the relationship between words and vectors  there are multiple ways  And these often  they perform the multiple different ways  often yield very similar results  And so this is important because it implies that this concept is very robust  It's not that dependent on the particular methodology you use  there are multiple methodologies that yield very effective results  Now as we step forward we'll start to dive in to the techniques to learn these word embeddings
o4YvZfDjKRo,Example of Word Embeddings  So  as we now start to become more comfortable with the concept of word embeddings  or the idea of mapping every word to a vector  we might want to think about how we might utilize those word embeddings or those word vectors to do analysis  to do natural language processing  What we mean by natural language processing is to take natural text or language and to try to make inferences or to make predictions based upon that text  Elsewhere in our lessons  we've introduced this concept of a Convolutional Neural Network  So  what we would like to do just for a moment is to examine how we might use this concept of a Convolutional Neural Network in the context of natural language processing  utilizing this Word2Vec or word to vector representation  So  at this point  let's assume that we have a codebook  which maps every word to a vector  So  we still haven't discussed how we are going to learn those embeddings or how we're going to learn the mapping from a word to a vector  we'll get to that  But before we get to that  it might be worthwhile to examine what we might do with those word vectors if we had them  because that will help motivate us for the work that it takes to understand how we learn these word embeddings  so these word vectors  So  let's assume for now that we have learned the codebook  So  we have learned a mapping from every word to a vector  If we've done that  then any document that is composed of N words can be mapped to N vectors  each of which is m-dimensional and size  Once we have achieved this  we now have taken our natural language  and we have mapped it into vectors numbers  and then once we have that we can do various types of analyses on them  For example sentiment analysis  question and answer  translation between languages  et cetera  So  let's think a little bit about how we might achieve this  So  let's assume that we have a document  which is composed of N words  a sequence of N words  W_1  W_2  W_3 to W_N through this Word2Vec concept  those N words will be mapped to N m- dimensional vectors  So  what we would like to do is apply  the concept of a CNN  a Convolutional Neural Network  You want to apply this concept to text  So  what we're going to do  if you recall the the Convolutional Neural Network  which we've considered elsewhere in the context of images  now we're going to consider the Convolutional Neural Network in the context of text  So  recall that with a Convolutional Neural Network  what we do is we take a filter  and we shift it through the data  In the context of images  that corresponded to a two-dimensional shift in the space of images  Here  we're going to consider filters  each of which is m by m times d in size  So  m corresponds to the dimension of the vector  and d corresponds to the number of words  the length  the number of words in the filter  So  for example  if we consider d equal to three  that corresponds to convolutional filters  Here  we have k convolutional filters  each of which is length three in the dimension of words  or you might think in terms of time  three time points along a sequence of words  Then  the m dimensions  which is the height corresponds to the dimensionality of the word vector  So  what we're gonna do is we're gonna take our filter here  the filter is highlighted  and we're going to convolve it or shift it to multiple positions along the length of our text  So  the way to think about this is that that filter in this case is of dimension three  and it corresponds to a concept which is related to three consecutive words  Then what we're gonna do is  we're gonna take that filter and shifted through our text  Whenever the filter  the concept reflected in the filter of three consecutive words is aligned or related to the associated taxed at the corresponding shift location in that text  we would expect a high correlation or connection between the filter and the text  So  through this process  we're going to take each of our k filters  and we're going to shift it to each possible location in our text  For every shift location  we're going to get a number  and that number is reflective of the match between the filter and the text  Remember now the text is represented by the word vectors  So therefore  through this process  we take the original text  and we map it into a set of N vectors each of which is k-dimensional  Then  we might do something like pooling  So  the idea here is that for each of those k dimensions of the output of this convolutional process  we might take the maximum value across all N positions  So  for each row of this matrix  each row corresponds to the degree of match between the corresponding filter in the text at the associated shift location  For each row of this n by k matrix  we might take max  We might take the maximum value  which means  and this is called a pooling step  that's called max pool  which would basically quantify which value across the text gives us the largest correlation between our filter  and the original text  We do this for each of the filters  and then we get a k-dimensional vector which is telling us the maximum correlation across the entire text  between each of the K filters and that text  So  this then gives us a k-dimensional vector  and then ultimately  once we have taken the text and mapped it to a k dimensional vector  that can then be sent through tools that we already have  which are mappings of vectors of feature vectors to classification decisions  So  we can take that k dimensional vector  send it through logistic regression  or a multi-layer perceptron to make a decision  So  the the idea here is that once we have mapped words to vectors  we then have significant opportunity to leverage tools that we have developed elsewhere in our lessons particularly  Convolutional Neural Networks  So  the Convolutional Neural Network  which previously was applied to images through two dimensional convolutions  when we analyze text is manifested in terms of one dimensional convolutions  Once we can do this  we can then use the power of the Convolutional Neural Network to analyze text  One thing that I'll just briefly note is that  if we're going to do learning  if we're going to learn using a Convolutional Neural Network  we will require labeled text  which means we need to for example  know the sentiment of every document that we're analyzing  Because with a Convolutional Neural Network  if you recall  we need labeled data for the training process  So  this is actually expensive in practice because it implies that every document that we're going to learn with  we would have to have the corresponding label for  for example  the sentiment  It is very expensive or time-consuming for humans to read every document  and then provide what we call the truth for the label  for example  the sentiment  So  therefore learning in a supervised way using labeled data is expensive  So  as we move forward in our analysis of natural language processing  we're going to be particularly interested in situations for which we can learn  for example  learn the word embeddings or the word vectors based upon unlabeled data  What that means is this is just natural text  we directly take the natural texts without requiring any human labeling  and to do learning from that  So  in the discussion that we've had thus far  it was assumed that the word vectors were available i e  they were already learned  However  you may think that we can generalize this  In particular  we can treat the word vectors as additional parameters that we need to learn  Then  in the context of the Convolutional Network  we can learn the word vectors  and the other parameters of the Convolutional Network simultaneously  The challenge of this as was hinted  which I'll now highlight further is that to do this requires labeled data  So  what that means is is that for every document that we analyze  we have to have the so-called true label that we wish to predict from that text  As I said  the access to label data is very expensive because it requires a human to read every document  and then to provide a label  So  as we move forward in our analysis of methods for learning word vectors  we're going to be particularly interested in methods that do not require labeled data  which means that we can learn the word vectors directly on a corpus of text without the need for any human to do labeling  It turns out that there are some very powerful methods to do that  which we will consider next 
vEy54Wq4Nrw,Neural Model of Text  So now that we have introduced this concept of word vectors or this concept of Word2Vec  Where we're going to take every word and we're going to map it to a vector  and we're going to do analysis in the space of these word vectors  And so again  the key idea here is that we take these words  which are just language  a part of a vocabulary and once we have mapped every word to a vector  we can then do analysis  We can do analysis on a computer  for example  So the key  now we want to spend some time on how we actually could learn these word vectors  how we can learn the mapping from a given word to a vector  So there many ways that this can be done  And our goal here is to learn the codebook  The codebook is a set of vectors where we have a mapping for every word in the vocabulary to a vector  And so our goal is to learn this codebook  and we would particularly like to do it in a way for which we do not need label data  So that means we do not need humans to read the documents  a priori and assign labels to them  And so we're going to assume access to a large or massive corpus of unlabeled documents  so just the raw documents  And so the idea is that if we learn these word embeddings or these word vectors  we would like to do so in such a way that each word in a given document is predictive of the presence of surrounding words  So most people if they understand language  if you gave them a particular word  they could tell you words that are related to that word  And therefore word that might be in the near vicinity of that word and a particular document  And so that's just a fundamental concept  And so what we are going to do is build models  such that those models are capable of taking a given word and making predictions about surrounding words  The way that this will be done will by utilizing these word vectors  And therefore good word vectors are word vectors for which using those vectors we're capable of predicting the words that might be surrounding a given word  So  that will actually be the fundamental concept that we're going to use to learn these word embeddings or these word vectors  So again  the idea is we want to seek the capacity to predict words surrounding a given word  So for example  let's consider word W sub n in a given document  So W sigma n just corresponds to the nth word the document  And let's let c(Wn) represent the code or the vector associated with word Wn  And so  as we've discussed elsewhere  we're going to assume that this is an M dimensional vector  An M dimensional code associated with that word  And so if that  if our codes are good  if the vectors that we have for our vocabulary are good  then using that vector  I should be able to predict words that are surrounding it  And the way that we can learn that is we can look at a corpus  We can look each word in that corpus and we can look at the words around each word  And what we would like to do is to learn word vectors in such a way that using those word vectors we can effectively predict which words might be in the vicinity of any given word  So the way that we're going to do this is using the tools that we have already developed  particularly the multi layer perceptron  So let's consider word Wn  and it's going to be composed of an m dimensional vector  So c(Wn) is the code associated with the word Wn  and c sub 1  c sub 2  through c sub m  correspond to the m components of that vector  Those m components of the vector  of the word vector are going to be the input to a multi layer perceptron  And so as we've studied elsewhere  what we do is  we take the input vector  The input vector here corresponds to the vector  the word vector of word w sub n and we're going to map that to a hidden vector which is d dimensional  h1 through hd  A d-dimensional latent vector  And now what we want to do is to take that d-dimensional hidden vector and then make a prediction about what words might be in the vicinity of word W sub n  So just to remind a little bit about the notation and a little bit about the multilayer perceptron  The way that this mapping from the word vector c1  c2  through the cm to the hidden vector h1 through hd  The way this is done  is that each of the component of the vectors c1  c2  through cm is multiplied by weights  So W1j corresponds to the weight between component 1 of the vector c1 and the jth hidden variable  So h sub j  so we multiply each component of the vector by a weight  We add a bias associated with the j hidden variable  b sub j  And then that  so we take the code  we take each element of the code and multiply it by a weight  We then add a bias  and then we add these upl and then the product and then sum is sent through a nonlinear function which we represent by g  Elsewhere we have looked at situations where g might be represented by a sigmoid function but there are other examples  For example  hyperbolic  tangent and others that can be used for g  And so to simplify on notation that we'll use elsewhere  we say that the hidden variables are represented by the code  c  which represents the vector associated with the word  multiplied by all of the weights which are represented here by w  Adding by sp then sending through a nonlinear function g  which ultimately gives us our hidden variables  This is the first layer in what we'll see is a multilayer perceptron  So examples that one might consider for that nonlinear functions of g  This is called a hyperbolic tangent  this is just one example  So what it does it maps a real variable x to a number between -1 and 1  And so what we're doing here concisely is we're taking the code  c  associated with the word vector  We multiply it times the matrix w  we add a bias  that process gives us a vector  Each component of that vector  is sent through a nonlinear function  which is here represented by the hyperbolic tangent which is then gives us our latent or hidden variable h  And so this function the hyperbolic tangent operates pointwise or separately on each component of the vector  And so this looks familiar  this is our multilayer perceptron  The output of the hidden variables h are then multiplied by another set of weights  which are represented by the matrix  U  And then ultimately at the top of this network  we're going to have V-1 outputs  Where V is the size of our vocabulary and as we'll discuss in a moment  this will become clear why we have V-1 outputs  But y1 through yV-1 correspond to V-1 numbers which are the output of this multilayer perceptron  And then just to kind of highlight how this is done in the same way that we did before  If we take each of the hidden units  h  if we look at the hidden vector which has hidden components  each component is multiplied by a weight  We then add a bias  and so  to write this concisely  we take U  which is our matrix of weights  we have a hidden vector h  we add the bias and then this gives us y  This is just a concise way of representing this  And so now what we want to do is to use this model to quantify the probability of word Wi  where Wi could be any word in our vocabulary  Wi represents the ith word in our vocabulary  So what we're interested in is the probability that the ith word wi would live in the neighborhood of the word (wn) which is represented by the codes C(Wn)  And so what we're trying to do here is to build a model which predicts the presence of surrounding words of a given word  So Wn is a given word  is the nth word in our document  And then we're asking the question  what is the probability that the ith word in our vocabulary  w(i)  what is the probability that it would be in the neighborhood of the nth word? And so the way that we're going to do this is by generalizing the concept that we previously used for logistic regression  So what we're going to do is we're going to take each of the outputs  y1  y2 through y V sub 1 for a vocabulary of size V  And we're going to set the Vth component Y sub v to zero  So we're going to always set the Vth component to zero  And then we're going to say that the probability that word  i  Wi  would be in the presence of the nth word is represented by this expression  This is called Softmax  and as we'll see in the moment it's a generalization of logistic regression  And  so what we're quantifying Is the probability that the ith word would be in the presence of the nth word  So the nth word is represented by this vector c1 through cm  And then the output of the model are going to give us the probability of this mapping  And so that operation at the top  at the end  to the right of the multilayer perceptron  this is something that we do often times  which allows us to quantify the output here of V variables  Where we have a vocabulary of size V  this is called the softmax function  The equation that you see at the bottom is the softmax function  The thing to notice about this function is that if we sum that probability over all i  over all words Wi  it sums to 1  So if we look at this probability  the probability of word i given the code input C  If we sum over all possible words in a vocabulary  it sums to 1 as probabilities must  This is a neural model for text  and so the input of the network is the particular word that we're examining in our document  And the output of the network tells us the probability with which each of the words in our vocabulary will be in the neighborhood of that input word  Remember the concept of Word2Vec  and remember that the idea is that if words are nearby and embedding space or nearby in vector space  then we would expect that those words are related to each other  And so this concept of a word in our vocabulary being in a neighborhood within the text of a particular word is related to this concept of relatedness or physical proximity of words  So this multilayer perceptron idea is going to give us the foundation for how we're going to do learning of the word vectors and also the parameters of this multi lab perceptron  So this basic neural model of text is going to be our foundation for learning the word vectors  And then learning how to make predictions about documents based upon the word vectors that we learn 
Xg7BhVHXqLo,The Softmax Function  So  in the previous lesson  we introduce this concept of a neural model of text  and that neural model of text leveraged the multilayer perceptron that we have studied previously  However  within the context of our model of text  we introduce this new function called the softmax  which we had previously not seen before in our discussion of the multilayer perceptron  So  this is a new tool  the softmax  and it's probably worthwhile to take a few moments to talk about it and also to connect it to what we have done previously  You recall that when we introduced the logistic regression and when we originally introduced the multilayer perceptron  we were talking about binary outcomes  two-dimensional outcomes  With the text  we have to generalize that because we do not have just two words  we have a vocabulary of V words  So  we have to ask the question  what is the probability of V possible outcomes? Not just binary outcome  The softmax function allows us to do them  However  I want to highlight that this softmax function is not really that different conceptually from the logistic function that we looked at previously for binary outcomes  In fact  it is a generalization  So  here  again  is our neural model of texts  Just to remind you  we have a word which is represented by the m-dimensional vector C_1 through C_m  So  what we're doing is we're taking a word and we're mapping it to a vector  and that vector is represented by the m components C_1 through C_m  W are the weights that give us the mapping from that input vector to our hidden units h  and U is a matrix of weights that gives us the mapping from the hidden units h to the V minus 1 outputs at the top or at the end of our neural network  Then  those weights  Y_1  Y_2 through Y_V minus 1  are sent through that function at the bottom  which is called the softmax  So  this may look complicated  It doesn't look like what we have seen previously  That's called the softmax  We're going to see in a moment that actually it's not as exotic as it might appear  So  with that softmax  we can then quantify the probability that word one  word two  word three  word V in our vocabulary  the probability that each of those V words would be in the proximity of the input word  The way that we're going to do this learning is by looking at text  So  with actual documents  we will know the word  and we'll know the words around it  So  we'll know what the truth is  What we would like to do is to learn a model for this neural model of text  which is characterized by weight parameters W and U  and also the word vectors  the C vectors  those all together constitute our neural model of text  So  what I want to do is to spend a few moments talking about the equation at the bottom  the softmax  So  recall the softmax  I just put it here again just for simplicity  Let's remind ourselves that the way that we set up this softmax is that we set Y sub v corresponding to the Vth word in our vocabulary  We set that equal to zero  So  the softmax is the equation at the bottom  So  I'm just rewriting this  Let's consider the case in which we had a vocabulary that was composed of only two words  So  imagine a very simple vocabulary where it was only composed of two words  Under that circumstance  that arguably complicated equation at the top simplifies considerably  So  we only have two output words  w1 and w2  The probability of the first word  w1  given the code C is the logistic function or the sigmoid function  sigma y_1  The probability of word two is 1 minus sigmoid y_1  which is 1 over exponential y_1 plus 1  So  consequently  if we look at our neural model of text  in many ways  it's just a generalization of the logistic model  So  this is our neural model of text  and this is the special case of that model when we have two words  So  the thing to notice is that in many ways this softmax operation is a generalization of the logistic function or the sigmoid function to the case in which we have a vocabulary of V words instead of just two words  But in the case of two words  this directly reduces to the sigmoid function or the sigmoid function that we're familiar with  So  therefore  we can think of the softmax  which is introduced new here  in the context of neural modeling of text  When we look at the logistic regression that we are much more familiar with  these are actually very close relatives  They're actually intimately related  So  the softmax operation is simply a generalization of the logistic link function  So  it will take a little bit of time to get comfortable or familiar with this softmax function  but actually  it is not a very fundamentally different concept from the logistic regression concept that we already have  So  now that we have the tools of this neural model of text with the softmax  which is a generalization of the logistic function  we're now prepared to ask the question  how can we use this model to learn the word vectors  and then also simultaneously to learn the parameters W and U associated with the multilayer perceptron model of text  We're going to learn all of those together using a text corpus as we move forward in our lessons 
DoNqOcmlg9c,Methods for Learning Model Parameters  So  we have introduced the concept of mapping each word in our vocabulary to a vector where a vector is just a set of numbers  So  every word in the vocabulary is mapped to a vector  and so consequently every document which is a series of words is represented by a series of vectors  Once we have that representation of a document in terms of vectors and numbers  we're then prepared to do analysis  But before we look into that analysis  the open question is  how do we learn the word embedding? So  those word vectors themselves  So  we're going to spend a little time focusing on that  It turns out that there are many different ways to learn these word vectors and so we're going to go through several of these  So  this is a very active area of research and continues to this day  but there are several methods that are now widely used  So  let's try to see how this might be done  Imagine a situation for which you have a document  the document is composed of a set of words  So  let's imagine a word n in a document  What we'd like to do is to predict what word n will be based upon the words in the neighborhood of it  For example  we might wish to predict what word n is based upon what word n minus two  n minus one and plus one and n plus 2 are  So  these are the two words before and the tours after word n  What we'd like to do is to build a model that is capable of predicting what the nth word is here based upon to two words before and the two words after  So  what we're going to do is worth understanding that our goal is to map each word to a vector  What we're going to do is we're going to assume that we have the vectors corresponding to the four words in the neighborhood of the nth word  So  assume that we have the word vectors for the two words before and the two words after  What we're going to do is simply average those word vectors and then we're going to send that vector of m numbers  so we're assuming an m dimensional vector  So  this is an m dimensional vector that is manifested after averaging the four words in the neighborhood of the nth word  and then we're going to send it through our familiar multilayer perceptron which is characterized by parameters W and U  and then at the end of that multilayered perceptron  we've talked about the concept of a softmax  The softmax is a way for which we can quantify the probability of each of the V words in our vocabulary  So  what we have now is a model which is going to take two words before  two words after the nth word  We're going to average the word vectors assuming we know what they are  Those are going to be represented in terms of a code which is composed of the components C_1 through C_m  We send that through a multilayered perceptron and then at the end of that multi-layer perceptron  it goes through a softmax which tells us the probability that word n is each of the v possible words in our vocabulary  So  this now allows us to build a predictive model for the probability of which word is our nth word  To give some more clarity on this  this is a piece of text that was written by Ernest Hemingway  and what we're looking at is a situation now where I have a neighborhood of words in green  three words before and three words after  What we would like to do is to predict the presence of the word game based upon the words  there  was  much  hanging outside  the  So  using our model  we have the words  there  was  much  before and hanging  outside  the  after we assume that each of those words is mapped to a vector  We average those vectors  We then from that average get a single vector which has M components that we represent as C_1 through C_m  Those components or those numbers are then sent through a multilayer perceptron  At the top of that multi-layer perceptron is a softmax that tells us the probability of each of the V words in our vocabulary being the missing word here game  Then  what we would like to do is to build a model such that the parameters of the model  which are characterized by the parameters of the multilayered perceptron as well as the word vectors yield good predictions  In other words  we want our model to be good  In this case  at predicting game as the missing word  If we can do this well  effectively  we will have learned very good vector representations for each word  So  for this to work  we have to learn good representations of each word in terms of a vector and that indeed is our goal  Another approach which is kind of a complement is  let's assume that the nth word is given to us  and let us assume that the nth word is represented by the vector with components C_1 to C_m  We again send this vector through our multilayer perceptron  At the top of the multilayered perceptron again  is the softmax  and then now what we would like to do is predict via the softmax which words in this case are towards prior to the nth word and two words after  So  what we want our model to do is to understand what the nth word is and then to be able to predict  in this case  what the two words prior to and after the nth word is  If we can build a model  again  that does this well  then implicitly  we have learned good vector representations for every word which is our goal  Again  to give some clarity on this  So  in this case  the word game is the input sent through the multilayered perceptron  At the top of which is this softmax which tells us the probability of each of the V words  and we would like the model to understand or to predict that if the word game is present  then words  there  was  much  hanging  outside  the  that those are all words that are consistent with the word game  By doing this  we effectively learn via the word vectors  We learn a semantic representation for every word in terms of the word vectors  So  now  to provide a little bit more details on this  so this model which was the first one I talked about is called a Continuous Bag of Words model  So  Continuous Bag of Words (CBOW)  and the reason it's called a Bag of Words model is because it actually really doesn't matter what the order of the words  there  was  much  hanging  outside  the  because if you think about what we're doing is we're taking those words and we're simply averaging them and then sending them into the multilayer perceptrons  So  therefore  the order is irrelevant  So  when the order doesn't matter  it's kind of like taking the words and throwing in them in a bag  It doesn't really matter what the order is  So  this is in the nomenclature is called a Bag of Words model  The other model  this is called a Skip-Gram  Here  we're looking at the word game and we're trying to pick the words around it  So  this is called a Skip-Gram  The two models that we're talking about which are two of the simplest are called the CBOW or the Bag of Words model and the Skip-Gram model  In both cases  what we're trying to learn are the set of vectors for each of the words which are represented by the matrix C  So  the matrix C is the set of all vectors for every word in our vocabulary  We use the notation C because we think of this as a code  This is a codebook for all of the words in our vocabulary  Then  we also would like to learn the parameters W and U of a multilayered perceptron and their corresponding biases b and beta  So  all of the parameters in this model  either the Skip-Gram model or the CBOW model are going to be learned based upon a large corpus of data  The thing to notice about this or to think about is that we can learn this model based upon the text itself  We don't need any human labeling of the meaning of the text  We can just take the text and by via the Skip-Gram or CBOW model  we can directly learn these word vectors 
uc9QZgWle8U,More Details on How to Learn Model Parameters  So  now having introduced two of the simple forms of models for learning  the skip gram and the bag of words model  I'd like to dive a little bit deeper into the concept of learning  So  of course this whole subject is about machine learning  and so we'd like to understand in more detail what that means  So  here our machine if we think about this as machine learning  The machine is the algorithm or the model that we've developed for the text  which is here  The principle concept is the idea of mapping each word to a vector  So  now  we want to dive a little bit deeper into what we mean by learning  So  by learning what we mean is we are going to take a corpus of data  so here a large set of documents and then based upon those documents we're going to try to learn the parameters of our model  So  this is called unsupervised learning  and the reason that it's unsupervised  what we mean is that  we're just going to take a corpus of documents and learn this predictive model  We don't need any human to label the meaning of the documents  So  the parameters of our model  the model I want to emphasize and you'll see this repeatedly in different parts of the lectures  or the lessons is this multi-layer perceptron  Is a very fundamental concept also called a neural network which is appearing in many places in what we're doing and will continue to appear in other aspects of the lessons  So  the parameters that we seek to learn are the parameters of this model  So  the model parameters are the set of vectors that represent each of the words which are represented by c  Then  we have a set of parameters of the multi-layer perceptron represented by W and U  Then  based upon this softmax model  we can then predict based upon the input word c represented by c here  we can predict which of the V words are most likely to be present next  and that is represented via this softmax model  So  in this model  we have an input and we have an output  and what we would like to do is to build a model such that whenever we give the input here  the input is represented by the vectors of the surrounding words  So  here we have six surrounding words  surrounding the nth word  We're going to average the vectors associated with them that will be our input  and then what we would like to do is to predict the output which is the presence of the nth word  So  we could do this with the CBOW  or the bag of words model  we can do this with the so-called Skip-Gram model  In both cases  what we're trying to learn is basically the same thing  We're trying to learn the parameters of the multi-layer perceptron and we're also trying to learn the word vectors  These two different models  the skip gram  and the bag of word models are just two different ways of doing that we will see other ways  So  this concept of learning word vectors can be done many different ways  But  no matter which way we use  our goal is to do model learning  So  now let's think about this a little more  So  basically  what we're trying to do is to predict the probability of words on the output  So  if we think back to this softmax and we think about this model  we think about to the right  what we're doing is that we have the probability of each of the V words based upon an input word c represented by the vector c  This is a fundamental concept whether we're using the bag of words or the skip gram model  The parameters are represented by Theta  and so here what we have is  we have the probability of the output words represented by our neural network  manifested via the softmax  and then we have an input word which represents the contextual word or words that are defining the input to the neural network  Then we're going to represent the input word or words by a vector  and then finally we have parameters theta  which are again the word vectors and then the parameters of the multi-layer perceptrons  So  what we're assuming here when we do learning is that  we assume that we're given m examples of pairs  So  if we look at the third line on the third bullet  we were assuming that we have m examples in our corpus where for each example the i'th example we have an input WN  which is the the input vector  and the i means ith example  and then Wout is the output vector again the ith example  We have m of these examples  This is what we call our training set  M here which is the number of examples and if we think about the Skip-Gram and the CBOW concept  This set of m and examples of in and output can be manifested just by looking at a large corpus of documents  and in that setting the number m the number of examples can be quite large  Then  what we do is we set up something which is called a cost function  and so  then that's represented by f  where f is a function of the parameters data and the data  The way that we represent this is simply the sum of the logs of those probabilities for all of the m examples  So  that the last equation is simply the sum of the predictions of our models for all input-output pairs in our training set and what we would like to do is to learn the parameters or seek the model parameters theta such that we maximize that function f  So  if we can do this and this is at the heart of the concept of learning  if we can do this well then that means that we have a model which is good at predicting the output words given the input words in here based upon m examples from our corpus 
g6ZCpS7Farc,The Recurrent Neural Network  So  we've introduced some basic models for natural language processing with neuro networks  the Skip-Gram model and the bag-of-words model and these are very nice models for learning those word vectors or those so-called word embeddings  But  there are other things that we might want to do which we cannot do with those tools and so this motivates something called the recurrent neural network  The recurrent neural network we're going to use here for text  it's actually a basic framework for representation of a sequence of data  so the recurrent neural net  recurrent means repeated over and over again and that will become clear as we move forward  But this recurrent neural network concept is a general one  it is not only used for text for natural language modeling but natural language modeling is a very important application  So  let's assume that we have learned our word embeddings or our vectors and now our goal might be to use them to perform text synthesis  So  what we're talking about here is the ability to automatically produce a text  So examples where this might be useful  are if I give you an image and your goal might be to automatically manifest a caption for that image  or to do language translation  to go from one language to another  So  if we think about that if we go from language A to language B  then we need to be able to synthesize the text and language in language B and so this requires the capacity to for synthesis  so thus far we really don't have those tools and so now we're going to dive in into understand them a little bit to try to understand how we might do text synthesis and this is done using something called RNN or Recurrent Neural Network  So recurrent neural networks as I said  are very powerful tools for text but they're also very powerful tools for anytime we're dealing with sequential data  so therefore they're an important tool to try to understand  So  this is the basic way that we might think about the use of a recurrent net  So  remember that W_n represents the nth word in a document and we may think of a document as a sequence of words  so here words were W_n minus one  word W_n  word n plus one et cetera  So  a sequence of words in a document  Each of those words is going to be represented by a vector of the concept that we have talked about previously  so each word is mapped to a vector and so those words  so what our goal is to have a model which can take the n minus one word in the vector associated with the n minus one word in  send it into a neural network  also use knowledge of the hidden vector at time at the previous word  so h_n minus one and then to be able to predict the nth word  So  the idea here which we will dive deeper in  so this will become more clear as we go forward  is that what we want do is we want to build a model such that given word in a sequence we can predict the next word  so for example given word n minus one we can predict word n  given word W_n  we can predict word W_n plus one  So  what we want to be able to do is to predict a sequence of words one after the other and so this is done with a neural network which is repeated over and over again  so right now this point is this figure might look a bit confusing  we'll dive deeper and it'll become more clear as we go forward  The most important thing to notice here is the recurrent nature of this setup  so notice we have a recurrent or repeated use of the neural network and repeated use of the Softmax and so that's why this is called a recurrent neural network  So  let's try to understand this neuro network a little bit more deeply  So  the neural network is characterized here by two inputs  so W_n minus one is the n minus one word in a document and we're going to represent that word via a vector or by an embedding  We're also going to assume that we have access to the hidden vector in the multilayer perceptron from the word or the previous word or the n minus one word again  those both are going to be concatenated together  so what we're going do  is we're going to take a vector for the n minus one word  so the vector associated with n minus one word one word and then we're going to take the vector associated with the hidden units associated with the previous word  we're going to concatenate them which means we're just going to stick them together and then we're going to send them into a neural network  The neural network is characterized by parameters W and then from this model we will predict the nth hidden units which again are represented by a vector  so the key thing to notice here  is that we have a word vector and the hidden units from the previous word are going to go into a neural network which is characterized by parameters W and then we're going to predict the hidden unit at the network associated with the next word or the nth word  so we're using information from the n minus one word to predict what's going to happen for the nth word  at this point we characterize that in terms of a hidden vector h_n  So  that hidden vector h_n is then going to be sent in the same way that we've done before into the multilayer perceptron and then to the Softmax and then it's going to predict what the nth word is  and then h_n is then sent forward to the next word in the sequence such that we can repetitively or in a recurrent way predict each word in the document  So  notice that the h_n minus one which is the hidden vector from the previous word  feeds into the model  the neural network model  for the prediction of the nth word and then in the process we get a new hidden vector h_n which is then sent forward into the next neuro network and then the associated next word W_n is also sent into that neural network and then they predict the next hidden unit h_n plus one  which then predicts the next word and this repeats or re-occurs and so therefore this is called a recurrent neural network  The key thing to notice though  that this architecture looks very much like what we've seen before and so one of the things that I hope is becoming more evident as we look at these neuro networks over and over again is that the basic structure of this recurrent net is a multilayer perceptron with some inputs predicting an output  our previous discussions of the Skip-Gram and the bag-of-words model had the same basic multilayer perceptron architecture  So while these are different models and they do different things  the heart of them  the underlying heart in each case is the multi-layer perceptron  So  just to introduce a little bit of notation which will be convenient later  So  we take the vector associated with the n minus one word and we take the vector associated with hidden the hidden vector associated with n minus one word  h_n minus one  We concatenate those together and what we're going to do is call the concatenation of those X sub n minus one  So that's just notation that X sub n minus one is simply notation for the concatenation of the vector for word n minus one in the hidden vector n minus one  this is then sent to a model for the hidden vector h_n which is a set of weights W multiplied by the input X_n minus one  we weight each of the inputs and then add it altogether  we add a bias and then we send that through a non-linear function  which here is called the Hyperbolic tangent  this is not the only non-linear function we can use  but it's a very popular one  we'll discuss this in greater detail later  so that is the model of our hidden units h_n  and then the hidden units h_n are then sent through a set of weights  so they're multiplied by weights and then added all together this is the process U h_n  we then add a bias and then this last step is the Softmax and through this model  through this two step where we take the input  we map it to a hidden vector and then the hidden vector is mapped via the Softmax to probabilities  this is our model and so the equations here are just simply ways of representing that and so what the model is doing the last equation is predicting what word n is W_n given the previous word W_n minus one and the previous hidden vector and this is what the multilayer perceptron is doing  So  the key thing to think about as far as what this model is doing  it's predicting the next word the nth word based upon two inputs  it's using the hidden vector h_n minus one from the previous word  and if you think about that what that is basically doing is it's telling us contextual information about which words were likely previously in the previous step  because if you look at this model the hidden vector h_n predicts which word is next and so if we think about what h_n minus one is doing  what it's doing is it's telling us which words were probable in the previous step  tells us contextual information the W_n minus one tells us what was exactly the previous word and so based upon the context provided by h_n minus one and based upon the previous word W_n minus one  we then predict what the next word W_n is  So  this is called a recurrent neural network the reason it's called a recurrent neural network is that this process represented by this figure is repeated over and over again in a recurrent fashion and this is a way in which we can synthesize or generate a sequence of data  here a sequence of words or text 
LHgp0w8wV78,"Long Short-Term Memory  So we've introduced this concept of a recurrent neural network  That recurrent neural network is a very natural extension of the multilayer perceptron  In fact  is simply a a repeated or recurrent usage of the basic a multilayer perceptron  So it's very simple concept  It's also a very powerful concept  However  it's been found that in the context of text  and the context of documents  and the context of natural language  which is what we're trying to analyze here  the recurrent neural network in its most simple form  which we've talked about in the previous lesson  is not as effective as we would like it to be  So now  what we're going to do is we're going to introduce a new form or a more complicated form of recurrent neural network  This is going to also build upon the concept of multilayer perceptron  but in a far more sophisticated way  So I want to say at the start  this lesson is is much more complicated than some of our other lessons  Probably this is the most complicated  The reason I want to spend some time talking about this thing called LSTM  or long short-term memory is because this represents the state of the art in natural language processing today  So while the methodology is somewhat complicated  it's worth studying and worth understanding because this technology is what's driving the recent revolution and natural language processing based upon neural models  This technology is truly remarkable  We'll talk a little bit more about that as we go forward  While this model was more complicated  I hope to demonstrate that it is understandable and perhaps not so difficult to understand  So again  we have the Recurrent Neural Networks  So we see a sequence of words  wn-1  wn  wn+1  This corresponds to the n minus one  the nth  and the nth plus one word in our sequence of a document  We again see the repeated use of the neural network and the repeated softmax  However  this model now has introduced some new things that we did not have in our previous simpler recurrent neural network  and the most important of these are what we call memory cells  So notice that in our simple recurrent net are relatively simple recurrent net  What we did was we propagated the hidden units h from the previous word to the next word  So we had Hn-1 which was feeding into the prediction for hn  which then predicted the nth word  We've introduced something new  which is called a memory cell  We'll describe this in some detail as we go forward  This is represented by c  the cell  Cn-1  Cn and Cn+1  and Cn+2  So we'll we'll drill down into that a little bit more  We again  as before  from the simple simpler recurrent neural net  we still have these hidden units  So we retain those  So the key thing that we've introduced that is new are these memory cells  and we'll drill down more deeply as we go forward  But the key thing to notice is that this setup  this recurrent neural network setup is basically the same  We again see the softmax  which allows us to predict the next word  What's going to change here is what's in yellow  the form of the recurrent net will change  So let's try to understand that  So to do this  we need to kind of take a little bit of a step backwards  and just kind of recall some notation because we'll need to use this notation as we go forward  So this is a bit of a review  So we previously had a word vector which represents a word vector for the n-1 word  and we had a hidden vector  which was representative of the n-1 word  We concatenated those  which means we just stuck them together  Those two vectors are stuck together  After that concatenation process  we call that cumulative vector Xn-1  That vector then goes into a neural network which is characterized by parameters W  and the we  so basically we take the parameter the components of the vector Xn-1  we multiply them by the weights  we sum them together  we get the outputs  We get the outputs we add a bias  and then that process is then sent through a non-linear function f which we'll talk about a little bit more  Then from that  we get are hidden units Hn  So this is the basic construct that we looked at previously  So now what I'd like to do is to just spend a moment talking about the form of that non-linear function  So previously when we introduced the recurrent neural network  we use the hyperbolic tan or tanh  The hyperbolic tangent  It's a widely used function in neural networks  It has a functional relationship which is shown in the figure  So the key thing to take away from that figure is that the hyperbolic tangent will always yield a number between minus one and one  The output of the hyperbolic tangent here represented by y is always between one and minus one  When the input is large positive  when when x is large and positive  the hyperbolic tangent tends towards one  Whenever x or the input is negative and large  the output tends towards negative one  This is a hyperbolic tangent  This is one example of that non-linear function f  We're going to use multiple forms of that function f  and so  therefore  it's worth spending a few moments talking about them  So one is the hyperbolic tangent  Another is another function that we have talked about elsewhere in our lectures  and that is the sigmoid function  So here the sigmoid function is represented by the sigma  That's called that Greek symbol is called sigma  and  therefore  we choose it to represent the sigmoid function  The sigmoid function is shown in the figure  So now notice importantly that this sigmoid function always lives between zero and one  The output of this sigmoid function is always between zero and one  When the input is large and positive  the output of the sigmoid function tends towards one  When the input towards the sigmoid function is negative and large  the sigmoid function has an output which tends towards zero  So it's important to notice that the hyperbolic tangent has an output which is between minus one and one  where the sigmoid function has an output which is between zero and one  Those differences are going to be important as we move forward in our in our discussions  Okay  so let's go back to this model  So this is now our recurrent neural network  and I want to spend a little bit of time walking through the neural network slowly and carefully because this is somewhat complicated  But it is understandable  But we need to go slowly and think about it  I also want to reiterate that this is the state of the art model for natural language processing  So it is certainly worth trying to understand  So again we're going to introduce the notation where Xn-1 is going to be the concatenation or the a combination of the vector for word n-1 and the hidden unit  Hn-1 from the previous step  So this is the same as we did before  So Xn-1 which is going to be the input to our model is the word vector from the previous word and the hidden vector from the previous word  Now what we're going to do is introduce not one neural network but actually three neural networks  Okay so this is where things start to get complicated  Previously  in our recurrent neural network  we had one neural network process  Here we introduce three  One of them is represented by i  the other represented by f  and the other represented by o  We\""ll explain what i  f  and o mean later  But if you look at what's going on here  this is not as wildly different as one might think from what we have done before  We take the input Xn-1  and we send it separately through three different neural networks  The parameters of the neural networks associated with the output i  or W sub i and b sub i where b sub i is the bias and Wi The weight parameters  we take the input X n minus one which is our concatenated vector  We multiply it by our weights  We sum them together  and then  we get an output which is then sent through a sigmoid function  We take the input X n minus one we multiply it by the weights associated with W sub i  We add those together  We add the bias  We send those through the sigmoid function and we get an output i sub n  The thing I want to emphasize or I want you to think about is that sigmoid function is operating on a vector  So  the outputs of W i times X n minus one that is a vector  it's like the hidden vector in our neural network  So  the sigmoid function is operating separately on each of the components that are the output of the neural network on the bottom left  So then  for the neural network associated with f  we have parameters W f and we have biases B f  We'd play the same game with the neural network  but now again  our nonlinear function is the sigmoid function  Sigma and then finally  the same thing for the output of the o or which will be corresponding to an output  So  the key thing to notice is that we're now introducing three neural networks  The basic functional form of each of those neural networks is the same  The detailed parameters are different for each of those three neural networks  So again  I want to emphasize that the non-linear function is the sigmoid function  which means that every component of the vector i  the vector f and the vector o  Those are each vectors  So  i sub n is a vector  f sub n is a vector  and o sub n is a vector  These are the hidden vectors that we saw previously in the multilayer perceptron  The thing I want you to notice is because each component of the vector is going through a sigmoid function  and because of the fact that the sigmoid function always has an output between zero and one  we know that each component of the vector i  the component the vector f and the vector o  Each component of those vectors is always between zero and one  that will be important later  So  these three neural networks which have outputs i  f  o  are what we call control networks  control neural networks  They're characterized by the set of parameters Wo  Wf  Wi  So  those are the set of weights and then the set of biases  Bo  Bf  and Bi  So basically  each of those three neural networks looks very much like what we've seen before in the multilayer perceptron  The three control neural networks that we represent here are going to be used to control the output of our neural network or control the output of our recurrent neural network as we'll see as we go forward  Okay  so now  we're going to introduce a fourth neural networks  So  with the LSTM or the Long Short-Term Memory  We're going to have four neural networks  So  three of the neural networks are shown at those three equations  These are the basic multilayer perceptron that we've seen before  Now  we're going to introduce a fourth which is going to give us a new estimate of our memory cell  which I'm going to represent as C sub n  So  this C is our memory cell and the subscript n means it's going to be for the nth one and the squiggly line on top of the sea that's called a tilde system notation  So  that squiggle on top of the sea is called a tilde  So  that's called tilde C n  What we're going to be using here is the hyperbolic tan as the output or the non-linear function in our network  So  in this case  we know that the every component of the vector C tilde n  it's going to be between minus one and one  as manifested by the properties of the hyperbolic tan  So now  what we're going to do the last step  which is a key step which will go through slowly and we'll try to really understand is  we're going to update the memory cell  So  remember that in this recurrent network  we have a hidden vector H n and we have a memory cell C n  Okay  So  let's now try to understand what this symbol  So  C this circle with a dot in it  So  if you look at what we're doing here  we're saying that C n  which is the nth realization of the memory cell  is a combination of C n minus one  which was the memory cell from the previous time  Then C tilde n which is our updated representation of the memory cell  So  C n is a combination of the previous memory cell C n minus one  and a new estimate which is C tilde n  Then  C n minus one and C tilde n  are operated on by the output's f  n and i from our control network  So  right now notation is very confusing  So  let's try to understand what this means  So  let's think about this  f n is the output of one of the control networks that we looked at previously  Remember f n is a vector where every component of that vector is a number between zero and one because of the fact that the f n was manifested by going through the sigmoid function  So  f n is a vector  It's represented by each of the components in those rectangles those are the numbers  Then C n minus one is the memory cell from the previous time stamp n minus one  The symbol circle with a dot  what this means is that we're going to take every component of f n  and multiply it by the corresponding component of C n minus one  Then after we do this  we're going to manifest a new vector which is the combination of f n and C n minus one  So  we take the vector f n which is a set of numbers each of which is between zero and one  We multiply it by the respective component of the memory cell  C n minus one and that gives us a new vector  That process  that operation is represented by f n circle Cn minus one  So  this is simply taking every component of f n and multiplying it by the corresponding component of C n minus one  So now  you can start to see where this control process is manifested  So  if we go back and look at this  what the f n is doing  it's controlling the contribution of the previous memory cell C n minus one in our update of the new memory cell C n and then the i n  which we'll talk about more in a moment  i n does a similar operation on C tilde n  Then we sum those two vectors together and then that gives us C n  So basically  what's happening is that  memory cell C n is a combination of the previous memory cell C n minus one and the new estimate of the memory cell C tilde the n  Then  the vectors f n and i n each of which is themselves and output of a neural network are controlling the degree to which C n minus one and C tilde n contribute to the new updated cell  The final thing that we need to think about is how we manifest the hidden unit or the output h n which then goes into the softmax  This is now the third control network which has an output o sub n  This is multiplied in the same way as before dot circle with the hyperbolic tangent of the memory cell  So  if you think about what is going on here  we have three control networks that are manifesting the output vectors o n  f n  and i n  We have an updated estimate of our memory cell C tilde n which also is represented by a neural net  Then  the three control networks are then operate on the memory cells  Then  the output o sub n control vector operates on the hyperbolic C n to give us our hidden unit h n  So  this is the basic construct of the Long Short-Term Memory  This is as I've said  the state of the art method for natural language processing and for many types of sequential modeling  It is certainly more complicated than anything that we've looked at before  So  what we're going to do in the next lesson is to walk through this model slowly  part by part to try to understand more fully how it works"
kJBW4Lhz5f4,Long Short-Term Memory Review  We've introduced this concept of the long short-term memory  This is a recurrent neural network which is used for modeling of sequences of data  Language which is represented by a sequence of words is a natural place for the long short-term memory to be applied  Within the context of natural language processing each word in the sequence of words of a document is represented by a vector  and so therefore in the context of documents the long short-term memory is operating on a sequence of vectors associated with the words in the document  Now the LSTM is the state of the art method for natural language processing today  and it has been used to achieve remarkable results in many settings  However  it is certainly more complicated than most anything else we've discussed in these lessons  and so therefore it is probably worth taking a step back to review the LSTM understand  and perhaps a little bit more deeply what the various components of the model are doing  So let's recall the long short-term memory  the LSTM  It's composed of three control neural networks  which have output vectors at time n in our sequence  On fn and In  Each of those three neuro networks  have the same basic form in terms of essentially a multi layer perceptron  and we noticed that the non-linear function for each of these control units is a sigmoid function sigma which means that  each of the components of the vectors on  fn and in are numbers between zero and one  In the long short-term memory  we've introduced a new vector in addition to the hidden vector h  we've introduced a memory cell which is represented by c  The memory cell is updated by yet a fourth neuronetwork which is which has a non-linear function corresponding to the hyperbolic tangent  Again a multilayer perceptron  but then the non-linear function is a hyperbolic tangent  which means that c till to n has components or has elements in the vector c till to n which are each a number between minus one and one  Those four neural networks in the top block are then used to update the memory cell cn  and to also update the hidden variable hn  The way that this is done  is by taking the control outputs from the f network and the i network to operate on the previous memory cell cn minus one and our new estimate c till to n  Then the output of that which is cn  our new updated memory cell is then sent through a hyperbolic tangent  and then is operated on by the output of the network with on  and that gives us the hidden unit  So a lot going on here  let's try to understand this a little bit more  So looking at the output of our update of our memory cell  we notice that what the vector fn is doing is it's controlling the degree to which we forget old memory cell components  So remember  that every component of the vector fn is between zero and one  So if the particular component of fn is near zero  then the corresponding component of the memory cell cn minus one is multiplied by a number which is near zero which means it essentially forgets that memory cell output  So  the fn allows the model to forget or to erase previous memory cell outputs  So  the reason we use the symbol f sub n is because this is a methodology for allowing the model to forget or to erase old memory cells  By contrast the in  controls the input of our new estimate c till to n  and so what we're doing is we're taking the previous memory cells  which is a vector of numbers cn minus one  we're multiplying them component by component by the vector fn which is telling us  and remember  that each one of those outputs of fn is between zero and one if a component of fn is near zero that means we're seeking to erase or forget the corresponding component of the memory cell  So F is called our for getting control  i is called our input control because it's controlling the degree to which our new estimate c till to n contributes to our new estimate of cn  and so again  every component of in is between zero and one  If the corresponding component of in is near zero  then the corresponding component of c till to n is not going to contribute significantly to our new representation of the memory cell  By contrast if a component of in is near one  then that means that the corresponding component of our new estimate c till to n is going to contribute significantly to our new memory cell cn  So the forget control in the eye  or the input control are controlling the degree to which our previous memory cell  and our new representation of the memory cell contribute to update the memory cell cn on the next time timestamp  Then the on is the output control  and what it's doing is it's taking the memory cell cn  it's sending it through a non-linear function hyperbolic tan which yields an output between minus one and one  and then the on is also a vector of numbers each of which is between zero and one  and each of the components of on are represented by the respective components of the output of the hyperbolic tan  and then that product of the output by the hyperbolic tan of Cn  controls the degree to which each of those components contribute to the next hidden vector hn  We then introduce a fourth neuronetwork which is our update of our memory cell which is represented by c till to n  So the simple single neural network in the original recurrent neural network  In the LSTM is controlled now by four neural networks  which allow the model to forget previous data in our sequence of data through the f control  They allow the ability to input new data through the eye control  and then the o control  controls the degree to which the memory cell goes to our output  here the hidden vectors  So we've introduced the same basic concept of recurrent model because this basic construct that we see in this schematic repeats recurrently  it's why this is called a recurrent neural network  However  it's got two outputs  it's got the hidden vector hn and it's also got the update of the memory cell cn  All LSTM parameters can be learned using unlabeled data  because basically  what this is is a model that allows us to predict the next word based upon the previous word  and we can do this for a sequence of words in a document or corpus  and there's no need for human labeling of the documents  So therefore  we can apply this model to unlabelled data  The word vectors may be treated as additional parameters in the model  and they maybe learned as well  So this LSTM model is a model that allows us to learn the word embedding vectors and then also allows us to learn a model which is capable of synthesizing text  which we'll talk about more in a moment  This ability to synthesize text is critical for applications like text translation  So in that case  the input text might be a sequence of English words  and then the output might be a sequence of French words  If we're going to translate from English to French  we have to be able to synthesize language  and this LSTM which allows us to synthesize a sequence of words is a way by which we can do this task of texts synthesis  This model  the LSTM is undoubtedly complicated  but it's built up of basic constructs that we have seen before the multilayer perceptron  It's a complicated model  but the reality is that it works very well in practice providing state of the art results for many applications including translation  In the moment  in our next lesson we'll show another application where the LSTM has proven to be very effective for text synthesis 
HshE44J_-rY,Use of LSTM for Text Synthesis  So  now that we have introduced this concept of long short-term memory  or the LSTM  it'd be worthwhile to spend a few moments to understand how this model is used in text synthesis and as has been discussed  this basic framework for texts synthesis has achieved remarkable results across multiple applications  One is text translation from language A to language B  another application is in the automatic captioning of images  So  the idea would be that you're given an image  and then the machine can automatically look at the image or analyze the image and then synthesize text that is improper language  that can describe the image  So  we're going to talk a little bit about that in this lesson  So  let's assume that we have learned the LSTM parameters based upon a large document corpus  a large set of documents  So  how can we use the LSTM to synthesize text? So  remember that the basic construct is a neural network which has as input the hidden units H  and the memory cell C  From the previous step  those are inputs  The previous word is an input  and then out comes the next hidden unit which predicts the next word  and then we also update the memory cell and then the updated memory cell and the updated hidden units go to the next neuron network  So  the way that we are going to do this is that we're going to repeatedly apply  or in a recurrent manner apply this basic construct  and this is of course why it's called a recurrent neural network  This BOS symbol that I'm showing here  that means beginning of sentence  So  whenever we initialize the process of synthesizing text  we tell the model through the symbol BOS that we're beginning the sentence  So  it understands we're at the beginning  and then the model will automatically end the sentence through an EOS  or end of sentence symbol  So  we have a beginning of sentence symbol and an end of sentence symbol EOS  Each of which are represented by vectors  which are also learned those vectors are analogous  or basically they play the same role as the word vectors that we've talked about previously  Then  the hidden units are sent through a softmax  and then through that softmax  remember we get a probability on the next word  So  in this case the first word w_1  The softmax tells us the probability of which word is first  So  what we could do is we can select that word which is most likely as manifested by the softmax  and then we'll say that that word is w_1  The hidden units are updated to h_1  the memory cell is updated to c_1  that then goes into the next network  the next LSTM network  What we do then is we take the word w_1  which we've already predicted  and we use it as the input to the next step in our recurrent neural network  So  we first predict the first word w_1  we then take the vector associated with w_1  and that is used as an input to the next neural network  the next layer in this recurrent process  Then the updated hidden unit h_2 is again sent through a softmax  we then again take the word that is most likely  that has the most highest probability out of the softmax  we call that w_2  W_2 is then sent down to be the input to the next layer  So  then we take the vector associated with w_2  we take h_1  we take c_1  we send it into our LSTM  and we do the same process  We get h_3 softmax  take the most likely word  that's w_3 and repeat  So  we can see that through this recurrent neural network  we can sequentially synthesize text  As I'll show in a moment  the quality of this synthesis is truly remarkable  Almost equivalent to what one might expect of a human  So  whenever we do this  we have to initialize the hidden units h_0 and c_0  So  typically  in real applications  we're going to initialize h_0 and c_0 to be consistent with some task  and so what I'm going to do is I'm going to give the example of the automatic captioning of a picture  So  let's look at this example  So  we see a picture of a bird  and we see some text that describes the bird  The thing I want you to recognize is that this text  this bird has read throat and breast  with a dark brown colored head and wings  That sentence was generated automatically by a machine  So  that sentence was not produced by a human  that sentence was produced by a machine  So  how how is this done? So  elsewhere in our lessons  we have learned about the deep convolutional neural network  The deep convolutional neural network is a framework by which we can analyze images such as this bird here  So  what is done is that the picture is first sent through a deep neural network  and then it manifests at the top of that neural network  It manifests a set of features which are characteristic of the image  Those features are then used to initialize the parameters of our long short-term memory  So  they initialize the memory cell c_0  That initialized memory cell c_0 is then sent through a long short term memory to synthesize a sentence which I showed you before  So  this is a so-called end to end method by which we can train a model to analyze images and then synthesize text simultaneously by taking two of the key components that we have discussed in our lectures  The deep convolutional neural network for analysis of images is used to extract features which predict the initialization of LSTM  With that initialization  the LSTM then synthesizes texts  So  the input is the image  the output is the text  and so to learn a model like this which is a combination of a deep convolutional neural network and a long short-term memory  recurrent neural network  what we do is we give the model examples of inputs  of images  and outputs of texts that were generated by humans  we have many examples of images and text  and then we use that data to learn the parameters of our convolutional neural network in our LSTM  Here are some examples that you can look at  Each of these are manifested by the machine  So  what is shown here are six examples of images  and then the text underneath that was generated automatically through the long short-term memory machine  So  if you look at these  it's remarkable I think  that the quality of the sentence is excellent  grammatically correct  and also if you look at the photograph  and if you'd look at the sentence  it's remarkable  I believe it is remarkable  The machine's ability to analyze the image and then to represent that image in English words that are meaningful  and also written in a way that is almost indistinguishable from what a human might do  This example now brings together two of the key concepts that we have discussed in these lectures that are driving some of the tremendous excitement in the opportunities for machine learning  We're taking the deep convolutional neural network for analysis of images in the LSTM  which is a state of the art method for synthesizing text  We're bringing these together to automatically caption images with a degree of accuracy and realism that is comparable to what a human might do 
7LGmUi3xS0I,Simple and Effective Alternative Methods for Neural NLP  So  we've introduced several different methods for analyzing text  so we introduced the skip-gram and the bag-of-words model for learning word embedding vectors  We've also introduced the LSTM model which is particularly effective at synthesizing text  at generating text  What's been recognized over the last several years is that while these complicated models particularly the LSTM are useful for many tasks and in fact are required for example for synthesis  it's been recognize that oftentimes very simple models work well for many tasks  So  the last example I want to give on the natural language processing discussions is to just try to demonstrate that for many natural language processing tasks very simple models much simpler than the LSTM and also the convolutional neural network which we've discussed elsewhere  Much simpler models than those can often yield state-of-the-art results  So  let's consider the problem of sentiment analysis  So  sentiment analysis is characterized by the situation in which we're given text  given a document which will represent the ith example of a document where we represented T_sub_i  so  T_sub_i is a set of words associated with the text from the ith document  and then we're also given a label  for example minus one plus one  where minus one may mean that the sentiment is negative and plus one may mean that the sentiment is a positive  Here we are considering a binary example but we can consider other than binary  So  this is a very common problem  we would like to build a model  a machine that can read the text represented by capital T and then it can predict the sentiment  So  the question is  can we do this task using relatively simple models? Models that are much simpler than the LSTM and the CNN  The answer is  that we can and so I'd like to just spend a few moments to demonstrate that  and this kind of underscores that for many tasks in natural language processing simple models work very well  So let's consider text T which is represented by N words w_1  w_2 through w_n  and what we would like to do is a very simple model  so  C w_1 is the code or the vector for word one  C w_2 is the code or the vector for word w_2  an C w_n is the code or the vector for word w_n  Those are the codes  are the vectors representing the words in our document  Let's do something really simple  Let's just average those words  Just take the average of the word vectors across all N documents in our all  average the word vectors for all N words in our document  so we get a single vector after we do that  and then let's just send this into a very simple logistic regression classifier  So here the inputs to the model R is the average of the vectors which will be represented by C_1  C_2  through C_n  those are the M components of the average  of the output of the average of our vectors  we're going to send that into a logistic regression which is characterized by the parameters w and then we're going to represent the probability that the label is plus one in terms of this sigmoid function  sigmoid of the logistic of Y  the probability of class label minus one as one minus that  This is a very simple model simple logistic regression  So  this is bringing together several concepts that we've talked about elsewhere in these lectures  this is logistic regression classifier characterized by parameters w and we're using the concept of word  vectors are word embeddings for each of the words in our document  we're building a very simple model  Take the average of the word vectors  send it to the logistic regression  predict the sentiment  and so that logistic regression is represented or the sigma function is represented by this sigmoid function that we've seen elsewhere in our discussions  So  this has been found as we'll see in a moment to be very effective in practice  The only parameters that we need to learn here are the word vectors for each of the V words in our vocabulary and the weights of the logistic regression  this is clearly much simpler model than the LSTM  It's much simpler than any of the recurrent neural networks  much simpler than the convolutional neural network  but it's been found to be despite its simplicity to be very effective in practice  So  the question that we would ask is  how could such a simple model work well? How could this work well? Now the question the problem that we're solving here is actually a fairly straightforward one  our objective is simply to analyze the sentiment of a document  So  the model to underscore this simplicity  If we look at what we're doing is we're taking the average of the word vectors for each word in our document  So that means that we're not using the word order at all  because if you change the order of the documents  if you change the order of the words in the document the average doesn't change  So  how can this work? So  what must be happening and we'll see in a moment what is happening  is that the model will assign near zero values to the embeddings or the vectors for all words that are not informative for the task of sentiment analysis  So  remember that we're learning word vectors for every word in our vocabulary  If a word in our vocabulary is not important for sentiment analysis what this model will do is it effectively it will implicitly and in practice set all words that are unimportant to the sentiment analysis task to an embedding vector of near zero  which means that  the contribution to the average of all words that are unimportant for sentiment analysis  their contribution will be zero  So therefore  what this model is doing is it's inferring the key words of importance to the sentiment analysis  Notice  I also note that using the average of the word vectors is not the only thing one can do  Alternatively  what you can do is you could do a component-wise maximization and so therefore what I mean by that is just that  if we look at every component in our word vector across all the words in our document  we simply take the maximum of that value across all words in the document  So a document is characterized by a set of word vectors associated with each of the words in a document  What we're going to do is  for each component of the vector  we're simply going to ask which word in our document can has the largest value for that component and then we'll just use that  So  this is what we call component-wise maximization  After we do that  we again get a vector  that vector goes into our logistic model with parameters W  and again we make a prediction  This is a very simple model  So  these are called simple word-embedding models or SWEM  So  that the average  we can either do an average or a component-wise maximization and we can do both actually  We can do an average and a component-wise maximization and then concatenate them  So  what I said was that  conceptually for this to work  implicitly what must be going on is that the model learns that most words are not important to sentiment analysis and consequently the associated vectors of those words are mapped to a zero embedding  Then only the relatively small number of words which are important for the sentiment classification task  only those with relatively small important words have non-zero embedding  So  what I'm showing here is  these are real results on sentiment analysis of the large corpus  SWEM  remember  is our simple model  We're showing SWEM max  which means that we're doing maximization of the components of the word vectors across the document  What we're showing is the frequency of each value in the word embedding and so what you notice is that in the SWEM model almost all components of the word embeddings are zero  So  notice that we're just showing a histogram of the value of the word embeddings and for the SWEM model  almost all words have word embeddings near zero  So you could see that they're almost all near zero and there's only a relatively small number of words that have non-zero value  and those are the words that contribute to the sentiment analysis  GloVe is a different method for learning word embeddings which is different than what we're showing here  It is not based upon the SWEM construct  and here you notice that the GloVe results has a much more broad distribution of values of the word embedding  So  one thing to take away from this  is that the values of the word embeddings or the values of the vectors associated with the words in our vocabulary  those vectors depend upon the method you use to do the learning and different methods will learn different vectors  So when you'd use the SWEM based idea  this simple idea which tends to map all of the or most of the words or the unimportant words to a zero embedding  you get a highly concentrated set of embeddings near zero where another approach using a different methodology here  GloVe  yields a very different set of word vectors  So  this is a very detailed plot not to this slide is just to give a sense of things  We have previously talked about a CNN or Convolutional Neural Network based model for text analysis  We've talked about the LSTM model for text  So what I just want to give you  and then I've talked about the SWEM  which is the very simple model  So  I just want to give you a sense of the complexity of these different models to underscore that there are many different ways of doing natural language processing based upon these neural models and depending upon what you choose to use  there's very different levels of complexity  So  you notice that the CNN has about 540 000 parameters  the long short term memory has 1 8 million parameters and then this very simple SWEM model has only 61 000 parameters  When we look at computational speed  we also notice that that complexity of the CNN and LSTM also translates into computational time  So we can just notice the speed is not important what exact task we're doing here  but what the key thing to notice is the relative number of parameters and the relative computational speed  Then on the bottom table  this is  again  not something to spend too much time looking at  but what we're showing is results of the CNN and the LSTM based models  So these are very complicated models on the accuracy of predicting the sentiment and then other metrics across the horizontal  the SNL1  the MultiNL1  WikiQA  Quora  MSRP  Those are different datasets and different natural language processing tasks  I give the reference in the figure so you can go take a look at this if you want  The key thing to notice here is that the SWEM based model  which is a very simple model  oftentimes performs as well  oftentimes better than these very complicated CNN and LSTM models  So  the key thing to take away is that this concept of learning embeddings or vectors for each of the words is a very powerful one  can be used across many different types of models  CNN  LSTM  SWEM and others that we've looked at  Oftentimes  we find fortunately that very simple models like SWEM can do very well on complicated but interesting tasks such as sentiment analysis  So as we consider various problems in natural language processing  we need to pay careful attention to examine the appropriate level of model complexity and to make sure that that model complexity is matched to the problem  Oftentimes  very simple models can perform very well 
omuyzDmNaf4,Natural Language Processing with PyTorch  Like computer vision  natural language processing has undergone a significant transformation with introduction of deep learning  Here  we'll introduce a few of the relevant concepts  In particular  Module 4 introduced the concept of a word embedding  which is a vectorized representation of a word that carry some meaning  Here  we'll have some fun with some pre-trained word embeddings and see how we can use them to find similar words and solve word analogies  We'll also train a very simple neural network model that learns how to use word embeddings to predict the sentiment of some movie reviews  We'll conclude with a brief overview of recurrent neural networks in PyTorch  For the assignment  you'll train a few different types of models on a top classification data set and compare the performance 
TiwvPlTSqss,Word Vectors and Their Interpretation  In previous parts of this class we have talked about the word to vec concept in the context of natural language processing  So the key idea here is that each word in the vocabulary will be mapped to a vector  and these factors are also sometimes called embeddings  So if wi represents the ith word in a vocabulary  what we're going to do is map it to a d dimensional vector  And we're going to do that for every word in our vocabulary  We've talked about this in the context of the long short-term memory for natural language processing  We're now going to extend this concept to what's called the transformer network or a attention-based network  And in so doing the hope will be that we'll gain some further insight into what these word embeddings or word vectors represent  So the key idea of this word to vec mapping  so word to vector  word2vec mapping  is we're going to take our vocabulary  which is composed of here v words  and we're going to map it to a collection of corresponding vectors or codes  So we use the notation c for code  and so if we have v words in our vocabulary  we now have v codes  each one of those codes is a d-dimensional vector  So after doing this  we represent our vocabulary of words by a code book  And again  a code book is a collection of the d-dimensional vectors for each of the words in our vocabulary  So there are many ways to learn these factors  these code vectors  And in fact  we have talked about those different ways of learning in other poor parts of this class  including long short-term memory  And we're going to  again  as I said earlier  we're going to extend this to a new type of model  the transformer  The key idea in all of these methods is that for each word in a given document we should be able to predict the next or surrounding words  So the idea is that the words have meaning  that meaning implies that a given word should indicate that a particular other word might be present with high probability  These word vectors  which we are going to learn  are meant to preserve or to achieve that concept  So what we want to do now is to try to achieve a further understanding of what these vectors mean  This will help motivate the model that we're going to talk about subsequently  So just to simplify the discussion  let's assume that each of these word vectors is ten dimensional just for simplicity  And so in this figure each of those boxes represents ten numbers corresponding to the ten components of this vector  And so the idea that we want to kind of think about is that there are underlying topics associated with each of these ten numbers and our ten dimensional word vector  Each of these topics represent characteristics of words  or what one might be trying to represent with words  And so if a given word is aligned with say the kth topic  then we would expect or what we expect that the corresponding component in our vector will be positive  will be a positive number because it aligns with that word  If the kth topic is not connected to the word  or does not align with the word  then we would expect the value of the kth element to be negative  and we'll see this further as we proceed  And so just notionally  let's look at a particular word here  Paris just as an example  And what I'm doing here is assigning themes or topics with each of the components of that ten dimensional vector  These themes or topics will be learned in the context of our machine learning  The assignment of particular names to these here  sport  politics  history  this is notional  We don't explicitly do that  But it's meant to give some understanding of the underlying concepts associated with this word vector  So if we look at the word Paris  historically Paris has played an important role  It's the capital of France  It has played an important role in politics  So we see that  notionally  here the second component of that vector is positive  which means that Paris is positively aligned with politics  Also Paris  as a consequence of its being the capital city of an important country  France  it has a very important role in history  So historically  Paris is important  So we see that the third component  which corresponds to history  is positive  The next or the fourth component  notionally  corresponds to action or a verb  Well  Paris is a proper noun  It doesn't have much explicitly to do with action  So we see that the action component is negative  Paris is a name  It's a name of a city  So the the next component  which corresponds to names  is positive  And so the idea that we're trying to reflect here is that the underlying components of the word vector have some topical or thematic meaning  We don't explicitly have to know what that meaning is  but it is uncovered through the machine learning  And so the idea that we want to gather from this is that what that word vector is doing is representing  component by component  The component either being positive if it's aligned with the topic  or negative if it's not aligned  is providing the underlying thematic meaning of the word  That's what this word2vec concept is meant to reflect  Now  what we're going to do next is take this idea of some meaning or association of what this word vector is actually doing  and use it to constitute a new form of machine learning for natural language  And this is a natural language processing model 
A-cJBuwLUx8,Relationships Between Word Vectors  So let's try to understand a little bit more about what these word vector means  So again  each component of the word vector corresponds to an underlying topic or theme associated with things that we assign words to  So here what I'm showing are pairs of words which have very similar meaning but also key differences  So if we look at the word king and queen  these are royal positions  They're very similar  except for the king is usually always a male and the queen is a female  So the idea that I'm trying to show here is that the vector associated with the word king and with the word queen are essentially the same everywhere except for one component which I highlight with the ellipse  and notice that for the component which is highlighted by the ellipse  the respective component is positive for king and negative for queen  So what we're supposed to infer from this is that component must represent gender  If we now look at the second example to the right  uncle and aunt  Those two words have essentially the same meaning with respect to a young person often  except for the fact that the uncle is generally a male and the aunt is a female  So the idea to notice is that the word uncle and the word aunt  the vectors associated with them are virtually the same for every component except for one  and in that one component  they are flipped  In the context of uncle  it is positive  in the context of aunt  it is negative  We infer from that  that that component must correspond to gender  and that component is consistently different between uncle and aunt and king and queen  So from this simple example  we get a sense of what the word vectors mean  So this gives us some ideas as to the underlying mechanisms that are captured by these word vectors  Another example is king and kings  and queen and queens  These are the same words except for the fact that one is singular  the other is plural  So the thing I want you to notice is that king and kings  the vectors are essentially the same for every component except for one component  In the context of king  it is positive  and in the context of kings  it is negative  So that component must represent singularity of a word  how singular it is  So in the context of king  it is positive  in the context of kings which is plural  it is not singular so it is negative  This relationship holds for king and kings as well as for queen and queens  I also want you to notice with the arrow that I've highlighted that that component is consistent between king and kings  and it's consistent between queen and queens  but it differs between king and queen  So that component with the arrow must represent gender  So the idea is that if we can learn these word vectors in ways that we will talk about  and if we look at pairs of words which we know are basically the same except for perhaps one area of distinction  we can then look at the word vectors  see what is similar between them  see what is different  and from that we can uncover some underlying meaning associated with these word vectors  So earlier in another part of the course  we showed this relationship  So what we're meaning to represent here is that each word that you see corresponds to an embedding or a word vector here in two-dimensional space  So the idea here is that man is to woman as uncle is to aunt  as king is to queen  So if this relationship that we are talking about here is true  which means that king is to kings as queen is to queens  even though king and queen are different types of words  we would expect a consistency in the relationship between these word embeddings or these word vectors in the D-dimensional space in which they are represented  and here we see on the right  king is to kings as queen is to queens  This is a concept of geometry that we expect to uncover in these word in vectors based upon their underlying meaning  and this relationship gives us some underlying intuition into what these word vectors actually mean 
1eUVC4bI7QE,Inner Products Between Word Vectors  So now we have re-introduced ourselves to this concept of mapping a word to a vector  we have gained some further insight into what this mapping it represents  So now let's move forward and actually start to model sequences of words or sentences or documents  So let's consider a situation where we have N words which are represented as W_1  W_2  through W_N  Those correspond to the N words  for example  in a sentence or a document  Based upon the understanding that we now have  every one of those words will be mapped to a d-dimensional vector or code  C_1 corresponds to the code associated with W_1  C_2 with the code associated with W_2  and C_N with the word W_N  One of the things that might bother you at this point is that this concept of mapping each word to a single vector to one vector is restrictive  in the sense that if you look at a word in the dictionary  it has multiple definitions and the definition that applies corresponds to the context in which the word is used  So this idea of mapping each word to a single vector  which implies a single type of meaning for that word  in the sense that we talked about earlier  seems restrictive because it does not take into account the context of the surrounding words  So what we would like to do now is develop a framework by which we can modify these word vectors in a way that takes into account the meaning implied by the surrounding words  So to do that  we need to introduce this concept of an inner product between two word vectors  This is a new concept that we haven't talked about very much thus far  We're going to try to introduce it in a relatively simple way  So again  we have an N words  W_1 through W_N  Those N words are mapped to N codes C_1  C_2  through C_N  Those mappings are true  In other words  the word W_1 is mapped to the code C_1 independent of all of the surrounding words  So in the sense that we talked about earlier  this is problematic  We're going to extend that  To do that  we need to introduce a little bit of notation  So again  C_1 corresponds to the code associated with word 1  It has little d components  and so in this figure  what we're meaning to represent is the d components of that word vector  C_1 1 is the first component of word of code 1  C_1 2 is the second component of code C_1  and C_1 d is the d_th component of that code  What we're going to do now is introduce the concept of an inner product between two codes  So let's consider codes C_1 and C_2 associated with word 1 and word 2 and we want to quantify the relationship between those two codes  The way that we do this is with something called an inner product  also called a dot product  The reason it's called the dot product is notationally  we see C_1  C_2  So this is an inner product or dot product and the figure that you see at the bottom is meant to depict what that equation is saying and hopefully the picture makes the equation meaningful  So now  what we're going to do is we have two codes  C_1  C_2  Each one of which has d components using the notation that we talked about before  What we're going to do is we're going to take the first component of code 1 and the first component of code 2 and we're going to multiply them together  We're going to take the second component of code 1  the second component of code 2 and we're going to multiply them together  The third component of code 1  third component of code 2  multiply them together  We do that for each of the d components  So what we do is we take code 1  code 2  we take each component one-by-one  d of those components  and we multiply them together and then we take each of those products  those d products  and we sum them together  That's symbol  that Sigma sign means summation  So this process of multiplication and then summing  which is represented by the equation in the middle  is called an inner product  For reasons that we're going to talk about subsequently  What the inner product is going to do is to quantify how similar code 1 is to code 2 through this inner product process  and of course  if we can do this for code 1 and code 2  we can do this for any two codes in our vocabulary  So if words W_1 and W_2 are similar  then we would expect that code C_1 and C_2 to also be similar  Because remember  in the sense that we talked about earlier  these codes  the components of those codes  represent the underlying meaning of the words  If word 1 and word 2 are similar  we would expect that the corresponding codes  C_1 and C_2 are also similar  In the sense that we're going to talk about in a moment  If code C_1 and C_2 are similar  then this dot product C_1  C_2 should be large and positive  in the sense that we'll talk about in a second  If code C_1 and C_2 are different  which means that the corresponding words  W_1 and W_2 are dissimilar  than we would expect that that inner product will be small or negative and we'll see this in a moment  But this idea of the inner product  which quantifies the similarity between words is fundamental to what is going to come subsequently and just to underscore this point  if words W_1 and W_2 are similar  the inner product C_1  C_2 will be positive and we'll see this in a moment  If two words are dissimilar  that inner product  C_1  C_2 will tend to be negative and this will become more clear as we proceed  This concept of inner product  and this concept of using the inner product to quantify similarity of words through their word vectors is fundamental to the neural processing that we're going to do subsequently for natural language processing  So this inner product is something that we're going to be using repeatedly in the discussion that follows  So in the highlighted block  that is the same expression that I showed before  It is a lot to carry around  and so what we're going to do is we're going to introduce a concise notation for the inner product  So the inner product between code C_1 and C_2 visually  henceforth  is going to be represented as you see it on the right  So this is going to be a representation of an inner product  Again  what this means is if I take code 1 and code 2  I take each of the d components of those codes and multiply them one by one and then we sum it together 
VcO3ja6WS2E,Intuition Into Meaning of Inner Products of Word Vectors  So let's continue this discussion of the concept of inner product to try to gain further understanding of what it means and how it can be used  So again  let's look at our 10-dimensional highly simplified representation of a word vector  We're doing it in 10 dimensions here simply because it's easy for us to show visually here  but I want to underscore that in practice when we actually do machine learning on natural language processing  the dimension of these word vectors could be on the order of 256 or five-twelfths  So generally  much larger than 10  We're using 10 here just for visualization  Furthermore  this is back to the example that we considered earlier  the word Paris  and I'm attributing meaning to each of the 10 components in the way that we talked about earlier  where we looked at pairs of words which are similar  We can uncover this meaning  Remember  we talked about gender  we talked about plurality  a word being plural  and so we have these various components  So the word Paris notionally is represented by a d here  a 10-dimensional vector  If Paris is positively associated with a given topic which is represented by each of the d or 10 dimensions  it is positive  and if it's not related  it is negative  Let's use this concept now to try to gain some further insight into what this inner product is doing  So let's look at word i and word j from our vocabulary  So this is the ith word and the jth word from our v-dimensional vocabulary  These are just two arbitrary words  What I'm showing you here in 10 dimensions  this notionally are the word vectors between those associated with those two words  The thing I want you to notice is that almost always for each of the 10 components  if the ith word is positive  the jth word is positive  that component is positive  So for example  if you look at the last component  the 10th component of word i and word j  it is positive in both cases  If you look at the first component of word i and word j  it is negative  So the thing I want you to notice is that these word vectors are similar in the sense that for almost all of the 10 dimensions  the sign  positive or negative  is the same for each of the components  With this understanding that each of the components represents some underlying meaning associated with the words  we would infer that since these word vectors are similar that these two words should have similar meaning  So the thing that I want you to notice is that if you multiply a negative number by a negative number  you get a positive number  Remember that the inner product is taking component by component multiplication  So if we look at  for example  the 10th component of this vector  it's positive in both cases  Positive number multiplied by positive number will give me a positive number  If we look at the first component  it's negative in both cases  Negative number by a negative number will give me a positive number  So the idea is that if the ith and jth words share almost always the same sign for here  for each of the 10 components  when we take those multiplications  they will almost always be positive  and when we sum them up to constitute the final inner product  that inner product will be positive  So we see that if words have similar meaning  which means that they have similar word vectors  then the associated inner product will be positive  The more similar the words are  the more positive that inner product will be  In contrast  if we look at these two words  word i and word j  and we look at  for example  the 10th component  the rightmost component  word i is negative  word j is positive  So we see that those are inconsistent with each other  If we look at the first component  word i is positive  word j is negative  So since the sign of each of the components of word i and word j is almost always opposite  the sign is opposite  when we take the product  that product will be negative  Most of the components of that inner product will be negative  When we sum them altogether  we would expect that we will get a negative inner product  A negative inner product implies that the ith and the jth words are dissimilar  they do not have the same meaning  So the key concept that we're trying to get through this inner product is that each of the components of the d-dimensional vectors associated with words have underlying meaning  If two words are dissimilar  they will in general have different signs on the respective d-components  and the inner product will be negative  If two words are aligned  the signs of each of the d-components will mostly align  and the inner product will be positive  So therefore  this inner product through the sign of the inner product  positive being similar  negative being dissimilar  tells us information about any two words in a vocabulary  Now  for the reasons that will become clear in a moment  it is not very convenient to have to work with inner products that are sometimes negative and sometimes positive  So this idea of a negative inner product and positive inner product tells us a lot about the relationships of the words  However  when we actually do the machine learning  it will be more convenient to only work with positive numbers  So to do that  we're going to remind ourselves of the exponential function  So here the x-axis represents the input and the vertical represents the output of a function  Here  the function is the exponential function  exp of x  Notice that when x is equal to 0  the x of 0 is 1  The most important thing to notice from this representation of the exponential function is that for every input  for any value of x  the exponential function is positive  However  there are other functions we could choose that would always be positive  The reason that we use the exponential function is that the more positive x is  the larger the exponential function is  The more negative x is  the smaller the exponential function is  So the exponential function is called a monotonically increasing function of the input x  Remember that the inner product  positive or large  represents words that are aligned  Negative represents words that are not aligned or are different  We want to preserve this idea of scale between positive and negative  So this exponential function is a way of doing it  The output of the exponential function is always larger if the input is larger  So if you give me two values of x  one negative and one positive  the exponential function of the positive input will always be larger than the exponential function of the negative input  So the exponential function preserves the meaning of positive and negative input  but it has an output  The output of the exponential function is always positive  So this is a convenient function for reasons that we will see in a moment  So as I said before  the exponential function for all x is always positive  Then as I was alluding to  the exponential function is a monotonically increasing function of the input x  Here  in the context of what we're going to do  the x is going to correspond to the inner product as we'll see in a moment  So if we then go back and wrap this up on the top left  is that inner product  I'm now spelling it out in all of its detail  But remember to the right is our notation of an inner product  The inner product is represented as a dot product  Again  the reason we use that notation is because the inner product is between c_1 and c_2 is just notationally c_1 c_2  If c_1 and c_2 is large  then the exponential function  because it's a monotonically increasing function  will also be large  The other thing is for all inputs  for any inner product  c_1  c_2  it is always positive  But it preserves this monotonically increasing nature which will be important because it therefore preserves this concept of similarity between words  If two words are similar  then the exponentiation of the inner product between those two words will be large  If the inner product between two words  c_1 and c_2 is small or negative  the exponential function will be small  So the idea through this exponentiation is we preserve the meaning of words as represented by their inner products  We choose to use the exponential function because it's always positive  As we will discuss in a moment  it's very convenient for the model that we're going to develop 
jK5Mh6d9HiE,Introduction of Attention MechanismÃ‚  So let's now try to put this all together  So what we would like to do is assume as we have before  that we have N words in let's say a sentence of length N or a document of length N  and those N words are represented as c_1  c_2  c_k  and then c_N  We now have this idea that the inner product is a measure of similarity between two word vectors  If the inner product is positive  those word vectors are similar  if the inner product is negative  those words are dissimilar  So what we're going to do is we're going to quantify how similar each of the N words in our sequence is to the kth word  and so the kth word in our vocabulary  So to do that  based upon our understanding that the inner product is a measure of similarity  we're going to take the inner product between c_k and c_1  c_k and c_2  c_k and c_i  and c_k and c_N for each of the N words in our vocabulary  So the first step is take the kth word and perform an inner product with each of the N words  and so we denote that as the dot products above each of those symbols  Now  those inner products could be positive and negative  If two words are similar  they're positive  two words are dissimilar  they're negative  As we'll see  it's somewhat inconvenient to have to deal with both positive and negative numbers  So we now know that the exponential function always will have a positive output  and it will preserve the meaning of the inner product from the standpoint that the exponential function is a monotonically increasing function  So what we're going to do is we take the inner product of the kth word with each of the N words  we then exponentiate those inner products so that we make them positive  and then the last step  which is above the top arrow  is we're going to get a relative representation for the strength of the inner product  So r_k arrow i represents the relationship of the kth word to the ith word  This represents the relative similarity of word k to word i  What we mean by when I say relative  it means how similar is the kth word to the ith word relative to all of the other N words in this N dimensional sequence  So the way that we calculate this relative relationship of the kth word to the ith word  is that we take the exponentiated inner product between c_k and c_i  and then we divide that by the sum of the exponentiation of c_k to all of the other words  So that is shown at the first equation and then the bottom equation with the Sigma is just a simplified representation  Now  the thing that I would like you to notice is that r_k arrow i  which is the relationship or how similar the relative relationship or relative similarity of the kth word to the ith word is always positive because the exponentiation is always positive  It's always a number between zero and one  So this is why we call it a relative relationship between word k and all of the other words  So now if we summarize what we're doing here  So the first step at the bottom is we take an inner product with the kth word with all N words  So we're quantifying how similar the kth word is to all N words with the similarity we now compute via the inner product  because if we recognize that it is a measure of similarity  The next step is we exponentiate  We do this because we want to make it positive and because of the fact that the exponential function is monotonically increasing  we preserve order of strength of inner product  Then finally  we normalize  So the final result of this is we're able to quantify the relative degree to which the kth word is related to each of the n words in that n dimensional sequence  I again remind you that the r  this relational that I call r here is relation  the relation of word k to any of the other words  that r is always positive because of the exponentiation at the bottom  Secondly  if we sum each of those rs  they sum to one  So the more related word k is to any of the words in that sequence  the more it's related  the stronger that r will be  if it's not related  it will be small and tend towards zero  So this concept of inner product exponentiation and normalization  is a key construct that we're going to be using repeatedly in the natural language processing framework that we're going to be talking about as we proceed  So this thing that I show on the left  this process of inner product exponentiation  normalization  is a lot to carry around symbolically and what we're going to do as we proceed  we're just going to summarize that by what you see at the right  again  inner products at the bottom and then we're going to map that to relative relationship of word k to all of the other n words  The representation to the right is meant to imply everything that you see at the left  So the interpretation that we have is what we've discussed  The r_k arrow i quantifies the relationship between word k and word i  The larger r is  the more they're related or the more they're correlated  the smaller r is  the less related they are  There is an underlying grammatical relationship between words in a vocabulary  So we are seeing that this r is a mathematical way of quantifying the relationship between words and those relationships between words are characteristic of language itself  So in the context of machine learning  we are going to learn or infer these word vectors  Those word vectors will be learned to preserve the relationship between the words  So now we're starting to see how natural language processing in the context of machine learning  and this concept of r which is a relationship between words  there are underlying relationships between words in a language  Those relationships are going to dictate how words are going to unfold in a document  So the machine  is going to use these inner products and these word vectors  as an intermediary way of representing these relationships between words and these word vectors are going to be learned  The way that we're going to do the learning  is with gradient descent  we've talked about that in other parts of the course  When we have large data  the number of word vectors we have to learn can be large  so we will use stochastic gradient descent  The final thing that I want to point out is that  when we take the inner product between the kth word and the nth word or we take the inner product between the kth word in each of the n words in a sequence  those inner products can be done in parallel  What that means is that  they can all be done simultaneously because they are not related to each other  they can be done independently  This is different from the long short-term memory recurrent neural network that we talked about elsewhere in this course  The recurrent neural network operates sequentially  one step after the other  So therefore  it is not well suited for parallel computing  because each of the steps has to be done sequentially  One of the really nice aspects of this framework that we're talking about here  is that the underlying inner product between word k and all of the other n words in our sequence  can all be performed in parallel  That is particularly important because modern computing platforms  in particular the graphical processing unit or GPU  is particularly well-suited for parallel computation  So this framework of taking inner products between here  the kth word and all n other words in our sequence  is particularly attractive from a computational perspective  There's a significant opportunity with GPUs to markedly increase the computational efficiency by leveraging this concept of parallelism 
rFqsuq66Gtc,Queries  Keys  and Values of Attention Network  Okay  so let's think about what we have talked about  We have a sequence of words W1 through Wn  We map each one of those words to an associated code C1  C2 through CN  We want to quantify how similar a Kth word  CK  here represented by a code CK  is to each of the N-words in that sequence  The way that we do that is through inner product  If two words are similar  the inner product tends to be positive  If two words are dissimilar  the inner product tends to be negative  We then quantify this relative relationship between word K and each of the N-words by exponentiating and normalizing  And we get these r values  rK arrow i represents how similar or the relative relationship between word K and word i  What we want to do now is to revise or refine the word vector for CK  So the thing that I want you to recall is that the mapping from the Kth word to the Kth code CK is done independent of the context in which that Kth word is used  So now this Kth word CK could be the Kth word in our sequence of length N  And what we would like to do is to modify the word vector for word K or the code for word K to take into account the context of the surrounding words  The way that we're going to do this is we now know that rK1  rK arrow 1 represents the relative relationship of word K to word 1  rK2 represents the relative relationship of K word K to word 2  and rKN represents the relative relationship of word K to word N  Each of those rs is positive and they sum to 1  We're going to use this idea to try to manifest a refined code for the Kth word that takes into account the context in which it is placed  Here the context is in terms of the N word C1 through CN  So what we're going to do is we're going to take rK1  rK arrow 1 and we're going to multiply it by code C1  We're going to take rK2 and multiply it by code C2  And then we're going to take rKN and multiply it by code CN  And then we're going to add these all together  and we're going to get a new representation for the Kth word C squiggly K  that squiggly is called a tilde  That that squiggly line above the C is called a tilde  This is C tilde K  The thing that I want you to notice here is we have a sequence of N-words  which are represented by the code C1 through CN  We have the Kth word  which is represented by the code CK  That code is independent that CK is not informed by the context in which that word appears  Here  that context is represented by the N word C1 through CN  We compute the relative relationship of word K to all of the N-words  which is the rK arrow i  rKi  which is the relationship of word K to word i  for i 1 to N  We then multiply those relative degrees by which the Kth word is related to each of the words  We multiply them by the corresponding code  So rK1 is multiplied by C1  rK2 by C2  rKN by CN  And so then we add them up  we sum them and then we get a new code CK  The thing that I want you to notice here  is that if CK is highly related to a particular word in our sequence of N  then the corresponding relative relationship quantified through r will be large  because it's highly related  That large r is then multiplied by the associated code  and therefore that highly related word will contribute significantly to the revised version of the code for word K  C tilde K  So by doing this  we are revising or refining the code for word K  But we're doing it in a way that takes into account its contextual relative relationship to each of the N-words in our N-dimensional sequence of words  And so this is a way in which we can revise or refine the code vector associated with any word taking into account the degree to which it is related to the other N-words in this case in our sequence  The thing that I want you to kind of think about is these relational numbers rK1  rK arrow 1  tells us how much attention whenever we do this mapping to C tilde K  how much attention we should pay to code C1? rK2 represents how much attention we should pay to code C2  rKN quantifies how much attention we should pay to code CN  How much attention we should pay when we get the new mapping C tilde K? And so this concept whereby the relative numbers  the rKis  rK arrow i  which is the relative relationship between word K and word i  tells us how much attention we should pay to word i or code i when we constitute this new representation of the Kth word  If the relationship is high  if r is large  then we should pay high attention to that word  If r is small  implying that that word K is not highly related to that word  then we should pay little attention  So this concept  this concept that we're looking here  is called attention  For reasons that I've hopefully articulated  this representation is called attention in neural network processing for natural language processing  It tells us how much attention as quantified implicit three through the inner product should be paid between word K and word i for constituting the new representation of the word CK  Which is taking into account the contextual relationship of surrounding words  And so what we have manifested here through this process is a mapping or a transformation of the original code vector CK  which is a mapping of a vector for every word here  the Kth word  independent of the surrounding words  Through this attention mechanism  we now get a C tilde K  which takes into account the surrounding context of other words  And I talked about this concept of attention when we do this mapping from CK to CK tilde  The code CK attends to those N codes to which it is most correlated as implied through the underlying inner product  And so I now want to introduce a little bit of notation  because this attention network is going to be fundamental to what we are going to do with these attention based networks for natural language processing  So the code CK that we are going to examine all of the other N-words with respect to is called a query  And so what we're going to do is we're going to query how related  the query CK  to each of the N-words  The N-words  the vectors associated with those N-words are called keys  And so the degree to which the query CK is related to the keys through the inner product will quantify how much attention should be put on the corresponding vector when we constitute the updated version of our code C tilde K  And then finally  the values are what the relative weightings rK arrow i for the relationship between word K and word i  Those numbers  those relational numbers  which are always positive and always sum to 1  they're multiplied by what we call values which here are the original code vectors in that N-dimensional sequence  And so we have queries  we have keys  and we have values  And so the way this works is we have a query  The query then is computed through the inner product  through each of the N-key  The relative weighting which is manifested through that process is then multiplied by the values  And then finally for the query  we we get a new representation C tilde K  which takes into account the context in which that Kth word is manifested  And that context is constituted in terms of the sequence of N-words here represented by the N codes C1  C2 through CN 
ubM-4lZLnJU,Self-Attention and Positional Encodings  Okay  so summarizing this we take the kth word  We map it with the kth word  which is represented by a code ck  which is independent of the context in which it is manifested  We map it through this attention mechanism to a new code c tilde k  which takes into account the relationship of that word ck  to all of the n words in the sequence  Just a little bit of notation  the r's  rk1  rk2  rkn  recall those are always positive and they sum to 1  that process of them always being positive and summing to 1  mathematically means that the final code c tilde k is a convex combination of the values  So the values remember were c1 through cN  when we weight them by those n relational numbers rk1 through rkn  this is called a convex combination of those codes  And so what we were talking about earlier  was we took an arbitrary word  the kth word  represented by a code ck  And through this inner product and through this attention mechanism  we were able to map it to a new code c tilde k  which took into account the context of the surrounding words  Now  we can do this for each of the n words in our sequence  So now  let's go back to the beginning  We have a sequence of n words  w1 through wn n words  We map each of those words to a vector c1  c2  through cN  those c's are what we call codes  The problem with this is  that mapping to codes is independent of the surrounding words  Now  what we can do is  we can take the first code c1  and it could play the role of our query  it can play the role of the ck that we were talking about before  And so now  the arrow  what this is meant to reflect  is that we could take the original sequence of n codes  and map it into a new sequence of code c1 tilde  c2 tilde  and cN tilde  those codes take into account the context of all other words in the sequence  So c1 tilde  now is a representation of the first word that takes into account the context associated with all other words  C2 tilde corresponds to a mapping or a vector associated with the second word  that takes into account the context of all surrounding words  And then finally  cN tilde is a code for representing the nth word that again  takes into account the context of all surrounding words  And so the thing that we should recognize through this attention mechanism that we've talked about earlier  is that it is a way in which we can map each of the words in our sequence to a vector  That vector here  c1 tilde through cN tilde  takes into account the original meaning of the original word vector  but then modifies it by the context in which it's employed  And so therefore  the vector associated with any word through this process  will be modified by the context in which it is placed and the same word in different contexts  In other words  with different surrounding words will be mapped to a different factor  Now  the one thing that I want you to notice is that this process is independent of the word order  and what this means is that if I take the original n words  w1 through wn  and I scramble the order or the word we use mathematically is permute  we permute the order of the words  So the first word becomes the nth word  the second word becomes the first word  etc  we permute them  This process of attention will give us the same word mapping after this attention mechanism  independent of the way in which we reorder the n words  So if you think about that  independent of the order in which those n words are manifested  the associated word vectors will be the same because this attention mechanism that we talked about earlier  pays no attention to the order of the words  So if we think about this  this is a problem because we know that the order of words matters  So the sequence United States  the next word that we would expect is different than if we had United and States  and they were not next to each other  So the word order matters  and so that word order should come into the way in which we do these word vectors  if indeed we want to capture contextual meaning  And so what we have done this far  is very effective in the sense that we have now constituted word vectors that take it to account the context of the surrounding words  The problem is  at this point we have not accounted for word order  and so with that understanding we need to make a modification or an augmentation to what we have done here  so that the model can take into account word order  and so that limitation we're going to address in a moment  But before we do that  let us try to summarize in a relatively simple diagram what we have done  So starting at the bottom  we have an input sequence of words  So this is n words  w1 through wn  first step is we embed each of those words to a vector  So every word is mapped to a vector  the problem with that is it does not take into account the relationship of that word to its surrounding words  In other words  it doesn't take into account context  So then we constitute the keys K  the values V  and the queries Q  and we have this thing we call an attention network  And then we have an output sequence  that output sequence takes into account the context in which every word is manifested  that output sequence is a sequence of vectors where we have a single vector for every word  and that vector accounts for the meaning of the surrounding words  But as we have talked about this attention network  if the input sequence is permuted or the order of the words is flipped around  the output sequence will be the same The vectors will be the same  they will be correspondingly flipped around  And so what this implies is that  the word vectors don't really pay attention to the order of the words  and we know in language  the order of the words have tremendous meaning  And so we need to modify this construct to take into account the order of the words  and so we're going to make several modifications to this setup that have been found to improve performance  So the first one is called a skip connection  So in the original setup that we have here  when you go from bottom to top  when we do the attention network and we get the output sequence  we have lost the original word embeddings at the bottom  And so it's been found convenient  you notice what we call a skip connection  So we have the input sequence  the input sequence is mapped to a sequence of word embeddings or word vectors  and then those correspond to the keys  the values in the query  So the word embeddings play the roles of the keys  the values  and the queries  So that is what those arrows are meant to depict  and then we go into the attention network  Now  the problem with that is we have lost the original word embeddings after we've done the attention  So the skip connection what it does is  it takes the original word embeddings  and it skips it above the attention network  And then what we do is  we add the original word embeddings to the output of the attention network  and then after we're done with that process  we normalize them with respect to each other  So this is called a skip connection  this type of network has been found to improve performance of these types of models  So the next thing though is to try to augment this setup  so that we take into account the position of the words  and the way that we do this is with what we call positional embeddings  So these are a few modifications to the basic construct to try to improve performance  and the thing that we're going to try to achieve here is to account for the order of the words  And so the way that we're going to do this is with positional embeddings  which we'll talk about in a second  but the thing I want you to notice is the way that this is going to be done  is we take the input sequence which is a sequence of words  Each of the words is mapped to a vector through this word embedding  and then those word embeddings are going to be added to positional embeddings  so that the P with the circle  our positional embeddings  which we'll talk about in a second  We add these together  and then the rest of the network is unchanged  And so the question now is  how do we constitute these positional embeddings  which reflect information about the order in which the words are manifested? So the equations are at the top position  our P-O-S  P-O-S is meant to reflect position  and it means the position of the word is the order of the word  Remember that we have a d dimensional word embedding  and so now what we're going to do is we're going to constitute d dimensional positional embeddings  And the sine and cosine correspond to these sinusoidal waves that many of us are familiar with  and that we'll talk about in a second  And so  P-E represents positional embedding  and what we're going to do is  through this positional embedding  we're going to constitute a new d dimensional word vector  which will reflect the position of the word in the sequence  So depending upon the position of the word  it will get a different vector  So let's try to understand what this is doing  Now  remember that we have d components of our embedding  and so what we're going to do is  we're going to constitute a d dimensional vector for every word which is connected to its position in the sequence of n words  And so remember that we have embedding dimension 1  embedding dimension 2  embedding dimension 3  all the way to embedding dimension d  The thing that I want you to notice  is that embedding dimension 1 will have an associated sine wave that has a relatively low frequency  so the undulations are large  Embedding 2 has also a sine wave  but a more quickly varying sine wave  Embedding dimension 3  again  has an associated sine wave but faster  And so the thing I want you to notice  is that as we move from embedding dimension 1  to embedding dimension 2  to embedding dimension 3  and downward to embedding dimension d  In each case  there is a sine wave associated with them  and the frequency at which that sine wave oscillates  is connected to the dimension  Dimension 1 oscillates slowly  dimension 2 faster  dimension 3 faster  and then what we're going to do is  the value of the positional embedding will depend upon the position of the word  And so along the horizontal axis is the position of the word in the sequence  And so the idea is that if you look at a word with a given position in our sequence  we're going to associate for each of the d dimensions of that embedding  we're going to associate a number  The number is going to be connected to the associated sine wave at that dimension  And so if you think about this  the position for the d dimensional vector that is going to be associated with each of the words in our sequence  will depend upon the order of that word in the sequence  or the position of that word in the sequence  And so what we have done here through this construct  is to take each of the n words in our sequence  and map each of those n words to a d dimensional vector  Remember that the original word embeddings that we were talking about  had some underlying meaning which was connected to language  Here  this is not really connected to language in any other way  other than that it reflects the position of the word in the sequence  And so the position of the word is meant to be reflected from left Tow write along the horizontal and is meant to reflect the first word  second word  third word  fourth word  fifth word  And then depending upon where the word is  we will pick off the corresponding component of the sine wave at each of the D components  And so the key thing to notice through this this mechanism is that each of the n words in our sequence is going to be encoded with a d-dimensional vector which reflects its position in the sequence  And in this way  we actually encode positional information through these positional embeddings or PE  Now this use of the sine wave and we then the use of the frequency of the sine wave connected to the dimension of the embedding  This is not the only way that this can be done  The key thing that we're trying to achieve here is a mechanism by which each of the words in our sequence can be mapped to a d-dimensional vector that reflects its position in the sequence  This use of a sequence of d sine waves of different frequencies is one way to do it  It is by no means the only way to do it  However  this is the standard way of doing it in natural language processing  This is proven to be an effective framework  Well  let's focus on the bottom and think about these positional embeddings  We have an input sequence which is a sequence of n words W1  W2  through WN  We map each of those words to a vector or an embedding  Those embeddings do not take into account context  The other thing is we have not really taken into account the order  What we're going to now do with these positional embeddings p in the circle  What we now have is for each of the n words in our sequence  we have an associated d-dimensional vector which reflects the position of that word in the sequence of n words  We now add the original word embedding to the positional embeddings  So now through that process of word embedding plus positional embedding we have taken the meaning of the word which is the word embeddings at the bottom  We have the position of the words which are the positional embeddings  We add them  So now we have meaning plus position  However  the meaning does not take into account context  It doesn't take into account the context of the surrounding words  So now we introduce the attention network  The intention network now is going to introduce new word embedding vectors which take into account the the connection of a given word to all of its surrounding words  Through the positional embedding  words that are nearby positionally  words that are nearby positionally will have similar positional embeddings  And so therefore their inner products will be high  Words that are far apart positionally will have inner products that will have word positional embeddings which are different  And consequently their inner products will be negative  And therefore those words will not be related  And so through these positional embeddings and through the meaning of word embeddings which are the word embeddings at the bottom  We are taking to account meaning and position  And then in the intention network  we now put those two together  The skip connection is something that has been found convenient to do in practice  And then finally the top  the final modification of this framework is to take a simple feed-forward neural network  which is at the top  And so what we do is we take the intention network  We add and normalize  And then each of the n vectors that are now in our modified sequence  c  c bar i  represents the ith such component in our sequence is fed through a simple neural network  which we've talked about elsewhere in the class  And then we again do the skip connection  add  and normalize  And so this hyperbolic ta and h we recall from elsewhere in the class is called the hyperbolic tangent  And its functional form is depicted  So this model is called the sequence encoder  In the next lesson  we're going to walk through each of these components  This sequence encoder is fundamental to everything that we're going to do with this type of model for processing of natural language processing  So it's important that we think through the meaning of the components of this model  So in the next lesson  we're going to walk through each of the components one by one  think about conceptually what they mean and why this type of model might be effective for natural language processing 
fMJl01wbmWE,Attention-Based Sequence Encoder  Okay  So in the last lesson  we introduced this new framework for modeling sequences of words  This framework is fundamental to what's called the Transformer Network  which is a modern way of doing neural network-based modeling of sequences of words  Very important that we understand each of its components because this is going to come up again and again  So let's walk through each of these components and review them and think about what they mean  So the first step that we're going to start at the bottom and work our way up to the top  So the first idea is that we're going to take a sequence of words  W1 W2  Wn  which correspond to the n words in a sequence  We're going to map each of those words to a vector  a d-dimensional vector  those are called word embeddings  And the way that we talked about at the very beginning  those word embeddings reflect meaning  meaning of the words  The components  those D components of the words  reflect meaning of the associated word  A very important concept  However  that word embedding in its simplest form does not take into account the contextual information of the surrounding words  The other thing is it does not take into account the order of the words  So the next step is this positional embedding concept  So now we're going to introduce new d-dimensional vectors associated with every word  However  these d-dimensional vectors are not connected to the meaning of the word  they're connected to the position of the word  And so depending upon the position of the word  each word will have a different positionally dependent d-dimensional vector  And now we're going to take each of the n vectors associated with the n words and each of the n positional vectors  each of which is d-dimensional  and we're going to add it to the word embeddings at the bottom  And so now  after this word embedding plus positional embedding  we have words that characterize the meaning of the word  We have word vectors which account for the meaning of the word as well as the position of the word  Now  if you look at this  this idea of word embedding plus positional embedding  it may be kind of counterintuitive that you can just do something as simple as this  something as just adding them together  And indeed it is not obvious that this is a good idea  But what we find in practice is that this works quite effectively  So at this point  it's not obvious that this would be a good thing to do  However  it is a relatively simple thing to do  And we always should favor simple models wherever we can  So what we're going to do is with this simple word embedding plus positional embedding  we hope that we can get a good natural language processing engine  And what is found in practice is that you can  So this is a relatively simple  Now  after we take the word embedding plus positional embedding  we've taken to into account the meaning of the word as well as the position of the word  However  those word embeddings are done out of context  They do not take into account the context of the words  So now each of those word vectors plays the role of the keys k  the values v  and the queries q  So each of those word embeddings now plays the role of k  v  and q  And they go into an attention network that we talked about earlier  The attention network takes into account the context of the words  So now we have the meaning of the word through the word embedding  We have position through the positional embedding  And now we have context through the attention network  And then it has been found convenient to not lose the original embedding vectors  And so we have what's called a skip connection  What that means is it skips around the attention network and then we add it to the output of the attention network  This has been found  these skip connections have been found to be effective  We then finally introduce a feed-forward neural network through which those vectors are sent  This has attractive features  You might ask  why are we doing this with a neural network? There are two reasons we do this  Through the process of the neural network  we provide  if you will  regularization or structure on this network  And this is manifested two ways  One way is somewhat mathematical  which I'll just note for the mathematically inclined  what it does is it restricts the output of the neural network to be constrained to the subspace of the vectors associated with the neural network  This is a mathematical concept which if you're not familiar with subspaces  that's okay  But the key thing is it tends to improve the performance  The other thing is through the hyperbolic tan  the tnh function  it restricts the output of that neural network to be between -1 and 1  And so this constraint on the values of the output of the neural network and therefore of this sequence encoder has been found to improve performance  Now  this process of attention neural network  So we take the word embedding  we then do the positional embedding  We add that together  And then those vectors  the sequence of n vectors  is then sent into the intention network  and then subsequently into the neural network with the skip connections  If we could do this one time  there's no reason why we can't do it k times  And so we can repeat this process k times  So k could be equal to 3  In practice  k is often equal to 6  And so this process of attention neural network with skip connections in between  this can be done repeatedly  And elsewhere in the class  we talked about deep neural networks  And it's been found in practice that the introduction of these deep networks yields good performance  And so this is our final deep sequence encoder  This is the key module that we will be using in this form of what's called a transformer network for the modeling of sequences of words  Now  once we have this sequence encoder  there are many things we can do with it  One thing that we might want to do is to take a sequence of n words and quantify the sentiment of those words  Are the n words in our sequence  in our sentence  or in our paragraph  are they reflecting something positive? Perhaps a positive review of a movie? Or are they reflecting something negative  for example a negative review of a restaurant? So sentiment analysis is a very common problem  Here we're going to say that the sentiment is binary  it is positive or negative  And so the equation that you see on the middle right is the sigmoid or logistic function that we have seen elsewhere in this class  And so one way that you might use this sequence encoder is you take the input sequence  you do the word embedding  you do the positional embedding  you do this attention plus neural network  you do this K times  After you're done  you have a sequence of n vectors or n codes which are reflected at the top  One very simple thing that you might do is just take the average of those codes  And that's what the equation at the top is meant to reflect  And we take the average of the sequence of codes and we represent that as V  Remember  this is a sequence of n codes  so the average is 1 over n times the sum of the codes  That vector V is a vector which is an encoding or representation of the original sequence of n words  We now introduce a new parameter  which is the vector e  And remember  the dot product V dot e is an inner product between the final average code V and the vector e  And then we can quantify the sentiment in this way  And now the vector e is an additional set of parameters that we need to learn in the context in this case of sentiment analysis  Another example that we have talked about elsewhere in the class is the idea of predicting the next word  So you have a sequence of n words  w1 through wn  Now what we would like to do is to predict the next word  w n plus 1  the n plus 1 word in the sequence  And so the question  is what is the probability that the next word  the n plus 1 word  corresponds to the kth word in our vocabulary? And so one way that we can do this is  looking at the left we have the sequence of n words that are mapped to word embeddings  Then we have the positional embeddings add  Then we'd go through this attention network plus neural network k times  And then again at the top we have a sequence of n vectors  We take the average of those  which we call V  And then on the middle right  we have this equation  We've talked about this elsewhere in the class  This is called the softmax  And  e k which is an additional vector  which is associated with the kth word  And so now we have a new set of parameters that we have to learn  which are the set of parameters e1  e2  through e v for a vocabulary of size v  And then through this softmax construct  we can quantify the probability of the next word  Through this sentiment analysis  and the predict the next word setup  are very simple problems that we might want to solve with this what we call transformer encoding network  However  this framework through this transformer encoding network is much more powerful than these simple examples  Which is now going to lead us into the next part of the class  which is the next lesson  which is going to address the key application of these transformer networks  Which is not simply predicting the next word but predicting the next sequence of words 
cNsHV9rsCzE,Coupling the Sequence Encoder and DecoderÃ‚  So in the last lesson  we showed how we can use this this sequence encoder  this transformer  to encode a sequence of words and then use it to predict the next word  But a far more ambitious task is not simply to predict the next word  but to predict the next sequence of words  And so there are several example problems that we might think about this might be used for  So imagine that the original sequence of N words corresponds to the N words in a sentence in English  And what we would like to do is to translate that to French  And so in that context  what we would like to do is to encode the sequence of N words in English and not predict one word in French  but to predict the entire sequence of words in French  which do the translation  So the applications that we talked about earlier  which just predict the next word  are inappropriate  Another example might be  given a large block of text  given a large sequence of words w1 through wN  where here the sequence N is very large  And imagine  what we would like to do is to provide a concise summary of the meaning of that original sequence of N words  So we'd like to give a summary of a set of words much less than N  Here  again  we have a sequence of words in  we have a sequence of words out  So we want to develop a methodology whereby we cannot simply predict the next word or predict one word  we want to predict many words  And so  the way that we do this is with what you see on the right  And so before we focus on the right  let's just remind ourselves of what we have on the left  So on the left is our our input sequence encoder  this is what we have talked about thus far  Let's just remind ourselves what we're doing here  We have a sequence of words w1 through wN  Those words  for example  could be English words  What we would like to do is to convert them to the same meaning  but in a different language  say French  And so  the first step is to encode the original input sequence  We take the original sequence of N words  say in English  We map them to word embeddings in the way we've talked about  Those word embeddings take into account the meaning of the words  but they do not place that meaning in the context of all other words  They also do not take into account the position of the words  So now we introduce these positional embeddings  The positional embeddings  for every word  we also have a d dimensional vector  but that d dimensional vector is connected not to the meaning of the word  but to the position of the word  We take those positional embeddings  we take the original word embeddings  we simply add them  We take the first word embedding at the bottom  add it to the first positional embedding  The second word embedding  add it to the second positional embedding  We do that one by one for each of the N words in our sequence  We now have taken into account meaning and position  but that meaning of the word does not take into account context  We then go into our attention Network  The attention network is a framework by which we can now get word embedding vectors or word codes that take into account not simply the meaning of the word  but the meaning in the context of all surrounding words  We have the attention network and a neural network  We do that K times in the context of a deep network  a deep encoding network  And then at the top  we have a sequence of vectors which take into account the meaning of the word  the position of the word  the context of the word  So now with that  let's say we now have encoded an input sequence of length N in English  Our objective now is to convert it into a sequence of words in French  And of course  we can do this in French  German  Spanish  any language you want  So the way we're going to do that encoding is not to predict just one word  we're going to predict the entire sequence  We're going to do that with something that you see at the right  Now  I'm going to go through this step by step  But the thing I want you to notice about the thing on the right is it looks a lot like the thing on the left that we have spent a lot of time on  So now  what we're going to do is we're going to go step-by-step through what you see at the right and try to understand what we're doing  Now  at the bottom of what you see at the right is going to be the words that have been predicted thus far by the model  And so the idea is that we have N words input on the left  These are our words  let's say in English  Now we want to produce French  So now what we're going to do is when we produce the French on the right  we're going to produce it one word after another  So imagine that we have predicted M words thus far  So we now have M words which have been predicted thus far in our translation from English to French  At the bottom on the right are the M French words that have been predicted thus far  So on the left  the input is the original words  On the right  the input are the words that have been predicted  for example  in the translation thus far  Now  if in the same context as on the left  remember we introduced repetition K times to constitute a deep encoding network  Well  there's no reason why we can't do the same thing on the right  So on the right now  we're going to do this J times  J could be a number as 1  2  5  6  a number that we choose  But the point is we repeat this repeat this process J times  and so that we're going to have K repetitions on the left  J repetitions on the right  And this is called a deep network  and the context  in the same sense that we've talked about deep technology in other parts of this class  The first thing I just want to underscore is that the input on the right  the input is at the bottom  are the words that have been predicted thus far  And the way that we do this is the most recently predicted word is to the left  The second most recently predicted word is the second word  and then to the right  And so  the way it's done is the leftmost word is the most recently predicted word  And the furthest to the right is a word that was predicted a while ago  And so the idea is that if we're going from English to French  on the left  we have a sequence of N words in English  On the right  we are decoding in French  At the bottom  on the right are the French words that we have predicted thus far  where the leftmost word is the most recently predicted French word  This is the second most recently predicted French word  etc  And so the idea is every time that we predict a new word  that input sequence on the bottom shifts to the right by one position  and then the new word goes in the leftmost position  So the input on the bottom right is always shifting to the right as we predict new words  I just want to briefly note that this model  the left hand side is called an encoder  encoder  What we're doing is we have an input sequence of N words  we are encoding them through the leftmost  and then what we want to do is take that encoding and then decode to a sequence of  for example  French words  So the right hand side is called a decoder  left is encoder  right is a decoder  This is sometimes called an autoencoder  this kind of construct  okay  So let's now walk through what we're doing  At the bottom  as I've said before  is the output word sequence that we have decoded thus far  where the most recent word is to the left  First step is we do the word embedding  We take every one of those words that we have decoded thus far and we map it to a word embedding  If the translation is from English to French  those words are in French  those word embeddings are associated with French words  So English words  French words  Spanish words  Chinese words  they will all have their own word embedding codebook  And so here  if we're doing English to French  the input sequence to our decoder are the French words decoded thus far  They are embedded with our French embedding codebook  Now  those embeddings in our d dimensional vectors in the sense that we've talked about before  they capture meaning of the word  However  they do not take into account the context of the surrounding words that we've decoded thus far  and they do not take into account the position of the words  So the next step is we introduce these positional embeddings  These are also d dimensional vectors  but the vectors are now not connected to the meaning of the word  but are connected to the position of the word  We take the word embedding at the bottom  add it to the positional embedding  Now we have the meaning of the word and the order of the word taken into account  So through this word embedding and positional embedding  we have meaning of word  we have position of word  but we don't have context of word  So based upon the tools that we now have at our disposal  we know that this attention network is good at modifying those vectors such that it takes into account the context of the surrounding words  And that context is manifested now through the positional embeddings and through the original word embeddings two ways  by the position of the words and the meaning of words are going to be reflected through the inner products  which are at the heart of the attention network  So now  we run our decoded sequence thus far through the attention network  We do the skip connection for reasons that we have talked about  So now  we have word embeddings that take into account the meaning of the word  position of a word  and context of the word  Now  what we would like to do now is decode  which takes into account the encoder that we started with  So remember  we have a sequence of N words in English  We have encoded that into a sequence of N vectors  That was the left-hand part of the original model  We want to account for that in our decoder  The way that we're going to do that is that the output  the N vectors that we got at the output of our encoder are going to be the keys and the values of an attention network  And then the output of the process at the bottom will be the query  So remember the query  the Q  the value V  and the keys K in our attention network  At the bottom  the keys  the values  and the queries are all the same  that's what the picture is meant to denote  The thing I want you to notice here is that the queries are coming bottom up here  but the keys and the values are coming from the output of the encoder  And then we take the keys  the values  and the query through this process  the KVQ  those go into the very same type of attention network that we have talked about before  We then do the skip connection because we have found that this tends to work well  This now employs attention on the final embeddings of the input sequence  And then we talked about the fact that this transformation  manifested through the neural network at the top  is convenient and found to be effective  And then this is repeated J times  The thing I want you to notice is this process from bottom to top of embedding  positional embeddings  and then attention is exactly the same kind of construct that we had before for our encoder  The thing that is different is that the on next step  we're going to do what I'll call cross attention  where the cross attention is using queries  Q  from what we have decoded thus far  And it is taking the keys and the values  K and V  from the encodings that we got at the output of our encoder  And we're going to talk about that in greater detail  And then at the top  we have our neural network  that is the same  So this basic construct of our decoder looks very much like our encoder  except for this piece right here  which I'll call the cross attention between the encoding at the output of our encoder and the query that we have encoded from our output thus far  So what we want to do is to spend a little bit of time to think about what is going on there  And then finally  at the top we have the Softmax that we've talked about before  And so now  this is going to predict the next word through the Softmax  That next word will now become the leftmost input at the output of this model  will be the leftmost input at the bottom of this network  And then the output thus far will be shifted to the right  And so again  we have the Softmax operation  And so  the key thing that is different relative to what we have talked about before is this concept of cross attention  And we're going to spend a little bit of time to talk about that in the next lesson 
Km45DNEa9mI,Cross Attention in the Sequence-to-Sequence Model  We have developed this sequence to sequence model  which is called the transformer  On the left  we have the input sequence which is a sequence of vectors corresponding to the input words  And then after K layers of this deep network  we have the output  the output of the encoder network  On the right  we have a decoder which produces the output sequence and so this is called a sequence to sequence model  We have an input sequence of words and then we have an output sequence of words  An example where this might be used is where we have an input sequence in one language and then we have an output sequence in another language  As part of this  we have what's called a cross attention component  And what we're going to do is take a little bit of a closer look at that  So at the top of our encoder  this is the encoding of the input sequence  We have an output of M vectors  And so if the input sequence is composed of say M words  at the top of the encoding network  we have M vectors  each of which is of Dimension D  These M vectors are the keys and values of our attention network  And then the decoded sequence or the output sequence  is at the bottom of this decoding network  And so these are representative of the queries  And so if we have decoded n-word so far  so up to this point  we have decoded n-words  then we have n vectors  vector 1  vector 2  through vector capital N  These are the vectors corresponding to the n-words words we have decoded thus far  each of these vectors is also D dimensional  The N factors  corresponding to the n-words decoded thus far  are the queries of this attention network  And the outputs of the encoding network at the top of the encoding network are the keys in the values of the attention Network  And so the reason this is called cross attention is that the queries are coming from the n-words that have been decoded thus far  and the keys in the values are coming from the input M words that are encoded at the top of our encoding network  And so this cross attention is manifested because the queries  the keys  and the values are different  And so whenever we do this attention network with those queries  keys and values  we then get an encoding of the n-words at the end queries are now mapped to a new sequence of n-words and vectors  each of which is is also D dimensional  And so what's happening here  is that the n-words that have been decoded thus far are represented via the attention mechanism in terms of the keys in the values  which are at the top of the encoding of the input sequence  So this is called cross attention  And so this composite model  where on the left  we're encoding the input sequence  And on the right  we are decoding the output sequence  This network is called the Transformer Network  And there are many applications of this and so as I mentioned one of the key applications is for what are called sequence to sequence model  Models where you have an input sequence of words  and then you want to have an output sequence of words  And so important application of this is translation  So you may have a sequence of words input in English  and then you would have an output sequence of words decoded for example  in German or any other language  This Transformer Network can be used in many settings  It could also be used in sentiment analysis and also to do things like predicting the next one or two words that one might type 
pGT6iJ_z5Vs,Multi-Head Attention  So in the context of the model that we have developed this far  the transformer network  This concept of attention has been fundamental to what we have done  and the attention is manifested in two ways as we've seen  One is self-attention  where we have a sequence of say  n words which are represented by n vectors  and the attention is done between those n words  And doing this by using this attention mechanism  we basically are modifying the vector representation of each word to take into account the characteristics of surrounding words  We have also looked at cross-attention  where we are evaluating attention between the output of the encoding arm and the output of the decoding arm  we're in a sequence to sequence model  we have an input sequence and we have an output sequence  In the original transformer network  there's this concept that generalizes this attention mechanism to what's called multi-head attention  And so what we'll do now is spend a little bit of time to try to understand this concept of multi-head attention  which is fundamental to the transformer network that is actually used in practice  Before we do that  let us just recall the various components of the attention mechanism  And so  in the attention mechanism  we have a sequence of words which are represented by codes 1  code 2  through code N  that's at the bottom  We then have a query  a vector Ck  which is query used to query relative to those n words  and so recall the query mechanism  We also have keys  So the n vectors C1  C2  through CN  corresponding to the input sequence are our keys  And then finally  we have values where the values here are also C1 through CN  and by combining the queries  the keys  and the values  we manifest the attention mechanism  which then transforms the query Ck into a new vector C tilde k  which places the context of that query relative to the n word vector C1 through CN  So what we want to do now with this so-called multi-headed attention  is to generalize this representation of the queries  keys  and values  And so the original attention mechanism that we have talked about  looks as shown here  So we have an input sequence of words  then each of those words is mapped to a word embedding  and recall that a word embedding just means a word vector or a vector for each word  Those vectors are then used as the keys  values  and queries  and then this manifest a self-attention mechanism  and then finally an output sequence  and recall that this output sequence places each word vector in the context of all of the word vectors that surround it  And so what we will do to make the subsequent discussion concrete  we will introduce a notation or a functional form  which says that this is the attention mechanism  It is a function of the keys  the values  and the queries  and then the output of this function is called the output itself  So we're going to use that notation as we generalize this to what's called multi-head attention  So now  recognizing what we just talked about  we looked at the queries as a set of n vectors  the keys as a set of n vectors  and the values V  as a sequence of n vectors or as a set of n vectors  Each of these vectors is d dimensional  the queries  keys  and values are fundamental to the attention network that we have talked about throughout these lectures  So now  if we consider the ith query or the ith one of these n vectors  what we're going to do is do a projection  what we call a projection of that query qi  so qi is a vector  it's d dimensional and it corresponds to the ith one of those n vectors  1  2 through capital N  What we're going to do is  we're going to multiply that vector qi which is d dimensional  times a matrix  where the matrix is composed of k rows  each one of the rows is also d dimensional  So what we're doing is  we're taking each of the rows of the matrix MQ  so MQ is a matrix M which is operating on a query q  So we called M superscript Q  this is a query matrix  What we're doing is  we're taking the query qi  and we're taking an inner product of qi with k vectors  each of which corresponds to the rows of the matrix that you see in blue  And so what this can be viewed as doing is  we're taking the query qi and we're mapping it on to what we call a linear subspace  which is spanned by the k rows of the matrix MQ  So let's try to get some interpretation or intuition about what's going on here  So recall that the embedding vector that is associated with each word  has an underlying let's say topical meaning  so this is notional  but we can again look at the word Paris  and this is now for illustration purposes  mapped to a ten-dimensional embedding vector  And then each of the components of that vector  each of the ten components of that vector represent a topic  and then if the word here Paris  is aligned with that topic  the associated component is positive  So for example  Paris is the capital of France  it has political significance  So the second component which notionally is associated with politics is positive  on the other hand Paris doesn't have that much to do with sports  So you see that the first component associated with sports is slightly negative  So you recall this is the intuition that underlies the word vectors  Now  remember that each of the  in this case the queries  corresponds to a vector associated with a particular word  and remember that each of the components of that vector represent topics  And so the way that we can think about this  is that we're taking the query q  and we're taking an inner product of qi here with row 1  row 2  and row k of the matrix MQ  and that we represent those rows as r sub 1  r sub 2  and r sub k  And so what we're doing is  we're taking the original query vector qi  and we're projecting it onto the k rows of this matrix MQ  And so a way that you can think about this  is that each of the k rows of the matrix MQ represent overarching  what we call meta-topics  And so what's happening is that each of the rows of that matrix  is selecting or emphasizing some of the components of the query vector qi  and essentially what we're doing is by using this matrix  we're highlighting certain topics or meta-topics that are important  And so this summarizes that  and so this matrix MQ which is represented in here in blue  which is composed of k rows  each row is of d dimension  is mapping the original query vector qi  which was d dimensional  to a new vector which is k dimensional  Those k dimensions of the new vector represent what we will call meta-topics of the original vector  So this is a general concept  this idea of multiplying the query times a matrix  and so doing  highlighting k meta-topics associated with that vector 
wuQIHnLr30w,The Complete Transformer Network  So in the previous lecture  we introduced this idea that we can take the original query  the ith query q_i which is of dimension d and we can map it into a new vector of dimension k by multiplying that vector q_i times a matrix of k rows  each row of which is of dimension d  where d is the dimension of the original word embedding  Now  as we've talked about  what this is basically doing is mapping the original vector q_i which is d dimensional  which corresponds to all of the d topics that can be characteristic of a word  We're mapping it into meta topics  where each meta topic is characterized by one of the rows of the matrix at the left in blue  where each row  the weighting of the components of each row highlight particular topics associated with the word embedding factor  Now  if we can do this one time with one particular matrix  M_i  so this is the ith example of the projection matrix  we can do this h times  So what we can do is we can take the original vector  the original query q_i  and we can multiply it times M_1  M_2 through M_h  where we have h different types of projection matrices of this form  So by doing this  what we're doing is we're taking the original vector q_i and mapping it onto h different linear subspaces which are defined by the corresponding matrix  which is highlighted on the top in blue  So what this is doing is through these matrices  M_1  M_2 through M_h  we are highlighting by each of them individually different subspaces  different sets of meta topics associated with the original word vector here q_i  So in this way  what we're doing is we're highlighting through these matrices M_1 through M_h  we're highlighting particular aspects or topical characteristics of particular words  Now  in the discussion that we have had to this point  we've introduced these projection matrices for the queries  but we can do exactly the same thing for the keys and for the values  So M_Q corresponds to a projection matrix for the queries  M_K corresponds to a projection matrix for the keys and M_V corresponds to a projection matrix for the values  We can do this multiple times  in other words we can have h different matrices  h different matrices for the queries  and h different matrices for the keys  and h different matrices for the values  Here what we're showing is with the subscript i  M_i  we're showing the ith example of them  So what we're doing by this projection  in each case for the queries  the keys and the values  we take the original sequence of vectors here  n vectors  each one of which is d dimensional  After this projection  we map those n vectors to a new sequence of n vectors  each of which is k dimensional  So now if we come back to this attention model  we again have an input sequence at the bottom  Each of the words in the input sequence is mapped to a vector  each word is mapped to a vector  we call that a word embedding  Then those word embeddings are the keys  the values in the queries K  V  and Q  Then we multiply those vectors times matrices M_K  M_V  and M_Q  So we're taking the original keys  values  and queries and projecting them onto linear subspaces defined by those respective matrices  and then once we have done that  we again do attention and then we get an output sequence  So the key thing that I want to try to communicate is that through this process of projecting onto these matrices  here the ith set of matrices  what we are doing is we are highlighting particular aspects of words defined by the characteristics of the rows of those matrices  Then recall that when we started the discussion of the attention network as the input of the attention  we had the keys  the values  and the queries  So now the modification is that the input to the attention network are the original keys  values  and queries  but now multiplied by respective matrices M_K  M_V  M_Q on the keys  values  and queries respectively  Here  we have the subscript i on each of those matrices because here this is the ith instantiation of those projection matrices  So therefore  we call this output i  Output i is looking at a particular aspect of the word embedding vectors which are highlighted by the respective matrices  MK  MV  and MQ  So this is the output for the ith sequence and we can do this  So now at the bottom  we recall that the output for the ith instantiation of the matrices is repeated  We can do this h different times for h different manifestations of those matrices and therefore we get output 1  output 2 through output h of these different attention mechanisms  We then take those output vectors  we just stack them one after the other  that's called concatenation so the concat means concatenation  So what we're doing is we're taking output vector 1  the next vector output 2  then output 3 through output h  we just stack them all together  That's called concatenation  Then we do what's called a linear transformation through the matrix WO  which corresponds to the output  and then this is the final output of the multi-head attention  So now to try to summarize  in the original attention mechanism  we had input vectors which corresponded to queries  keys  and values  and through the inner product mechanism  we did what we called the multi-headed attention  What this means is that we do the attention multiple times  here we do it h times  and the reason we call it multi-head is that the matrices M_i_K  V  and Q correspond to the ith instantiation of this projection or this head and so this is called multi-head attention  So in the transformer network that we have talked about  it is been found that this multi-head attention provides value in the quantitative performance  So the reason that this works well is that each of these heads  the ith head  M_i  K  V  and Q focuses on particular aspects  particular characteristics of the words  and therefore when the attention is imposed  the words  the attention mechanism is attending to particular characteristics of the words  So if we have h heads through h versions of these matrices  M_i  so we have M_1  M_2 through M_h  these allow the attention mechanism to attend to h different aspects  or components  or characteristics of words  or language  This is the final transformer network  On the left  we have an input sequence of words  On the right  we have an output sequence of words  On the left  we repeat this K times to manifest a deep network and on the right we do it J times to also manifest a deep network  and key to both the encoder and the decoder is this concept of attention  Now what we have done and what has been found to be effective is that we've generalized this attention to what's called multi-headed attention and this is the final transformer network which does sequence to sequence encoding for natural language processing  Again  very important application of that might be translation  where you have an input sequence in one language and you're trying to output a sequence in another language  To summarize  the key components of the transformer network is that it's composed of an encoder and decoder arm  this is for sequence to sequence modeling  The key component of both the encoder and the decoder is this attention mechanism  The encoder is based upon self attention and the decoder is based upon both self attention and cross attention  Within the attention mechanisms  this concept of multi-headed attention has been found to be very effective and it's used in practice  The training of the transformer is because of the way that the transformer network is designed  there is significant opportunity for parallelism when doing the training  we talked about this earlier  Within the attention mechanism  there are many computations that can be done in parallel  So with modern computing  graphical processor units  which can be used to exploit this parallelism  it has been found that in practice  the transformer network is very efficient to train  This is to be contrasted with another tool that we've talked about for natural language processing  the long short-term memory  the LSTM  Recall that the LSTM was a recurrent neural network for which the data are modeled sequentially  one after the other through the recurrent neural network and so the recurrent neural network relative to the transformer has the disadvantage that it does not allow you to exploit parallelism when doing training 
TWI19UruijI,Introduction to Reinforcement Learning  We're going to talk about an area of machine learning called reinforcement learning  This is an area of Machine Learning that recently has generated tremendous excitement  In that  it was one of the fundamental technologies in a machine learning solution for playing the game Go  Go is an ancient game from Asia  which consists of a very complex board of two colors  You can see that on the cover of a Nature article here  So very complex board game  This is a game that previously was thought that there was no way that a machine could play this game as well as a human  What was demonstrated using reinforcement learning and deep learning was that indeed  it is possible to develop a machine that can perform and play the game Go with the proficiency  which exceeds that of humans  So this was a fundamental breakthrough in Machine Learning and it's generated a lot of the excitement that we now see in Machine Learning  Interestingly  it was based largely on reinforcement learning using techniques that we're going to now discuss  So this gives us hopefully some motivation for trying to understand reinforcement learning  The problem I guess that one might realize real quick is if you try to study reinforcement learning is that it is a rather mathematical field  So you have some rather complex mathematical equations  which you can see here  The point of this slide is not to scare you  it's just to show that if we attack the problem of reinforcement learning or try to study reinforcement learning  let's say head on by trying to understand the mathematics  there's a danger that we may lose the opportunity to understand the underlying foundations of what reinforcement learning is doing  which actually  is far simpler than these equations might imply  So what I'd like to do is to walk through an example that should hopefully be intuitive to most people  which will give us a sense of what reinforcement learning is doing  So let's consider a situation of a medical doctor or an MD  who is tasked with trying to treat a patient  So the goal here is that the doctor would like to develop a policy or a framework by which in this case  she can optimally treat the patient  So what we mean by optimally is most effectively from the standpoint of improving the health of the patient but also doing it in a cost effective manner  So we assume in our setup that the doctor has a set of actions that they may perform  So you can think of actions in the case of the doctor might be medications that would be prescribed  procedures that might be performed  These are actions that a doctor may take  Each of those actions will in general impact the state of the patient  Here  the state of the patient is characterized by the health or by the parameters or the data of health that is presented to the clinician  So what the doctor would like or desires is a policy that achieves a good balance between achieving good outcomes for the patient  and then also doing this at low cost  This could be applied in many medical settings for example  in the control of diabetes or also in procedures or optimal flow of workflow in an operating room  So this is a very fundamental problem in medicine as we'll see in many fields  So the way that we're going to set this problem up is that our doctor is going to see or observe the patient and in particular  we'll observe the state of the patient  That state is represented by a vector  which we'll call S  The state of the patient are whatever variables we have to characterize their health  So for example  there are temperature  blood pressure  perhaps laboratory tests  et cetera  So the state S are whatever data that we have to characterize the health of the patient  So that we'll call S  and that is observable to the doctor  Then we assume that the doctor has a set of actions that she may take  We'll represent that set by A  Then there are a set of actions a_1 through a_m that the doctor may take  So these actions may be again  prescribing a particular medication  suggesting a particular procedure  et cetera  These are actions that a doctor may take  Now  there is randomness in how a patient in state S  So given a patient in state S  there is some uncertainty or randomness in how that patient will react to the action that is selected  The reason that that randomness is present is because there may be some randomness to the disease itself  But in addition  the state S  which is a vector of some parameters that characterize the health of the patient  those parameters may not be sufficient to characterize every aspect of the health of the patient  Therefore  there are some missing aspects of the health of the patient  which are not captured by S  Those missing aspects of the health manifest some randomness because those missing characteristics are different from patient to patient  So we introduce a probability distribution  which we represent by P  which is the probability of when the patient is in state S and the doctor takes action A that the patient will transition to state s'  So s' is a new state of health of the patient  So for example  the patient is presented to the doctor with a certain state of health S  which are characterized by their parameters  for example  again  the temperature  blood pressure  et cetera  of the patient  The doctor then takes an action  For example  prescribes a medication  and then the doctor sees what happens  Then in this case  the state of health of the patient changes to s'  which might be a change in the temperature  change in the blood pressure or what other laboratory tests we have  So given this probability P(s a s')  what we would like to do is to devise optimal actions for the doctor to take  So here  to define what we mean by optimal  we're going to define a reward function  R(s a s') is the reward provided whenever the patient is initially in state S  The doctor suggests action A  and then the patient goes to state s'  This is what we call a reward  So for example  if the state of health improves  in other words  if s' is better than S after taking action A  then the reward should be high  If the state s' is worse for the patient  then it was at state S  the reward should be low  This reward should take into account the state of health of the patient  Initially  state s subsequently state s'  it also should take into account the cost of the action  So this reward takes into account the reward to the patient from the standpoint of health but also the cost of providing that care  So the thing to note about this is that this reward function R(s a s') reflects the immediate impact to the patient of taking action A when the patient is in state s  and then the patient transitions to state s' 
TbvhGcf6UJU,Reinforcement Learning Problem Setup  Let's continue describing the way in which we setup or solve reinforcement learning problems  So the the MD  who's in this case is our example to try to introduce the concept of Machine Learning  the concept of reinforcement Learning  The MD interacts with the patient through a series of actions and rewards and observations  So here are the state of health of the patient at time t minus one is S sub t minus one  The doctor takes an action  prescribes a medication  looks  considers procedure etc  takes an action at time t minus one  Then a reward is manifested because the state of health has changed to S sub t  Then an action is again taken by the Dr  We get a reward RT and then the chain the health changes to S t plus one  So the thing to notice about reinforcement learning is that the interaction  in this case of the doctor with the patient  is characterized by a sequence of states  actions  rewards  then new states  and then a new action reward  and then a new state  So it's  sequence  action  reward  series  state  action  reward followed sequentially  The goal of the doctor and the goal of reinforcement learning is to develop a policy that defines for the doctor the optimal action to take when presented by a patient in state s  So the goal of reinforcement learning is to learn this policy  and the policy will effectively define the standard of care  how a doctor should pick care for a patient  The optimal policy should maximize the average reward over time and it should account for the patient outcome and for the patient costs  or the cost of delivering of care  The policy should be non-myopic  which means that it should think ahead  not only considered the most immediate impact to the patient  but also look at the long run impact to the patient  In particular  in the context of this non-myopic characteristic  we would typically like to weight the impacts in the near term more importantly or more highly than what happens in the long run but nevertheless with a non-myopic policy  we want to think about the reward immediately as well as into the future  So the challenge that we have when we solve a reinforcement learning problem is that we typically do not know that underlying distribution  P(s a s) prime  Recall that P(s a s) prime corresponds to the probability that when we're given a patient in state s  and we take an action a that the patient will transition to state S prime  That underlying probability is typically unknown  So the question then is  how can we learn a policy that is optimal without having access to that underlying probability P(s a s) prime  So conceptually  what we can do is  we can just experience the world and then try things and record what happens  Then the idea is to try to adapt the policy such that over time we reinforce actions that were good or that lead to good outcomes and we discourage actions that lead to poor outcomes  So this is in fact the heart of the term  Reinforcement Learning  So in Reinforcement learning  we're going to just experience the world  We're going to see the state of the world  we're going to take actions  and we're going to see the new state and we're going to see the reward  and we're going to do that repetitively  The idea is is that  through experiencing States and actions and rewards and new states  we hope to learn a policy which is optimal  Optimal in the sense of achieving an outcome for the patient or reward that is on average very good  good for the patient and also good from the health system from the standpoint of keeping costs under control  So Reinforcement learning is the formulation of this fundamental challenge  So the fundamental challenge is that we're going to experience the world by a series of states  actions  rewards and new states  and then new action  new reward  new state  We're going to do that repeatedly  Then what we would like to do based upon that experience is to learn an optimal policy  This is a very fundamental problem  this is what reinforcement learning is  and the reinforcement methodology that we're going to talk about is going to address this challenge  The thing that is important to recognize is that while we have used health and medicine has our underlying thematic example largely because it's understandable to most people  intuitively understandable  This construct of trying to develop an optimal policy over time is manifested in other settings  So for example  consider the case in which you are operating the machine floor of some factory  and what you would like to do is monitor the health of the machines and also determine your maintenance schedule  So for example  when should you take a particular machine offline and repair it or tune it up  It would probably be not a good idea to wait for machines to break  and so therefore what is the optimal way to monitor the health of Machines and what is the optimal way to maintain that health  This is also a reinforcement learning problem  Another example is investing  So if you look at the price of of companies over time  you would like to make good investments  It's very difficult to understand the underlying statistics of the stock market  So what you might try to do is to develop a policy  a policy that guides which stocks you buy  which stocks you sell  You would like to do this in a way that over time you develop a policy which on average yields a positive outcome in investing  So the thing that I just wanted to highlight from this slide is that while we're going to use medicine as our underlying theme as in the past and as we move forward in these lectures  this construct of reinforcement learning is very general  We also talked about it in the context of the game Go and a machine playing a human in a game like Go  These are all sequential decision type problems and reinforcement learning which we're going to now start to dive into in greater detail is a fundamental machine learning framework for solving each of these problems 
7P53AOKtVdQ,Example of Reinforcement Learning in Practice  So this framework of reinforcement learning which is characterized by the goal of developing a policy of taking actions when the world is presented to us in a certain state  taking actions such that over time  we get a large reward on average  So our goal in reinforcement learning  is to learn that policy  the policy of which actions to take when we're in a given state  Now  I recognize that this construct may seem rather abstract perhaps  hard to fully grasp  So what I'd like to do now  is to give a concrete example which is hopefully easily understood  which will try to make these somewhat perhaps abstract constructs more understandable  So let's consider again our doctor as an example  Let's assume that we have a diabetes doctor  So this is a doctor who is dealing with diabetic patients  So the goal of this doctor  might be to develop a regimen to keep the health of the diabetic patient under control  So let's consider how we might set this problem up as an reinforcement learning problem  Where again  the doctor over time we'd like to try to devise a policy which was able to specify the optimal action to take for any state of health of a diabetic patient  So what we're going to do in this example  is characterized the state of health of the patient which remember is a vector denoted by s  This is the state of health of the patient  Let's represent the state of the health of the patient from the standpoint of a diabetes doctor as the minimum and maximum glucose concentration from the previous day for that patient  So these are now two numbers  So from the standpoint of the doctor  the state of health of the patient on the previous day  is characterized by two numbers  The minimum glucose concentration from the previous day and the maximum glucose concentration from the previous day  Now  given that state  given those two numbers  the doctor would like to specify which action to take ideally to get the glucose level of the patient under control  So here in the context of our diabetic doctor  let's assume that there are two types of things that the doctor can control from the standpoint of medication  The rate of continuous insulin supply  So let's just say that we have a diabetic patient who was being supplied insulin  The doctor can specify the rate of continuous insulin supply as well as the bolus dose  So these are two things a doctor can control  So the action that the doctor can take in this case  would be setting the rate of continuous insulin supply and the bolus dose  So then our patient  is going to get a reward  The reward is the reward the patient gets when initially  the patient is in state s  we take action a  and this patient health or state goes to s prime  So what we would like to do is to devise that reward r  such that if s prime which is the subsequent health of the patient characterized by the minimum and maximum glucose concentration  If the s prime is better than s  then the reward should be high and if the s prime is worse than s  In other words  the health of the patient has deteriorated  the rewards should be low  We assume that we may specify what that reward is  We or the doctor specifies what that reward is  So now  to again make this further work towards making this concrete  Let's again look at our state  The state is again characterized by the minimum and maximum glucose concentration from the previous day  we're going to discretize that continuous range of values into bins  So what this means is that  while the minimum and maximum glucose concentration levels from the previous day  could be let's say any continuous number  what we're going to do is we're going to break it up into bins or ranges  and if the min and max glucose concentration levels are within a particular bin  we're going to assign the health from the previous day to that bin  So this is a discretization of the states of the health of the patient  In a similar way  the action which in this case  are the rate of continuous insulin supply and the bolus dose  those are also in general continuous numbers  We're going to again discretize those continuous numbers into bins or ranges  So instead of being concerned with what is the exact number for the rate of continuous insulin supply or the bolus dose  we're going to ask which bin is that going to be specified into  So therefore  what we're doing by this setup  is we're taking the state of health of the patient and we're discretizing it or replacing it into bins  Likewise  the actions that the doctor can take are also discretized or placed into bins  So for both  the state of health and for the actions  we're going to simplify our solution via a discretization step  So then with that done  we're going to define something called a Q function  The Q function is a function of the state and the action  This is now an n by m matrix  So if the continuous range of states is broken up into n bins and the continuous range of actions is broken up into m bins  we're now going to have a table which is an n by m matrix  which is represented by this function Q  which is a function of s and a  Now  s and a are discrete  So consequently  Q (s  a) is an n by n matrix  This matrix is meant to denote as we'll see subsequently  as the value of taking action a when in state s  So this Q function which will be really important as we move forward in reinforcement learning  is the value of taking action a  when the patient is in state s  So now  the key goal of reinforcement learning is to learn this Q function  The key thing that we want to achieve in reinforcement learning  is to learn this table or matrix Q(s  a)  Because if we learn that Q function or learn that Q table  then if the patient is presented to us in any state s  which is now discretized in the way that we talked about  then what we're going to do is we're going to choose the action a  which maximizes that Q functions  so that we can maximize the reward to the patient  So the goal of reinforcement learning  is to learn this Q function or this Q matrix  The way that we're going to do that  is we're going to just experience the world  So remember that the way that reinforcement learning works we have in this case  a patient presented to us in state s  the doctor specifies an action a  the patient transitions into a new state s prime  and then a reward is manifested for that patient  then a new action is taken and the patient transits into a new state  This happens repeatedly  Through this repeated sequential process of state action new state reward  state action new state reward  that sequential process over time  Our goal is to learn this Q function or this Q table  Q(s  a)  which is a good representation of  sorry  the value of taking action a when the patient is in state s  So the challenge then is  how do we learn this table Q(s  a) based upon experience? So the way that we're going to do this  is we're going to initialize the table Q(s  a) in some way  So the way in which we initialize it  can be done in multiple ways  If we have some intuition or some prior knowledge about which state  which actions a are good for particular states s  we may reflect that in the way in which we initialize that table or we may just initialize the table Q(s  a) at random  So we initialize it in some way  The initialization  is not particularly important  We then have a patient presented to us in state s  We choose an action a and the patient transitions to state s-prime  and then we observe a reward  The reward is the reward that is manifested whenever the patient is initially in state s  doctor takes action a and the patient transitions into s-prime  The initial state s and the new state s-prime are observable after we take action a  the reward is also observable  Now  based upon those observations  we now want to update our Q-function  So the key question is  if we take these measurements  we have a patient in state s  we take action a  we transition the state s-prime and we observe this reward  how should we update the Q-function to try to learn a better model? So the way that we might do this is as follows  So let Q-old  Q-superscript old  be our old table  So we have some table which is an approximation to the value of taking action a when the patient is in state s  So this is our old representation of the table  We now have taking some measurements  We have a state action  we have a state s  and an action a  we transition to state s-prime and we observe this reward  How should we update our Q-function so that we're going to update it with what we'll call Q-new  which is our new table? So what we're going to do is  we're going to take the difference between the reward and our old Q-function  So r  s  a  s-prime is the reward that we observe and Q-old is our old representation of the value of taking action a when the patient is in state s  If the reward r is larger than our previous representation of the value of taking action a in state s  then what we should do is we should increase the value of that action  So what you see here is we take the old Q and then we add the difference between the new reward and our old Q-function  Then if r  our new reward  is larger than our old Q-function  then we increase the value of the Q function and Q-new  If the r is less than the old Q-function  which means that our old Q-function overestimated the value of taking action a in state s  then we are going to diminish the Q-function  So what we're doing here is rather simple construct where we're going to adjust the value of the Q-function depending upon how the new reward r is in value relative to the old Q-function  Q-old  So this is a rather simple  I think rather intuitive construct  This parameter Alpha is called the learning rate  and it is what it says  it tells us the rate at which we're going to learn or the rate at which we're going to adjust the Q-function based upon our observations and that Alpha or the learning rate is a number between zero and one  So if we look at this equation  this equation is going to be at the heart of reinforcement learning  We're going to build upon it  but this is really an important equation for us to understand  So if we look at the new Q-function  again  what it is equal to is the old Q-function plus a weighted version of the temporal difference  If the temporal difference is positive or this TD is positive  that means that the immediate reward that we see r is larger than our previous estimation of the value of that Q-function  which means that we probably underestimated the value previously and we should increase it  and that's what this equation does  If r is less than Q-old  then that would imply that we overestimated the value of the old Q-function  Then in that case  r minus Q-old is a negative number and therefore the temporal difference takes the old Q-function and diminishes its value  So this temporal difference  what it's doing is it's looking at the immediate reward r comparing it to our old estimate of the value of taking action a in state s  Then the new Q-function is an adjustment of the old one based upon the difference between the new reward r and our old estimation of the value of taking action a in state s  Again  this Alpha is called the learning rate  It controls the relative balance between our old estimate of the Q-function and our new estimate based upon the observed reward  If the temperature difference is positive  then we take the old Q and we increase it to reflect the fact that the value of taking action a in state s is higher than we thought previously  If the temporal difference is negative  then the old Q-function is diminished we need to update the new Q-function  This equation which is I think hopefully relatively intuitive is going to be at the heart of reinforcement learning  So what we're going to do is  in this case  our doctor is going to see a patient or patients  is going to see states of health  take actions  see new states s-prime  is going to measure reward and it's going to do this repeatedly  and then over time through this equation  is going to learn a Q-function which quantifies the value of taking action a for any given state s  Then after this Q-function is learned  then this effectively will constitute a policy for our doctor  because then  for any state of health of the patient after this Q-function is learned  the doctor will know how to choose the action that is most valuable to the patient or choose the action that has the most value for a given state  So this summarizes this  Now  one problem that we need to think about  is that this setup only accounts for the immediate reward r  It doesn't account for what might happen subsequently to the patient  So let's think about this a little bit  So what we're trying to do with this simple equation is trying to learn a Q-function which is good at predicting the immediate reward to a patient  Now  in medicine in many fields  you may have a situation where the immediate reward could be good  but then the long-term effects could be bad  So one example of this is  if you're a doctor and you have a young woman who is your patient  you have perhaps a situation where you might be able to prescribe a medication which might immediately make that young woman better  and therefore  the immediate reward would be positive  very positive  But there may be some side effects to that medication which may prohibit that woman from subsequently having children  So therefore  the long-term impact of that action could be very bad  So therefore  whenever we look at this Q-function  if we only learn it based upon the immediate reward  then we have a very serious limitation  because we do not take into account the long-term consequences of actions  So this is if we recall  previously we talked about the goal of developing a non-myopic policy  What that means is that that's a policy that takes into account the immediate reward as well as long-term consequences  This setup that we have here  while it's very simple and perhaps intuitive and attractive  it is not a good solution because it only takes into account the immediate reward and does not take into account effects down the road  In other words  this is a myopic  this solution is myopic and we do not therefore want it  So what we would like to do is to take this relatively simple solution and extend it in a rather simple way which will allow us to develop a non-myopic policy 
zKCr6uDlq1o,"Reinforcement Learning with PyTorch  So far throughout this course  we've mostly focused on supervised learning which means we're learning from a data set of input and output pairs  While supervised learning is a powerful framework  it's not always appropriate for every setting  For example  when our goal is for an agent to learn behaviors while interacting with an environment  a data sets of the often behavior often doesn't exist  Instead  we want to be able to learn through experience or \""trial and error \"" The field of reinforcement learning tackles exactly this problem  Like in vision and language  deep learning has made a big impact on reinforcement learning as well  These notebooks will introduce some basic reinforcement learning concepts with OpenAI Gym  We'll see a couple of environments implemented in Gym and how we might try to learn to solve them  Your assignments will be to adapt one of these agents to learn with the new algorithm "
1foc4sbmQb8,Q Learning  So this framework that we have developed to this point is called Q learning  The reason it's called Q Learning is because our goal is to learn this Q function  which is a table that describes the value of taking action a in state s  And so the heart of reinforcement learning in many applications is trying to learn this Q function through a series of state  action  new state and reward  new action  new state  reward  etc  State action  state reward  state action  state reward a series of these and then the goal through that experience is to learn the Q function  And so now what we'd like to do is to try to understand this in a little bit more detail since this is at the heart of reinforcement learning  Recall at the beginning of this discussion of reinforcement learning  we talked about AlphaGo  the capacity of a machine to play the game Go with performance that exceeded that of the best go players in the world  Interestingly the solution methodology as we'll see later was really based upon Q Learning  So this Q Learning is a very  very fundamental construct  And so it is characterized by this equation at the top  And so what were again  doing is sequentially updating the Q function by making observations of the state  taking actions  observing new states  getting reward and then doing that repeatedly  And so after we learn this Q function  so after a series of state action  state reward in doing that for a very long time  we estimate a Q function  So let's say that we have done that for a very long time  And in fact  when we think about our medical application this in fact is what medicine is about  the doctors interact with patients  They prescribe actions given states of health of the patient  they see what happens they do this again  And over time medicine gets a good understanding of value of taking action a and state s  After learning the Q function through that long series of interaction with patients in the case of medicine  We now can constitute a policy  and that policy is represented as a function Pi  which it tells us which a In to take when the patient  in this cases in state a as in state s  And so what is done here is we have our table  Q(s a)  that table reflects the value of taking action a in state s  And so now if we're a doctor  what we're going to do is we're going to say  okay  what is the state of health of the patient? What is s? Let's choose the action that maximizes the value to the patient  And so therefore what we're going to do is we're going to take this Q table  and the optimal policy is simply that action for every state that maximizes the value to the patient  And recall the underlying reward that we're using here takes into account the value to the patient from the standpoint of health improvement  as well as the cost of delivering that care  So again  I'm showing you the Q Learning equation  This is really very foundational  so let's spend a little bit of time talking about it  This gamma is what we call a discount factor  r sub t is the immediate reward of taking action a sub t in state s of t  And then the second term max over a of our old Q  So this is now the expected reward of taking an optimal action after the patient has transition into the new state  so the new state st + 1  And so this term in brackets is the immediate reward plus a discounted version of the future reward  The reason we discount it is because the immediate situation  the immediate reward rt is typically more important to us than subsequent rewards  The parameter gamma  which is our discount factor is a number between 0 and 1 and it just tells us how much we care about the future relative to the past or to the immediate situation  And that gamma is something that can be selected  So if we think about in the context of our patient and and we think about our young woman  We can ask that young woman how important is it that you have children in the future  And if it's very important to that patient then gamma is going to be large  is going to be near 1 because future rewards are important to that patient  If the young woman says  I'm not so concerned about having children in the future  the thing that's most important to me is that I get healthy right now  Then that gamma that discount factor might be rather small  which would mean that the reward to the immediate reward to the patient is far more important than that subsequent reward  And so the thing to notice is that we will get a different policy for every selection of gamma  And so in some sense or in every sense the doctor has the capacity to build a policy which is tailored to the desires or the needs of the patient from the standpoint of how much they value immediate reward versus subsequent award  And so this discount factor is something that is very important and is selected depending upon the preferences of the of the person or the situation of immediate reward versus subsequent reward  This alpha is our learning rate  and it basically guides how quickly the Q function changes over over time  And so remember we're observing the world the way we observe the world is we have a state  we take an action  we have a new state and we have a reward  We are experiencing the world  and then as we experience the world we're taking our old representation of the Q function Q old and we're mapping it into a new representation Q new  The question is how quickly should we change that? How quickly should we let every new measurement change the value of the Q function? If alpha is large  which means that the learning rate is high  Then you could have a situation where the Q function is changing dramatically from new situation to new situation and perhaps that's not a good idea  So in that case you might want the learning rate to be small  alpha to be near 0  which would mean that the rate at which the Q function changes as a function of time is smooth  And so this again the learning rate is something that one can set  And so the Q old is our previous representation of the value of taking action a and state s  This term r plus the discount factor times the max of Q old is are non-myopic representation that we get to observe of the reward of taking action at and in state s of t  And so this this is what we call Q Learning  this is at the heart of reinforcement learning  And as I said earlier and as we will show subsequently this was at the heart of the breakthrough technology that we talked about at the beginning the AlphaGo and the ability of machine to play the game Go at the capacity of the best humans in the world  This is at the heart of reinforcement learning as applied to many other applications including as we discussed monitoring of equipment and other applications  Okay  so if we look at this this this expression  this equation that we're using a Q Learning  which hopefully while it looks perhaps if you just glance at it  it looks complicated  But hopefully through the way in which we have derived it  it is relatively intuitive  Whenever we evaluate this thing we do not actually have to set the next action at plus 1 equal to that that maximizes the Q old  What we can do is we can just evaluate because we have the old table  So we just choose the action a that it maximizes Q old  and then we can update our table our Q table into Q new  And then we can choose the next action at + 1 to optimize or maximize the new Q function  And so therefore  whenever we actually implement this when we observe state action reward  state action reward  state action reward  What we do is at every step we update the Q function through the observing of the immediate reward and our estimate of the discounted future reward  After we update our Q function we can then choose the next action  which is optimal as reflected by that Q function 
BmayUdDaDYM,Extensions of Q Learning  So Q-Learning  which is our goal of trying to learn this Q-function  where again  the Q-function is an approximation that we hope to learn of taking action a when the agent or in our example  the patient is in state a  So the Q-function reflects the value of taking action a in state s  This is at the heart of reinforcement learning  Reinforcement learning is a field that has been studied for decades  So it is actually a very well-studied field  and there have been many different ways in which the Q-function has been learned  many different extensions of this basic construct  and so I want to briefly discuss a few of the extensions and also some of the practical ways in which Q-Learning is implemented  So again  we showed this fundamental equation of Q-Learning  which is basically updating the old Q-function to a new Q-function  based upon the immediate reward r_t  and our estimate of the expected or average future reward  which is reflected by max over the actions of the Q_old for the new state s_t plus 1  with the discount factor Gamma  As we discussed the learning rate  Alpha describes or characterizes how quickly learning is done  how quickly the Q-function changes and practice what one finds effective is to set Alpha equal to 0 1 or 1 over 10  So this is something that has been found to work well in practice  Another method that has been studied in reinforcement learning is called SARSA  and just want to briefly talk about this  It is a rather simple extension and it has been shown to be effective in some situations  The Q-Learning  which is guided by this equation  is based upon the assumption that we see the patient or agent  we sometimes call the agent  In state s_t  we take action a_t  we get reward r_t  and then we move to the new state s_t plus 1  So we're in a state  we take an action  we get a reward  and then we transition into a new state  which we call s_t plus 1  So it's state  action  reward  state  So this is represented as SARS  state-action-reward-state  So SARSA is basically the exact same equation with a rather small modification  So in SARSA  we assume that we have access to the previous state s_t  the previous action a_t  the reward r_t  the next state s_t plus 1  so that's the same  plus we get to see the next action  a_t plus 1  So SARSA simply means we have state-action-reward-state-action  and we're going to learn based upon that  The second equation is what is used in SARSA is actually exactly the same as what is used in Q-Learning  which is based upon SARS  The only difference is instead of taking the max over a of the old Q-function  which is the optimal action for state s_t plus 1 based upon the old Q-function  we're simply going to instead of taking maximization  we're going to take the next state a_t plus 1 and plug it in for a  So this is SARSA and the top one is called Q-Learning or SARS  These are two different approaches that one might consider  This is just to show that there's not one way to do this and in fact  there are many other ways that people have studied over time  but they're all based upon this basic construct  Now  let's think a little bit more about our learning procedure  So the way that reinforcement learning works again  it's a series of states  actions  rewards  and new states  Then based upon those new states  new actions  new rewards  new states  It's a sequence of state  action  reward  states  So the question that one might ask is that  as you are engaging in this process and as you are updating your estimation of the Q-function through Q_new  through this sequential process represented by this equation  What actions should you take? So in other words  when you take state s  you're in state s  you take action a  you get an immediate reward  and then you transition into a new state  and now you are going to take another action  Which action should you take? Which action would be beneficial from the standpoint of learning the policy? Now  one way that you might think about this is that we have a Q-function  and that Q-function or that Q-matrix or Q-table is going to be sequentially updated as we do this Q-Learning  So one thing that is kind of intuitive is that what we might do is always choose the optimal action  based upon our current estimate of the Q-function  In other words  no matter what state we're in s  let's choose the action which maximizes the Q-function or the action that will give us the most value  based upon our estimation of the Q-function  So the problem with this is that we never have the opportunity through this process to just explore  So one of the fundamental constructs of reinforcement learning is this concept of exploration and exploitation  So in exploration  the exploration is the idea of just trying different actions  Let's just try things  try different actions because we might be surprised  we might learn something that we didn't expect  So if you think about the Q-function that we're optimizing or learning in the way that we are  it is heavily  in fact  entirely based upon all of our previous experience  So therefore  it might be biased and we might not get to see actions which would be beneficial  So exploration  the concept of exploration  What that means is  it's the concept of taking actions which may not be optimal  based upon our current estimation of the Q  They may be actions that we may have never taken before but we may be surprised  we may be surprised in a very beneficial way  So therefore  there is benefit to exploring  Exploitation is the concept that we're going to exploit the Q-function that we now possess  and we're going to choose actions which are optimal as based optimal for a given state and therefore  we're going to exploit all of our experience  So therefore  in reinforcement learning  there's a paradox of exploring versus exploitation  So what has been found to be effective in reinforcement learning is something called an Epsilon-Greedy approach  So the way that this works is that we choose a probability  a number Epsilon  so Epsilon is some small probability  it's a number between zero and one  typically small  We have a coin with probability Epsilon  we get heads  with probability 1 minus epsilon  we get tails  So every time  when we're about ready to choose our next action  we flip our coin  If we get heads  which has probability Epsilon  then what we're going to do is  we're going to choose the next action a_t plus 1  completely at random  So we're not going to use the Q-function at all  We're just going to choose the next action at random with probability 1 minus Epsilon  which is the probability of us getting tails in my example  we're going to exploit our previous estimation of the Q-function  What we're going to do is we're going to choose the action  which maximizes the expected reward or the expected value for a given state s_t plus 1  So with probability Epsilon  we're going to explore  With probability 1 minus Epsilon  we're going to exploit  The idea here is that where at random  with probability Epsilon  going to take some actions which may seem very unusual  and therefore will be not constrained by our understanding of the Q-function  The value of this is that every now and then  we may discover something  we may discover an action that we never considered before  which is very valuable  So this Epsilon-Greedy framework has been found to be a very effective tool to improving the learning in the context of reinforcement learning  So as I said  this framework allows exploration with a probability Epsilon  which is something that one can choose 
ZKBLirP5jo8,Limitations of Q Learning  and Introduction to Deep Q Learning  So thus far we have introduced the concept of Q-learning  which is a fundamental technology that underlies reinforcement learning  While the technique is relatively easy to understand  as we discussed it has limitations  So now what we're going to do is to discuss something called  deep Q-learning  It's called deep because it's going to be based upon deep learning technology  and as we'll see it's very closely related to Q-learning  so we call it deep Q-learning  This technology that we're going to now discuss was responsible for the breakthrough that we discussed earlier  whereby AlphaGo using a reinforcement machine learning technology based upon deep Q-learning was able to defeat the finest players of go in the world  So let's think back to our Q-learning  which is characterized by this equation  So again  when you look at this equation  at first it looks perhaps a bit daunting  but as we've discussed as we've built up our understanding of this  basically what we're doing with Q-learning is adjusting this Q-function which is a representation of the value of taking a given action in a particular state  We are sequentially learning this Q-function by trying to fit it to a non-myopic representation of the reward of actions in a given state  in that non-myopic representation of the reward is represented by the immediate reward r_t  plus a discounted version of expected future rewards which is that max over the action of our old representation of the Q-function  So this framework is very powerful  however  it has some limitations  In particular  it assumes that we can represent all of the states and all of the actions in a matrix  We talked about when we looked at our example of diabetic control that the way that we constituted that matrix was by discretizing the states and actions  This becomes practical  it becomes impractical as the number of states and the number of actions becomes very large  which is often the case in practical problems  The other thing is is that by using a tabular approach which is what we're doing with Q-learning  we have no capacity to generalize across different types of environmental types  So while Q-learning is a powerful concept  it has important limitations  this is going to motivate us to introduce this concept of deep Q-learning  So with deep Q-learning  we're no longer going to make any assumptions about a tabular representation of the states and actions  So in this case  the states could be infinite in size  in other words they could be essentially arbitrary and they're variation and the actions can as well  in the way that we're going to do this and the reason we call deep Q-learning  is that we're going to introduce a deep neural network where the input to the neural network is the state and here the state is characterized by N numbers  so the I state is S_i1 through S_in which are just the N values of that state  For example  if the state is an image that we might be looking at  the state of the system is characterized by an image  in that case N corresponds to the number of pixels in that image  So what happens is that the state S_i in this case for the ith state  goes through a deep neural network which we've discussed elsewhere in this class  then at the top of that neural network we have actions  the actions that the agent might take  In this case we're assuming three actions  So a1  a2  a3  but the number of actions could be large  could be much larger than three  So the idea is that this neural network  this deep neural network is a representation of the Q-function  The input to that deep neural network is the state  The output at the top of the neural network is the value of each action for that state  so this is why it's called deep Q-learning  because this deep neural network is going to represent the Q-function  So the important thing to notice is that we're no longer going to make any assumption that Q is tabular  We're going to represent it by a deep neural network  That deep neural network is characterized by parameters theta  the neural network parameters are theta  So the Q-function representative of that neural network is denoted Q of s  a so that the s is the input  that's the state  a is a particular action which is the output in this case  In this figure we have three actions  Then theta are the parameters of that neural network  Now our goal is to learn the parameters theta  the learning challenge that we have is to learn the parameters theta  The way we're going to do this and the way deep Q-learning is going to work  is actually very similar to Q-learning  So what we're going to do is we're going to introduce what we call a cost function u  which is a value function or cost function on the parameters theta  So if you look at this first equation  the second term r_t plus gamma times max over a  q  that is exactly the non-myopic reward that we had previously  So that part looks the same  What we're going to do is instead of using the old table q  in our previous notation it was q_old  for representation of our old table  Now what we're going to do is we're going to take the neural network and use the old parameters theta_o  Those are the old parameters  So what we're going to do is we're going to say that  in state s the value of action a is the immediate reward r_t plus a discounted version of an estimate of the future reward which is our Q-function with the old parameters theta old  Then what we're going to do is we're going to update the parameters theta  such that our Q-function which is represented by that deep neural network matches the non-myopic reward  So if we look at this cost function what we're doing is we're taking q  our neural network which is characterized by unknown parameters theta and we're calculating the difference of it to the non-myopic reward and then we're taking the square of it  So what we would like to do is to minimize that function  minimize u  If we minimize u  that means that the Q-function characterized by parameters theta which is our deep neural network  is a good model for representing the immediate reward plus the discounted future rewards  That's what this equation is doing  So what we would like to do is to minimize u  we would like to choose or determine the parameters theta that minimize u  So the way that we do this  is by introducing a cost function which is shown in the figure below  u of theta is the difference between the neural network with parameters theta and the non-myopic reward which is r  t plus discounted future reward and what we would like to do is to minimize this  The way that we're going to do this is via gradient descent  We're going to take the gradient which we've talked about elsewhere in previous lectures  but basically what we're going to do is we're going to calculate the slope of the line and we're going to move  if you see the arrows  the arrows are moving in the negative direction of slab opposite to slab  and so the objective here is to try to minimize this cost function  So this type of optimization problem is something that we find often on neural networks and we've talked about elsewhere in this class  So to summarize  again the top equation is the cost function  we're basically trying to fit our deep neural network with parameters theta to a non-myopic representation of the reward at time t  The way that we're going to minimize u  is by doing something called gradient descent  we're going to move in the opposite direction of the gradient  So if we look at the second equation or the bottom equation  what we're going to do is we're going to update the model parameters  So the new model parameters are the old model parameters minus a scalar Alpha  Alpha is equivalent to our learning rate and the triangle symbol corresponds to gradient or slope in a high-dimensional space where in this case it's the space in which the parameters theta live  What we're going to do is that negative sign  negative Alpha times the gradient  What that means is we're going to move in a direction opposite to the slope  So if you look at the arrows the way they're pointed in that figure  they're pointed in a direction which is in the negative direction of the slope  So this update equation which is the bottom equation is a standard equation that we've talked about elsewhere fundamental to machine learning  What we're going to do is sequentially update the model parameters via this equation  So we have the old parameters  we move in a direction that is a negative slope  We then update the parameters those are now theta new and then we repeat  So you see multiple arrows  Three arrows in the figure that corresponds to three steps of gradient descent  So that triangle symbol mathematically is called a nabla or gradient  and the way to think about this it's a multi-dimensional slope  What it means is a slope in multiple dimensions more than one dimension  So now the learning process corresponds to taking the gradient that nabla or triangle symbol of that function u which is the first equation  it turns out that that expression for u has a gradient that we can actually write down  So the last equation here  what we're doing is we're actually evaluating that gradient of u  it turns out that it has the expression that you see at the bottom  It looks somewhat complicated  but as we're going to talk about in a moment  it actually looks very much like something that we're familiar with  It has a term in brackets which we'll talk about in detail in a moment and then it has a gradient with respect to the Q-function  So that last term is a multidimensional slope  So now  if we look at this expression that we've derived for our deep Q network  If we look at the first few terms that we put in brackets here  this looks very much like the Q-learning that we did previously  If you look at it  it has the immediate reward rt plus a discounted version of the future rewards gamma times max of the Q-function and then minus the old Q-function  So that is the temporal difference that we had previously  Then the thing that is different about this is that we now multiply this by a modification which accounts for the neural network parameters theta and that corresponds to the gradient of the Q-function with respect to the model parameters theta  So one may show rigorously which we'll do in the next lesson that this is actually almost exactly the same as Q-learning  We'll get to that in a moment  So this introduces us to the concept of deep Q-learning  which as I said it was at the heart of what AlphaGo did  In our next lesson we're going to dive in and understand it further 
kyCibTQQQBE,Deep Q Learning Based on Images  In our previous lesson  we introduced the concept of deep q-learning  Now what I would like to do is to talk more about deep q-learning for the special case in which the reinforcement learning is done based upon images  And as we talked about in the AlphaGo scenario  the machine was able to learn how to play a board game  The way that that was done was a picture was taken off the board of the Go board at any point in time  And then that image  that picture of the Go board was then given to a reinforcement learning algorithm analyzed via deep q-learning and then it decided the next optimal action to take in playing the board game  And so therefore  this concept of doing deep q-learning based upon images is fundamental  It's in fact  you could also imagine that this would be used in medicine where for example  a radiologist might have access to an image of a patient  And then based upon that image the goal is to determine what's the next best course of action for that patient  And so this analysis of reinforcement learning based upon images is very fundamental  It's at the heart of deep q-learning and so we'd like to look at this in some more detail  So if we look at the function that we're trying to optimize  again  Q is going to be represented by a deep neural network with parameters theta  The input to that network is the state  here the state is going to be an image  It's actually a picture so it might be the picture of a Go board  And then the action is the next action you're going to take in the game  What you would like to do is to have that cue function be good at estimating the immediate reward rt  plus a discounted version of the future award  And so this cost function  this u is trying to make Q match the non myopic reward  And so as we've talked about  what we're going to do is we're going to represent that Q in terms of a deep neural network  Where s  the state which in this case is going to be an image is input to the neuron network  And then at the top of the neural network the output is the value of that state as reflected by the different actions  And so the  and here we are showing it for three actions  And so again  the goal is to try to fit that deep neural network to the non myopic reward  which is characterized by the next immediate reward as well as a discounted future reward  If you think about this  this can be viewed as supervised learning  The reason that that is true is that we in the context of experiencing the world  So remember that the way that reinforcement learning works is we are in a state  we take an action  we transition to a new state and we see a new reward  And so we see the immediate reward which is represented by rt  we have access  we get to see in learning that immediate reward rt  We also get to see which state we transition into which is State St + 1  And so then we can evaluate the value of that new state based upon maximizing over actions  And so therefore  the immediate reward rt  and the previous model parameters theta old and the new state St + 1 are available to us  And for the second term  the non myopic reward is observable  And so therefore  this can be viewed as supervised learning where we're trying to teach a machine through the supervision to match the Q function  which is a deep neural network to the observed non myopic reward  In the context of imaging  the way that that Q function is going to be represented is in terms of a deep convolutional neural network  And so we've talked about this elsewhere in this class  So I won't dwell on that too much  but I just want to point out that this deep convolutional network which we have introduced elsewhere in this class now makes an appearance in the context of q-learning  And so if we look at this figure  the Q neural network is represented by from left to right  a deep neural network  Where the input is the image and then we have a sequence of convolutional layers  and then at the end we have a sequence of so-called fully connected layers  So the input is the state and then the output is the value of each action for the particular state or particular image as input  So this is a very nice use of deep convolutional neural networks  And then what we would like to do is to take this deep convolutional neural network  And we're going to learn the parameters  theta  by trying to match the output of that deep convolutional neural network to the non myopic reward  which is shown in the bracketed part of this equation  And so interestingly  this basic construct which is relatively easy to understand with the tools that we have  So the deep convolutional network is something that we've talked about elsewhere and in this class  We now have the tools to actually develop an AlphaGo algorithm  And it turns out that the original AlphaGo technology which subsequently has been improved significantly over time  but the basic construct was based upon this deep Q network that we've talked about here  And so it's interesting that even in a relatively introductory class to deep learning  which we've shown here in the context of reinforcement learning  We basically have introduced the underlying technology that was responsible for one of the most significant breakthroughs in machine learning in recent decades  And in addition to applying the Go game this same technology has been used to automatically play many different digital game  here these are different types of Atari games  And again  the machine based upon this deep q-learning framework has been able to play these games with a proficiency  which is better than the finest players of Atari games in the world  So this is a very  very important technology  The use of these games  the Go game or these Atari games  of course  these are not very practical applications in the real world  But if you can play a game like Go very well  If a machine can play a game like Go very well or if a machine can play a these Atari games very well  These are basically adaptive algorithms to choose the best next course of action  And so these games are ways in which we develop the technology but where it will ultimately be used where it can make an impact on people's lives  Is to for example  augment clinicians and we talked about at the beginning of these lessons about how these these reinforcement learning could be used for automatic control of medication for diabetic patients to perhaps help clinicians with that task  So this technology which has been demonstrated using games has very profound potential applications that is likely to impact people's lives across the globe 
OAOGY8wZOjI,Connecting Deep Q Learning with Conventional Q Learning  So this is our last lesson in the context of reinforcement learning  This lesson is by far the most technical and mathematical  and consequently  it may not be for everybody  So this is perhaps a bonus lesson for those people who have a deeper mathematical and technical background  If you don't have that background  it's okay  you don't really have to watch this  but for those who would like to get a deeper understanding of reinforcement learning  this last lesson is meant to achieve that  So if we think about what we've done in the last series of lessons in the context of reinforcement learning  we basically have done two things  We've done something called Q-learning  and when we introduced Q-learning  we assumed that we could represent the states and the actions in a quantized form or discretized form  which meant that we could represent the Q function  Q  which was a function of the state and action  we can represent that as a table  as a matrix  So the first thing that we did in our early lessons was introduce this concept of Q-learning  which is assumed that we had a discrete and finite set of states and actions  We then talked about the fact that there's some serious limitations to that because the number of states that one might have could be enormous  So if you think about the go game  if you had to quantize all of the different states of the go board  it could be just absolutely enormous  and so therefore  Q-learning is really not going to be a good solution  So the next thing that we talked about to generalize Q-learning was this concept of deep Q-learning or deep Q-networks where instead of representing the Q function as a matrix or as a table  we represented it as a deep neural network  and then our goal was to learn the parameters of that deep neural network  So we've done two things  Q-learning  where we had a quantized tabular representation of the Q function  and Deep Q-networks  where the Q function was represented by deep neural network  When we looked at the equations for the deep Q-learning setup  there was some hints that this might be similar to Q-learning  So what we're going to do in this last lesson is to dig in a little bit deeper into the mathematics and show that indeed deep Q-networks have a very intimate relationship to Q-learning  and in fact  you could argue that deep Q-networks are a form of Q-learning  So recall  that when we developed our deep Q-networks  so deep DQN is Deep Q-network  which is a representation of the Q function  the update equation is shown as the first equation  and so this equation was the result of doing gradient descent  Remember  that the Q function is a function of the parameters Theta  In the bottom equation  we take the new parameters Theta new  and we explicitly plug them into the Q function  So in the top equation  we have our update rule for the parameters Theta  Theta new  and then on the bottom what we're going to do is  we're going to take those parameters  Theta new  and we're going to plug them in to the Q function  So that's what the bottom equation reflects  Again  I mentioned that this session or this lesson is definitely the most technically involved  but hopefully  I expect a fairly substantial group of you will be able to follow along  So in any case  the bottom equation reflects the Q function with our new representation of the model parameters  which are based upon the gradient descent rule  which is the first equation  So to continue  what I've done here is  I have repeated that Q function with the representation of Theta new  So that's exactly the equation that we had at the bottom of the previous slide  What we're going to do is  we're going to define something which I call Delta Theta  which is the change in Theta  So if you look at the first equation  you can see that the Q function is represented in terms of parameters Theta  which are equal to the old parameters  Theta sub old  plus some change to the parameters  which we call Delta Theta  and that expression Delta Theta directly comes from the first equation  So from this  we can write the first equation  which is relatively complicated  We can write it simply as the third equation here where the Q function at parameters Theta new is equal to the Q function at Theta parameters old  plus Delta Theta  and Delta Theta are defined in the equation above  So this representation is just a compact way of representing this  Now  for those of you with a little bit of background in calculus and the mathematics  you'll recall something called the Taylor's series expansion  So again  this is for people with a little bit more mathematical detail  some understanding of calculus  but this last equation is a mathematical fact  and it's called a first-order Taylor series expansion  What this says is that  if the change in parameters Delta Theta is relatively small  then we can approximate the Q function at parameters Theta old plus Delta Theta  we can approximate that in terms of a sum of two terms  The first term is the Q function evaluated at the old parameters  and then the second term is the Delta Theta  which is the change in the parameters that we defined in the second equation  we take the transpose of that and multiply it by the gradient of the Q function  So this expression is a fundamental expression of calculus  It is accurate as long as the change in parameters Delta Theta is relatively small  So if we now look at this bottom equation and we plug in the expression for Delta Theta  which is the second equation  So we're going to take the bottom equation  we're going to plug in the definition of Delta Theta  and what we see is that the Q function evaluated at the new parameters Theta old plus Delta Theta is equal to the Q function at the old parameters plus Alpha  which is analogous to a learning rate  times an expression in brackets  which is the immediate reward plus discounted future rewards minus the old value of the Q function  and then that is multiplied by something called the L2 norm of the gradient  which is the last term  So now  if we define a new parameter Alpha prime  which is equal to Alpha times the L2 norm of the gradient of the Q function evaluated at the old parameters  and we now plug that definition of Alpha prime  the first equation now becomes the bottom equation  So now Alpha prime is defined in the way that we had  and what we see is that the new expression for the Q function is a weighted combination of the old Q function  and a weighted version of the immediate reward plus a discounted version of the expected future rewards  So this bottom equation is exactly Q-learning  So what this tells us is that in the regime when the change of the deep Q-network parameters Delta Theta  when that is small  what we're effectively doing is what this bottom equation reflex  which is essentially Q-learning  and so what we have seen is that at least in the context of a first-order Taylor series expansion  deep Q-learning is intimately connected to Q-learning  So this summarizes this  So we effectively are manifesting something exactly like Q-learning when we are doing deep Q-learning particularly in the regime in which the change of parameters Delta Theta is small  So now  the top equation  which is Q-learning  we don't actually implement this because we don't have the Q-function in tabular form  When we do the update in DQN  the deep Q-network  we do not actually implement this top equation  which is Q-learning  What we do is we implement the bottom equation  which is the update of the model parameters  But what we've shown is that doing the bottom  which is the update of the model parameters and the regime in which the change in the model parameter is small  this is effectively exactly Q-learning  So what this shows is that this deep Q-network  which we adopted in the latter part of these lessons  is intimately connected to what we talked about at the beginning of these lessons  the Q-learning 
Q7hlcuCGbxE,Machine Learning Lifecycle  Now let's walk through the phases of developing publishing a machine learning model  Think of MLOps as a lifecycle management discipline for machine learning  Its goal is a balanced process for approach to the management of resources  data  code  time  and quality to achieve business objectives in meet regulatory concerns  Some of the concepts from DevOps translate directly to MLOps  When software developers work on a project  they don't all work on the same code at the same time  Instead  they check out the code they intention workout from a code safe  they merge it back with their task is finished  Before the code is returned to the safe  the developer checks that nothing has changed in the main version and then unit test the updates before merging the code back together  The more frequently these changes are merged with the main code  the less chance that is of divergence  This process is called Continuous Integration or CI  In a busy development team  this happens tens of times a day  Another process favored by developers is Continuous Delivery or CD  This is a method for building  testing in releasing software in short cycles  Done this way  the main development code is almost always production ready  it can be released into the light environment at anytime  If it is not done this way  the main code is like a race car with its wheels off and its engine out  you can go fast  but only after is put back together  Continuous delivery can be done either manually or automatically  Continuous integration of source code  unit testing  integration testing in continuous delivery of the software to production  or important processes in machine learning operations too  But there is another important aspect to MLOps  that's right  data  Unlike conventional software that can be relied on to do the same thing every time on the ML model  can go off  By this we mean that its projective power wins as data profile changes  which they inevitably do  So we can build on continuous integration and continuous delivery  and introducing new term Continuous Training or CT  Continuous training is the process of monitoring  measuring  retraining and serving the models  MLOps differs from DevOps in important ways too  Continuous integration is no longer only about testing  invalidating code and components  but also about testing and invalidating data  data schemas and models  It is no longer about a single software package or service  but a system  The ML training pipeline that should automatically deploy another service  the model prediction service  Uniquely  ML is also concerned with automatically monitoring  retraining and serving the models  Another concept that transfers well from software development machine learning is technical debt  Software developers are familiar with time  resources  and quality trade offs  They talk about technical debt  which is the backlog of free work that builds up  because sometimes they have to compromise on quality in order to develop code quickly  They understand that  although there may have been good reasons to do this  they have to go back and fix things later  This is an engineering version of the common saying  putting off until tomorrow what is better than today  there is a price to pay  Machine learning could arguably be considered the high interest credit card of technical debt  These means that developing and deploying in the ML system can be relatively fast and cheap  but maintaining it overtime can be difficult and expensive  The real challenge isn't building an ML model  it is building an integrated an ML system and continuously operating it in production  Just like a high interest credit card  that technical debt with machine learning compounds  it can be incredibly expensive and difficult to pay down  Machine learning systems can be thought of as a special type of software system  So operationally they have all the challenges of software development  plus a few of their own  Some of these include  multi-functional teams  because ML projects will probably have developers in data scientists working on data analysis  model development and experimentation  Much in functional teams can create it all management challenges  Machine learning is experimental in nature  You must constantly try new approaches with the data  the models  and parameter configuration  The challenge is tracking what worked and what didn't in maintaining reproducibility while maximising causerie usability  Another consideration is attaching and the mouse system is more involved than passing other software systems  Because you're validating data  parameters  and code together in a system instead of unit testing methods and functions  ML systems deployment isn't a simple as deploying enough line train  ML Model as production service  ML systems can require you to deploy a multistep pipeline to automatically retrain and deploy models  And finally  concerns with concept drift and consequent model decay should be addressed  Data profiles constantly change  if something changes in the data input  the projective power of the modeling production will likely change with it  Therefore you need to track summary statistics of the data and monitor the online performance of your model to send notifications  or rollback with values deviate from your expectations  Technical batch builds up in the ML system for many reasons  So we'll be looking at ways to mitigate that throughout this course 
VugQE-SRHa0,MLOps Architecture and TensorFlow Extended Components  Now it is time to go over the main phases of a machine learning lifecycle and map them to the components or tasks within MLOps  When we look at machine learning projects  we identify three main phases  A discovery phase  a development phase  and a deployment phase  For the discovery phase  identifying the business need in its use case allows for a clear plan of what a machine learning model will help us achieve  This phase is crucial because it will establish the problem or task that needs to be solved and how solving it will affect the business and the users consuming the product or solution argumented by machine learning  This phase is also when data exploration happens  recognizing why datasets are needed  whether the needed data is readily available and sufficient to train a model  and whether external datasets would be beneficial and how to acquire them  All of these are considerations that involve the data exploration step  Then  depending on the tests to be performed  an algorithm is chosen by the data science team  The combination of data availability in algorithm along with the decision of buying versus building the solution  becomes an important consideration for feasibility assessment where the team tries to uncover any problems that may arise during the development phase  One example is that for the specific use case and question  the data is available historically but not for inference time  In that case  the particular scenario might make the use case infeasible for ML and the more through analysis may have to be performed before the use case can be pursued further  Another aspect of the discovery phase is prioritizing the different use cases that the business has that can become potential ML projects  but that discussion is out of the scope of this course  Now  for the development phase  you may ask  how does development start on this chart during data exploration? Shouldn't we wait until the result of the feasibility study? What happens in reality is that even for data exploration and algorithms selection  some proofs of concept will need to be developed  and that is what we refer to here  After the feasibility assessment gives the go-ahead  the real development starts  All the data steps such as cleaning  extracting  analyzing  and transforming  will be implemented during the data pipeline creation  The data pipeline evolves  ensuring that all the operation is needed on the data for both offline and streaming  for training and reference also will be performed consistently to avoid the rescue  After the data is ready  building and evaluating the model begins  I say begins because these steps may need a couple of iterations until the data scientist is happy with the results and ready to present them to the main stakeholders  Considerations include  the use case should be revisited because the learning algorithm isn't capable of identifying patterns on the data for that task  Data should be revisited because the model either needs more of it or needs additional aspects and your features maybe from the existing data  Some additional transformations are needed to improve the model quality  Or even a different algorithm is perceived as a better choice  There are numerous possibilities  This iteration will happen as many times as needed until the model reaches the desired performance  After results are presented and stakeholders are satisfied with how the model is performing  it is time to plan for model deployment  This is when the following questions will likely arise  Which platform should host my model? Which service should I pick for model serving? How many loads should the cluster have so you can scale and take care of all the demand in a cost effective manner? Operationalizing and monitoring the model will allow for maintainability and avoiding model decay  as we discussed  Having a strategy in place to detect concepts of data drifts will allow signaling when the model should be retrain or data should be adjusted or argumented  Ensuring that your pipeline considers all the necessary tasks for health checks and alerts is the most effective way to avoid your satisfaction from the user's consuming your models projections  Focusing on the development and the deployment phases  we see that they have multiple steps  For data exploration  for example  that is data extraction  data analysis  and data preparation  The model building comprise a streaming  evaluation  and validation  Deployment requires hosting the train model and serving it and having a prediction service ready to handle requests  Finally  monitoring to allow for continuous evaluation and training based on the performance results at a given point  The level of automation of this steps define the maturity of the ML process  which reflects the velocity of training your model is giving you data  or training your model is giving new validations  Many ML professionals build and deploy the ML models manually  We call this maturity level zero  All the data scientists perform continuous training of their models by automating the ML pipeline  This is maturity Level 1  Finally  the most mature approach completely automates and integrates the ML training  validation  and deployment phases  This is maturity Level 2  You and your team have probably begun or still are at maturity Level 0 and that's nothing to worry about  Our goal here is to help you automate your processes and move up the automation ladder with the suite of tools and services available at Google Cloud  Stay tuned and have fun 
UYv36HV3Sng,Introduction to Containers  Let's start by introducing containers  In this video  you'll learn about the key features of containers and the advantages of using containers for application deployment compared to alternatives such as deploying apps directly to virtual machines  You'll learn about Google's Cloud Build  and then you can see how you can use it to build and manage your application images  It's now not very long ago  the default way to deploy an application was on its own physical computer  To set one up  you'd find some physical space  power  cooling  network connectivity for it  and then install an operating system  any software dependencies  and then finally the application itself  If you need more processing power  redundancy  security  or scalability  what'd you do? Well you'd have to simply add more computers  It was very common for each computer to have a single-purpose  For example  a database  web server  or content delivery  This practice as you might imagine  wasted resources and it took a lot of time to deploy and maintain and scale  It also wasn't very portable at all  Applications were built for a specific operating system and sometimes even for specific hardware as well  In comes the dawn of virtualization  Virtualization helped by making it possible to run multiple virtual servers and operating systems on the same physical computer  A hypervisor is the software layer that breaks the dependencies of an operating system with its underlying hardware  and allow several virtual machines to share that same hardware  KVM is one well-known hypervisor  Today you can use virtualization to deploy new servers fairly quickly  Now adopting virtualization means that it takes us less time to deploy new solutions  we waste less of the resources on those physical computers that we're using  and we get some improved portability because virtual machines can be imaged and then moved around  However  the application  all of its dependencies and operating system are still bundled together and it's not very easy to move from a VM from one hypervisor product to another  Every time you start up a VM  it's operating system still takes time to boot up  Running multiple applications within a single VM also creates another tricky problem  applications that share dependencies are not isolated from each other  the resource requirements from one application  can starve out other applications of the resources that they need  Also  a dependency upgrade for one application might cause another to simply stop working  You can try to solve this problem with rigorous software engineering policies  For example  you could lock down the dependencies that no application is allowed to make changes  but this leads to new problems because dependencies do need to be upgraded occasionally  You can add integration tests to ensure that applications work  Integration tests are great  but dependency problems can cause new failure modes that are harder to troubleshoot  and it really slows down development if you have to rely on integration tests to simply just perform basic integrity checks of your application environment  Now  the VM-centric way to solve this problem is to run a dedicated virtual machine for each application  Each application maintains its own dependencies  and the kernel is isolated  So one application won't affect the performance of another  One you can get as you can see here  is two complete copies of the kernel that are running  But here too we can run into issues as you're probably thinking  Scale this approach to hundreds of thousands of applications  and you can quickly see the limitation  Just imagine trying to do a simple kernel update  So for large systems  dedicated VMs are redundant and wasteful  VMs are also relatively slow to start up because the entire operating system has to boot  A more efficient way to resolve the dependency problem is to implement abstraction at the level of the application and its dependencies  You don't have to virtualize the entire machine or even the entire operating system  but just the user space  Again  the user space is all the code that resides above the kernel  and includes the applications and their dependencies  This is what it means to create containers  Containers are isolated user spaces per running application code  Containers are lightweight because they don't carry a full operating system  they can be scheduled or packed tightly onto the underlying system  which is very efficient  They can be created and shut down very quickly because you're just starting and stopping the processes that make up the application and not booting up an entire VM and initializing an operating system for each application  Developers appreciate this level of abstraction because they don't want to worry about the rest of the system  Containerization is the next step in the evolution of managing code  You now understand containers as delivery vehicles for application code  they're lightweight  stand-alone  resource efficient  portable execution packages  You develop application code in the usual way  on desktops  laptops  and servers  The container allows you to execute your final code on VMs without worrying about software dependencies like application run times  system tools  system libraries  and other settings  You package your code with all the dependencies it needs  and the engine that executes your container  is responsible for making them available at runtime  Containers appeal to developers because they're an application-centric way to deliver high performance and scalable applications  Containers also allow developers to safely make assumptions about the underlying hardware and software  With a Linux kernel underneath  you no longer have code that works in your laptop but doesn't work in production  the container's the same and runs the same anywhere  You make incremental changes to a container based on a production image  you can deploy it very quickly with a single file copy  this speeds up your development process  Finally  containers make it easier to build applications that use the microservices design pattern  That is  loosely coupled  fine-grained components  This modular design pattern allows the operating system to scale and also upgrade components of an application without affecting the application as a whole 
WnTQTSvpmOs,Containers and Container Images  An application and its dependencies are called an image  A container is simply a running instance of an image  By building software into Container images  developers can easily package and ship an application without worrying about the system it will be running on  You need software to build Container images and to run them  Docker is one tool that does both  Docker is an open source technology that allows you to create and run applications in containers but it doesn't offer a way to orchestrate those applications at scale like Kubernetes does  In this course  we'll use Google's Cloud build to create Docker formatted Container images  Containers are not an intrinsic primitive feature of Linux  Instead their power to isolate workloads is derived from the composition of several technologies  One foundation is the Linux process  Each Linux process has its own virtual memory address phase  separate from all others  Linux processes are rapidly created and destroyed  Containers use Linux namespaces to control what an application can see  process ID numbers  directory trees  IP addresses  and more  By the way  Linux namespaces are not the same thing as Kubernetes Namespaces  which you'll learn more about later on in this course  Containers use Linux cgroups to control what an application can use  its maximum consumption of CPU time  memory  IO bandwidth  other resources  Finally  Containers use Union File Systems to efficiently encapsulate applications and their dependencies into a set of clean minimal layers  Now let's see how that works  A container image is structured in layers  The tool you use to build the image reads instructions from a file called  ''The Container manifest '' In the case of a Docker formatted Container Image  that's called a Docker file  Each instruction in the Docker file specifies a layer inside the container image  Each layer is  ''Read only '' When a Container runs from this image  it will also have a writable ephemeral top-most layer  Let's take a look at a simple Docker file  This Docker file will contain four commands  each of which creates a layer  At the end of this discussion  I'll explain why this Docker file is a little oversimplified for modern use  The From statement starts out by creating a base layer pulled from a public repository  This one happens to be the Ubuntu Linux runtime environment of a specific version  The Copy command adds a new layer containing some files copied in from your build tools current directory  The Run command builds your application using the make command and puts the result of the build into a third layer  Finally  the last layer specifies what command to run within the container when it's launched  Each layer is only a set of differences from the layer before it  When you write a Docker file  you should organize the layers least likely to change through to the layers that are most likely to change  By the way  I promised that I'd explain how the Docker file example you saw here is oversimplified  These days  the best practice is not to build your application in the very same container that you ship and run  After all  your build tools are at best just cluttered and deployed Container and at worst  are an additional attack service  Today  Application Packaging relies on a multi-stage build process in which one Container builds the final executable image  A separate container receives only what's needed to actually run the application  Fortunately for us  the tools that we use support this practice  When you launch a new container from an image  the Container Runtime adds a new writable layer on the top of the underlying layers  This layer is often called the Container layer  All changes made to the running container  such as writing new files  modifying existing files  and deleting files are written to this thin writable Container layer in their ephemeral  when the Containers deleted the contents of this writeable layer are lost forever  The underlying Container Image itself remains unchanged  This fact about Containers has an implication for your application design  Whenever you want to store data permanently  you must do so somewhere other than a running container image  You'll learn that in the several choices that you can choose from in this specialization  Because each Container has its own writable Container layer and all changes are stored in this layer  Multiple Containers can share access to the same underlying image and yet have their own data state  The diagram here shows multiple containers sharing the same Ubuntu 15 04 image  Because each layer is only a set of differences from the layer before it  you get smaller images  For example  your base application image  maybe 200 megabytes  but the difference  the next point release might only be 200 kilobytes  When you build a container  instead of copying the whole image  it creates a layer with just the differences  When you run a container  that Container Runtime pulls down the layers it needs  When you update  you only need to copy the difference  this is much faster than running a new virtual machine  It's very common to use publicly available open source Container images as a base for your own images or for unmodified use  For example  you've already seen the Ubuntu Container Image  which provides an Ubuntu Linux environment inside of a container  Alpine is a popular Linux environment in a container  noted for being very  very small  The NginX web server is frequently used in its Container packaging  Google maintains a Container Registry  gcr Io  This Registry contains many public  open source images and Google Cloud customers also use it to store their own private images in a way that integrates well with Cloud IAM  Google Container Registry is integrated with Cloud IAM  So for example  you can use it to store your images that aren't public  Instead  they're private to your project  You can also find Container images and other public repositories  Docker Hub Registry  GitLab and others  The open source Docker command is a popular way to build your own Container images  It's widely known and widely available  One downside  whoever a building Containers with a Docker command is that you must trust the computer that you do your builds on  Google provides a managed service for building Containers that's also integrated with Cloud IAM  This service's called  ''Cloud Build '' and we'll use it in this course  Cloud build can retrieve the source code for your builds from a variety of different storage locations  Cloud Source Repositories  Cloud Storage  which is GCP is Object Storage service or git compatible repositories like GitHub and Bitbucket to generate a buildup Cloud Build  you define as series of steps  For example  you can configure build steps to fetch dependencies  compile source code  run integration tests  or use tools such as Docker  Gradle  and Maven  Each build step and Cloud build runs in a Docker container  Then Cloud build can deliver your newly built images to various execution environments  not only GKE  but also App Engine and Cloud Functions 
GQvNv9uhmbw,Introduction to Kubernetes  Now  let's introduce a popular container management and orchestration solution called Kubernetes  Let's say your organization is really embraced the idea of containers  Because containers are so lean  your coworkers are creating them in numbers far exceeding the counts of virtual machines you used to have and the applications running in them need to communicate over the network  But you don't have a network fabric that lets containers find each other  You need help  How can you manage your container infrastructure better? Kubernetes is an open source platform that helps you orchestrate and manage your container infrastructure On-premises or in the Cloud  So what is Kubernetes? It's a container centric management environment  Google originated it and then donated it to the open source community  Now  it's a project of the Vendor Neutral Cloud Native Computing Foundation  It automates the deployment scaling  load balancing  logging  monitoring  and other management features of containerized applications  These are the features that are characteristic of a typical platform as service solutions  Kubernetes also facilitates the features of an infrastructure as a service  such as allowing a wide range of user preferences and configuration flexibility  Kubernetes supports declarative configurations  When you administer your infrastructure declaratively  you describe the desired state you want to achieve instead of issuing a series of commands to achieve that desired state  Kubernetes job is to make the deployed system conform to your desired state and then keep it there in spite of failures  Declarative configuration saves you work  Because the system is desired state is always documented  it also reduces the risk of error  Kubernetes also allows imperative configuration in which you issue commands to change the system state  But administering Kubernetes as scale imperatively  will be a big missed opportunity  One of the primary strengths of Kubernetes is its ability to automatically keep a system in a state that you declare  Experienced Kubernetes administrators use imperative configuration only for quick temporary fixes and as a tool in building a declarative configuration  Now that you know what Kubernetes is  let's talk about some of its features  Kubernetes supports different workload types  It supports stateless applications such as an Nginx or Apache web server  and stateful applications where user in session data can be stored persistently  It also supports batched jobs and demon tasks  Kubernetes can automatically scale in and out containerized applications based on resource utilization  You can specify resource requests levels and resource limits for your workloads and Kubernetes will obey them  These resource controls like Kubernetes  improve overall workload performance within the cluster  Developers extend Kubernetes through a rich ecosystem of plugins and add-ons  For example  there's a lot of creativity going on currently with Kubernetes custom resource definitions which bring the Kubernetes declarative Management Model to amazing variety of other things that need to be managed  The primary focus of this specialization though  is architecting with Kubernetes because it's provided as a service by Google Cloud  So extending Kubernetes is not within our scope  Because it's open source  Kubernetes also supports workload portability across On-premises or multiple Cloud service providers such as GCP and others  This allows Kubernetes to be deployed anywhere  You can move Kubernetes workloads freely without a vendor login 
TbRVJKDC1rM,Introduction to Google Kubernetes Engine  Google cloud's managed service offering for Kubernetes is called Google  Kubernetes Engine or GKE  So why do people choose it? What if you begun using Kubernetes in your environment  but the infrastructure has become too much of a burden for you to maintain? Is there anything within Google cloud platform that can help you? Absolutely  totally  that's going to be Google Kubernetes Engine or GKE  GKE  let's talk about what it can do  It will help you deploy  manage and scale Kubernetes environments for your containerized applications on GCP  More specifically  GKE is a component of the GCP compute offerings  It makes it easy to bring your Kubernetes workloads into the cloud  GKE is fully managed  which means that you don't have to provision the underlying resources  GKE uses a container-optimized operating system  These operating systems are maintained by Google  And they're optimized to scale quickly and with a minimal resource footprint  The container-optimized OS  it will be discussed later on in this course  When you use GKE  you start by directing the service to instantiate a Kubernetes system for you  This system is called a cluster  GKE's auto upgrade feature can be enabled to ensure that your clusters are automatically upgraded with the latest and greatest version of Kubernetes  The virtual machines that host your containers inside of a GKE cluster are called nodes  If you enable GKE's auto repair feature  the service will automatically repair unhealthy nodes for you  It will make periodic health checks on each node in the cluster  If a node is determined to be unhealthy and requires repair  GKE would drain the node  In other words  it will cause it's workloads to gracefully exit and then recreate that node  Just as Kubernetes support scaling workloads  GKE support scaling the cluster itself  GKE seamlessly integrates with Google Cloud build and container registry  This allows you to automate deployment using private container images that you've securely stored in container registry  GKE also integrates with Google's identity and access management  which allows you to control access through the use of accounts and role permissions  Stackdriver is Google Cloud system for monitoring and management for services  containers  applications  and infrastructure  GKE integrates with Stackdriver monitoring to help you understand your applications performance  GKE is integrated with Google virtual private clouds or VPCs  it makes use of GCP's networking features  And finally  the GCP console provides insights into GKE clusters and the resources  and it allows you to view  inspect and delete resources in those clusters  You might be aware that open source Kubernetes contains a dashboard  but it takes a lot of work to set it up securely  But the GCP console is a dashboard for GKE clusters and workloads that you don't have to manage  And it's more powerful than the Kubernetes dashboard 
myr_VzA3tS8,Compute Options Detail  In this last lesson  you learn more about the computing options available  In a previous module  I briefly introduced your choices for running compute workloads in GCP  Now that we know more about how containers work  we can compare these choices in more detail  The services are Compute Engine  GKE  App Engine  Cloud Run  and Cloud Functions  At the end of this lesson  you'll understand why people choose each  Compute Engine offers virtual machines that run on GCP  You can select predefined VM configurations  At the time this course was developed  these virtual machines could be as large as 160 V CPUs with more than 3 terabytes of memory  You could also create customized configurations to precisely match your performance and cost requirements  Virtual machines need block storage  Compute Engine offers you two main choices  persistent disks and local SSDs  Persistent disks offer network stores that can scale up to 64 terabytes and you can easily take snapshots of these disks for backup and mobility  You could also choose local SSDs which enable very high input/output operations per second  You can place your Compute Engine workloads behind global load balancers that support autoscaling  Compute Engine offers a feature called managed instance groups  With these you can define resources that are automatically deployed to meet demand  GCP enables fine grained control of costs of Compute Engine resources by providing per second billing  This granularity helps reduce your costs when deploying compute resources for short periods of time  such as batch processing jobs  Compute Engine offers preemptible virtual machines which provide significantly cheaper pricing for your workloads that can be interrupted safely  So why do people choose Compute Engine? With Compute Engine you have complete control over your infrastructure  You can customize operating systems and even run applications that rely on a mix of operating systems  You can easily lift and shift your on-premises workloads into GCP without rewriting your applications or making any changes  Compute Engine is the best option when other computing options don't support your applications or requirements  App Engine has a completely different orientation from Compute Engine  App Engine is a fully managed application platform  Using App Engine means zero server management and zero configuration deployments  So if you're a developer  you can focus on building applications and not really worrying about the deployment part  So you can simply use your code and App Engine will deploy that required infrastructure for you  App Engine supports popular languages like Java and Node js  Python  PHP  C#   NET  Ruby  and Go  You could also run container workloads  Stackdriver monitoring  logging  and diagnostics  such as debugging and error reporting are also tightly integrated with App Engine  You can use Stackdriver's real time debugging features to analyze and debug your source code  Stackdriver integrates with tools such as Cloud SDK  cloud source repositories  IntelliJ  Visual Studio  and PowerShell  App Engine also supports version control and traffic splitting  App Engine is a good choice if you simply want to focus on writing code  and you don't want to worry about building the highly reliable and scalable infrastructure that'll run on  You can just focus on building applications instead of deploying and managing the environment  Use cases for App Engine include websites  mobile apps  gaming backends  and as a way to present a RESTful API to the Internet  What's a RESTful API? In short  it's an application program interface that resembles the way a web browser interacts with the web server  RESTful APIs are easy for developers to work with and extend  And App Engine makes them easy to operate  Finally  the main topic of this course  Google Kubernetes Engine  We learned that Kubernetes is an orchestration system for applications in containers  It automates deployment  scaling  load balancing  logging  and monitoring  and other management features  Google Kubernetes Engine extends Kubernetes management on GCP by adding features and integrating with other GCP services automatically  GKE supports cluster scaling  persistent disks  automated updates to the latest version of Kubernetes  and auto repair for unhealthy nodes  It has built-in integration with cloud build  container registry  Stackdriver monitoring  and Stackdriver logging  Existing workloads running within on-premise clusters can easily be moved on to GCP  There's no vendor login  Overall  GKE is very well suited for containerized applications  Cloud-native distributed systems and hybrid applications  Kubernetes and GKE are discussed in depth throughout this course  Web requests  or cloud Pub/Sub events  Cloud Run is serverless  it distracts way all the infrastructure management so you can focus on developing applications  It's built on Knative  an open source Kubernetes based platform  It builds  deploys  and manages modern stateless workloads  Cloud Run gives you the choice of running your containers easier with fully managed or in your own GKE cluster  Cloud Run enables you to run request or event driven stateless workloads without having to worry bout servers  It abstracts away all the infrastructure management such as provisioning  configuring  managing those servers so you can focus on just writing code  It automatically scales up and down from zero depending upon traffic almost instantaneously  so you never have to worry about scale configuration  Cloud Run charges you for only the resources that you use calculated down to the nearest 100 milliseconds  So you don't have to pay for those over provisioned resources  With Cloud Run you can choose to deploy your stateless containers with a consistent developer experience to a fully managed environment or to your own GKE cluster  This common experiences enabled by Knative an open API and runtime environment built on top of Kubernetes  And it gives you the freedom to move your workloads across different environments and platforms  either fully managed on GCP  on GKE or anywhere a Knative runs  Cloud Run enables you to deploy stateless containers that listen for requests or events delivered via HTTP requests  With Cloud Run  you can build your applications in any language using whatever frameworks and tools you wish and deploy them in seconds without having to manage and maintain that server infrastructure  Cloud Functions is an event-driven  serverless compute service for simple single purpose functions that are attached to events  In Cloud Functions  you simply upload your code written in JavaScript or Python  or Go and then GCP will automatically deploy the appropriate computing capacity to run that code  These servers are automatically scaled and are deployed from highly available and a fault-tolerant design  You're only charged for the time that your code runs  For each function  invocation memory and CPU use is measured in the 100 millisecond increments  rounded up to the nearest increment  Cloud Functions also provides a perpetual free tier  So many cloud function use cases could be free of charge  With Cloud Functions  your code is triggered within a few milliseconds based on events  For example  a file is uploaded to Google cloud storage or a message is received from Cloud Pub/Sub  Cloud Functions can also be triggered based on HTTP endpoints that you define  and events in the fire based mobile application back end  What are some of the use cases for Cloud Functions? They're generally used as part of a microservices application architecture  You can also build symbols  serverless  mobile IoT backends  or integrate with third party services and APIs  Files uploaded into your GCS bucket can be processed in real time  Similarly  the data can be extracted  transformed and loaded for querying in analysis  GCP customers often use Cloud Functions as part of intelligent applications  such as virtual assistance  video or image analysis  and sentiment analysis  So which compute service should you adopt? A lot depends on where you're coming from  If you're running applications on physical server hardware  it will be the path of least resistance to move into compute engine  What if you're running applications in long-lived virtual machines in which each VM is managed and maintained? In this case  you'll also find moving to compute engine is the quickest GCP services for getting your applications to the cloud  What do you want to to think about operations at all? Well  App Engine and Cloud Functions are good choices  You can learn more about the differences between App Engine and Cloud Functions in the courses under a developing applications with Google Cloud platform  I hope that this course so far is help you understand why software containers are so beneficial  Containerization is the most efficient  importable way to package you an application  The popularity of containerization is growing very fast  In fact  both Compute Engine and App Engine can launch containers for you  Compute Engine will accept the container image from you and launch a virtual machine instance that contains it  You can use Compute Engine technologies to scale and manage the resulting VM  And App Engine flexible environment will accept the container image from you and then run it with the same No-ops environment that App Engine delivers for code  But what if you want more control over your containerized workloads than what App Engine offers? And denser packing than what Compute Engine offers? That increasingly popular use case is what GKE is designed to address  The Kubernetes paradigm of container orchestration is incredibly powerful  and its vendor neutral  and a abroad and vibrant community is developed all around it  Using Kubernetes as a managed service from GCP saves you work and let's you benefit from all the other GCP resources too  You can also choose Cloud Run to run stateless containers on a managed compute platform  And of course  if you're already running Kubernetes in your on-premises data centers  moving your GKE is a great choice  Because you'll be able to bring along both your workloads and your management approach 
DU7IZMciHJE,Kubernetes Concepts  In this lesson  we'll lay out the fundamental components of the Kubernetes operating philosophy  To understand how Kubernetes works  there are two related concepts you need to understand  The first is the Kubernetes object model  Each thing Kubernetes manages is represented by an object  You can view and change these objects  attributes  and state  The second is the principle of declarative management  Kubernetes expects you to tell it what you want  the state of the objects under each management to be  It will work to bring that state into being and keep it there  Formally  a Kubernetes object is defined as a persistent entity that represents the state of something running in a cluster  it's desired state and its current state  Various kinds of objects represent containerized applications  the resources that are available to them  and the policies that affect their behavior  Kubernetes objects have two important elements  You give Kubernetes an objects spec for each object you wanted to create  With this spec  you define the desired state of the object by providing the characteristics that you want  The object's status is simply the current state of the object provided by the Kubernetes control plane  By the way  we use this term  Kubernetes control plane  to refer to the various system processes that collaborate to make a Kubernetes cluster work  You'll learn about these processes later in this module  Each object is of a certain type or kind  as Kubernetes calls them  Pods are the basic building block of the standard Kubernetes model  and they're the smallest deployable Kubernetes object  You may be surprised to hear me say that  Maybe you were expecting me to say that the smallest Kubernetes object is the container  Not so  every running container and Kubernetes system is in a pod  A pod embodies the environment where the containers live  That environment can accommodate one or more containers  If there is more than one container in a pod  they are tightly coupled and share resources including networking and storage  Kubernetes assigns each pod a unique IP address  Every container within a pod shares the network namespace  including IP address and network ports  Containers within the same pod can communicate through localhost  The famous 127 0 0 1 IP address that you pranked your friends with back in the 1990s  A pod can also specify a set of Storage volumes to be shared among its containers  By the way  later in this specialization  you'll learn how pods can share storage with one another  not just within a single pod  Let's consider a simple example where you want three instances of the NginX Web server  each in its own container  running all the time  How is this achieved in Kubernetes? Remember that Kubernetes embodies the principle of declarative management  You declare some objects to represent those NginX containers  What kind of object? Perhaps pods  Now it is Kubernetes job to launch those pods and keep them in existence  But be careful  pods are not self healing  If we want to keep all our NginX Web servers not just in existence  but also working together as a team  we might want to ask for them using a more sophisticated object  I'll tell you how later in this module  Let's suppose that we have given Kubernetes a desired state that consists of three NginX pods always kept running  We did this by telling Kubernetes to create and maintain one or more objects that represent them  Now  Kubernetes compares the desired state to the current state  Let's imagine that our declaration of three NginX containers is completely new  The current state does not match the desired state  Kubernetes  specifically it's control plane  will remedy the situation because the number of desired pods running we declared as three and zero while presently running  three will be launched  The Kubernetes control plane will continuously monitor the state of the cluster  endlessly comparing reality to what has been declared and remedying the state as needed 
IPlP2R7l-Nc,The Kubernetes Control Plane  In the previous lesson  I mentioned the Kubernetes control plane  which is the fleet of cooperating processes that make a Kubernetes cluster work  Even though you'll only work directly with a few of these components  it helps to know about them and the role each plays  I'll build up a Kubernetes cluster part by part  explaining each piece as I go  After I'm done  I'll show you how a Kubernetes cluster running in GKE is a lot less work to manage than one you've provisioned yourself  Okay  here we go  First and foremost  your cluster needs computers  Nowadays  the computers that compose your clusters are usually virtual machines  They always are in GKE  but they could be physical computers too  One computer is called the master and the others are called simply  nodes  The job of the nodes is to run pods  The job of the master is to coordinate the entire cluster  We will meet its control plane components first  Several critical Kubernetes components run on the master  The single component that you interact with directly is the kube-APIserver  This component's job is to accept commands that view or change the state of the cluster  including launching pods  In this specialization  you will use the kubectl command frequently  This command's job is to connect to kube-APIserver and communicate with it using the Kubernetes API  Kube-API server also authenticates incoming requests  determines whether they are authorized  invalid  and manages admission control  But it's not just kubectl that talks with kube-APIserver  In fact  any query or change to the cluster state must be addressed to the kube-APIserver  Etcd is the cluster's database  Its job is to reliably store the state of the cluster  This includes all the cluster configuration data and more dynamic information such as what nodes are part of the cluster  what pods should be running  and where they should be running  You never interact directly with etcd  Instead  kube-APIserver interacts with the database on behalf of the rest of the system  Kube-scheduler is responsible for scheduling pods onto the nodes  To do that  it evaluates the requirements of each individual pod and selects which node is most suitable  But it doesn't do the work of actually launching pods onto nodes  Instead  whenever it discovers a pod object that doesn't yet have an assignment to a node  it chooses a node and simply write the name of that node into the pod object  Another component of the system is responsible for then launching the pods  and you will see it very soon  But how does kube-scheduler decide where to run a pod? It knows the state of all the nodes  and it will also obey constraints that you define on where a pod may run  based on hardware  software  and policy  For example  you might specify that a certain pod is only allowed to run on nodes with a certain amount of memory  You can also define affinity specifications  which cause groups of pods to prefer running on the same node  Or anti-affinity specifications which ensure that pods do not run on the same node  You'll learn more about some of these tools in later modules  Kube-controller manager has a broader job  It continuously monitors the state of a cluster through kube-APIserver  Whenever the current state of the cluster doesn't match the desired state  kube-controller manager will attempt to make changes  to achieve the desired state  It's called the controller manager because many Kubernetes objects are maintained by loops of code called controllers  These loops of code handle the process of remediation  Controllers will be very useful to you  To be specific  you all use certain kinds of Kubernetes controllers to manage workloads  For example  remember our problem of keeping three engine x pods always running  We can gather them together into a controller object called a deployment  that not only keeps them running  but also lets us scale them and bring them together underneath our front end  We'll meet deployments later in this module  Other kinds of controllers have system-level responsibilities  For example  node controller's job is to monitor and respond when a node is offline  Kube-cloud-manager manages controllers that interact with underlying cloud providers  For example  if you manually launched a Kubernetes cluster on Google Compute Engine  kube-cloud-manager would be responsible for bringing in GCP features like load balancers and storage volumes when you needed them  Each node runs a small family of control-plane components too  For example  each node runs a kubelet  You can think of kubelet as Kubernetes agent on each node  When the kube-APIserver wants to start a pod on a node  it connects to that node's kubelet  Kubelet uses the container runtime to start the pod and monitor its lifecycle  including readiness and liveness probes  and reports back to kube-APIserver  Do you remember our use of the term container runtime in the previous module? This is the software that knows how to launch a container from a container image  The world of Kubernetes offers several choices of container runtimes  but the Linux distribution  that GKE uses for its nodes  launches containers using container D  The runtime component of docker  Kube proxies job is to maintain network connectivity among the pods in a cluster  In open-source Kubernetes  it does so using the firewalling capabilities of IP tables  which are built into the Linux kernel  Later in this specialization  we will learn how GKE handles pod networking 
ruClwamT-R0,Google Kubernetes Engine Concepts  Next  we'll introduce concepts specific to Google Kubernetes Engine  That diagram of the Kubernetes is control-plane had a lot of components  didn't it? Setting up a Kubernetes cluster by hand is tons of work  Fortunately  there's an open-source command called cube ADM that can automate much of the initial setup of a cluster  But if a node fails or needs maintenance  human administrator has to respond manually  I suspect you can see why a lot of people like the idea of a managed service for Kubernetes  You may be wondering how that picture we just saw differs for GKE  Well  here it is  From the user's perspective  it's a lot simpler  GKE manages all the control-plane components for us  It's still exposes an IP address to which we send all of our Kubernetes API requests  But GKE takes responsibility for provisioning and managing all the master infrastructure behind it  It also abstracts away having a separate master  The responsibilities of the master are absorbed by GCP  and you are not separately billed for your Master  Now let's talk about nodes  In any Kubernetes environment  nodes are created externally by cluster administrators  not by Kubernetes itself  GKE automates this process for you  It launches Compute Engine virtual machine instances and registers them as nodes  You can manage node settings directly from the GCP console  You pay per hour of life of your nodes  not counting the master  Because nodes run on Compute Engine  you choose your node machine type when you create your cluster  By default  the node machine type is N1 standard one  which provides one virtual CPU and 3 75 gigabytes of memory  Google Cloud offers a wide variety of Compute Engine options  At the time this video was made generally available maximum was 96 virtual CPU cores  that's a moderately big Virtual Machine  You can customize your nodes  number of cores  and their memory capacity  You can select a CPU Platform  You can choose a baseline minimum CPU platform for the nodes or node pool  This allows you to improve node performance  GKE will never use a platform that is older than the CPU platform you specify  If it picks a newer platform  the cost will be the same as the specified platform  You can also select multiple node machine types by creating multiple node pools  A node pool is a subset of nodes within a cluster that share a configuration  such as their amount of memory or their CPU generation  Node pools also provide an easy way to ensure that workloads run on the right hardware within your cluster  You just label them with a desired node pool  By the way  node pool are GKE feature rather than a Kubernetes feature  You can build an analogist mechanism within open-source Kubernetes  but you would have to maintain it yourself  You can enable automatic node upgrades  automatic node repairs  and cluster auto-scaling at this node pool level  Here's a word of caution  Some of each node CPU and memory are needed to run the GKE and Kubernetes components that let it work as part of your cluster  For example  if you allocate nodes with 15 gigabytes of memory  not quite all of that 15 gigabytes will be available for use by pods  This module has a documentation link that explains how much CPU and memory are reserved  By default  a cluster launches in a single GCP Compute Zone with three identical nodes  all in one node pool  The number of nodes can be changed during or after the creation of the cluster  Adding more nodes and deploying multiple replicas of an application will improve an applications availability  but only up to a point  What happens if the entire Compute Zone goes down? You can address this concern by using a GKE regional cluster  Regional clusters have a single API endpoint for the cluster  However  it's masters and nodes are spread out across multiple Compute Engine zones within a region  Regional clusters ensure that the availability of the application is maintained across multiple zones in a single region  In addition  the availability of the master is also maintained so that both the application and management functionality can withstand the loss of one or more  but not all zones  By default are regional cluster is spread across three zones  each containing one master and three nodes  These numbers can be increased or decreased  For example  if you have five nodes in zone one  you will have exactly the same number of nodes in each of the other zones for a total of 15 nodes  Once you build a zonal cluster  you can't convert it into a regional cluster or vice versa  Regional and zonal GKE clusters can also be setup as a private cluster  The entire cluster that is the master and it's nodes are hidden from the public Internet  Cluster masters can be accessed by Google Cloud products such as Stack driver through an internal IP address  They can also be accessed by authorized networks through an external IP address  Authorize networks are basically IP address ranges that are trusted to access the master  In addition  nodes can have limited outbound access through private Google access  which allows them to communicate with other GCP services  For example  nodes can pull Container images from Google Container Registry without needing external IP addresses  The topic of private clusters is discussed in more detail in another module in this specialization 
ZPPjW4mqI8I,Deployments  Deployment subscriber desired state of pods  for example  a desire state could be that you want to make sure that 5 engine X pods are running at all times  It's declared of sense means that Kubernetes will continuously make sure the configuration is running across your cluster  Kubernetes also supports various update mechanisms for deployments  which I'll tell you about later in this module  Deployments declared a state of pots every time you update the specification of the pods  For example  updating them to use a newer container image  a newer replica set is created that matches the altered version of the deployment  This is how deployments roll out updated pods in a controlled manner  All pods from the old replica set are replaced with newer pods in a new replica set  If the updated pods are not stable  the administrator can rollback the pod to a previous deployment revision  You can scale pods manually by modifying the deployment configuration  You can also configure to deployment to manage the workload automatically  Deployments are designed for stateless applications  Stateless applications don't store data or applications state to a cluster or to a persistent storage  A typical example of a stateless application is a web front end  Some back end owns the problem of making sure that data gets stored durably  and you'll use Kubernetes objects other than deployments to manage these back ends  The desired state is described in the deployment YAML file containing the characteristics of the pods coupled with how to operationally run these pods and handle their lifecycle events  After you submit this file to the Kubernetes Master  it creates a deployment controller which is responsible for converting the desired state into reality and keeping that desired state overtime  Remember what a controller is  It's a loop processes created by Kubernetes that takes care of routine tasks to ensure the desired saving an object  Or set of objects running on a cluster matches the observe state  During this process  a replica set is created  A replica set is a controller that ensures that a certain number of pod replicas are running at any given time  The deployment is a high level controller for pod that declares it's state  The deployment configures a replica set controller to instantiate and maintain a specific version of the pods specified in the deployment  Here's a simple example of deployment object file in YAML format  The deployment named my-app is created with three replicated pods  In the spec template section  a pod template defines some metadata and specification of each of the pods in this replica set  In the pod specification and images pulled from Google Container Registry and port 8080 is exposed to send an accept traffic for the container  Any deployment has three different lifecycle states  The deployments progressing state indicates that a task is being performed  what tasks? Creating a new replica set  or scaling up or scaling down a replica set  The deployments complete state indicates that all new replicas have been updated to the latest version in are available and no old replicas are running  Finally  the failed state occurs when the creation of a new replica set could not be completed  Why might that happen? Maybe Kubernetes couldn't pull the images for the new pods  or maybe there wasn't enough of some resource quota to complete the operation  Or maybe the user who lost the operation lacks permissions  When you apply many small fixes across many rollouts  that translates to a large number revisions an to management complexity  You have to remember which small fix was applied to which roll out  which can make it challenging to figure out which revisions roll back to when issues arise  Remember earlier in this specialization we recommended that you keep your YAML files in a source code repository  That will help you manage some of this complexity 
t7d2pbLidqU,Ways to Create Deployments  You can create a deployment in three different ways  First  you create a deployment declaratively using a manifest file  such as a YAML file you've just seen and a kubectl apply command  The second method creates a deployment imperatively  using a kubectl run command that specifies the parameters in line  Here  the image and tag specifies which image and image version to run in the container  This deployment will launch three replicas  expose port 8080  Labels are defined using key and value dash hash generator  specifies the API version to be used  and dash hash save dash config saves the configuration for future use  Your third option is to use the GKE workloads menu in the GCP console  Here  you can specify the container image and version  or even selected directly from the container registry  You can specify environment variables and initialization commands  You can also add an application name  and namespace along with labels  You can use the view YAML button on the last page of the deployment wizard to view that deployment specification in YAML format  The replica set created by the deployment ensure that the desired number of pods are running  and always available at any given time  If a pod fails or is evicted  the replica set automatically launches a new pod  You can use the kubectl get in the scribe commands to inspect the state and details of the deployment  As shown here  you can get the desired  current  up to date and available status  of all the replicas within an deployment along with their ages  using the kubectl get deployment command  Desired  shows the desired number of replicas in the deployment specification  Current is the number of replicas currently running  Up-to-date  shows the number of replicas that are fully up-to-date as per the current deployment specification  Available displays the number of replicas available to the users  You can also output the deployment configuration in a YAML format  This is a useful trick  Maybe you originally created a deployment with kubectl run  then you decided you'd like to make it permanent managed part of your infrastructure  Edit that YAML file to remove the unique details of the deployment you created it from  Then you can add it to your repository of YAML files for future deployments  For more detailed information about the deployment  use the kubectl describe command  You'll learn more about this command in the lab  Another way to inspect a deployment is to use of GCP console  Here you can see detailed information about the deployment  revision history  the pods  events  and also view the live configuration in YAML format 
5pwc2DYlKQU,Services and Scaling  You now understand that a deployment will maintain the desire number of replicas for an application  However  at some point  you'll probably need to scale the deployment  Maybe you need more web front end instances  for example  you scale the deployment manually using a kubectl command or the GCP console by defining the number of replicas  also manually changing the manifest will scale the deployment  You can also autoscale the deployment by specifying the minimum and maximum number of desired pods along with the CPU utilization threshold  Again you can perform auto-scaling by using the kubectl autoscale command or from the GCP console directly  This leads to the creation of a Kubernetes object called horizontal pod Autoscaler  This object performs the actual scaling to match the target CPU utilization  Keep in mind that we're not scaling the cluster as a whole  just a particular deployment within that cluster  Later in this module  you'll learn how to scale clusters  Thrashing sounds bad  and it is bad  It's a phenomenon where the number of deployed replicas frequently fluctuates  because the metric used to control scaling also frequently fluctuates  The horizontal pod Autoscaler supports a cool-down or delay feature  It allows you to specify a wait period before performing an under-skilled town action  The default value is five minutes 
8QpDGVGml6M,Updating Deployments  When you make a change to a deployments port specification such as changing the image version an automatic update rollout will happens  Again  note that these automatic updates are only applicable to the changes in port specifications  You can update a deployment in different ways  One way is to use a kubectl apply command with an updated deployments specification YAML file  This method allows you to update other specification of a deployment such as the number of replicas outside the pod template  Another way is to use a kubectl set command  This allows you to change the pod template  specifications for the deployment such as the image  resources  or selector values  Another way is to use the kubectl edit command  This opens a specification file using the Vim editor that allows you to make changes directly  Once you exit and save the file  kubectl automatically applies the updated file  The last option for you to update a deployment is through the GCP Console  You can edit an employment manifest from the GCP Console and perform a rolling update along with this additional options  Rolling updates are discussed next  Let's look at how a deployment is updated  When a deployment is updated it launches a new ReplicaSet and creates a new set of Pods in a controlled fashion  First  new Pods are launched in a new ReplicaSet  Next  all Pods are deleted from the old ReplicaSet  This is an example of a rolling update strategy also known as a rampt strategy  It's advantage is that updates her slowly release  which ensures the availability of the application  However  this process can take time and there's no control over how the traffic is directed to the old and new Pods 
uyDUmW2ormk,Rolling Updates  In a rolling update strategy  the Max unavailable and Max search fields control how the pods are updated  These fields are final range for the total number of pause within the deployment  regardless of ReplicaSets  The max unavailable field let's you specify the maximum number of pods that can be unavailable during the roll-out process  Deployment can either be absolute or percentage  For example  you can say you want to have to pods unavailable during the process  Specifying Max unavailable at 25 percent means you want to have at least 75 percent of the desired pods running at the same time  The default Max unavailable is 25 percent  Max surge allows you to specify the maximum number of pods that can be created concurrently in a new replica set  For example  you can specify that you want to add two new pods at a time in a new ReplicaSet  and the deployment controller will do that exactly  You can also set max surge as a percentage  Deployment controller looks at the total number of running pods in both ReplicaSets  old and new  In this example  a deployment with the desired number of pods as four and a Max surge of 25 percent will allow a maximum of five pods running at any given time  In other words  it'll allow 125 percent of the desired number of pods  which is five  Again  the default Max surge is 25 percent  Let's look at the deployment with the desired number of pods set to 10  Max available set to 30 percent and Max third set to five  Deal ReplicaSet has 10 pods  The deployment will begin by creating five new pods in a new ReplicaSet based on Max surge  When those new pods are ready  the total number of pods changes to 15 with Max unavailable  it's set to 30 percent  The minimum number of pods running regardless of old or new ReplicaSet is 10 minus 30 percent  which equates the seven pods  A total of eight pods can be removed from the old ReplicaSet  This keeps a minimum total at seven pods  five in the new ReplicaSet  and two in the old ReplicaSet  Next  five more pods are launch in a new ReplicaSet  This creates a total of 10 pods in a new ReplicaSet and a total of 12 across all ReplicaSets  Finally  the remaining two pods and the old set are deleted  This leaves 10 pods in a new ReplicaSet  There are a few additional options  such as min ready seconds and progress deadline seconds  min ready seconds defines the number of seconds to wait before a pod is considered available without crashing any of its containers  The default for min ready seconds is zero  meaning that as soon as the pod is ready  it's made available  Another option is progress deadlines seconds  where you specify the wait period before a deployment reports that it has failed to progress 
cIlKSD8ptAk,Blue-Green Deployments  Recreate is a strategy type where all the old pods are deleted before new pods are created  This clearly effects the availability of your application because the new pods must be created  and will not be available instantly  For example  what if the contract of communication between parts of your application is changing  and you need to make a clean break? In such situations  a continuous deployment strategy doesn't make sense  All the replicas need to change at once  That's when the Recreate strategy is recommended  We haven't yet discussed how to locate and connect to the applications running in these pods  especially as new pods are created or updated by your deployments  While you can connect to individual pods directly  pods themselves are transient  A Kubernetes service is a static IP address that represents a service or a function in your infrastructure  It's a network abstraction for a set of pods to deliver that service in  It hides the ephemeral nature of the individual pots  Services will be covered in detail in the networking module  A blue green deployment strategy is useful when you want to deploy a new version of an application  and also ensure that application services remain available while the deployment is updated  With a blue green update strategy  a completely new deployment is created with a newer version of the application  in this case  it's my-app-v2  When the pods in the new deployment are ready  the traffic can be switched from the old blue version to the new green version  But how can you do this? This is where a Kubernetes service is used  Services allow you to manage the network traffic flows to a selection of pods  This set of pods is selected using a label selector  Let's take a closer look at how this is implemented  Here in the service definition  pods are selected based on the label selector  Where pods in this example belong to my-app and to version v1  When a new deployment  labeled v2 in this case  is created and is ready  the version label in the services change to the newer version  labeled v2 in this example  Now the traffic will be directed to the newer set of pods  the green deployment with the v2 version label  instead of the old blue deployment pods that have the v1 version label  The blue deployment with the older version can then be deleted f The advantage of this update strategy is that rollouts can be instantaneous  And the newer versions can be tested internally before releasing them to the entire user base  For example  by using a separate service definition for test user access  The disadvantage is that the resource usage is doubled during the deployment process 
JRsN4Oz36QU,Canary Deployments  Hi  I'm Eoin Carroll and I'll be teaching a couple of lectures in this course  in this video  we're going to be covering Canary Deployments  The Canary method is another update strategy based on the blue green method  but traffic is gradually shifted to the new version  The main advantages of using Canary deployments are that you can minimize the access resource usage during the updates  And because the Rolla is gradual  issues can be identified before they affect all instances of the application  In this example  100% of the application traffic is directed initially to (my-app-v1)  when the Canary deployment starts  a subset of the traffic 10% in this case is redirected to the new version (my-app-v2)  When stability of the new version is confirmed 100% of the traffic can be routed in new version  but how is the stone? In the blue green update strategy previously covered  Both up and version labels were selected by the service  so traffic would only be sent to the pods that are running the version defined in the service  In a Canary update strategy  the service selector is based on the application level and does not specify the version  the Selector in this example  covers all pods with the app  my- label  This means that with this Canary update strategy version of the service traffic is sent to all pods  regardless of the version label  this setting allows the service to select and direct traffic to pods from both deployments  Initially  the new version of the deployment will start with zero replicas running and overtime as the new version is scaled up  the old version of deployment can be scaled down and eventually deleted  With the Canary update strategy  a subset of users will be directed to the new version  this allows you to monitor for errors in performance issues as these users use the new version and you can quickly rollback minimizing the impact to your overall user base if any issues arise  However  the complete rollout of a deployment using Canary strategy can be a slow process and may require tools such as STO to accurately shift the traffic  There are other deployment strategies  such as AB testing and shadow testing  but these strategies are outside the scope of this course  A service configuration does not normally ensure that all requests from a single client will always connect to the same pod  Each request is treated separately and can connect to any pod deployment  This potentially can cause issues if there are significant changes in the functionality between pods  as may happen with the Canary deployment  To prevent this  you can set the session affinity fields to client IP in the specification of the service  If you need a clients first request to determine which pod will be used for all subsequent connections  so that's roll ish  Next  let's discuss how to rollback updates  especially in a rolling update and recreate strategies  you rollback using the cube  itll rollout undo command  A simple rollout undo command will revert to deployment to its previous revision  You can rollback to a specific version by specifying the revision number  If you're not sure the changes  you can inspect the road out history using cube  cube ctl rollout history command  The Cloud Console does not have a direct rollback feature  however you can start cloud shell from your console and use these commands the Cloud Console also shows you the revision list with summaries and creation dates by default  The details of the ten previous replica sets are retained so that you can rollback to them  you can change the default by specifying the revision history limit under the deployment specification 
WdqdebPmPuQ,Managing Deployments  When you edit a Deployment  your action normally triggers an automatic rollout  But if you have an environment where small fixes are released frequently  you'll have a large number of rollouts  In a situation like that  you'll find it more difficult to link issues with specific rollouts  To help  you can temporarily pause these rollouts by using the kubectl rollout pause command  The initial state of the Deployment prior to pausing will continue its function  But new updates to suit the Deployment will not have any effect while the rollout is paused  The changes will only be implemented once a rollout is resumed  When you resume the rollout  all these new changes will be rolled out with a single revision  You can also monitor the rollout status by using the kubectl rollout status command  What if you're done with a Deployment? You can delete it easily using the kubectl delete command  You can also delete it from the GCP Console  Either way  Kubernetes will delete all resources managed by the Deployment  especially running pods 
Ejpxgrv_9n8,Lab Intro  In this lab  it's more of the basics of using deployment manifests  The first task that you'll learn to perform  is to create a deployment manifest for a pod inside of the cluster  You'll then use both the GCB Console and Cloud Shell to immediately scale pods up and down  The next task will be to trigger deployment roll out and a deployment rollback  Parts can be of various service types  Cluster IP  node port  load balancer  which can be defined in the manifest  You'll perform a task where you define service types in the manifest and verify load balancer creation  In your final task  you'll create a new canary deployment for the release of your application 
FQWTa3XjyWg,Jobs and CronJobs  In this lesson  we'll explore Jobs and CronJobs in more detail  let's start with jobs  A job is a Kubernetes object like a deployment  a job creates one or more pods to run a specific task reliably  In its simplest form  a job will create one pod and tracked the task completion within that pod  When the task is completed  it will terminate the pod and report that the job has successfully finished  Consider a scenario where you're transcoding some video files using a single standalone pod  What if the node that the pod is running on  suddenly shuts down? What will happen to the task that was being performed? It will be lost  when the potd is terminated for any reason  it won't be restarted  Jobs provide a mechanism to hail this type of failure  Unlike other Kubernetes controllers  jobs manage a task up to its completion rather than to an open ended desired state  In a way  the desired state of a job is it's completion  Kubernetes will make sure it reaches that state successfully  In this scenario  the first step in this process involves the user uploading a video file to a Web Server for conversion or transcoding  The Web Server creates a job object manifest for this transcoding task  a job is created on the cluster  The Job controller schedules a pod for the job on a Node  and the job controller monitors the pod  If a node failure occurs and the pod is lost  the Job controller is aware that the task has not been completed? The job controller reschedules the Job Pod to run on a different node  The job controller continues to monitor the pod until the task completes  When the task was completed  the job controller removes the finished job  and any pods associated with it  There are two main ways to define a job  non parallel and parallel  Nonparallel jobs create only one part at a time  of course  that pod is recreated if it terminates unsuccessfully  These jobs are completed when the pod terminates successfully or  if it completion counters defined when the required number of completions is reached  Parallel jobs are jobs that have a parallelism value defined  where multiple pods are scheduled to work on the job at the same time  And they also have a completion count defined  they're used for tasks that must be completed more than once  Kubernetes considers parallel jobs complete when the number of pods that had a terminated successfully  reaches the completion count  A second type of parallel job for processing work queues can also be defined  which you'll see later  Let's start by looking at how to create a simple  nonparallel job  Here's a simple example of a non parallel job  which computes pi to 2000 decimal places  a job object is specified through its kind  Within a job spec  there's a pod template  this is where a pod specifications and its restart policy are defined  In this example  the restart policy is set to Never  This means that if a container in a pod fails for any reason  the entire pod fails  and the job controller will respond to this pod failure by launching a new pod  The other restart policy options that can be used for jobs  is to set the restart policy to on failure  In this case  the pod remains on the node  but the container is restarted  Where application failures are possible or expected  the back off limit field can be used  Back off limits specifies the number of retries before a job is considered to have failed entirely  The default is 6  this allows you to halt jobs that would otherwise get stuck in a restart loop  Failed pods are recreated with an exponentially increasing delay  10 seconds  20 seconds  40 seconds  and so on  up to a maximum of 6 minutes  In this example  if the pods continues to fail four times  the job will fail with backoff limit exceeded given as the reason  If the pod succeed before the back off limit is reached  the counter is reset  Like other Kubernetes objects  the job object can be created using a kubectl apply command  Alternatively  a job can be created using the kubectl run command  A nonparallel job is where a single pod is created to start the job or task  The pod is created as usual and the job or task finishes successfully  If the pod fails for any reason  it's restarted ensuring that the job gets another opportunity to finish successfully 
TfkPs9VvPrY,Parallel Jobs  Now let's discuss parallel jobs  The parallel job type creates multiple pods that work on the same task at the same time  Parallel job types are specified by setting the spec parallelism value for a job greater than one  As I mentioned earlier  there are two types of parallel jobs  one with a fixed task completion count  and the other which processes a work queue  If you want to launch multiple pods at the same time for parallel jobs  along with a fixed completion count  you can add completions along with parallelism  The job controller will only launch the maximum number of pods at the same time specified by the parallelism value and will continue restarting pods until the completions count is reached  In this example  the controller will launch up the two pods and parallel to process tasks until three pods have terminated successfully  The controller will wait for one of the running pods to complete successfully before launching the next pod  The job controller will track successful completions  When the specified number of completions has been reached  the job is considered complete  If the remaining number of completions is less than the parallelism value  the controller will not schedule new pods as there are sufficient remaining pods running at the point to complete the desired total for the job  What is a parallel job with a work queue? Let's look at that next  In a worker queue parallel job  each pod works on several items from a queue and then exits when there are no more items  Because the workers  the pods themselves detect when the work queue is empty and the job controller doesn't know about the work queue  It relies on the workers to signal when they're done working by terminating  You create a parallel job to process a worker queue by specifying a parallelism value and leaving spec completions unset  In this example  with parallelism set to three  the job controller will launch three pods simultaneously  The job will consider all tasks complete as soon as any of these three pods successfully finishes its task  At some point  one of the pods terminate successfully  The applications running in the remaining pods detect this completion state and finish  causing the remaining pods to shut themselves down  One pod terminating the job successfully is considered to be finished successfully  Like other objects  jobs can be inspected using a kubectl describe command  The pods can be filtered using the kubectl get command and label selector  Job details can also be viewed from the GCP console  Jobs can be scheduled either from a command line or the GCP console  This is done by changing the parallelism value  Pods can fail all the time  One way to limit pod failure is through the backoffLimit described earlier  Another option is to use the activeDeadlinesSeconds setting  where you set an active deadline period for the job to finish  The deadline counts starts from the job starts  If the deadline is reached  the job and all its pods are terminated with a deadline exceeded reason  activeDeadlinesSeconds has precedence over backoffLimit  You can use either one of these kubectl delete commands to delete a job  When you delete a job  all of its pods are also deleted  If you want to retain job pods  set the cascade flag to false during the deletion  Jobs can also be deleted directly from the GCP console 
8NcBcsIdPSA,CronJobs  Now that you understand jobs  is there any way to schedule them? If you guessed a Cron job you are right  Cron job is a Kubernetes object that creates jobs in repeatable manner to a defined schedule  Cron jobs are called that because they are named after Cron  the standard Unix Linux mechanism for scheduling a process  The schedule field except the time in the Unix Linux standard format for scheduling a Cron job  In this example  a Cron schedule has been set up under the Cron job specification to run every one minute  The job template defines the job specification  just as I told you about in the previous lessons about jobs  If the job defined in this benefits is scheduled to start every minute  what happens if the job isn't started at the scheduled time? By default the Cron job looks at how many times the job has failed to run since it was last scheduled  If that failure count exceeds 100 and an error is logged and the job is not scheduled  this will prevent an error with a Cron job resulting in an endless accumulation of failed attempts overtime  This behavior can be controlled using the starting deadlines seconds value  Instead of looking at the number of failed attempts to run the jobs since it last successfully run  you can define the starting deadlines seconds attribute which defines a window of time to sum the number of failed attempts  This changes the window that the controller examines  Now  instead of looking back to the last time the job was scheduled when starting deadlines seconds is set  the window starts at starting deadlines seconds before it know  Depending on how frequently jobs are scheduled and how long it takes to finish the define test  the Cron job might end up executing more than one job concurrently  You use the concurrency policy value to define whether concurrent executions are permitted  With the values allow  forbid  or replace  In the case of forbid  if the existing job hasn't finished the Cron job won't execute a new job  With replace  the existing job will be replaced by the new job  This policy only applies to jobs that were created using the same Cron job  Other Cron jobs in their jobs aren't considered or effected  You can stop execution of individual jobs by a Cron job by setting the suspended property to true  When this is set all new job executions are suspended  Suspended executions are still counted as missed jobs  The limit to the number of successful and failed jobs to be retained in history is configured by the fields  successful jobs history limit and failed jobs history limit  Cron jobs operate in the same manner as the job itself  You can create  inspect  and delete Cron jobs using Kubectl command shown  You'll learn how to execute a Cron job in the labs 
vu6eLFmrwaA,Overview  In the previous modules  we learned about basics of Machine Learning Operationalization  That is ML Ops  Whites needed various ways off its implementation and its impact  In this module  we will be discussing a Google Cloud product AI Platform Pipelines that makes ML Ops easy  seamless and scalable with Google Cloud Services  Let's start  Throughout this module  we will learn about what a machine-learning pipeline is and what is important parameters are  The basics of AI platform pipelines  a Google Cloud product to build ML Pipelines  and how it is different from the regular ML Ops pipelines  We will also discuss what is the technical stack behind this product and what is the ecosystem around it that makes it very collaborative and scalable  Let's start with an overview of ML Ops  We will briefly discuss its impact on artificial intelligence as a field and try to understand what are the challenges that you may face while building such Pipelines  Head Google  we believe in democratizing machine learning and always target these three important factors while building any AI solutions  The first one is simple  Make the solution's simple that anyone can take advantage of them regardless of their technical capabilities  AI solutions should be simple that even a business audience can understand and implement them  The second one is fast  Make them fast you can iterate and succeed more quickly  AI cycles should be as agile as possible  Build fast and feel fast  More experimentation leads to better results and helps in making a robust production grid solution  Third  and the last is useful  Make them useful in solving your real business problems  Without business impact  any technical solution is of very little use  When we talk about AI solutions  these are the three common problems that we usually face  The first one is deployment  Many times  infrastructure is very brittle and doesn't scale as you need to grow  Complexity in the infrastructure also increased the complexity in deploying machine learning systems or pipelines  You need a robust toolbox to build  manage and deploy a machine-learning pipeline  There can be different from a regular CICD pipeline  This toolbox should be compatible and tightly integrated with your Infrastructure  This may contain model replanning and human validation in the loop  Kubeflow and TensorFlow extended  The effects are the grid solutions for this problem  The second one is talent  Very limited talent is currently available in machine learning  and you may need to lower the bar to build AI Apps  You need a solution that can be reused and can be re-implemented by anyone  You can also create a tiered architecture in your organization  While one segment is building solutions from the beginning  the second segment is using them and customizing them for various other business problems  and the third segment is just consuming them  The third common problem is collaboration  You need an ecosystem in your organization  a community to extend support  enabled specialized skills to flourish for both publishers and consumers and see the market with the content  You may also need access to a marketplace where you can publish your solutions or Pipelines and anyone in your organization can take it  modify it as per their needs  and use it  Such a marketplace should be created with a target to make developer experience better  Let's start discussing a few potential solutions  First one is using a framework to build and deploy your Pipelines  Kubeflow or TensorFlow extended TFX are two common ones  They are very easy to start with and very exhaustive to cater arrange of your needs  Kubeflow provides out of the box support to a lot of common ML frameworks like TensorFlow  PyTorch  Caffe  and XGboost  TFX is state of the art technology and provides Google's best practice on TensorFlow  It integrates tightly with TensorFlow and supports end-to-end deployment from data ingestion to serving  These services are swappable and scalable  Scale is very important today  You never know when your requirements for computation increased by a 100X or even 1000X  users can increase exponentially as your ML Pipeline should be robust enough to take care of  You cannot just rely on a fixed Virtual Machines with a dedicated processing power and running large machines will be costly to deploy as well as to maintain  These frameworks can support different platforms as well  These can work On-premises as well as Cloud  Given the requirements to use different models at different platforms and integrate back to your legacy environment  these gelato [inaudible] very well with your Infrastructure  Second is building reusable pipelines  Biggest question is  why do I need to build the same components again and again to create my pipeline? Why can't I use PBL blocks and stitch them together to get my job done? Why can't the Machine Learning pipeline follow the ID of Lego where we have defined blogs? Can you just need to know what suits you the best? These pipelines like Kubeflow or TFX  provide you with a capability to reuse the already built components than starting from scratch by reinventing the wheel  You can search  learn  and then replicate the already successful Pipelines  This will help you to solve the actual user problem rather than going through the unnecessary pain  The third and the last one is a collaborative ecosystem  A place where you can publish your pipeline on notebooks and then anyone else can take it and use it  AI hub is one example of that Google Cloud provides  It is a personal  as well as the public marketplace for your solutions  Where you can publish your work either internal to your organization or to the whole world based on your likes  You can search for already available pipelines and then directly deployed on your Cloud instance  You can modify it as well as based on your use case  This deduces the effort to start solving a problem from scratch  For example  if you're solving a problem to predict customer churn for your finance business unit  but the pipeline available is trained on retail data  Then you can take it and modify it to make it useful for you  Then you can publish it internally that anyone in your organization can actually use it  AI provides you a very clean  simple  and fast implementation  It just needs one click of a button to deploy pipelines on Google Cloud  If you want to make it hybrid  which means to make it work on both Cloud and On-premise  then you can do that as well  If we make thing easier to explain what this ML Ops or ML Pipeline ecosystem looks like and we can compare it with the Android ecosystem  In this analogy  you can consider Google Cloud Infrastructure as your hardware  like Google Pixel phone in Android ecosystem  where you can deploy things  where actually use things  Google Cloud Services as Google Mobile Services to take advantage of pre-built libraries to build any app  ML Pipelines using Kubeflow or TFX with Android operating system and Software Development Kit  SDK as a high level abstraction to orchestrate your solution  Finally  you can compare AI hub to a Play Store to provide you a marketplace or collaboration tool to publish your stuff and share it with others  That's how the analogy with your Android ecosystem  and your Google Cloud ML Pipelines Ecosystem will look like 
4Vy_bq1oWyg,Introduction to AI Platform Pipelines  Let's learn about basics of ML Pipelines and Google cloud product AI platform Pipelines  ML pipelines or machine-learning pipelines are portable  scalable ML workflows based on Containers  ML pipelines are composed of a set of input parameters and a list of tasks  Each task is an instance of a pipeline component  You can use ML pipelines in many ways  You can apply ML operationalization techniques to automate a repeatable process run various ML experiments to try different sets of hyperparameters and number of training steps  You can also reuse such pipelines  go train a new model for a new problem altogether  This provides you a capability to scale your ML work across the organization or teams  Pipeline components are self-contained sacks of code that perform one step in a pipelines work flow  such as data pre-processing  data transformation  model training  and so on  Components are composed of a set of input parameters  a set of outputs  and the location of a container image  A components Container Image is a package that includes a component's executable code and the definition of the environment that the code runs in  Each task in a pipeline performs a step in the pipelines workflow  Because tasks are instances of pipeline components  tasks have input parameters  outputs  and a Container Image  Task input parameters can be set from the pipelines Input parameters are set to depend on the output of other tasks within this pipeline  Kubeflow pipelines or L platform pipelines  uses these dependencies to define the pipelines workflow as a directed acyclic graph  also called as tags  For example  consider the pipeline with the following tasks  Step 1  Pre-process  This task  prepares the training data  It does all the formulations  future generation and etc required for the data preparation  Step 2  Train  This task uses the pre-process printing data from step 1 to try in the machine learning model  This ML model can be from any of the ML frameworks like Tensor Flow  pipe doors  Scikit-learn and others  Step 3  Predict  This task  deploys the training model as an ML service and gets predictions for the testing data set  Step 4  confusion matrix  This task uses the output of the prediction task to build a confusion matrix to check the model performance  like for precision  recall and other performance matrix  In bath to step 4  this ROC step can also be performed  This task uses the output of the prediction tasks to perform receiver operating characteristic  ROC curve analysis  To create the workflow graph  a platform pipelines analyses the task dependencies  The pre-processing task does not depend on any other tasks  It can be the first task in a workflow or it can run concurrently with other tasks  The greening tasks relies on the data produced by the pre-processing task  Training must occur after pre-processing  The prediction task relies on the green model produced by their printing task  Prediction must occur after the training tasks or the printing step  Building the confusion matrix and performing ROC receiver operating characteristic analysis both rely on the output of the prediction task  They must occur after prediction is complete  Building the confusion matrix and performing ROC analysis can occur concurrently because they both depend on the output of the prediction tasks  but they are independent of each other  Based on these analysis  the AI platform pipeline systems runs the pre-processing  training  and prediction tasks sequentially and then runs the confusion matrix and ROC tasks concurrently  If he briefly discuss about current process for building MLOps or ML operationalization pipeline  It consists of setting up a cluster  In GCP  It will be GKE  Google Kubernetes Engine  Then we need a Cloud Storage bucket to store the data  followed by installing Kubeflow pipelines  Then we need to configure it and set up the port forwarding  Finally  we need to create a process to share the pipeline with your team or organization  This is a very long step and it takes lot of time  Sometimes missing any one step can cause errors and confusions  This raises a question  can we automate these whole process to make MLOps a seamless and easy experience  can we make it one peak deploy? This question raised the need of a new product  A product to deploy robust  repeatable machine learning pipelines  along with monitoring  auditing  version tracking  and reproducibility  It should deliver an enterprise ready  easy to install  and secure execution environment for all your machine learning workflows  This product should democratize use of Infrastructure to Data Scientists  Hence  we built a product AI platform pipelines  This product provides you a one push button installation via the Cloud Console  You need not to go to step-by-step and follow the process  but just one click on one button to create your cluster and configure your pipeline  It includes all important enterprise features for running ML workloads  like pipeline versioning  automatic data tracking of artifacts and executions  cloud logging and visualization tools  Al platform pipelines provides seamless integration with Google Cloud managed services such as BigQuery  data flow  air platform training and serving and cloud functions  This will help you to build an integrated end-to-end system with different needs  Also  you will get many pre-built pipeline components are pipeline steps for ML workflows with easy construction of your own custom component if needed  Al platform pipelines has two major parts  First  the Enterprise ready Infrastructure for deploying and running structured ML workflows that are integrated with Google Cloud services  The second is pipeline tools for building  debugging and sharing pipelines and its components  You can access the AI platform pipelines from the AI platform panel in the Google Cloud Console  The installation process is lightweight and push button and the hosted model simplifies management and use  Al platform pipelines runs on a Google Kubernetes Engine GKE cluster  A cluster is automatically created for you as a part of the installation process  but you can use an existing GKE cluster as well  The Cloud AI platform UI lets you view and manage all your clusters  You can also delete the Pipeline's installation from a cluster and then re-install  retaining the persisted state from the previous installation while updating the pipeline's version  Al platform pipelines gives you secure and authenticated access to the pipeline's UI via the Cloud AI platform UI with no need to set up what forwarding  You can also give access to other members of your team or your organization  It is similarly straightforward to programmatically access a pipelines cluster via its rest API service  This makes it easy to use  The pipelines has SDK from cloudy AIP platform notebooks  for example  to perform tasks like defining pipelines or scheduling pipeline run jobs 
cqmJoI4QwYg,Concepts  Now let's learn about the concepts of Google Cloud product AI platform pipelines  We will discuss the technical stack behind AI platform pipelines and how it works  Let's start  With a platform pipelines  you specify your pipeline by using the Kubeflow Pipelines SDK or by customizing that TensorFlow extended TFX pipeline template with the TFX SDK  The SDK combines the pipeline and submits it to the pipelines rest API  The AI pipelines rest API server  stores and schedules the pipeline for execution  AI pipelines uses the Argo workflow engine to run the pipeline and has additional micro-services to record metadata  handle component IO  and scheduled pipeline runs  Pipeline steps are executed as individual isolated parts in a GKE Google Kubernetes Engine cluster  which enables the kubernetes native experience for the pipeline components  The components can leverage Google Cloud services such as data-flow  AI platform training and prediction  and BigQuery for handling scalable competition and data processing  The pipelines can also contain steps that perform sizable GPU and TPU computation in that cluster  directly leveraging GKE auto-scaling and node auto provisioning  The Kubeflow pipelines SDK is a lower level SDK  that's ML framework neutral and enables Direct Kubernetes resource control and simple sharing of containerized components  as shown in the pipeline steps  The TFX SDK is currently in preview mode and is designed for ML workloads  It provides a higher level abstraction with prescriptive but customizable components with predefined machine learning types that represent Google Best Practices for durable and scalable machine learning pipelines  It also comes with a collection of customizable TensorFlow optimized templates  developed and used internally at Google  consisting of component archetypes for Production Machine Learning  You can configure the pipeline templates to build  train  and deploy your model with your own data  automatically perform schema inference  data validation  model evaluation  and model analysis and automatically deploy your train model to the AI platform prediction service  To make it easier for developers to get started with Machine Learning pipeline code that TFX SDK provides templates or scaffolds with step-by-step guidance on building a production machine learning pipeline for your own data  With a TFX template  you can incrementally add different components to the pipeline and iterate on them  TFX templates can be accessed by the AI platform Pipelines getting started page in the Google Cloud Console  The TFX SDK currently provides a template for classification problem types and is optimized for TensorFlow with more templates being designed for different use cases and problem pipes  A TFX pipeline typically consists of multiple pre-made components for every step of the ML workflow  For example  you can use example gen for data ingestion  statistics gen or stat gen to generate and visualize statistics of your data  example validator and schema gen to validate data  transform for data pre-processing and trainer to train a TensorFlow model  The AI platforms pipeline U I lets you visualize the state of various components in the pipeline  data step  statistics  and more as shown in this slide  AI Platform pipelines also supports pipeline versioning  It lets you upload multiple versions of the same pipeline and group them in the U I so you can manage semantically related workflows together  Grouping makes it easy to identify such pipelines and provides a very clean interface for further sharing  You can easily see this in this screenshot  AI platform pipelines supports automatic artifact and lineage cracking powered by ML metadata and rendered in the U I  You can see that the list shown in the image has multiple types of artifacts  Things like models  Data Stacks  and modeling evaluation metrics  When clicking on a particular artifact  you have access to the details of that artifact as shown on the screen right now  With AI platform Pipelines U I  it's easy to keep track of artifacts for an ML Pipeline  Last one is lineage tracking  Just as you wouldn't code without version control  you should not print your models without lineage tracking  Lineage tracking shows the history and versions of your model  data and more  You can think of it like an ML stack trace  Lineage tracking can answer questions like  what data was this model trained on? What models were trained on this data-set? What other statistics of the data that this model trained on and so on  That's all about the concepts and basic understanding of AI platform pipelines  We will next discuss when to use these pipelines 
94evBUo8EiE,When to use  In the last section  we discussed the AI Platform Pipelines technical stack  Now let's learn about when to you use this product and what are the common themes behind the selection of this product  Let's discuss what AI Platform Pipelines as a product enable  These are the top three features that AI Platform Pipelines enable out of the box  Workflow orchestration  a rapid  reliable  and repeatable experimentation  and then ecosystem to share  reuse  and compose your work  Let's discuss these three features in the day  First one is a workflow orchestration  This feature provides you to orchestrate your pipeline as a workflow  In such workflow  each and every block has a designated task to do and it can be sequential or parallel in nature  After the end of one task  another task will trigger in-sequential mode and multiple tasks will trigger at a time in parallel workflow  This feature provides you to actually visualize your pipeline and various components used in it as a laboratory  You can see a sample pipeline on the screen  The user interface provides rich details on the pipelines execution in a step-by-step process  It creates a tree-like architecture to visualize the pipeline orchestration and identify any potential errors  You can easily review what parts have run successfully with the green on the top right  Any error or warning will change the status as well as visualization  There  like a red cross for failure or yellow exclamation for wanting  The UI provides information about the pipelines execution  such as logs and performance goals  This reduces the need to go back in your code or manually check the logs or see performance goals that you saved in a storage bucket  You can directly check the results in this pipeline UI itself  You can click on any block and see the important artifacts right there  For example  you can see the receiver operating characteristic  ROC curve on the screen  After the model has been drained  after clicking on the green task in the pipeline  You can get more details with one click option ostensible  It's not just the metrics  you can also view details of the pipelines execution with its input/output parameters from every run  This will provide you more details like what data you used with a path to Google Cloud Storage bucket for model training and as well as evaluation  In a nutshell  these are the important parts of an AI Platform Pipeline constitutes of  Containerized implementations to give microservices like architecture  Each and every task can be single node are distributed to make it fast  A containerized task can invoke any other Google Cloud Services like Cloud AI Platform creating a prediction  Dataflow of pipelines like transformations or Dataproc  These steps can be specified using Python SDK and input parameters can be provided to invoke the pipeline with these specified parameters  Such design helps to decouple the execution environment from the code runtime  Unlike Spark  for example  where if the cluster has Spark 2 3  all your code should be linked with the same version  This will also allow easy reproducibility of the code between the development and production environments  All the things you test will be exactly the same in production  You can also encapsulate the complexities of Containers as much as possible  Users will not see the Containers if they don't want to  Each component in the pipeline can have its own version of runtime  different languages  and libraries  This helps make composition easier  You can see a sample queue flow pipeline on the screen  You can easily feel the containerized architecture and how they are connected with each other in either sequential or parallel mode  Second feature is to provide rapid  reliable  and repeatable experimentation  This is a very important feature for any organization to run quick experiments and make decisions accordingly  Any experimentation will require a loop where you can take any example solution  modify it  and then run it  Then you can save it for future or share it with your right audience  Every experiment is usually logged with three important things  First  configs  parameters or inputs  second  are the metrics  It can be anything like accuracy  latency  etc  The third one  user added labels and annotations  It is a good practice to provide search capabilities to find old runs  Search runs should be logged to keep track of metrics and outputs  Such experimentation methodology will help you to rapidly iterate over your ideas without spending a lot of efforts or time on building things from scratch  Such thinking will make the selection process simple and fast for management to make the right decisions and identify the right target  Here is an example to show the interface of AI Platform Pipelines  where you can see all the previous experiments and compare the metrics to check for the gains  You can select any pipeline from your needed group  clone it  edited and submit it  You can run any selected pipeline and see all its parameters are metrics  You just need to click on the clone button  to clone or copy the pipeline  and it will create a copy for you to modify it  It is as simple as that  You can quickly run that experiment with new parameters or new data or create a new experiment altogether  Third useful feature that AI Platform Pipelines enable  is to share  reuse  and compose a new pipeline  This feature is on the line of reusable pipeline concepts discussed in the previous section in context of generic ML pipelines  It is always good to use the existing solutions rather than reinventing the wheel  Hence  sharing and reuse becomes very important in any enterprise setting  Each and every development requires money and time  No organization wants to solve the same problem again and again  This feature will help you to keep track of all the parameters of your experiment  You can easily run your experiment for different settings and save configurations for individual runs in a database  You can see them easily reproduce your results  It is very easy to compose or swap any standard artifacts  interfaces  or share best practices with your audience 
q9THx--fAFw,Ecosystem  Now  let's learn about the ecosystem  and driving force behind MLOps  How can we collaborate  and what tools will be available to make collaboration easier for developers? We have already discussed the top problems any AI team or companies are facing today  You need to lower the bar to build AI apps to work on the talent  Or you need a community to extend enable specialized skills to flourish  both publishers and consumers  seed the market with content  and access marketplace from developers experience  Or you need flexibility on ML frameworks [inaudible] algorithms where to run  manage  or [inaudible]  One of the most important factors to address for better collaboration  and business impact is the ecosystem  Let's discuss how the AI platform will provide you the right ecosystem to collaborate with the right audience within your organization  If we think about any ML project life cycle  it starts with data ingestion  Usually  software engineers work on building data pipelines and doing initial feature engineering  They connect the right databases  and procure the data  The software engineers work with data scientists to split that data in right buckets to train  and test the models  They worked together to explore various hyperparameters  and tune them to generate the right output  Finally  they publish their models to the right audience  Then  software engineers work with the management team to convert that output into a business logic  This is usually a generic flow  We need an ecosystem to enable software engineers to build AI pipelines standing on the shoulders of the community  If we connect the previous flow with a robust ecosystem where you can search the right solution  and use it  then the whole flow will significantly shorten them  In such a case  you can take a reusable pipeline  and need not to make a model  [inaudible] hyperparameters  and publish it  Rather  you can just take that model  connected with the right data  print that model again  and deploy it  Then you can generate the right business logic out of it  This is an important  and very crucial factor to scale your machine learning efforts across the organization within limited budget  and other challenges  To provide you the right place to collaborate  and create your own ecosystem  Google Cloud came up with AI Hub as a product  This is a one place for everything  from AI  to experimentation  to production  You can share your pipeline on notebooks within your organization  This will help you to scale up things  and reduce the experimentation time significantly  You can use AI hub for your various needs  There are two major ways to use it  First  is to share your public content  You will find assets published by either Google or our partners  Google publishes their unique EIS to common problems  In the partner section  anyone can create  and share their solution to monetize it  Second  and the important one  is to share your private content  Consider it as your internal solution repository  You can safely  and securely share content experiment solutions within your organization  This content won't be searchable  or available in public  In a nutshell  AI hub with AI platform pipelines  provides you the right tool kit for fast  and simple adoption of AI  It's a scalable way of implementing  or doing machine learning  or artificial intelligence  You can easily search and find the right pipelines for your business problems  You need to start from the scratch every time  that's not the thing  You can start with already set milestones  Once you find the right pipeline  you can easily deploy it with just one click of a button on Google Kubernetes Engine  or somewhere else on Google Cloud  Once it's deployed  you can customize it as per your needs  You can either add  or remove features  You can change parameters  or anything else  You can adjust it as per your experimental needs  Then you can run that pipeline to test the output  If you're happy with the results  if those results are matching your business requirements  you can then run it in production on Google Cloud with serverless  and scalable architecture  Finally  you can securely publish your new pipeline back on the AI hub  so that in future anyone else use it  and they need not to reinvent the wheel again  This will create a network effect  Over a period of time  you will see the actual impact on ground  You will feel that your overall deployment time has been reduced significantly  and you can run many more experiments  and target the right business problem  than figuring out the basic things  again and again 
xZpyjp5IVo0,System and concepts overview  In this module  we will learn how to train  tune and serve a model manually from the Jupyter Notebook on AI Platform  Here's the agenda for today  We will first briefly go over all the components of the model development process and how they interact with each other  Then we will focus on the dataset creation part of the story  Next  we will explain how to write AML model in [inaudible] learn  how hyperparameters can be tuned with AI platform and how to package the model ancient Docker Training Container  Then  we'll show you how to build  push  train and tune the model  using the Training Container  Finally  we'll go over how to deploy the model  as a rest API on AI Platform inquiry it  When you're building a Machine Learning model  you have three steps essentially  Step 1 is to create your dataset  which will be used to build the model  After you create a dataset and apply any needed transformations to it  you build the model  When the model is built and performing as expected  The last major step is to operationalize it  which means you train it at scale and deploy it  Operationalizing the model on AI Platform  consists itself of three main steps  The first step is to implement a tunable training application  which requires writing your model into a trained up by file  This will consists of the training code  in the configuration of the parameters  The second step  is to package the training code into a Docker Container  with all the training code dependencies  Operating Systems  libraries  assets  etc  These Docker Container  is used to kick off the training at scale on AI platform  The definition in configuration of this Docker Container  is specified in a file called Dockerfile  The last step is to specify the training configurations  such as the hyperparameter ranges to be tuned i1`1` n the config yaml file  We'll go over the details of each of these three steps in the next sections  When developing an ML model  developers and data scientists  usually develop most of their code on Jupyter Notebooks  On AI Platform Notebook  is a configurable Jupyter Notebook server on AI platform  This is a typical flow  Load the training data from BigQuery  You can then store the training files  which have been already transformed in Cloud Storage  package the training code into a train py file  This call will be pushed as a Docker image into Container Registry  trigger the trainee on AI Platform training  AI Platform also stores the training artifacts such as the trained model in Cloud Storage  Deploy the training model using AI platform prediction  so the model can be served  AI platform prediction does that  by retrieving the saved model from Cloud Storage in deploying it as an API  You probably noticed that I mentioned only the orange boxes in this diagram  not the white ones  This is because the description I just gave you  covers a manual process for the mouse tabs we discussed  The focus of this course  is how to automate this process  which includes services to allow for version control of the source code  continuous integration and deployment in pipelines  Here's a different view of how the components we've discussed so far  Let's see with the help of this diagram  how we are going to use them and how they interact with each other  At the center  that is the Jupyter lab Notebook  where we experiment much with our code  and interact with all the other components prominent  Again  this components are BigQuery  which you can use to create repeatable splits of the dataset  train validation in testlets  Cloud Storage  where we export the dataset splits as CSV files and CSV files are being consumed by the model  We also store the training models in Cloud Storage  The next component is Container Registry  or GCR/IO  where we store the training Container that packages are training code  The Docker containers  AI Platform Training  which is in charge of running the training containers for training in hyperparameter tune  and finally  AI Platform prediction  which takes a train model stored in Cloud Storage and deploys it as a recipe either inquiry  In the next section  we'll dissect each bar of this diagram and learn how to use each of these components  In the experimental phase that we have discussed so far in this module  every step of the process is done manually  Which means that  we manually run a cell in the Notebook for a given action of the process should be triggered  Such as building the training container around the training code or pushing it to Container Registry  In the next module  we'll see how to automate this process  We want each push to the code repository  that contains our training code  to trigger a rebuild of the assets that constitute our Machine Learning pipeline  Training Container  hyperparameter tuning and all those  We can then extend these Automation to automatically retrain the model  exporting the newly trained model to the model Registry and deploying it  into specifies serving infrastructure  This is called continuous integration in continuous delivery  or CICD 
o0MyjvrqdiQ,Create a reproducible dataset  Let's now dive into the details  We will start by explaining how to create repeatable splits of the dataset using BigQuery and how to store them in cloud storage as CSV files  Let's look at two components of the diagram at the piece of the diagram  which will allow us to split the dataset  First  we will use BigQuery to create repeatable splits of the dataset  train  validation and test splits  Then we will store the CSV files exported from the dataset in cloud storage  These CSV files will then be consumed by the model  and we also store the trained models in cloud storage  Throughout this module  we use the cover type dataset from the Machine Learning Repository in University of California  Irvine  The goal will be to predict the forest cover type from a number of features  such as the elevation  slope or soil type  For this exercise  the dataset has been already pre-processed and stored  ready-to-use in BigQuery  Next  we will see how to run queries from the Jupyter lab notebook to split the dataset into train and validation sets  export the results as CSV files and store them in cloud storage  That's what the dataset looks like  As you see  the Target column  which is the cover type  is a categorical variable  There will be a classification problem  Remember that in machine learning  the best practice is to split the dataset into three sets  A training set used by gradient descent to learn the model parameters  A validation set used to choose the best hyper parameters and best architecture for the model  and finally a typeset used to report the performance metric of the final model  Usually  the performance of the model on the training and validation sets is much better than on the typeset  Because we use this as splits during training and toning  To get a valid sense of model performance  we need to evaluate it on data that has been never seen by the model  the type split  How do you craft a query to create a split of the data? For example  if you want to have 80 percent of your dataset in the training set  you can use the RAND function within the where clause of the SQL statement  RAND is a SQL function that returns random numbers greater than or equal to zero and less than or equal to one  in a uniform fashion  Which means that if we filter the rows with this clause  we'll get roughly 80 percent of the data because 80 percent of the time  that number is smaller than 0 8  Let's try it  it's that simple  However  we need a better way of knowing which data belongs to which bucket  That being training  validation  and testing  This will enable us and our colleagues to repeat our experiments because the method we just saw won't give you the same result each time you trigger that query  We need a better approach to splitting our dataset in a way that allow us to reproduce exactly the same results  Looking at it even closer  in this example  a simple random function  we just grab a new set of five randomly selected rows each time you run the query  This also makes it difficult to identify and split the remaining 20 percent of data for validation and testing  In addition  the dataset might be already sorted  which could add bias interests sample because adding an ORDER BY also creates problems when you use mini-batch gradient descent  As I mentioned  for Machine Learning  repeatedly sampling the data you have is ideal so that your experimentation across different architectures and [inaudible] can be consistent  One way to achieve this is to use the last few digits of the Hash function on the field that you are using to split your data  A Hash function return a value like December 10th  2018 into a hashed numeric value  This hash value will be identical for every order December 10th  2018 value in the dataset  One hash function available in BigQuery is FARM_FINGERPRINT  If we hash the date  this approach is now repeatable because the FARM_FINGERPRINT function will return the same value every time it is invoked on a specific date  You can be sure you will get the exact same 80 percent of data each time  Here's an example of splitting an airline dataset based on the date  If you want to split your data by arrival airport  for example  so that 80 percent of airports are in the training data set  compute the FARM_FINGERPRINT on arrival airport instead of date  Looking at the query here  how would you get a new 10 percent data sample for evaluation? Change that less than eight in the query above  to equals eight  If a testing data  change it to equals nine  This way  you get 10 percent of samples in evaluation and 10 percent in testing  We can also take the module of the hash  and because the hash is constant  you get the same hash and the module then is the same  Now that is the question of what you hash on  You will want your hash of dataset on the chosen label  or on a feature that you use for training  Because you might introduce a bias in your training set  and you may also lose some precious information  You need to hash on the field that you absolutely won't use for the classification or regression task  and that's also granular enough for your desired module split  What does granular enough mean? Well  for instance  if you decide to hash on a date field  and you choose the year of the date  In that case  you have all the data from one year in your training sets  another data from a different year in a third set  That might not be good  because something might have changed between those two years  You want to have data from these two years in both your training set  and your test set  so that you don't introduce a bias by re-splitting strategy  You're probably now wondering  how am I going to find a way to balance all these nuances? What if we just concatenate all the fields into a big string? That way  we just need to use the two JSON string function to create a field that's not really meaningful and hash on these new field  That's what we will do in the lab later  Now that we have a strategy for splitting the data  we need to concretely run the query from the notebook  One possibility is to use the bq command  The command shown  bq query  creates the training data split using hash  as we discussed  and it stores this split into another BigQuery table whose name is specified with the destination table argument  Here  the table is called cover type dataset training  Now that we have our data as a BigQuery table  we can export that as a CSV file into Google cloud storage by using the bq extract command  where we need to specify the BigQuery table that was just created and the path of the cloud storage bucket that you want to export the data to  We'll use the same process for the validation split  except that we change the name of the table  and an important note  we also change the modules here so we don't have overlapping splits  Now let's see how we write and train the model  Actually  we'll both train the model  and tune the hyper parameter using AI platform 
VA4kWwJc9Uo,"Implement a tunable model  So far we have gone through how to create rebuildable splits of the data-set using BigQuery and how to store these sites in Cloud Storage  Now  let's look at the details of how to implement a Scikit-learn model that can be trained in tuned on AI platform  Just to recap  in the previous section we learn how to write in execute SQL queries on the AI platform notebook  to create our trained csv and valid csv splits and store them on Cloud Storage for training  In this section  we will learn how to implement the train py we discussed earlier  This py contains a tunable model code  Also  we will see how to write the config yaml file for tuning the hyperparameters  Remember that ML models are mathematical functions that relate an input to an output  To do that mapping  ML relies on parameters and hyperparameters  A parameter is a real valued variable that changes during model training  These are the weights that can be associated with the features that the model is using to create the associations with the output or label  A hyperparampter is a setting that is set before training and does not change afterwards  at least in general  because the learning rate is a hyperparameter and it can change from one epoch to another based on the chosen optimizer  Finding the best value for the hyperparameters is called tuning the model  AI platform training performs both training and tuning for us  As I mentioned  we will use Scikit-learn to develop our Machine Learning model example for this session  Scikit-learn also known as Sklearn is a popular Machine Learning library for Python  and it features various classification regression in clustering algorithms  Let's look at what a train py file should contain  It needs a scikit-learn pipeline instance  but we want to add a pre-processing step to the pipeline  so let's define that first  It's preprocessor is defined with two simple transformations  One  the Standard Scalar  which normalizes numeric features by subtracting the mean and dividing the result by the standard deviation  Two  a One Hot Encoder  which will as the name suggests  create one hot encoded vectors from the categorical features  Then  we define the pipeline object by adding the preprocessor followed by the learning method definition  In this case  a simple classifier trained using stochastic gradient descent  The file also new score to set the values of the hyperparameters before training  We add those by calling the pipeline set parameter method on the pipeline object  Some of the parameters that this method takes are classifier alpha  which is the L2 penalty parameter  or a regularization term and the number of gradient descent steps on classifier max iter  For sub gastric solvers such as stochastic gradient descent and atom  know that this parameters determines the number of epochs  or how many times each data point would be used not the number of gradient steps  Go to trigger the training from the training set using the pipeline method  pipeline fit  Here  we pass the features in x train and the labels in y train  Finally  go to evaluate the model performances on the validations split using the method pipeline score  Again  features from the validation set are passed in x validation and their labels are passed in y validation  That's essentially how you write your code to train a Machine Learning model scikit-learn  Now  let's talk about our next step in this exercise  which is to find the best value for our hyperparameters  Let's return to our piece of the code that sets the hyperparameters  As we saw  here we have two hyperparameters we find  alpha in max number of iterations  What should we set alpha to? 0 01? 0 1? What about the number of epochs? 200? 300? The building hyperparameter tuning feature of AI platform will help us with this  You would train many combinations of these values and determine the best combination for all problem in our data set  Hyperparameter tuning is personally one of my favorite features on the AI platform  We only need a few changes to our existing workflow to benefit from this advantages  All we need to do is  one  convert the hyperparameter to command line argument  Two  setup cloudml hypertune to record our training metrics  For TensorFlow models  the AI platform Training Service monitors TensorFlow Summary events generated by the training application and retrieves the metric  If the model was built with frameworks other than TensorFlow  such as in this case  which uses cycle line  or uses a custom container  you need to use the cloudml-hypertune by them Package to report the training metric app platform training  Step number three  make sure to export the final model  Finally  step number four  supply a YAML file containing and range of hyperparameters on the training job  Let's go into the details of each of those steps  For the first step  making the hyperparameter a command-line argument  we can use the fire by Cam package  which we import with import fire  Python fire is a library for automatically generating command line interfaces or CLIs with a single line of code  If you package your training code in a function such as train  evaluate  in your passage of fire  you will pass the command line arguments to the function argument with the same name  Next  you need to capture the performance metrics often model after it has been trained with this special combination of hyperparameter values  To do that  you import the cloudml-hypertune Python package from Google  is simply capture the tuning metrics  Using the report hyperparameter tuning metrics of a hypertune instance  In the back-end  hypertune will simply write this metric on the file system where the training Container is executed  at a location where the AI platform training knows retrieve it  That's how AI platform training can compare the various runs of the training Container with different hyperparameter values  You don't need to save the train model during the hypertune task  It may save you some time not to do so  However  you still need to save the model for using the final best values of the hyperparameters  For that  a simple trick is to retrieve the best hyperparameter values  retrain the model with these values in a known-to-new mode  and then check whether they hypertune flag is on or off to decide whether it's time to save the training model  Also note that  when you are implementing machine  when modern Python and easy way to save the train models is to pickle it using pickle background  After the model is serialized as a pickle  we exported to a Cloud Storage bucket using the \""gsutil cp\"" command that we invoke in Python with the sub process column  The last step is to create the training entry configuration file that we call config yaml  The config yaml file is where you specify the hyperparameters to be turned  in the range of values for each of the hyperparameters should be explored  You need to specify the type of the hyperparameter  For instance  discrete  For the discrete number of values should try  A double  for a range of values to explore  either in non-linear or logs scale  You can also specify the maximum number of hyperparameter combinations you want to explore using the max trials field  We use the max parallel trials to specify the maximum number of parallel trials to allow  But there is a trade-off here between speed and optimality  because the hyperparameters from training  we use previous rounds to determine the next best combination of hyperparameter values to be tried  For example  if you said max parallel trials to one  you have the longest training  because each hyperparameter value be tried individually  but each new trial would benefit from information of all the previous brands  You need to specify the name of the metrics  that [inaudible] from training we use for tuning the hyperparameters  Here  it is accuracy  This name should be the same name you use in the hyperparameter metric diagonal argument  when you capture the tuning  metric new training code with the hpertune package within trained up [inaudible] Finally  you launch the Training and Tuning  using the gcloud command  where you specify where the config yaml file resides  through the config command line argument "
AMWwXjiTCRs,"Build and push a training container  At this point  we know how to write the train py in the config yaml files  which will contain the training code and the training configuration respectively  The last stage before we can train the model is to wrap our training container into a Docker image  That's what we'll look at now  Wrapping our training code into a Docker container involves writing a Docker file that specifies the training code dependencies in terms of both the operating system in the libraries that the code in train py needs in order to execute successfully  When we have that file  we can build the container  You can think of this step as a compilation step  the results are inexecutable  which is the container image  Then we'll need to push this artifact  the container image  to Container Registry  from which AI platform training we'll retrieve it during training  The Docker file specifies which operating system to use with the \""FROM\"" keyword  FROM must be the first non command instruction in the Docker file  FROM can appear multiple times within a single Docker file in order to create multiple images  An image has a specific operating system  so you can choose the one that you want for your container  Then the sequence of runs statements followed by standard bash code is used to provision the image with other tools and libraries needed to run the training code  The four libraries we will need for our training code  are fire for exposing the command line argument  the hypertune library to capture the scoring metric  scikit-learn because our pipeline is in scikit-learn and pandas  Please note that you can put as many run commands in the Docker file as you want  The \""WORKDIR\"" command specifies what the current working directory should be  when the container is executed  In this example  we chose forward slash app  The COPY statement is used to load the training code  here train py into the container  In this example  train py will be copied in the container file system at the location forward slash app  forward slash train py because the current working directory was set to forward slash app with the WORKDIR command  The \""ENTRYPOINT\"" command specifies the command to be run within the container when the container is executed  Any additional arguments passed to the command that executes the container RUN will be forwarded to the ENTRYPOINT command as command line arguments  That's how you pass the hyperparameter values  to our training container  This command needs to be run from the directory that contains the Docker file in the train py and you need to specify the container image URI  This URI is then passed on the gcloud builds submit command in the tag parameter  The URI is built from the name of the registry  here  gcr io for Container Registry  followed by the project name  Know that every Google Cloud project has its own attached Container Registry  followed by a container name of your choosing  You also need to specify the dollar sign training app folder that contains both the Docker file in the train py file  Here is our final Docker file for this exercise "
mAK9KklkRYY,Train and tune the model  So we have push tune training Containers are Container Registry  Now we need to actually trigger the trainee so that we can train into in the model  Let's see how we can do that  In the previous section  we wrote the trained up indocker file and we learnt how to package all our training code and all that needs to be known and available to train our model into that little docker container  We also saw how to push it to container registry  Now  we will learn how to retrieve that image from the registry so we can train and tune our model at scale using AI platform  We can execute our trainer package locally  After we ensure everything is working  we use the GCloud command to submit the training job to AI platform  To train in the Cloud  we run the command GCloud AI-platform jobs submit training and choose a job name  This command has two main parts  First  you need to provide the URI of the container image  The URI should be exactly all you defined when you're building and pushing the container with GCloud builds submit  Then you need to provide a path to the config yam1 file  which contains the information about which hyperparameters to tune in which a range of values to try for each of them  Note they're scaled to your option here  There are more details in the documentation  but you can choose from predefined tiers or create your custom tier  Basic  for example  uses a N1 standard four virtual machine  You can also choose a virtual machine image we support four accelerators  such as GPUs and TPUs  You can set additional parameters when triggering a Cloud training jobs such as the path to the training and validation data sets  and whether you want to use hyperparameter tuning  You can check the documentation for more details  After you launch the command from the Jupiter lab  training will start  After the training job is complete  the console will display the various runs with both the hyperparameters and performance metrics  You'll see the different trials or runs of the training with the different parameters that have been trained along with the target metric  In this example  we find the accuracy for different values of the maximum number of iterations  For example  200  500 and again  200 and 500  tried with different values of Alpha  If you click on the job from the list  you see more information about that particular job  The best model from the training job would be the first one on the list of hyper-tuned trials because the trials are sorted in descending order of the metric chosen  In this case  the best accuracy will be at the top  For this example  the best model is trained with a max iteration of 500 in a value for Alpha of 0 00028  These achieve the best accuracy of 70 2 given their number of trials  of course  Now that we know what the best values for the hyper numbers that we tuned or  our goal is to train a model with these hyperparameter values  save the model and deploy it to start getting predictions  There are two ways to do that  We can just look at the values on the console in anther them manually  we are studying a new training job  or we can do that programmatically  In the lab  we'll be doing it programmatically  so let's look at that  To retrieve the best parameter values  use the Google API client as shown here  This example uses the API client to retrieve the best value for the hyperparameters alpha and makes either from the first trial  Remember that the first trial  trial number zero is the best one  So we look at the first trial and retrieve the corresponding values for the hyperparameters  After we retrieve the best values for the hyperparameters we need  we should use them to retrain the model  save it and deploy it  To retrain with the best set of parameters  use the same command as we did for the training in 20 phase  but this time  make sure you omit the config argument  Instead  you'll pass the actual best parameter values through the command line after the backslash argument  Remember to also set nohptune  so that when the code is executed  hptune is set to false  The training model will then be saved to Cloud Storage at the end of the run  Remember that in train the Pi  we had a conditional statement looking at the hptune value to decide whether to pick on the model and export the model to Cloud storage  Here we'll execute the code that does that when hptune is equal to false 
_ffzSGKgJcE,Serve and query the model  Now that we know how to train into and models using AI platform  Let's see how to deploy them as rest APIs so you can send predictions from the model  Will be talking about the last component in our system  AI platform prediction  AI platform prediction retrieves the trained model and saves it as a pkl in cloud storage  It can then be deployed as a REST API that can be query from anywhere  including the Jupiter lab notebook  Deploying a model requires two steps  First we need to create a model object  For that  we use the command git Cloud AI platform models create and assign a name to the model  Next  we run a similar command  which is gcloud ai- platform versions create to create a version of the model  At this point  you tie the actual model saved as a pkl on some cloud storage bucket to the version through the origin argument  which is the path to where the model pkl is located  We're using scikit-learn so we need to specify that in the framework argument  and because we do that  the gcloud ai platform command knows that you should expect a pkl  If we're using [inaudible]   you'd expect a checkpoint or saved model instead of a pkl  Finally  the last step of our lesson is to set a prediction requests that can be done easily through the command gcloud ai platform predict  For the prediction  it is necessary to specify both the model in its version which were created previously  We also need to pass the examples for which we want to receive predictions from the deploy model  and that's it  This conventional return the prediction results for the examples present in the json and input file 
ZD3Ai0BHYAQ,System and concept overview  In this module  we will automate the training and tuning process with described before using this time Kubeflow pipeline  Instead of having to trigger every single step of the process manually from the Jupiter Lab Notebook  we can trigger the entire process with a single click after we have expressed the various steps as a Kubeflow Pipeline  Let's start by describing some high level concepts  It's very easy to think that machine learning code  then machine learning model development take up most of the work  In reality  most of the machine learning systems are large  complicated and distributed  And most of the effort is in the DevOps work that surrounds the actual model code development  Kubeflow was developed to use Kubernetes to standardize and streamline the DevOps work around machine learning  We know that the machine learning process can be split into sequence of more or less standard steps  and model training is just one of them  These steps can be arranged in a pipeline of tasks that are organized into a directing  basically graph  So for instance  here is a very simple and relatively standard machine learning pipeline  Is just a strain of tasks that starts with loading the data from a data store  possibly analyzing and extracting the raw data  And then a series of tasks that are linked to model development that is  model preparation  model training  model evaluation  and validation  The last task is after the model has been trained to export it so that it can be later deployed  The DevOps involved in the machine learning process can be quite difficult  why? Because during model development and experimentation  the part of the code that includes data preparation  model training  and model evaluation will change significantly  The part of the pipeline that concerns model development will change to produce different models with different performances  This model must be compared so that the best model is saved and deployed to prediction  We need an infrastructure that's flexible enough to allow these different versions of the model development pipeline to coexist in the same environment  That's a tricky DevOps problem  So for that Kubeflow provides standardized platform to build this machine learning pipelines  There are some very good aspects of the platform  It leverages containers and Kubernetes so it can be run anywhere Kubernetes is already running  especially on premises with Anthos with Google Kubernetes Engine  It's open source  it's cloud native  it's composable  portable  and standard  which means that you can create a component  reuse it in a different pipeline  and share it with your colleagues  To sum up  Kubeflow allows you to orchestrate the entire machine learning workflow  You can share and re-use the components in a very standardized way  that allows for rapid and reliable experimentation  Okay  let's look at what constitutes a Kubeflow pipeline  In a Kubeflow pipeline  each of these machine learning tasks is conceptualized as a docker container  Each task corresponds to a docker container that's executed  What kind of tasks will these containers execute? This tasks can be data import  model training  model serving  or modal evaluation like the task we saw earlier in the simple chime ML pipeline  This is useful because containers are really portable  so you can run these tasks anywhere you can run a container  Also available are prebuilt containers that allow you to use them  any resources from Google Cloud  an AI Platform such as Dataflow  Dataproc  BigQuery  AI  Platform Training  and AI Platform Prediction  The simple Python SDK allows you to describe the pipeline in code  The Kubeflow UI allows you to visualize the pipeline and view the different tasks  You can click each of these tasks for more information  such as where the corresponding container is located  In this case  the pipeline as a first data processing stage  and then a second stage which is feature engineering  In this pipeline  we decided to train various models on the same data set to see which one performed best  This is done by the next component  which looks at the model training output to score which model performed best and then deploys it  This is another example of what a machine learning pipeline that is a bit more complex than the single chain of machine learning tasks we started with  It's also interesting that after the tasks I've been executive  some artifacts are saved  These artifacts of different characteristics  For example  if you look at the artifacts for the training component  you might see the various performance matrix  the various training curves  the RC curves  etc  From the UI  you can see all the configs  and all the inputs  and outputs of each of the components  So you will never say  I have trained a model  but I can't remember which parameters I trained that model wave or which data set I used for the model  All that information is saved so you always have access to it  In terms of coding  what does it look like to implement a Kubeflow pipeline? Well  you can simply do it in the notebook if all the corresponding tasks are already available as Docker images pushed to a container registry  Start by importing the Kubeflow Python SDK  then implement the pipeline by composing the various tasks with each other  creating a graph  And then compiled by applying to a YAML pipeline description file which you upload the Kubeflow cluster  All that can be done directly in the notebook  You can also upload the pipeline description file manually through the Kubeflow UI  After you upload the pipeline  just enter the run parameters where the input is located  where the output is to be stored  which project it is  the Google Cloud region  where the pipeline will be executed  the location of the training data  the hyperparameters values  etc  Do that and then create the run  and the pipeline will run with these parameters  So that's the basics of writing and executing a Kubeflow pipeline 
QmoD_ELFMgo,Describing a Kubeflow Pipeline with KF DSL  Let's now look at how to use the Python SDK to describe a Kubeflow Pipeline  Kubeflow offers a Domain Specific Language or DSL that allows you to describe in Python code  how Kubeflow tasks organize themselves into a dependency graph  We described this DSL next  In the Python code  we start by importing the Kubeflow Pipeline package with the import KFP Studio  Then we have it function which we called here covertype _ train  in which we described the Kubeflow Pipeline itself  A Kubeflow Pipeline function  must first be decorated with the decorator  The kfp dsl pipeline decorator  We will name the pipeline and also describe what it does  This metadata is then available through the UI when we upload the pipeline  Next we choose the arguments the pipeline will takes as input and these arguments will become the pipeline run parameters in the Kubeflow UI  Here you see the translation in the UI of this pipeline arguments  So that's the general structure  Now we need to understand what goes into the body of this Kubeflow Pipeline function  How do we concretely describe the pipeline? In the body of the pipeline function where we describe the pipeline  there are essentially two steps  In the first step we instantiate the Tasks as Kubeflow OPs to be composed  These OPs  represent Docker containers that are executed when the tasks are run  In the second step  we composed these OPs  That is  we specify the order to run these tasks in an URL output of one component is passed to the input of another component  Now  how do we create these components and how do we compose them concretely? Here we have a prebuilt component that triggers a training job on AI platform training  To instantiate the corresponding OP  we use the mlengine_train_opconstructor function  It takes several arguments and then creates an operation called  here train _model that we can retrieve outputs from  There is also a second component  which we create by using the evaluate_model_ opconstructor function  Now  how do we pass the output of the first OP to the input of the second OP? This is simple  we just take the firstOP train_model and retrieve its output dictionary  Many outputs are stored in the dictionary  such as the output_ gcs_path or the job_dir  Then we pass that output to the input of the second component as an argument to the evaluate_model_opconstructor  So that's relatively simple  Now  you can imagine cases where you want a special component to be executed only if the output of another component is below or above a certain threshold  Typically  you might want to deploy a model only when the component that evaluates your model as an output for the metric value that's above a certain threshold  Kubeflow as a construct for that  which is this context block with kfp dsl condition  It allows you to pass the triggering condition  which called also involve outputs of previous components  Every Kubeflow component or task coincides with a docker container being run  In practice  there are very different ways to create and use this components  The prebuilt components allow you to simply load the component from and new RI and compose them  More complex  but more flexible  The lightweight Python components allow you to focus on writing the code for the task while all the containerization is performed for you  Finally  the custom components give you more flexibility  however the cost is that you have to write both the task code and the containerization code  We will discuss now each of these components in detail 
QbczPuEphUY,Pre-built components  Let's start with the simplest components  the pre-built components  GitHub has a repo full of pre-built components  To use them  you need a URI to the component yaml  which is the component description  Here is an example of such a component description  It essentially describes where the component Container is through a URI that points to the Container Registry  The second part specifies the component run parameters  You have to pass these arguments to the corresponding Kubeflow op into pipeline code after you load the component  Here is the corresponding pipeline code to load pre-built components  First  you instantiate a component store by providing the URI of the GitHub repo  where the component descriptions are  Then using that store  load the components by their names  For example  here we're loading a component that allows us to query bigquery  Another component that allows us to trigger training jobs on AI platform  The last component that allows us to deploy trained model on AI platform  For special components such as the bigquery op  we can look at the documentation on the GitHub repo  The documentation will tell you how that component op must be instantiated  For the bigquery component  you need to provide a SQL query  the project ID  a dataset ID  and several other arguments  Let's look at the AI platform training component  Its arguments are the Google Cloud project ID  the region where you want to do the training  the URI of the container that contains the training code  and the job  dir that you want to export the model to after training  To deploy component is similar  You need to specify  again  the project ID  the model ID  the version ID  and the location of the same model that you want to deploy  All the operations of training  tuning  and deploying a model on AI platform can be represented by Kubeflow pre-built components  You can compose components as functions by using the op outputs to change the output of one component to the input of the second  For example  this is how you set up the AI platform training component to do hyperparameter tuning  You first retrieve the training and validation data path from the output of the previous component and pass that data to mlengine_train_op  Then  for hyperparameter tuning  you also need to provide a dictionary  HYPERTUNE_SETTINGS that specifies which of the hyperparameters of the model you want to tune  The HYPERTUNE_SETTINGS dictionary looks like the image on the slide  It's a Python dictionary representation of the training config yaml file we introduced in a previous module  It allows you to specify the maxTrial and maxParallel trial numbers and the name of the hyperparameters that you want to tune  This is the same AI platform training component  but set up to do the final training with the best parameters values directly passed to it  After you retrieve the best run hyperparameters  you want to retrain your model with these  but not in a tuning mode  To do that  use the same component but with different training arguments and don't specify the HYPERTUNE_SETTINGS dictionary this time  Note also that the --hptune flag is now set to false  Here we see the graph of a Kubeflow pipeline using all these pre-built components and how we can visualize the component composition described in code in the pipeline 
QhKIaK4ReUA,Lightweight Python Components  That was a discussion of the lightweight components  the one that are already completely written for you  You only have to load them  and compose them  Now let's look at components that are a little bit more complex  but also a little bit more flexible  The lightweight Python components  Suppose that you have two Python functions that you want to wrap into two Kubeflow components  You don't really want to write the full Dockerfiles  and build and push the Docker images into some Container Registry for just two small Python functions  The Kubeflow SDK allows you to do that easily without writing any of the containerization code  These two functions are in helper_component py  The first function takes a project_id and a job_id as input and then retrieves the best training run  that is  the values of the best hyperparameters  The second function takes a dataset_path  and a model_ path where the saved model has been exported under metric_name to evaluate our model  It returns the matrix computed on that dataset for this model  Now  we want to wrap these two simple small functions into two Kubeflow components  For that  we'll use the func_ to_container_op helper function that we import from kfp components  The helper function takes as input the function that we want to wrap into a Kubeflow component along with a base container image  Behind the scenes  it loads our Python code into this container  This is why we need to specify the base_image  that func to container op will wrap the function with  We didn't use and compose the lightweight component as the pre-built one using the handle returned by func_to_container_op  Now  we already use these lightweight components the same way we use pre-built components  We get the handle to the component from the func to container op  Now we just pass the arguments  which are the arguments of the original function here the project_id  and the job_id for retrieve best run op  For example 
TNIaPKDu678,Custom components  But now  what if the task is much more complex? Maybe you need many files  Maybe it's written in a different language  Maybe it's go  in that case  use custom components  For custom components  you write the code that prescribes the behavior of the component  along with the code that creates the container  which encapsulate all the dependencies of your code  You have to follow several steps to achieve that  The first step is to write your own code  and because you're in charge of writing your own container  it can be in any language  Then you write a Dockerfile to package this code into a Docker container  Build and push the Docker image to container registry  so that the container is available  Then write your own description of the component in the same way as prebuilt component description files we saw on GitHub  This description files essentially specify two things  the URL of the corresponding image on container registry  and the run parameters to the component  The last step  is to use the description file to load the component into the pipeline  So  let's see how to do this in code  Here we have main py  to query BigQuery  and create the train and evaluation splits from the dataset  We could have used the BigQuery built-in component  but here  we are just demonstrating how to do it from the beginning using custom components  Now  we need to wrap that code into a Docker image  For that we write a Dockerfile that specifies the setup we need for the base container so that our main py can be executive within it  Let's look at this Dockerfile  first it imports an OS with the FROM statement  Then we use the RUN statement to run a few bash commands to install the required software and libraries  After what  we use the COPY statement to copy our code and use the WORKDIR statement to change the current working directory  Finally  we use the ENTRYPOINT statement to specify the command to be executed when the container is run  The third step is to write your own component description in yaml  You first specify a name and a description for your component  All that information should appear later on in the QPro UI after the component is loaded  Then you set where your container image has been pushed to  this means that the container image must have already been pushed  In addition to the image URI  you also specify the arguments with which the container will be executive  These arguments can be crafted from the input parameters as shown in this example with the Input Bucket input field  These arguments will be passed to the ENTRYPOINT command specified in the Dockerfile  Now you have everything ready  You have the code  the container for your component has been pushed to the registry and you have your component description somewhere  How do you load that custom component into the pipeline? Well  it's very simple  you can use the load_component_from_file helper function  to which you just need to give the URI of that component description file  As a result  you obtain your component op  which you compose as any other component  So it's not very different from what we have seen before  There's just a little bit more work to write a containerization code  Finally  you can compile  upload and run the pipeline  we'll dissect that in the next section 
#NAME?,Compile  upload and Run  Now that we have reviewed the different types of Kubeflow components  the trade-offs between them  and how to describe a Kubeflow pipeline in code  The last step is to know how to compile the pipeline  upload the result of the combination to the Kubeflow cluster and create a run  The first step is to build and push all the containers that might be needed by the various components of the Kubeflow pipeline  These are not necessarily containers representing Kubeflow components  They can be ancillary containers that are needed by these Kubeflow components  For instance  this AI platform training components will trigger a training job on AI platform  That component  which is itself a container  needs yet another container  The training container that contains the training code  the train py file  Then we'll be run when you start AI platform training  AI platform training will first try to retrieve the training container from the Registry  so that training container must already be in a Registry before this component is run  Here we see the code to build and push the training container to the Registry using G-Cloud builds submit  Step two  we build and push the base containers that are needed by all the Python lightweights ops  Remember  a Python lightweight ops wraps a Python function into a base container  For that  we need to specify which base container to use  The wrapping of the code within the container is done behind the scenes  But we first have to push that base container to the Container Registry  Here  the containers we will use for lightweight components need the Python packages  fire  scikit-learn  pandas  and KFP to be installed  We use the G-Cloud builds submit command to build and push the container to the Registry  Step three  we compile the Kubeflow pipeline  which means that we use the DSL-compile command to which we pass the Python file that contains the pipeline description  Basically the DSL-compile command will transform the Python code describing a pipeline into a YAML file  which can then be uploaded to AI platform  Step four  is to upload the pipeline to the KF cluster  We can do that from the Kubeflow UI or we can do it programmatically by using the KFP pipeline upload command plan  In that case  we need to specify the Kubeflow cluster endpoint  which we can retrieve from AI platform pipelines in a common line  In both cases  the goal is to upload the YAML file that we have compiled and that describes the Kubeflow pipeline to the cluster  We can verify that the pipeline has been properly uploaded by using the KFP pipeline list command to list all the pipelines that have been uploaded to the cluster  Last step  how do we run the pipeline? We can use the KFP run submit command  which we need to specify the end point of the cluster to as before  We also need to give experiment underscore name so that each run of the pipeline is organized into an experiment  It's like a folder where you can sort all of your runs  We need to provide a run_ID  the ID of the pipeline  and all the run parameters that we want to set  All these run parameters correspond to the arguments of the function that defines the pipeline in the Python file  Also  these arguments are exactly what you see in the Kubeflow UI  when you create a run from the pipeline  Here we have the project_ID  the data-set_ID  the Google Cloud region  etcetera  Okay  lab time now 
TdtFxXo2zpg,Concept Overview  Hi  My name is Kyle Steckler  and I'm a Data and Machine Learning Instructor at Google  In this module  we'll be talking about CI/CD for Kubeflow Pipelines  We already know how to build an automated Kubeflow Pipeline  but how can we integrate this pipeline in a continuous integration stack? The goal is to rebuild pipeline assets immediately when new training code is pushed to the corresponding repository  Let's start off by discussing some of the core concepts that are involved in this process  Here's the idea  We experiment first with the model code  Then when we're ready  we push new code to our GitHub repo  and all the assets are rebuilt and available in production automatically  We want this to happen without having to track or manually trigger all the changes  Continuous integration simply refers to automating the process of this rebuild anytime code changes  In theory  every Container corresponds to a self-contained directory in a repository  When the code changes  we want this Container to be rebuilt and push to the Container Registry automatically so the new version is available for consumption in the production environment  Now GitHub uses several triggers to start a new Container build and push  But first  we need to connect our GitHub repo to Cloud Build  Cloud Build allows us to monitor pushes to a specific branch  tags  or pull requests to trigger a rebuild  We'll discuss how to build configuration files that will tell Cloud Build what Container to rebuild based on a trigger  But first  let's review the high level end-to-end process  Here's the full CI/CD stack for a machine learning system  After experimentation  the code is pushed to a repo  that's triggering a rebuild of all the assets  which are then pushed to an artifact repository like Google Cloud Registry  The models are then retrained using the new training images that have been pushed into the registry  If the model meets the criteria  they're deployed to AI platform prediction where there API is monitored  In this module  we'll learn how to configure the continuous integration stack on Google Cloud so that the assets of our previously built Kubeflow Pipeline are rebuilt when code changes are pushed  Let's dive in 
GiBG9_6w3hw,Cloud Build Builders  With Google Cloud  the CI/CD stack is powered by Cloud Build  One of the main components of Cloud Build is cloud builders  so let's take a look at these  What are cloud builders? These are cloud configuration or provisioning actions that are packaged as Docker containers  Now there's many cloud builder actions  Some very common actions are building a Docker image from a Docker file  pushing a Docker image into Google Cloud registry  deploying a VM instance on Compute Engine  or uploading a Kubeflow pipeline to AI platform Pipelines  Now there's two types of cloud builders  standard builders and custom builders  Standard builders are already packaged configuration actions that are really common  such as building a Docker container and pushing that Docker container to a registry  All these prepackaged configuration actions are in the GoogleCloudPlatform/cloud-builders repository  these are available for you to use  Custom cloud builders are special configuration actions that you have to package into Docker containers yourself  generally these are pushed to your own Google Cloud registry  Let's take a closer look at the standard cloud builders  Here are some of the common standard cloud builders  Docker is used to build and push containers  while gcloud is used to perform various configuration actions on Google Cloud  Here we can see an exhaustive list of all the configuration actions  we have Docker  Git  GKE-deploy  Cube Control  etc  I encourage you to take a look at this repo and see all the prepackaged configuration actions that are available to you  Now if we examine how the standard cloud builders are actually implemented  we see they boil down to two parts  One  a script that executes the configuration actions  And two  a Docker container that wraps the script with all the dependencies that it needs to be executed  Here's the Docker file  it simply provisions the OS with what's needed to run the notice sh script and an entry point that launches the script  The script simply invokes the Docker command and passes it all the arguments that were passed when the Docker container was executed  So what exactly is a custom cloud builder? With custom builders  you must write your own Docker file and configuration script  For example  here's a Docker file to provision a CPU deep learning container with the cube flow SDK  You might want to use a custom builder image for things like downloading source code or packages from external locations  using an external tool chain  or caching any necessary libraries  With custom builders  you need to push the container yourself into your own project container registry 
#NAME?,Cloud Build Configuration  Remember that the goal is to run a cloud builder whenever a trigger is detected  Well  this raises the question  how does Cloud Build know which cloud builder to run? Well  the answer lies in a Cloud Build configuration file  We tell Cloud Build which builders to run  in a cloudbuild yaml  cloudbuild yaml describes the cloud builder to be run  and what arguments should be passed to the entry point command  defined in the corresponding docker file  We do this with two things  first with name  and second with args  The name is the URI of the corresponding cloud builder container  and the args  contains the arguments to be passed to the entry points  How do you trigger the build itself  and execute all the build steps described in a cloudbuild yaml file? Well  we simply use the gcloud builds submit command  pass it the cloudbuild yaml file  as well as some  what we call a substitution variables  which we'll discuss a little bit later  Let's take a look at one single step  one single cloud builder in the configuration file  We have  first of all  the name  Again  this is simply the URI of the cloud builder container  in container registry  Here we say  I want my first step to execute that docker cloud builder container  We also provide args  which are just the arguments to be passed to the entry point  We actually have a third field here  dir  and dir specifies the current working directory within the container  from which the entry point will be run  Let's take a little bit of a closer look at this dir field  The dir field allows the use of persistence  This is very important  Imagine that we run each of the steps  each of these containers individually  Well  each container has its own file system  so the assets that might be created  produced  or returned within the container directory during one step  might not be available to the next container  This is a problem  because some of these steps might need some assets  For example  a container might compile some code  and another container might push that code elsewhere  In a situation like this  you need persistence and some asset sharing between containers  By default  the current working directory will be slash workspace and any assets that are written here will be shared across all the steps executed during a build  If you change that directory to something else  this is no longer the case  You have to be very careful if you change this directory  Another important and very useful mechanism for the cloud builder  is substitution  Cloud Build allows you to substitute variables before each of the individual cloud builders are run  In your cloudbuild yaml file  you can use variables with a dollar sign  followed by an underscore and a variable name  Then when you invoke the gcloud build command  you can replace these variable values through the substitutions command-line argument  This is incredibly useful for creating generic and reusable configuration files  What exactly is the difference between the cloudbuild yaml file for a custom builder  versus for a standard builder? Essentially nothing  The only difference is that the container URI will probably point to your own project container registry  The rest is exactly the same  Simply pass whatever arguments you want to the container  specify the current working directory that you want the command to be invoked within the container  Another commonly used trick  is to have our configuration script taking inputs from environment variables  We'll actually use this in our lab  to compile our Kubeflow pipeline  Cloud Build needs a way to specify environment variables when it runs the container  To do this  we have the env field  where we can specify the name of an environment variable  as well as its fixed value  You'll frequently use environment variables in conjunction with substitution fields  These two mechanisms combined really help us create generic and reusable Cloud Build configuration files  We also need to tell Cloud Build to push the container that has been built  Very often and especially with machine learning pipelines  you'll have to build and push multiple training containers  To do that  you need to first build the containers using the standard docker cloud builder  then in an additional field  you need to specify the tag of all the containers to be pushed  Now  note you don't actually need to specify where to push these containers  because the location is already given by the name of the container image that you built  If you forget this field  your container will simply be built  but not pushed  so not available in production  So it's very important to remember to push 
jBUfYN5plOY,"Cloud Build Triggers  Now that we know how to specify the configuration actions for Cloud Build and how to invoke the build manually using the gcloud command  let's see how to trigger that build from GitHub triggers  Recall that to trigger a build manually  you simply download the repo that contains cloudbuild yaml as well as your training container code and run gcloud build  submit from it  We want to automate this process when new code is pushed to GitHub  Here's an example of how you would do a manual build from a notebook  Remember that substitutions are incredibly powerful  especially if your configuration script reads environment variables  How do we automate that with a code push? As mentioned before  Cloud Build can manage multiple actions on GitHub  such as pushing code to a new branch  creating a tag  or pull requests  We want to configure Cloud Build to execute the build using these triggers  The first step towards this automation is to link the GitHub repo with the Google Cloud Project  How do you do that? It's actually pretty easy  You just go to GitHub and activate the Google Cloud Build app  During this process  it's going to ask you which repository you want Cloud Build to monitor  You can select all repositories  or just a specific repo  on the Google Cloud side of the process  you'll need to specify which repo you want to monitor with Cloud Build  Here we simply select our MLOps repository and click Connect repository  Now we have a link between Cloud Build and our GitHub repo  Cloud Build is aware of various GitHub actions  The next step is to specify the Cloud Build  what action to take when one of these triggers is detected and we set this up within Cloud Build  You simply select the type of event you want to monitor and the source repository  After this  you need to specify the location of the cloudbuild yaml file in your repo and you do this in the field Cloud Build configuration file location  The UI allows you to set the substitution variables  here we're setting Base Image Name  Component URL  search prefix  and endpoint  These are the substitution values that we'll use in the upcoming lab  If everything is set up correctly  you can see the trigger in the Cloud Build UI  Here  one repo is being monitored the MLOps on GCP repo and it is monitoring the creation of a new tag  When I create a new tag  Cloud Build will be run with this configuration that lists all of my cloud builders  If I tag my repo and push the new tag  the build will be triggered  Additionally  you can monitor this process and each of the steps  For example  here  the first training code image is being built and pushed  Next  the base image for all of the lightweight components that are built and pushed  Then the kubeflow pipeline is compiled into a yaml file  Finally  that same kubeflow pipeline is uploaded to the cluster  Each of these steps corresponds to a step in the Cloud Build file  After you've tagged your code and pushed your tag to GitHub  the build will complete  Everything is triggered automatically  Now you can see the new images that are rebuilt and pushed to the registry along with an updated version of your kubeflow pipeline  Basically you just changed the code in your repo and tag all the assets that depend on this code and they're automatically rebuilt for you without you having to think  \""What should I rebuild now that I've changed my code?\"" It's all done for you  Next time you run your pipeline  you will run the pipeline from all the assets that have been uploaded in the new containers "
ThnCx4MTVNw,(Optional) Intuition  Hi there  In this video  we start learning machine learning  During the first lesson  you will get the understanding of what machine learning is and why is it popular and widely used nowadays  At the end of the lesson  you will know machine learning basic concepts  types of machine learning problems and tasks  main concept of supervised and unsupervised learning  And finally  how machine learning is used in the industry  Machine learning is a big part of data science which allows you to utilize data you might have and gain some knowledge from it  From this perspective  we can see machine learning as a set of the algorithms and tools  which allows us to process data in order to gain some information to influence the processes described by this data  That is why the first question we should discuss is  where do we get the data? What kind of data can be used for the analysis? Where can we get it? Basically  there are a lot of different data holders in many fields such as IT companies  telecommunication and banks  Let's take a look at the example  For example  search engine collect a lot of data about users' interactions with their service  This is the data about queries user asks  Data about users  country from where user came  the language used in query  time of service usage  data about search results users got  data about page clicked and skipped by user and many more  There are huge amount of data that companies like Google collect every hour  Social networks also store a lot of information  There is some data about users  social and demographic informations  posts  photo  videos and links  information about user social graph and posts  user liked or disliked and many more  Banks are also huge data holders  For example  they store information about each user's transaction  Just imagine how many times we do something using credit cards or make online payments? All this data is accurately collected and safely stored  Of course  mobile operators have a lot of information to store  For example  data about service usage like incoming and oncoming messages and calls  mobile data access  Wi-Fi usage and so on  Not only cell phones are data sources  Nowadays  there are a lot of devices that can create data regarding actions happening on the device and device state  Smart home appliances  cars or industrial sensor are definitely a good example of such devices  These devices can not only generate data but also send them to the Internet  and this information is also very interesting for the analysis  Apart from the business and industries  there are other data holders like the ones used in science  For instance  in bioinformatics there is a lot of data about genomes that is securely stored for the studies  So now  you get a clue about the data that can be used for the analysis  The second question is  how we can utilize this data? What can we gain from the analysis of such data? Again  let's take a look at the examples  The first and very standard approach for machine learning used in the banking field is the credit scoring  It's crucial for banks to decide whether they should provide a loan to a certain customer  and this decision can be made automatically  Based on the data of the loans provided and history of balance  one can build a pretty precise model that will estimate the risk of a default  The second approach is the demand forecasting  This problem isn't limited to banks  but it's also applicable in different fields  For example  for banks  it can be demand for cash in ATMs  If you're speaking about the retailers  it can be demand for products  Another good example is a churn prediction and prevention product  If you are working in BDC then most likely you would prefer to keep as many clients as possible to increase your revenue  Thus  having a good model for churn prediction gives you a chance to prevent losing your clients  The next example is a search quality itself  The ranking problem can be also solved with the help of the machine learning  To find documents relevant to users query  search engines use ranking models based on the historical data of users' preferences in terms of the queries and results of the search  Recommendation of the products  services or media content are also an example of the machine learning based service  Such system both have to improve users' experience and increase some business KPI  search revenue or engagement  Hence  some historical data about users preferences and interests  one can create a recommendation model that will suggest to users some products which they might like  Probably  you can already imagine how many different problems can be solved with help of the machine learning  Now as the last I'm going to share a couple more nice machine learning applications with you  This is an example of race and gender recognition service  You can upload your own photo and check how well it works for you  For sure  a lot of you got to use this service for traffic jam forecast or navigation  Such services also use machine learning techniques  And there are a lot of nice services and technologies that are actively developing or being tested now like self-driving cars  testing for advanced product optimization and control  medicine robotics and many others  All these services can benefit from using of the machine learning on one or another way  So what is the machine learning itself? Arthur Samuel gave the following definition of machine learning in 1959  Machine learning is a field of study that gives computers ability to learn without being explicitly programmed  I think this is a very nice definition that is still correct  Machine learning is a subfield of computer science  It evolved from the study of pattern recognition and computational learning theory in artificial intelligence  Machine learning is a recent state that allows us to utilize data for building smart models and services that are widely used in many fields  In this video  we have discussed many examples of the machine learning usage for different purpose in many fields  Now you'll notice there is a lot of data that is already collected and stored by many data holders  This data can be used for analysis  and we can get many benefits from such an analysis  Machine learning is a suitable in modern technology for doing that  Sounds interesting? In the next video  you'll learn basic concepts of machine learning  See you 
wb6kEUGV7TA,(Optional) Basic concepts  Hi there  we continue our dive into machine learning  In this video  we will meet machine learning basic concepts  First we will discuss mathematical objects and concepts which we will operate  And then we will see how to work with these concepts on the examples  When we speak about machine learning problem in general  most often we work with the following abstractions  There are some objects for which we need to predict and estimate a known value  Because it's really an answer or a target  We denote object by x symbol and we denote the answer or target by y symbol  So the X is the space of the object and the Y is the space of the answers or target  In other words  X is an input space and Y is an output space  Let me quickly remind you the age and gender recognition problem from the previous video  In this example the object is a photo with my face and the answer is age and gender  The amount of all the photos used for the age and gender recognition in this service is a X  and the unknown age and genders are Y  Here is the model prediction is fully correct so the real answer is equal to the model answer y*  Eash result is represented by the vectors of features  So object x is an n-dimensional vector or numerical features that represents an object  It's features can be not only numerics  but anything that you can process with the help of the computers  In our example is age recognition raw pixels of the photo could serve as the features  The next concept to discuss is a dataset  A dataset is a collection of objects and answer pairs where each object is represented by a vector of features  And each answer is a value we should predict for this object based on the features  Each dataset consists of a number of objects and answer pairs  To solve machine learning problems in general means to create a model an algorithm which will give us answers to the objects  in terms of face recognition  we are looking for the algorithm that will give us the correct age answer based on the given photo object  What we are trying to create is during the process solving machine learning problem is the algorithm  The algorithm should help us to create some function that will be able to get the answer based on the features of the object  Mathematically speaking  we are looking for the method from the object space to the answer space  So a is an algorithm which is mapping from X to Y  A is a space of algorithms that belongs to the same algorithm family  For instance  linear models or decision trees  This algorithm space is also called hypothesis space  We will discuss details later on  What you should get for now is the endurance and model train  Basically  we are looking for the exact algorithm  a  from the family of algorithms A  In an ideal scenario  this algorithm should give us the correct answer for each object  But in the practice  this is hardly possible  Very likely our algorithm will make mistakes for some objects  From this point of view  we are working with optimization tasks  We know that we are probably not finding ideal algorithms  but most likely we can find the optimal one  An algorithm which will makes as few mistakes as possible  In other words  an algorithm that will have a minimal loss  How to figure out whether this algorithm is good enough for our problem solving? First of all  they have to measure it numerically  And for this purpose we use the loss function  The loss function is the function we use for measuring the loss of algorithm a on X dataset  The more mistakes our algorithm does  the bigger loss we have  So the idea behind loss function is the usage of such function to measure algorithm quality  For example  when you saw in the age recognition problem  our algorithm sometimes makes mistakes like you can see in the example  Here we use loss to measure how correct the algorithm is  So in the first photo I am 24 years old instead of 27  which is nice  but it's still a mistake  In the second I am 29 instead of 27  which is quite sad  but the mistake or loss is less  Thus we have learned the basic concept  Now it's time to discuss what we need to find the optimal algorithm  How exactly do we train machine learning models? This is pretty simple  basically all we need to do is to choose the exact algorithm from the family of algorithms  Actually there is the small logic trick here  because for choosing the exact algorithm from a chosen family  First we have to chose that family  but let us postpone this question now  If you summon give us a good family of algorithms or even easier assume we know only one family of algorithms  That is why we do not need to make any choice  Okay  how do we choose this algorithm? We used a loss on our dataset as a selection criterion  The lower loss we have  the better our algorithm is  So the logic behind module training is the following  First you have to prepare an X dataset  which is a set of object and answer pairs  The object should be represented by the vector of the features  Then you have to choose the family of the algorithms  And minimize the loss function Q to get a lump sum algorithm for the A family  And for loss minimization you can use suitable optimization technique you like  Now you got the idea of the machine learning problems  basic concepts and model training on logic  Let's try to apply it to the exact tasks  We start from the credit scoring problem where banks should decide whether they should give a credit to the  For this problem  we can take the end client as an object  And as the answer  we can use the binary value 0  If a client is reliable enough and bank should give him a loan and 1 if bank shouldn't  In this case  reliable means that client will pay loan back in time  So what is the machine learning problem here? We have to find the algorithm  which will predict whether the client is reliable or not  for each client based on the clients future representation  It can be combination of age  gender  profession  education  loan history and many more  Based on the values of these features  the algorithm should give us the better answer 0 or 1 for each customer  Our next machine learning task to discuss is handwritten digit recognition  This is a very popular task  the idea is to recognize the digits that was written by hand  There are some open datasets with pictures of hand written digits  here you have an example of one of them  Each picture is represented by 64 pixels  this is 8x8 pixel metrics  The objects here are pictures  which are pictures of the original digits written by someone  Here  features are all pixels of each picture  and the answer is a digit from 0 to 9  Our model should be able to recognize the initial digit by the picture  or in other words  produce the digit by its picture  Moving on  the next example is an exchange rate forecast  I bet many of us will be happy to know beforehand what exchange rate will be for any pair of the currencies in the future  Can help this to some point  In this case  the object will be pair of the currencies  for example euro and dollars  As for our features we can use the data about the previous exchange rate  For example  for each day of the previous year  Such kind of data is called the time  It's answer will be the exchange rate for the pair of the currencies for a specific period of time in the future  Here model should be able to predict the exchange rate for a given pair of currencies for the future  Query-based search is also an example of a machine learning program  Here is an adjunct you can use to query and documents layer  And as an answer  we use the rank  because the documents first are given query  Here we can use any data regarding your documents or query as the features  Query language  document language  words presented in the documents  query words  and many more  The model should estimate a reasonable run for each document query pair  By reasonable here  we mean that better documents would have a higher rank  Thus  the exact rank is not that important  The order is much more important than the exact rank here  It turns out that you have just supervised learning problems  Supervised learning is a class of machine learning problem whereas there is a subset of objects where you have the right answer for each object  Such a subset of objects we call a training set  We will discuss it in the next videos in details  Credit scoring is in the example of binary classification problem  Here we have objects from two classes only  reliable and unreliable customers  This model should choose one of two classes as an answer  Digit recognition is the example of the multi-class classification problem  Here we have more than two classes of objects and for each object  the model will choose 1 of 10 classes  The exchange rate forecast is the example of the regression problem  In case the model should predict not 1 class from the limited set of class but the real number  because it is problem regression  And the document ranking is an example of the ranking problem  Here we also do not have any class  but the answer here is not a real number  You should find the optimal rank for the object you have in mind since the order of the object sorted by rank is much more important than the exact rank  You have done a great job  In this video you have learned basic concepts of machine learning  You have learned what we call objects and answers  features and datasets  algorithm and loss function  You also have got the intuition about how to train models and met with the supervised learning concept  In the next video  you'll learn standard types of the machine learning problems  And go deeper into the supervised learning problem  the big part of machine learning problems  Meet you there 
-2IIWuAYeVQ,(Optional) Types of problems and tasks  Hi there  in this video you will learn standard types of the machine learning problems and tasks  You will learn how machine learning problems are structured  How to identify each type of the problem  What are the difficulties of each particular problem type? There are forced and there are machine learning problems  Supervised learning  unsupervised learning  semi-supervised learning  and reinforcement learning  Let's take a look at each of them  The first problem is called supervised learning  You have already met this concept in the previous video when we were learning how to apply machine learning to the greatest current problem  handwritten digit recognition  exchange rate prediction  and a query based search  Supervised learning assumes that we have the right answer for a subset of objects  This is a class or label in case of a classification problem  Like binary liability of a customer in a credit scoring problem  And this is the real number in case of regression problem  Like the exchange rate in exchange rate forecasting problem  Here is the answer  our output space is a real numbers  In this kind of problem we know exactly the right answer for the group of objects  We call this group of objects a training set  So you train your model over this training set  The model learns the dependencies between the object features and the labels from the training dataset  And then you apply this model to the test dataset so that the objects and labels doesn't take part into a training process  After that  you measure the loss or the train and test dataset  The lower the loss you get  the better model you have  One important thing to be mentioned is the object from the test dataset may differ significantly from the training dataset objects  And this is actually a big problem  Because the more objects from the test dataset differ from the object from test training dataset  the more complicated it is to train the precise model  And speaking about the module quality is the logic here is the following  The less loss we got  especially on the test dataset  the better model we made  Often  during such model training processing phase  conceptual machine learning problems  underfitting and overfitting  There are really interesting and complicated problems  You will face them very soon in the second lesson  And we are moving to the next step of machine learning problem called unsupervised learning  If in the supervised learning  we have the right answer for the set of objects  In unsupervised learning the situation is vice versa  In unsupervised learning problem  we have no labels  No answer for the object  but we still have to figure out what data structure we have  So the problem statement here is the following  We need to split our objects into the groups called clusters  Where a cluster represent the data structure of that data  This means that the objects inside of the cluster should be similar to each other  And the objects from different clusters should differ significantly  The trick here is that generally we do not have any information regarding the dataset cluster structure  We do not know how many clusters there are in the dataset  which size clusters should have  There is no information about the current cluster structure  This is actually a big problem  because the results of the unsupervised learning itself are very subjective  thus you can spend the day to enter clusters in different way  And there is just no way to check which is the right one and which is not  We will return back to this later in the second lesson  The next type of the problem is semi-supervised learning  It stands in between of the supervised and unsupervised learning  Here we know the answers only for a limited number of objects  But we don't have any answers for the majority of objects  Thus we can learn some information about dataset structure from the labeled part of the dataset  But  as the majority of objects have no labels  unsupervised algorithms and techniques are more applicable  Reinforcement learning is the last type of the machine learning problem to discuss  Here we do not have any answer for the object beforehand  Nevertheless  the model can interact with the dynamic environment in order to get some feedback  For instance  answers for the chosen object  In most cases  getting such feedback is complicated and expensive  That is why you cannot just get this answer for all the entrepreneur dataset beforehand  And then work with this kind of task the same manner as with the supervised learning  Here a model can ask for feedback only a limited number of times or a limited of times within the certain period of time  That is why reinforcement learning requires special algorithms and techniques differs significantly from the supervised learning  Let me sum up  in this lesson  you have learned four types of machine learning problem  supervised learning  unsupervised learning  semi-supervised learning  and reinforcement learning  You have learned the specific of each type of the problems and why these types differ significantly  During the girth we will mostly on the supervised and unsupervised learning as one of the most basic machine learning problem types  In the next video  you will learn the supervised learning problem  see you there 
wdGiekwXM2w,(Optional) Supervised learning  Hi everyone  in this video  we are going to go deeper into supervised learning problems  About how supervised learning problem looks like from the previous videos  In this video  we will learn the types of supervised learning problems  and see what difficulties you might face when you work on such kind of tasks  Let me quickly remind you of supervised learning problems  Binary classification  multi-class classification  regression  and ranking  Binary classification is one of the most popular problems in supervised learning  This classification problem might seem very simple If you have a data set similar to the one you see in the example  definitely the task isn't that complicated  You have entries from two different class  and you have to create some rules which will help you to distinguish between the objects from different class  And this example is pretty intuitive  you can see that the classes are linearly separable  So the only thing you had to do is to find the appropriate line to separate the objects  Such an algorithm family is called linear  the decision linearly depends on your future  Thus  when the right line is found  your decision rule is pretty simple  Say  object above the line is derived from the first class  and objects below the line are derived from the second one  This is a nice example  but in most cases  classification problem might look much more complicated  For example  if you have to solve the following problem  it will be not enough just to draw the line  Intuitively  you guessed that the linear model just isn't suitable for this data set structure  and we should consider another algorithm family  When you work with two or three-dimensional data set  data set where each object is represented by a two or three-dimensional vector  It's usually pretty simple to visualize your data  and decide what algorithm family suits you more  When you work with higher-dimensional data sets  the problem became more interesting  There are a lot of nice algorithm families that allows you to solve complicated classification problem  You will deal with them in a second lesson  and we are moving on to the multi-classification problem  Multi-class classification problem is kind of the problem  Where you have more than two class of objects in your data set  This problem seems to be more complicated compared to the binary classification  but good news for you  Multi-classification problem can be reduced to the binary classification one  There are different strategies to build a multi class classification model based on the binary classification  Let's consider one versus all and one versus one strategies  According to the one versus all strategy  you have to create as many binary models  as many classes as you have in your data sets  Each model should be trained to distinguish one specific class from the rest  This decision rule should be applied over this binary model result  and so the final answer depends on the whole set of models  According to the one versus one strategy  you train a model for each unique pair of class  Thus  each model is binary  and it tells you whether your object looks more like a object or more like b object  And the final decision also depends on the full set of binary models  The next type of problem is called regression  here  as you remember  the answer is not a class  but the real number  For example  age  exchange rate  or probability of something  often  As we have discussed earlier  the choice of algorithm family is a very important choice  And in some cases  it will trigger the quality of your model  let me demonstrate it by the following example  Assume we work with this data set  and we need to train the model to fit the data  If you assume that this dependency is linear  you will get the following picture  The model is not so bad  but definitely the dependency is a bit more complicated than it is linear  So our model is too simple to restore this dependency  this is an example of the underfitting  We will get back to the underfitting problem later on in the second lesson  Okay  let's try to use more complicated model  for example  quadratic  Subject to the picture  looks better now  we cannot make any conclusions or the proper estimations  but still  we can see that this model restores the dependency much better  Should we pursue the complication of the model for getting better result? Well  let's try  if we built a even more complicated model  we could get a picture like this  Definitely  this model is too wavy  and it doesn't really reflect our data  this is how overfitting might look like  The model overfits the training set  and it loses the generalization power  overfitting will be our topic of discussion later on  So the last type of supervised problem is called ranking  here  you should find the optimal rank for our objects  And you should remember that  in comparison to the regression problem  here the exact rank is much less important than the order of the objects  For working through this problem  you also had a different approach  the first approach is called pointwise  Therefore  each object in the training data set have an exact rank  Say this is a numerical rank  like 1  minus 2  or 15  or an ordinal rank  like first  second  third  And the model here should optimize to my exact rank of each object  So in the pointwise approach  the ranking problem is approximated by the regression problem  The next approach is called pairwise  here  we can reduce the ranking problem to a classification one  and you do it according to the following scenario  For each pair of the objects  you should make a binary decision  if the first object is better than the second  or vice versa  So for this  you can use the binary classifier  which tells you which object from the selected pair has a higher rank  based on such comparisons  you then make the final rank  And the last approach is called listwise  here  you directly optimize the value of pointwise or pairwise measures  and average them over all queries in the training data  In this video  you have learned the problem types of the supervised learning  You have learned how to solve multi-class classification problems  with the help of the binary classification  You have got the introduction of underfitting and overfitting problems  And you have learned a different approach to work with the ranking problem  Hope you enjoy it  see you in the next video  where we will discuss the approach and the difficulties of the unsupervised learning 
Um0v5t8jjcQ,(Optional) Unsupervised learning  Hi there  In this video we will discuss unsupervised learning  In this video you will learn types of unsupervised learning problems and see what difficulties you might face when you work on such kind of tasks  So what are the unsupervised learning problem types? There a number of different tasks that classified as unsupervised learning problems  Clustering  anomaly detection  dimensionality reduction  and data visualization  Clustering problem is a problem where we should structure our data into several groups called clusters  exactly like you see in the picture  Here I take all the customers and split them into the groups  The complexity of clustering boils down to uncertainty  When we deal with supervised learning  it's always easy to say if we were good in modern or not  They had the right answers first time [INAUDIBLE] we can check the correctness of our model at least for that object  So everything is pretty certain  Model loss can be higher or lower  but we always know what an ideal model should predict for each object  When we do the clustering we can get different clusters by different models  Sometimes it's pretty easy to say if our cluster are good  reasonable  meaningful  But sometimes we can face situations when you can see this instance formula  What clustering seems better for you? We can group these heroes  for example by gender  by families  by their race  And actually regression is what is the natural grouping for these subjects  So the clustering is always subjective  And the question is how to estimate the clustering quality and how to compare different clustering models between each other  is actually good question  We will return back to this example when we discuss clustering and quality metrics  Another unsupervised task is anomaly detection  While doing the clustering  sometimes you will get the object which are very hard to classify to any groups  It may look like the picture  Here I have split some objects into the groups  But for some objects  we cannot decide to which group we should put them  These objects are very likely to be different from the rest  apart from the clustering result  We call these objects anomaly  There are a lot of practical applications for doing anomaly detection  For example  you could find users who significantly differ from your other clients  in particular  And from your target audience in general  Having such users you can spend more time to figure out who these users are and how to interact with them  In manufacturing anomaly detection is also widely used  You can find anomaly states of the environment and if your model can not only find but also predict them beforehand you can save a lot of money and time in case of failure  The next test to discuss is dimensionality reduction  Sometimes we have to work this high dimensional data  It could be different kinds of data  for instance pictures  texts  videos  and many more  Before making the first analysis of such data  sometimes better to reduce dimensionality or the data service minimum informationalist  This is really helpful both for data and [INAUDIBLE] and during the descriptive data analysis and for modeling  During dimensionality reduction we would like to reduce the initial dimensionality  but preserve as much information as it can  Data visualization is the last type of the unsupervised task we are about to discuss  This is also very subjective task  Actually it's a general characteristic for the most unsupervised problem  For data visualization  it also have kind of poor quality criteria which sometimes sounds like it should look good  Of course  nobody gives you a formal definition of goodness  For examples there are the data visualization results for the data set of hand written digits  Probably you remember from the beginning of the lesson that each object is represented by matrix  So after a matrix direct transformation one by one  The initial dimension of this dataset is 64  So this is very hard to visualize data of such dimension  And for figuring out how this dataset looks in the 2D or 3D space  which is the most convenient space for people to observe  we should reduce the dimensionality  And the data visualization can only be done after such reduction  Let's take a look at the examples  This is the first example with the data visualization  This is the second one  And actually comparing these two examples we can easily see that the second one is much better because here we can see that object from different class can be separated into different groups easily  So the result of data visualization reflects the initial data structure  Bad news is that such reflection is sometimes very hard to be formally defined and directly simulated  In this video you have learned unsupervised learning problem types  You have met this clustering problem  The problem where you have to group data into separate clusters  The anomaly detection  where you have to find the objects which differ from the other objects significantly  You have a dimensionality reduction problem where you have to reduce the dimensionality of data with minimal information loss  And you have learned the data visualization approach of high dimensional data sets  In the next video we will move on to the machine learning business application  Hope to see you there 
EbNJ05EVXs0,(Optional) Business applications of the machine learning  Hi everybody  this is the final video of machine learning introductory lesson  You have already learned a lot  You know which data you can use for machine learning analysis  you know machine learning basic concepts  You also know how to classify machine learning problems and tasks  and know quite a lot about supervised and unsupervised learning problems  You have done a great job and this is the time to discuss machine learning business specifications  What is business value of machine learning based services? Let's take a look at some examples  Traditionally  let start from the credit scoring problem  For the credit scoring problem  we built a prediction model that gives the greatest score for each potential customer  Based on the predicted credit score  a bank can make a decision of providing a client with a loan  If a client is reliable enough to get a loan  the bank still should estimate the amount of money and the reasonable period of the loan  The credit score can be used to support this decision  The higher credit score a client gets  the better condition he or she could achieve  That is why a credit scoring is a very reasonable service for banks  Precise scoring model brings to a bank the following value  First of all  a bank can give more credit to reliable clients  So a bank will make more money from profitable loans  Secondly  a bank can give less credit to unreliable clients and this reduces the amount money lost due to unprofitable loans  One more thing to be mentioned  if a bank uses a model for a credit scoring instead of hand processing made by the staff  the bank will be able to make such decisions very accurate  unbiased and fast  So a bank will be able to process more client correctly  and more client means more earnings  Moving on to the next example  product recommended system  Today  it's hard to find an online shop without the product recommendation  It's  it might look like the example  For instance  if an online shop can provide client with relevant recommendation and client will discover new  interesting products that he or she hasn't seen before  but will then purchase  So if a client has a good customer experience  then probably he or she will come back more often and they will spend more time and make more purchases  Churn prevention is also an interesting approach for a business  Precise churn prediction model with reasonable prediction horizon brings  for instance  to a telecom operator  a lot of value  Having a list of potential churn clients leads to launching of relevant and the proper retention campaigns  So  if a retention campaign is successful enough  some clients will be preserved  Those churn rates will go lower  and the amount of clients will get bigger  Finally  the more clients a company has  the more revenue it could gain  Audience targeting is an example of unsupervised machine learning problem  This is also a very useful service from some businesses  One can do audience targeting  and if it gets reasonable results  it can bring a lot of value to business in general  First of all  advertisement can be done over the targeting results  Thus  a company can make relevant advertisement for each segment of users  The more precise  the better advertisement a company creates  the more revenue it will make  The targeted churn prediction can also be done based on the audience targeting  If business analytics has reasonable target in forklines  they will have more chances to generate a good enough hypothesis regarding the reasons of the churn for each target group  Based on this hypothesis  marketing specialists could make a real and retention complaints  for each segment of churn users  And finally  the better campaigns they have  the more clients they will keep  And more users potentially mean more revenue  as always  And the last task I would like to briefly discuss is the anomaly detection for the manufacturer  Anomaly detection model for manufacturing process can bring a lot of value for a company  If such model can detect an anomaly beforehand  it would have to prevent the failures  Thus  the more failures will be detected and prevented  the less amount of money will be spent on elimination the failure consequences  In this video  we have discussed a lot of business application for machine learning models  Now we know how machine learning algorithms and techniques can be applied to real world problems and bring some value for your business 
6vcdaVuTDT0,"Introduction to large scale machine learning  Hello  my name is Pavel  and during this week  you will learn how to use machine learning on big data  Sounds interesting to you? Let's go  This week  you will learn why machine learning should be used on big data  You will also know Spark MLlib  and learn how to use linear models on large scale to predict events  and learn some techniques for improving quality of prediction  In this video  I will tell you how to solve the problem of big data sampling in the right and the wrong way  You will get the answer to the question  why you need a large scale machine learning? And why you should study Spark MLlib? So  imagine that you have a huge data sets with billion of events  the first idea that comes to my mind  what if you take a small piece of the events from the data set  load it into a local machine and work with it and somehow  create models on it  make some predictions  All that you did last week with the help of scikit-learn  This idea is not so bad  to do this  You need to pull out a random split  random subset of the examples from your data set  and use them  However  there is one pitfall I want to tell you about  So  you have a giant log of user's activity in the Internet  You don't load 10% of it to your computer  and use to predict the gender of a person  Suppose that a visitor chosen by us  once read an article about fishing  This can tell us that this is a man  \""Stereotypes\""  What is the problem here? If a person made 100 clicks on your site  then in the sample he will have about 10 of his clicks  In the 10 clicks selected randomly  the site about fishing will fall with probability of 10%  and with a 90% probability  we will miss this important event  Sample correctly  in this case  you need to sample by selecting not the events but the users who created these events  Why do we need a complete data set? Let's figure it out  If you train your model on a very small data set  your model can easily become overfitted  Consider this example  where we distinguish boys and girls  You train a complex model  it determines correctly the classes for all the boys and girls  but if you try to classify  another data set using this model  you will see that this quality is far from 100%  Such an event is called overfitting  and training on big data is one of the ways to solve this problem  The second reason  why you need large data  is a very complex model for solving your problem  in which there are a lot of parameters to be automatically configured  and a large data set will allow you to train it to correctly predict an event  distinguish between kittens and dogs  and avoid mistakes  The quality of the prediction as usual  increase from a number of examples  Therefore  the more data you have  the more curious you get  If you have a giant data set  then trying to train the model on the entire data set  and see what quality will be in this case  make sense  Another case  where large data can really help you  when you need to predict a rare event  For example  if someone throw a coin  and it stood upright  of course  this can be predicted  because a coin is a random number generator  But  this is a good example of rare event  The facts are such rare events are very important  and sometimes critical for business  for example  the fall of service  if you can write a program that predicts a drop half an hour before it happens  you will save the company a lot of money  99% of all operations in the bank are performed by honest  law abiding citizens  However  amongst them  there a small percentage of frauds who try to steal money from someone  If you write a program that will catch such fraud  banks will get their profit  You need a lot of data here  because a number of fraudulent operations  compared to the number of legal ones  is vanishly small  We have figured out why we need large data  Now  why do we need a separate library of machine learning for large data? What it will give for us? First  it can build a predictive model using all data that it can get  Terabytes  ten terabytes  petabytes of data  only you need to deliver a sufficient number of machines  Second  it can be distributed to apply this model to the same terabytes and petabytes of data  If your algorithm can classify your data within 1 000 hours  then if you paralyze it on 1 000 processors  the whole classification will take one hour  compare the classifications that lasted 40 days and one hour  the choice is up here  So  complex machine learning models have a bunch of settings that you set by yourself  The so-called hyper-parameters  machine learning on large data allows you to run the selection of hyper-parameters  distributing them across the cluster  Some machines will train and check the quality of classification  with one hyper-parameter  another machine will train model with another hyper-parameter  thus  you can advance of the fact that you have a giant part of machines which you have access to  So  what library do I talking about? What kind of library do I mean? Well  of course I'm talking about Spark MLlib  In Spark 2 0  Spark MLlib works on top of the Spark SQL library  You already know it  You started it during the last course  What are the advantages of Spark SQL? The first advantage is simplicity  Is there anybody here not to know how to use SQL? It is integrated as a bunch of sources  It can read data from files  write data into files  read from Hive  write to Hive  use any external databases  The third advantage is its performance  The Spark SQL library runs 10 times faster than the MapReduce  because it doesn't store the intermediate data on the disk  and if you can cache the data  save it into the memory  then the rate of speeds will be treated as one to 100  Just think about it  What as advantages of Spark MLlib? Actually  Spark MLlib was inspired by the library scikit-learn  with which you have already got acquainted I hope  What are its advantages? First  unified way of processing data and apply in models  Any model  any transformation  they all are applied in the same way  The second advantage is an easy deployment for large data  You just need to run your program  it will pump out your data set  distribute it and process it  and the third  a very important quality  is a large number of sub-party libraries that are written by other people  Often professional scientists  who are interested in developing something for Spark  The fact is that Spark is now a hot topic  and everyone interested and trying to write something for it  That are the advantages  And so  what have you learned today? How to incorrectly and correctly circumvent the problem of large data? Why you need a large scale machine learning  and why you should study Spark MLlib? Stay with us  In the next lecture  I will tell you how to train the simplest models on the Spark MLlib  It will be interesting "
eGopmXP7j5A,First example  Linear regression  Hello  my name is Pavel and now we talk about how to use Spark MLlib  We will make our first step in studying machine learning of Big Data  Let's go  To begin with  we will build a linear regression  And in this lesson you will learn how to prepare data for Spark MLlib tasks so that you can use Spark MLlib  learn how to make predictions using linear regression  and estimate the accuracy of this prediction  You need some kind of a dataset in order to build our prediction  I generally like to ride the bike  Every day I go to work and back by bike  And so  I gladly found a dataset for renting bikes  And let's try to analyze it  I put it in HDFS  so you can download it via Spark DataFrame and see what there is  There are 16 columns in it which store a lot of rented bicycles on different days  You can divide this dataset into three parts  Firstly  it is a date data and all that can be extracted from it  Some good data engineers made a good thing for us  They created a lot of features from the data  season  year  months whatever the day was  a week day or weekend  what number it had  Secondly  it is the data about the weather  Weathersit means what the weather was like that day? One means sunny  clear  good  And four  rain  snow  snow storm  typhoon  tsunami  and apocalypse  Temperature means the temperature  Predicting your question about the scales of the temperature Celsius or Kelvin  I want to ask you what renting service we're dealing with? The one on the moon? Of course not  The temperature is normalized  Zero means -8 degrees  And one means +98 degrees  All the rest indications are intermediate  The next is so-called Baret temperature which is a very interesting thing  This value is combination of temperature  humidity  and wind  How people perceive it? It varies from the -16 when zero to +15 when one  Then next value is humidity  Zero usually somewhere in Sahara Desert and one is 100% humidity after the rain  And the speed of the wind  when zero means calm no both swim under sails  and one is the most terrible windmills that has ever been recorded during those two years of measurements  And three columns of objective function mean rental statistics  How many random passengers rent bicycles like this? Wow bicycles  I should rent one of them  How many of them already registered customers use bikes? And how many people rode the bikes today in total? I think you shouldn't be bothered with separation of registered users and unregistered users  Let's predict only the last column  Let's look at that schema of the data we have downloaded  We took it from the CSV file  In fact  this CSV file doesn't contain information in which format which comes to start and they are stored in the form of strings  So those integers  floating points  dates  all is kept in the form of strings  Well  let's fix it  First  you should throw away all unnecessary fields  type of the record number  date  random number of bicycles rent  Secondly  you need to translate all the integers fields to integers  Thirdly  convert all the floating point numbers to double  And fourthly  I changed the CNT column of the numbers of bicycles to the standard name label  Let's see what we have got  As you can see  the number of columns have become much smaller  I selected the last column in the way we want to learn to predict  We predicted by a linear regression  Here is the simplest form  It looks like this  There is a function from one variable  In our case  linear regression will depend from 12 variables but the principle is the same  Prediction at output is simply a linear combination of the input features with some weights  Before we begin to train regression  we need to divide the data in the train and test those that all our quality measurements are done in the test  Or we can make it one  two complicated functions that will predict one of those examples to good to be true  And thus accordingly  we can train it to get 100% quality but still miss the mistake  We divide our dataset into a train and test with proportion of 70 to 30  70 goes to the train and 30 goes to the test  And we have trained our linear regression  At the input one  we should receive a certain vector of values  Output is a number  Where do we get this vector? It's a vector that is fed from the input must be in the special column in a DataFrame which has a vector type  In order to collect it from the variable values  we need to use VectorAssembler which specifies input column with the output columns  and then apply it to our train sample  And that's what we have got  We have those 12 columns that were at the beginning  We have added to them one column with our feature vector and now we can use it  Let's throw all the other columns because they are useless  Leave only features and the labels  Create an object of linear regression and train it on your training samples  So  now you need to apply it on your sample  Let's see what happens  By default  the linear regression prediction are written in the same DataFrame as input and codes the prediction  If you compare the labels to the predictions  you will see that for many labels you have not so mistaken  For the first example  we have made a mistake about 50 elements and for the later we also make a mistake at 15  That is sometimes we predicts the labels very well  Let's now to maintain the experimental integrity  see how accurately we will predict everything on the test sample  You can also see there are values that are close and there are values where you even be mistaken for 500 bicycles  It's not good  We can calculate the quality of prediction of regression using a special evaluator  First  with the help of this evaluator we can calculate R squared so-called coefficient of determination  which describes how well our linear regression pass through all of the examples  So the coefficient of the determination is 0 76  is it good or bad? Well  let's take a look  Here in this picture  there are different distributions smoothed by a linear regression for which different estimates of the accuracy of R squared are given  The left most has an accuracy of 0 99  When almost one when all points almost directly fell on our straight line  Our accuracy is 0 76  So most likely  our situation is similar to the picture in the middle  Another way to estimate the mistake is to measure the mean error  For example  the root mean square error  It is the root of the sum of squares of all the deviations divided by N  The standard deviation is 910 bicycles that is you are mistaken for whole 1000 bikes  Wow  lots of  Well  let's try to construct some useful metric that will be understood by your possible customers  For example  they are ready to store extra 300 bikes in their warehouse just in case  How many errors containing less than 300 bicycles did you make in your predictions? For this  you should substract the real number of bicycles from the predicted one and take the absolute value of the residual  In all cases  when the mistake was less than 300 bikes put one  in the remaining cases  zero  And now just calculate the average of this ones and zeros  We have predicted correctly inserted 2% of cases whereas accuracy of prediction was within 300 bikes  This is a business-friendly metric and customers can decide whether to use this prediction in their work or not  So let's sum up  in this lesson  you have learned how to prepare data from Spark MLlib tasks  make predictions using linear regression  and evaluate the quality of the predictions  And at the next lesson  let's pay more attention to the architecture of Spark MLlib library  How it's arranged? What part it has and how it can be used? Stay with us  It will be interesting 
lECb8emmN0M,How MLlib library is arranged  Hello  my name is Pavel  and now you will figure out how the Spark MLlib library works  You'll want to know this  I'm sure you do  let's go! So in this lecture you will learn how Spark and MLlib works  what transformers are and why they are needed  what estimators are  and how to use pipelines in machine learning  Actually  Spark MLlib was inspired by one of the best machine learning libraries that I met in my life  that's called scikit-learn  Of course  it doesn't repeat it exactly  However  it works on very similar concepts  But designed to work with large data sets  So the main object that Spark MLlib works is a DataFrame that you have already known for a long time  And all the transformations that MLlib does with data I enclosed in transformers  What are transformers? What are they necessary for? Of course here we are talking not about Autobots or Decepticons  Transformers are objects that transform one DataFrame in to another DataFrame using the transform function  This is general definition  There are many different kinds of transformers  And in the last lesson you started one of them which collected columns into one vector  it's called VectorAssembler  You remember it  don't you? Here is our DataFrame and here is our feature vectors as the last column  These transformers are used for data preprocessing  text preprocessing  feature creation  and many other things  I think you will met them many times in our course  Another important concept of Spark MLlib it is call estimators  What are they? Estimators are the algorithms that are required to be trained on your data before usage  The main function of an estimators is fit  At the input it receives a DataFrame  At the output it returns the transformer  which applies the learned algorithms tith the data  You're already familiar with at least one estimator  Remember the moment I told you about the LinearRegression  please take look  Did you see after creating the regression object  we fit it to the data  At this point  the LinearRegression learns to predict the demand of bicycles for different days  And the results are saved in a special model  The model is already a transformer  Let's see how we supply it  You take the training sample  apply a transform to it  and you have a new column  prediction  Learning algorithms  as you know  include regressions  classifications  clustering algorithms  However  some processing algorithms are also training  for example  the MinMaxScaler  MinMax takes vector features  and every feature  whatever it is  rescales from 0 to 1  Let's see how this works  When creating a MinMaxScalar object  you need to specify which column it will work with  In this case  it will receive features on the input  And output column will be scaled features  You train it on the data set which you have  It learns to determine minimum and maximum values for each of the vector element  And then you apply it to the train data set  As you can see that the 1s we had in the first column turned to be 0s after scaling  Because we initially enumerate the seasons from 1  And now we enumerate them from 0  I hope it's clear  I want to tell you about one common mistake of data scientists  which is easy to make and very difficult to find later  It was done by me  my colleagues  my subordinates  and my bosses  Please train estimators only on training sample  Never train them on a full sample or on the test  Even if it is same MinMaxScaler which determines the minimum and maximum  Even if it seems completely quiet and cautious  In my experience  there are few very annoying cases when we train the estimators and the result got models incredible accuracy of 90%  95%  While the other models gave us accuracy of about 70%  80%  I understand that it's very difficult to catch that mistakes  And if you make this mistake and your boss will not be able to find and reject it  then you will deploy this model into production  You will tell your customers about incredible coolness of your algorithm  But in practice  the quality will be very mediocre  And your business customers will be unhappy  Please try to avoid this  The last thing I would like to talk about is pipeline  Here I draw the transformations you created when dealing with the input data  query on them  and make predictions on them  We first converted the types to get rid of the lines  Then everything was merged into one feature vector  trained  and applied linear regression  It would be desirable to train and receive segments of actions  then to save  and then to apply in production all at once  So you don't have to remember and reproduce the steps of processing by yourself  This can be done by a special tool called pipeline  Pipeline stores discreet segments of all the actions that should be applied  In this pipeline you can keep both transformers and estimators  Estimators should go last  There is the vector assembly transformer and the linear regression estimator in our transformation list  We can add them to our pipeline  But what about the first transformation? You converted the type of the columns by Spark DataFrame methods  Could you add this conversion into our pipeline? Actually  you can  You can turn this transformation into a transformer and put into our pipeline as well  You can do this as unusual by SQL transformer  I hope that you all remember that the DataFrame can be processed both through the DataFrame API and with help of SQL conversion  That's the transformation you did  You choose useful columns and fed their correct types  But the same thing could be done with SQL conversion  which specifies the types for each column  For the column cnt we'll also change the name to label  Inside SQL  the transformer registers the received DataFrame  a temporary DataFrame with a name THIS  And then apply the resulting SQL conversion through the input table  But you forgot one more transformation  which was so small that it could be easily forgotten  The fact is that after you created our feature vector  you discarded all the values you didn't needed  You left only the features and label columns  It's possible to reproduce this discard with another SQL transformer  How you should you think it looks like? Well  of course it should like this  You select two columns from the current table  that's all  Now those four actions can be collected in a single pipeline  First SQL transformation features a sampling  second transformation and then logistic regression training  Such a pipeline is an estimator  let's train it  You get a train pipeline model as a output  This model is already a transformer  And it can be used both in the training and on the test sample  If you have a good memory  you will remember that in the past video you saw exactly the same numbers that I show you for predictions  Let's evaluate the quality of prediction with r squared and rmse  They turn out to be the same as they were expected  And now  you can save our trained pipeline to a file on HDFS  When you need to use the model in production  you will download the pipeline from the file  apply it to your data  and get the same results  You can apply your model to new days which the weather forecast is known  and receive estimates of the required numbers of bicycles  What is the future of this approach? In fact  when we apply our transformations in the production code  we don't know what steps the pipeline contains of  If tomorrow you decide to use another feature to predict the demand or another type of regression  then you will just need to save a new evaluator model to a file  And then change file name with a model to a new one  And all the rest code that works in production will not be changed at all  This seems to me  this is a very cool achievement  You can work on the quality of your model  deploy it when you have to  and don't waste time at all  That's all for today  In this video  you have learned how Spark MLlib works  what transformers  estimators  pipelines are  And so how all it works in the real life  In the next video  I will tell you how the training of linear regression is arranged from the inside  how you can use a terabyte data set for your machine learning  Stay with us  it will be interesting 
m31K9yGf_fQ,How to train algorithms  Gradient descent method  Hello everyone  my name is Pavel and now we'll talk about how machine learning algorithms are trained on big data  What exactly happens when you call the feed function? So far  the topic has remained a blank box for us  now we will open this box and find what's inside  My goal's to explain you the intuition of the training methods that are being used at the moment  In this video  I will talk about learning algorithms and show you three most popular methods that work with distributed data sets  The first is an analytical solution to the problem  secondly  there is a method of gradient descend  and thirdly  the method of stochastic gradient descent  Here we go  here's our linear regression  for which we are trying to predict the values of labels for different sets of features  We try to do this in an optimal way  what does the optimum mean? When observed how well the linear regression work  we measure its quality with the mean square error  RMSE  here is it  Let's draw this line as so to use this error  let's find the minimum RMSE  However  we cannot do it without math  so it's time to introduce some mathematical notations  Let's label the features by x  and the labels are y  we will mark the example number in training set with an upper index i  x is a vector  and y is a integer  so minus infinity to plus infinity  We want to build a prediction of how many bikes today will go to the rental  Prediction is linear combination of input features with some weights  w0  wn  Let's add 1 to the vector x and w0 into the feature vector  And then we will be able to write down our equation in a short form  That y  the product of the feature vector and the vector  So we'll search for the optimal radial through the minimum of this function  which is called the error function  what kind of function is it? The fact is the same mean square error  it's just results square extraction and dividing by n  each with the minimum as the same point as RMSE  Our task is to find this vector of parameters which tests this minimum  The solution of the problem can be found in several ways  The very first and very obvious method is an analytical solution  Let's show how the error function depends on the parameters in the one dimensional case  On the x axis  either the parameter is aside  and the mean of the error function along the y axis  Here is the minimum of the function  at the minimum point  the derivative is zero for each of the coordinates  The derivative can be found analytically  simply by taking the derivative of our function  and equating it to zero  it will be in this form  In fact  we have obtained a system of linear equations  which can be solved as a school task  Everything goes fine  operations are fluent  and linear regression can be closed  but what are disadvantages? The main problem here  the complexity of solution depends cubically on the number of features  If you have 10 features  then you will have about 1 000 operation  2 000  5 000  maybe 10 000  If you have 100 features  then you will have 1 million operations  5 million  10 million  it doesn't matter  And if you have 1000 features and it's not that limit of machine learning  then the number of operation will reach billions  which is quite a lot  The second problem is  this method is not universal  it is suitable for searching for linear regression  but the method doesn't work for classification  I would like  of course  to find a silver bullet  and use it to solve different problems  And the silver bullet has already been found  it's called gradient descent  For a function of many variables  we can take the derivative with our variables  and compose the gradient vector  This vector has one remarkable property  at any point where it is counted  it points to the direction where the function grows fastest  but it also has another property  If you add a minus to it  then the vector will indicate the direction where the function decrease most quickly  and this can be used to find our solution  We take our current value  start with some random point  and subtract gradient from it  Gradient is multiplied by a factor that specifies the rate of convergence  If its multiplier is too large  we'll always miss the center  if it's too small  we will never reach it  and get the algorithm counted when pigs fly  that's how it works out  And here is an example of a two dimensional surface  but the circles I have designated is also called level lines  Let our arrow be 10 000 for external level line  5 000 for the second level line  1 000 for this third level line  500 for the fourth  and 300 for the fifth  And with each iteration  our point  our vector of features approach closer and closer to our minimum value  How is this calculated in practice  we have our data frame with features and labels  The features are x  as I have already mentioned  and the labels are y  For each pair  for each coordinate we can calculate the gradient value separately  At this point  we combine everything to get full value of the gradient  and make one gradient step with it  What are the advantages of this method  it is universal  and it's complex proportion to the number of features  grows linearly  But there is one problem  the method can become stuck in the local medium as the spit of converge is not the greatest  Therefore  another method was invented  or rather  the improvement of previous method  which is called stochastic gradient method  Gradient descent looks like this  in the stochastic gradient method  we start by mixing all the data that we have  And then for each point  we count the same difference that we have  And this difference is just our gradient  through which we make our gradient descent  constructing from our current weight of features  That's method I am talking about  here's such a method  what advantages does it have? Converts much faster  because in the usual gradient descend  in one pass are calling to the data  we make a one-step approximation  we need several iterations to learn  In stochastic gradient  for the same one pass in the whole data set  has time to build some fast approximation  so the acceleration of of convergence  The second advantage is that the chances to get into the global minimum  and not the local one  is much higher  If the function has a global minimum  the gradient descent can get stuck in it  A stochastic gradient can jump out of it  due to its random walks  And the third advantage is online training  that is  data can come to us gradually  Such an approach will take into account how the price of car rental business evolves over time  And the last one  there was another improvement in the stochastic gradient  The fact that almost no one uses a stochastic gradient on any single point  The stochastic gradient considers by small bars of complete data frame  which are called mini batch  We calculate on it our gradient  and with the help of the update  our vector  Then we consider gradient on another mini batch  and again update our feature record  and so on  That's all I wanted to tell you today  you have learned how the problem of learning is formulated  How it's solved analytically  and how it can be solved by a gradient and stochastic gradient  I hope that it was useful  In the next video  I will tell you even more advanced machine learning techniques on big data that converge even faster  and give it more accuracy  stay with us 
YZ6ij0EUmKY,How to train algorithms  Second order methods  Hello everyone  my name is Pavel  and now we'll continue our conversation about how much our algorithms are trained on big data  In this video  I will tell you about the ways to train the second order learning method that work faster than linear methods  I put here a portrait of Issac Newton  the man who came up was one of the most popular ways of learning algorithms  it's amazing  isn't it? Now I will tell you that the second method explains the intuition of the Newton method  and tell you method of BFGS and L-BFGS  which are the approximate method of Newton  and are used on the big data  In the end  I will tell you a secret  how linear regression actually trains itself  through the Newton method  Imagine the same problems that we solved earlier  we have our curve  we want to build some approximation  find the minimum value of this function  This can be our function of errors when we solve the problem of machine learning  When we use the gradient descent to build the tangents to this curve  and work down the tangent  the second order methods say  let's build curves  Second order times for each such curves at the selected point  the first and second derivative are equal to the derivative of the original function  what kind of curve could this be in? Curves of the second order tangents are parabolas  now we're looking for the minimum of this parabola  and this is second step of our approximation  After this  we again build a tangent parabola  but oriented to this new plane  The meeting of this parabola is step number three  and so on  until it converges  The formula looks very simple on the one dimensional case  we take the current value of our parameters  and subtract from it the ratio of the first derivative through the second derivative  and these we are moving to the minimum  How does it look in the two dimensional case  let's see  we construct our paraboloid tangent to the point of initial approximation  Its minimum is the point of approximation number one  Now we'll build parabola number two  and also move to the minimum by the following steps  The method  as I have said  works much faster  if a linear stochastic gradient needs 10000 steps  this method can easily call for six steps  How to move from a one dimensional case to a multi dimensional case  The first derivative  as you already know  has to be replaced by a gradient  Which is a vector containing derivatives with respect to all variables  And the second order derivative must be replaced by the  This is a matrix that contains the derivative for each of the coordinates  In the state of substructions  the ratio of the derivative from the current ratio to get the new one  We need to subtract the gradient multiplied by the inverse hashing metrics  This will move into our optimum  however  there is a problem in this algorithm  The point that the calculation of the inverse [INAUDIBLE] as well as the source of the analytical solution requires n in the cube of operations  We are faced with the same problem that for 10 features  we need to make about 1 000 operations  for 100 features  1 million  for 1000  1 billion  and so on  this is extremely expensive  We often have methods that contain much more features  hundreds of thousands  and sometimes millions  To solve this problem  a special method was invented  it's called BFGS  or the Broyden-Fletcher-Goldfarb-Shanno algorithm  It seems to me that these names are great for using them as a tongue twister  It says  let's look for the ratio of the hashing approximately  we'll use the real hashing ratio we obtained in the previous step  and find a new ratio  adding a small addition  those are values that you have before  The additive is a product of two matrices  one of which contains two columns  and the second two rows  we will need those in the future  How I find this additive  it can be found from the so-called secant equation  This equation specifies that two nearby points in the gradient change is equal to the difference of the coordinates of points  multiplied by the hashing  If we substitute our values of the inverse hashing in this formula  and the old hashing plus additive  then we can find this additive from the second equation  Excellent  we have learned how to build the approximate value of the hashing  and how to solve our problem  However  if there are many features  still many operations are required  it's not a cube of n anymore  but a square of n  That is  10 features need to store and import 100 values  whereas 100 attributes is just 10 000 values  if there were 1 million 1 000 features  In general  it also lot  it also requires a lot of RAM  imagine storing a matrix for a million values  And if you have 10 000 features  then you store a million values  it's just awful  To solve this problem  the algorithm L-BFGS was created  The solution to this problem  if the L-BFGS algorithm  which stands for Limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm  In principle  there is no need to remember this decoding  everyone calls this L-BFGS  how does this work? As you remember  in the BFGS algorithm  we presented our inverse hashing value as a sum of the previous inverse hashing value plus an addition  This addition can be presented as a sum  as a product of two matrices  where each of them has two columns or two rows  The fact that the value of the hashing in the previous step can also be decomposed the same way as the value that was before  and thus  we can come to the initial value of the hashing  By the way  a unit matrix multiplied by some parameter alpha is usually taken as an initial gradient  And there  one trick is done  and all intermediate hashing corrections are discarded  only last ten values remain  And in the end  we get the last ten values plus the initial approximation  Of course  this is some kind of approximation of the hashing  But nevertheless  experience shows that this approximation works really well and quickly converts  If we put this inverse hashing in our formula  and then open the brackets  then it turns out that each action can be done in linear time  For example  to multiply a gradient by a matrix u that contains two rows  and a column of lengths n  we need to do 2n operations  In the end  we get a vector of length 2  to multiply the vector of length 2 by the matrix u  we need to make 4n operations  That is  everything became linear  and now at the end  let me tell you a secret  how linear aggression is trained  Inside it  there are simply is a condition that asks whether we have many features or not  If the number of features does not exceed the specified threshold  and the specified threshold is 4 000 features  then we use an analytical solution  It looks for the outputs of a system of linear iterations  if the number of features is more than 4 000  then L-BFGS algorithm is used  So in this lesson  you have learned what optimization method exists  and I explained to you the linear method  the second order optimization method  You have learned gradient descent  stochastic gradient descent  Newton method  BFGS  L-BFGS  and in the end we have learned how linear regression works  Well  we'll close with the topic of optimization  figure out how it works  and it's time to move on 
ifgs0uypC78,Large scale classification  Logistic regression  Hi there  I'm Pavel  now we're going to discuss how linear classifiers handles the large data sets  Let me present a simple classification task with a picture  Let's say you have two different types of dots  with two different colors  and you need to spread them up using the linear surface  To predict what type of color of dot belongs top  either the first or the second one  what kind of task can they be in  For example  to find the gender of the visitor of your site to show ads that interesting for women or men  Or understand what you have in the picture  a kitten or a dog  that may not be important  but it's extremely exciting  Here is another example  you work in a financial company that gives loans  But before you give out a loan to somebody  you first need to assess its ability to pay it back  Your company must be sure that the client is not a fraud  In this video  I will teach you how to solve the problem of credit risk assessment  Besides  you will learn how to use logistic regression  working with lush data sets  and how to estimate its accuracy  So there are thousands of banks worldwide that make money  give loans  and issue credit cards to people and companies  We have a data set contains information how people pay loans back  If a credit department fails to forecast properly the cases when people can pay back loans or go into default  the bank will lose a lot of money  On the other hand  if it estimates the credit risks correctly  then the bank may make a lot of money  Though the main task of the credit department is to forecast the credit risk accurately  Namely  the probability that some people may not repay the loans  Let's load the data that we have  the credit risk data  data has 25 columns  that is a large data set  So I'm not going to show it on one screen  I just break into pieces  First  there is general information about people  the first column is identifier of a person  the second one is credit card limit  The third column is a gender  the fourth provides information about education  the higher it is  the more educated the person is  The fifth column show marital status  single or married  the sixth one gives you an age  Next there is a graded history with the bank  then we have six columns which contains information graded history  including the latest payments  Some columns have negative figures  this means that the person repaid the loan and made an extra payment on the card  overpaying it  If the person has 0 on their account or a negative value  this is a good client  If the value is positive  it means the client owes money to the bank  this is a warning sign  The following six columns shows the total amount of a person's debt as a payment history within several months on the credit card  You can see that people have different accounts and amounts to pay  One person has 3 000 and another 50 000 per month  we have data for six months  from April to September  The last set of columns shows how the person has been paying the debts in the last six months  Someone paid 1 000 rubles  another person paid 2 000  and there was a person who made a payment of 36 000 rubles to the bank  Finally  the most important column that shows information  that determines if the person will pay his loan payment next month or not  1 is the person will miss a payment and will still owe the money  and 0 means that everything will be fine  and there will be no default on the loan  Let's move to the classification  at first glance  the task of classifiers is to predict which class we should expect  1 or 0  Besides the classifier  can answer a second question  namely  what is the projected probability that someone may not repay their credit line  Probability is more important for the financial institutions than just 0 or 1  Using the data  a bank can decide what steps to make next  If their default probability is 0  then no steps they should take  If the default probability is 30%  perhaps the person could be forgetful and need a reminder  If the probability is about 80%  there is a need to talk with the person and ask him about the payments past due  Maybe the bank should offer a lower interest rate for the loan  and then everything will be fine  Finally  the probability make 100%  then most likely there is nothing they can do  but the bank can sue to get their money  Unfortunately  this is an unpleasant situation  but this is business implies a credit risk for banks  We can use the logistical regression to determine the probability  It sounds strange that a classifier for some reason is called regression  For the truth is that the main proposal of logistic regression is to estimate the probability  The probability is continuous quantity  and its estimate  we can also call the regression  So let's go back to our first picture  we have two different types of dots and we try to split them up using linear surface  And the surface separates 0 from 1 digits  good people from those who don't pay their loans back  And here is a particular perpendicular results points to the direction of the surface  The more a person is away from this surface  the more exactly we can predict the client's ability to pay back  Logistic regression levels out our probability and gradually makes a change from 0  or the people who will definitely repay  to 1  or people who will not repay at all  Let's introduce some notations  we can set our features using x vector that includes x1  x2 and xn  where these xs have the different features  We also use 1 as a constant term to make calculations simple  Target function  it takes 0 or 1 value  the weight filter at the same time is a perpendicular to our dividing space  it says the weight that should be multiplied by x  By the way  if we multiply x by this weight vector  then we get the projection of our x to r on our vector  Which will gie us the operating surface distance  and the site where our dot is located  The further the dot is away from the surface  the greater production value of w by x will be  On the one hand  if the dot is on the side of red dots  the value will be positive  and on the other hand  if it's on the side of blue dots  the value will be negative  We'll predict the full probability by so-called sigmoid function  it will take distance from separation surface as a param  Let's look at sigmoid function  where I observed the distance to the surface along one axis and the sigmoid value along the other  We can see as that the value tends to minus infinity  the value of sigmoid is equal to 0  And when it tends to the plus infinity  the value of sigmoid equals 1  This function is very convenient  as it mostly converts distance into probability  I'm not going to show you a logistic regression error function  because you started it last week  To find its minimum in large data sets  you can use a second order method called L-BFGS that you are already familiar with  so let's move on to the solving process  First we need to arrange our data  all these values which are stored as strings  we need to convert into a number using SQL transform  Now we need to get all the values and feature into one feature vector using a vector assembler  We use the assembler to get the features vector  and then move all columns using that select method  The new data set should have only two columns  including the assembly plane features and the labels that we will be estimating  Next we will sweep our data into the testing and the training sets  30 percent is going to testing and 70 percent is for training  Now we train logistic regression using the feed method  Then we apply the trained regression to our data using the transform method  We build prediction for both the test and the training samples  let's see here what has happened  Transform method has added three new counts to our data  the first one is rawPrediction  what is that? This is the distance of our object to the separation surface  the distance has both positive and negative directions  We can see that both the first variable and the second variable are equal  but only if there is minus sign  The second column is the probability  namely the rawPrediction  that substituted in the sigmoid function  We can see that for each person  the probability to pay off the loan is more than 0 5  For the first row  the probability equals 0 7  if we sum the first and the second probabilities in each row  we get 1  And the total probability of all values is always 1 0  Finally  the last prediction is very simple to get  we take the y value and check if it's more than 0 5 or less  I made the prediction for the first class  let's look at the predictions  For all the predictions of the default for the first five records  the probabilities were less than 0 5  so all predictions equal 0  According to our algorithm  the first five people on the list will repay their loans  If we compare our prediction with the label that we originally have  we will see that this is not always the case  For the last two people who have numbers three and number four  we can see they actually didn't repay their loans  Let's evaluate the quality of this classifier using the binary classifier emulator  Using it  we estimate the area under the ROC curve on both testing and training samples  The area under the ROC curve is above 72%  is that bad or good? Let's look at the ROC curve  it probably looks like 72% of the area under the curve  Of course  it's much better than a random prediction  but at the same time  it's much worse than if it had the quality of 90% and 95%  In other words  the quality is not perfect  but we still can predict  So in this video  you have learned a credit risk assessment that's very important for all financial institutions in the world  In addition  you have learned how to train logistic regression when you handle a lush data set  and check its performance  That would be all for today  I'm Paulo  come back here and you will never regret 
Yamz14tjcxQ,Regularization  Hi  my name is Vladimir  And in the following videos  we'll talk about two important topics  how to simplify your model and how to simplify the data at hand  Namely  we're going to talk about regularisation and a few unsupervised techniques  PCA decomposition and K-means clustering  While unsupervised learning is considered to be more of an art than science  the formal one regularization is one of the key concepts in machine learning  And we're going to start with it  Let's first understand why simplicity is so important and formally define what we usually mean by simple model  In case of building a supervised machine learning model  the general principle of keeping things simple can be expressed in the following way  Always choose the simplest model among those with the same test sample error  Moreover  simple model actually tend to have a smaller test error which we'll discuss a bit later  Now  let's think of the meaning of simple on a few examples  Simplicity of linear model which is effectively at least of its weights  can be define as number of non-zero weights  In case of decision trees  the simpler model is the one containing less nodes and less splits  Going forward  we'll be talking more about linear models  as they were extensively discussed in the previous lesson  Among the most important reasons for using simpler models is their relative inability to over-fit  Just to remind you  over-fitting is a situation random model fits too well to training data sample  and makes rather poor predictions on the test sample  Another important reason is easier maintenance of simple models  For example  they are easier to interpret  and interpretation is crucial for some applications  Also sometimes models are used to tens of thousands of predictions per second  An efficiency of this variation becomes really important  So  I hope it was convincing enough  and now you're ready to think on how you can achieve it  Let's be more specific and think of a particular task  You have number of features and a binary outcome variable y which you want to predict  One of the basic ideas that comes in mind is to restrict the number of features allowed for the modelling  The most simple way is to make use of statistical tests that verified hypothesis of independence of each feature and y in turn  The main statistical test for this task Is chi-squared and F-test  So one can simply disregard the features deemed as independent from Y  But the problem here is that you can't go further and choose the good subset among the rest of variables  because the results of statistical tests are incompatible  You simply can't say which dependency is stronger  Turns out there is a better way for restraining a model  which works by modification of a learning process  To understand how it works  let's remember how models are actually learned  Consider a logistic regression model for our task  The learning boils down to minimizing the arithmetic loss function  L(w)  on the training sample  Intuitively  what you want is to constrain the process  and make it converge towards more simple solution  even if the value of L(w) is suboptimal  To achieve that  let's introduce additional so-called regularization term to our task  by simply adding it to L(w)  This term depends on the model weight and should penalize complexity of the model  There are numerous ways to choose this function  but probably the first that comes to mind is just the squared length of w vector  Note that we deliberately don't include intercept w0 here  Using this regulizer is actually called L2 regularization and lambda here controls the extent to which regularization is applied  One good property of L2 regularization is smoothness  which allows us to apply the previously discussed optimization methods for L(w) prime  A good way to think about regularization is that now it becomes expensive to have large coefficients in the model  So large weights will be associated with those features benefit from which outweigh the penalty term  This  at least intuitively  looks like a reasonable thing to do  Turns out in many application it really helps with generalization  To be precise  you can often find a good value of lamda that increases the accuracy on a test sample  Even though having small weight is good  it's even better to have more weight exactly equal to zero  One of the most common ways to achieve that is to use L1 regularization term  The only difference from the previous case is that now you're using weight modulus instead of squares  I guess it's not really obvious why would such a function guarantee sparsity of the solution  The reason comes from the fact that now there is no derivative in 0 for our function  So now 0 becomes a kind of a special point in our optimization task  Needless to say that vanilla gradient descent methods need adaptation to be able to deal with the task  There's quite a few ways to make this adaptation  Not going too deep into details  Spark uses the solution based on the so-called proximal operators  It boils down to using gradient descent update coupled with special projection operation  The projection makes weights that deviate not too far from 0 to become 0 again  which implies sparsity in the initial model  To go further  there's useful generalization  which is called elastic net  It's basically a complex combination of the previous two regularization terms  Setting Alpha to 0 or 1  you can get vanilla L2 or L1 correspondingly  So what else is important to know about regularization? Turns out that the way the features are scaled can significantly affect the result  Imagine for example two features  one of which has wide range of values going all the way from o to 100  while the second one changes only from 0 to 1  Obviously  the slight modification of the corresponding weights have different outcome on prediction  What is wrong here is this both these coefficients get penalized in the same way  We can't wait to overcome this obstacle easily scale features  There are two common ways  the standardization and mean mark scaling  The former implies division by standard deviation and the optional mean subtraction  The latter is a linear transformation that makes minimum value and maximum value to become 0 and 1  Another thing that I've never mentioned yet is the way lambda and alpha values should be set  I guess that the answer is simple  They should be treated as hyperparameters of the model  and fitted against the data  Now  I suggest you to take a look at the example considered in the previous lecture and add regularization to the model  So  here you can see the Spark code that fits the linear regression model to the bicycle data  Note that I explicitly set the value of elastic net parameter and the regularization parameter to 0  so you could get an unregularized model  Add to score of this model on a test sample is around 075  Note that some weight parameters go as high as 10 000  To set out two regularization you should leave the value of alpha equal to zero while increasing the value of regparam which stands for lambda  Here you can see that with lambda equal to 100  you get a different solution with slightly better score  This time you can see that the value of rates at most 3 400  which is an expected outcome of using L2  Net  let's use L1 regularization  setting the alpha equal to 1 and the regParam=50  The resulting score is the same as the initial model  Though  you can see that two weight have ended up being exactly 0  which  again  is an expected outcome  So in this video  we've learned what are L1 and L2 regularization and the reasons for using them  Important to remember that L1 not only helps fight over-fitting  but also introduces sparsity which is a useful property  They think of regularization is really important as it often apply to other clauses of models such as support vector machines or neural networks
wbp5DDSCrUw,PCA decomposition  Let's talk a little bit about unsupervised learning techniques  Remember  that the difference from supervised learning is the absence of a target variable  In general  these techniques are applied to reveal a hidden structure in the data if it exists  It's particularly useful in multidimensional settings when our intuition fails us miserably  And we can't just plug the data and see everything straight away  Often  these techniques become a part of a larger task helping to direct further analysis  Let's be more specific  If we don't have clear targets  then it's sensible to have some questions about the data that we'd like to answer using unsupervised learning techniques  Here  you can see a few examples of such questions  The first one  can we reduce dimensions while preserving information? It makes sense when we are faced with too many features and expect that not all of them contain useful information  The word information here is little bit tricky because it's not really defined and based on the definition  you might want to do different things  The second question is about clusters in the data  Sometimes  the data at hand is not homogeneous and there could be groups of points that can be clearly distinguished  Usually  these groups should be treated differently and it's important to understand if they exist  Each of these tasks usually gets set in formal terms and is solved by an appropriate method  Now  let's focus on the first question and discuss one of the basic methods to handle it  Before going into formalities  let's build some intuition  Take a look at this picture that shows points in two dimensional space  Imagine that we have to throw away one of these dimensions  Which one would you pick for throwing away? Note that effectively  we are choosing which of the two projections is more meaningful for us  Well I guess that many of you would throw away the X_2 as the size of projection on X_1 seems to be larger  Okay  so far so good  But what if I ask you the same question for this picture? I personally would be puzzled  as our previous logic doesn't seem to work here  Looks like none of the initial axes is a good candidate for elimination  But if you don't like the initial axes  let's change them first  Here you can see my suggestion on how we could do that  In this new axis  our logic starts working again and we can easily disregard X_2 prime  Let's analyze this little example as it illustrates a few important points  One of them is that our initial axis might not present the information in the best way  Typically  the measurement are given to us and we seldom can affect the process they were collected  And other point is that intuitively we can see the future to be more informative in case it accounts for more variance in the data  That is after projection on that axis  The variability in the data doesn't change much  Turns out there is a method that rotates the space and finds a good directions with this property  The method is called PCA or principal components analysis and we'll discuss it in more detail  The main parameter characterizing how data is spread around its mean is variance  By now  you all should be familiar with it  but we'll go further and define covariance which is basically a measure of linear dependency between variables  By the way  correlation is just a properly normalized covariance  and variance is covariance of a feature with itself  Calculating covariances for all the pairs officious reveal spatial structure of the data and allows to identify those desired directions we've discussed earlier  Let's see how it works more formally  Remember  that we are dealing with a multidimensional space  so we have a matrix of all features which is denoted as X in this slide  Each row of a matrix is a point  So this matrix shows four points in three dimensional space  Computation of all the covariances among all the variables is a simple matrix multiplication  The covariance matrix has the size of number of features by number of features which is three by three in my example  It is a symmetric matrix which has variances of corresponding features on the diagonal  As I said previously  this matrix is all we need to find those directions  projection on which the variance is preserved in the most effective way  To do it  we rely on the so-called eigenvector decomposition of a matrix  This is a mathematical technique that can always be applied to a symmetric  non-negatively defined matrix  Effectively  it decomposes the matrix in a product of three matrices the way it is shown in the slide  You can see  that the third matrix is just a transposition of the first one and the second matrix has a particularly simple diagonal structure  The values of numbers on the diagonal are always non-negative  And I suggest rearranging them in non-increasing order  So number one is the maximum among them and number N is the minimum  The colons of the first matrix are called eigenvectors or principal components  While value of the corresponding numbers are called eigenvalues  For our task  the principal components are exactly those directions we are looking for  Let's take the first principal component  Turns out that if we project all the data in this direction and calculate the variance of the projection  we are going to get exactly number one  Moreover  you can't find a better direction projecting on which you'd get a better variance  Go back to our picture  X_1 prime and X_2 prime are exactly the first and the second principle components that this method finds for us  Additional property of these vectors is that they are diagonal and thus  forming your bases that we can consider instead of the initial one  So  now we are ready to approach the task of dimensionality reduction in the following way  First  we get principal components applying this cause technique  Then  we just take a few first principal components and project everything on the subset that defined  By the way  it's exactly what we did in our small example  And so  the only question left is how many components to pick? To answer this question  we need to see how much variance out of the total is going to be left after the projection  This value is usually called explained variance  Total variance is the sum of all numbers while they explained variance is the sum of selected ones  Of course  it makes sense to pick the components with larger numbers first  For example  we might want to take a subset that explain at least 80% of variance or just stop at the point when they explain variance starts growing rather slow  If you take a look at the graph that shows how not explained variance depends on number of components  Typically  it resembles a hand of a person stretching from shoulder to wrist  There is often a point where the slope of the graph changes  This points correspond to an elbow in our imaginary hand and picking the number of components called an elbow method  Important note on PCA is that  it's better applied to standardized data  It effectively means that we apply decomposition to correlation matrix rather than covariance matrix  By standardization  we make features to be of comparable scale which eliminates biases towards the features with large absolute values  So  in this lesson  we've learned about one of the approaches to the task of dimensionality reduction  The approach is called PCA and it tries to preserve the total variance in the data  The method identified this set of new directions called principal components  The dimensions are reduced by simply picking the subset of these components  And this procedure is guaranteed to preserve variance in the most effective way  The last thing that I want to note is that there are other procedures for dimensionality reduction that strive to preserve other properties of the data like relative prioritize distance between the points  There is no other right method and the choice of technique depends on your final goal 
AXgfeV568c8,K-means clustering  Let's continue talking about unsupervised learning techniques  Now  we'll focus on a different question  can we find any clusters in the data? I think it's the time to discuss what clusters are  Formally  what we usually understand by clusters is a group of points which are more similar to each other than to other groups  Among many definitions of what this similarity might mean  we'll consider the most useful one  A group of points is considered to be a cluster  if the distances among them are much smaller than the typical distance of points outside of the clusters  To understand what I mean  look at this picture  On the left  you can see set of points that doesn't show any particular clustering structure  while on the right  set of points clearly consist of two clusters  Of course  if we could always just take a look at the picture  there would be no need for any techniques  but unfortunately  if we are dealing with multidimensional data  they are absolutely required  As I've told you previously  all the unsupervised learning tasks need some formalization  To formalize a clustering task  we should first define the metric  In other words  we need to say  how we are measuring distances between our points in space  I suggest to stick with simple Euclidean measure  For now  though  you of course  can use all other kinds of metrics  Another simplification that will do for now  is supposed that we somehow know the number of clusters in the data  The parameter k will stand for this number  Let's associate each cluster with its central point  Mu with an index i  To calculate the coordinates of its central points  we just need to average the coordinates of all the points in the cluster  On the other hand  a set of central points defines all clusters by a simple rule  For each point  we find the closest center and assigned it to the corresponding cluster  Imagine that we have some cluster candidates  let's define the function that measures how good the clustering is in the following way  Take for each cluster  total sum of distances from points to their center  and add up these values for all clusters  In case the value of k equals to one  we don't have any options and just assign all the points to one cluster  In this case  the value of function V is the sum of length of green segments  In case the value of k is equal to two  and the clustering is done properly  that is we distinguish the left group from the right  The value of V is a sum of length of red segments  So  at least in this example  the function V seems to be much smaller for proper clustering compared to a wrong one  Now  we are ready to formulate our task in case of a fixed and known k  Our task is finding such centers of clusters that minimize the energy function V  Turns out  the task is finding a real global minimum is np-hard  We kind of have to try all the possible clustering options which typically are of a large number  The most reasonable alternative is a simple gradient Lloyd's algorithm which is guaranteed to converge to a local minimum  Let's see how it works  First of all  we fixed the value of k  and initial centers of k clusters  The next steps  are we assigning all points to the clusters according to current centers? Followed by recalculating the centers again  These two steps are iterated  until the centers stop changing from iteration to iteration  Let's see it again on the picture  First  we assign initial centers  then assign each point to a cluster  then recalculate the centers and so on  Just in a few iterations here  the cluster center stop changing  and we found the desired clustering  Nice and easy  Moreover  this algorithm is easily adapted to distributed settings  When you start at points on Hadoop cluster  namely  cluster assignment can be done in parallel on the map step  while new centers can be calculated on the reduced step  Let's address some important issues of the algorithm  The first and the simplest one is scaling  The recipe is straightforward  Data should always be standardized before applying kmeans  Otherwise  the distances are dominated by features with large values  and the result of a clustering is usually useless  Another issue is a strong dependence on the initial centers  It's been shown over time that in many examples  that a purely random initialization doesn't work well  Fortunately  there is a good heuristic called kmeans++ that seems to work much better  The heuristics makes use of the fact that closely selected centers tend to work poorly  So the idea to make the process iterative  picking one center at a time  making sure that the next center is not close to already selected  To achieve it  every time  the probability of which point to become a center of a cluster is proportional to square distance to the closest already chosen center  So  the further the point  the more the probability  To make this process easier to execute in parallel  There's been proposed the better version of kmeans++ called kmeans parallel  The idea is to reasonably reduce a set of candidates for becoming a center  and then to apply kmeans++ on the small set of candidates  This heuristic is implemented in Spark MLlib  Well  looks like there is an elephant in the room  Of course  I completely forgot to tell you how to define the number of clusters  This is the most tricky question as there is no right answer to it  I'll tell you the simplest approach that is called  The Elbow method which is similar to what we've discussed previously  Remember  that we've been minimizing the V function  when k equals to one  it gets its maximum value which we'll call V total  The idea is to try different values of k and see how small the value of V function becomes as a fraction of V total  So  we could build this simple graph showing reduction of V for various values of k  and look at the so-called elbow point  Just to remind you  this is the point where the marginal reduction becomes rather small  In this example  the elbow point is k equal to three  which indicates that the most reasonable number of clusters is three  Actually  there are many other ways to answer this question  and I encourage you to at least get familiar with them  So  today we've learned about the task of finding clusters in the data  We discussed formal definition of a cluster along with the formal definition of our task  Then we talked about kmeans method  with different heuristics to define initial sentence  and to define the number of clusters 
93PD9CqcZTg,Feature Engineering for Texts  part 1  Hello  Let's get started  This lesson is about Feature Engineering for Texts  By the end of this lesson  you will be able to make good feature representations for texts  If you're puzzled why this task is so important  let me show you a list of machine learning problems with texts  Facing these tasks in real work is quite common  Dealing with the first task  you will see that used for assigning a category for each document  it can be applied in an online advertising  Imagine a set of web pages in internet  millions of webpages  Can you get the ad for each page relevant for the content and topic? Got the idea? Ever done the document classification? Perfect  cause your first step to solve this task is to do the similar operation  that is  categorization of these webpages  The second problem is spam filtering  The technologies for sorting out spam emails are widely used by all major email providers such as Google  Microsoft  Yahoo  and so on  Do you happen to know that we do not get 90%  just think about it  90% of spam messages because they have already been filtered by such machine learning based technologies  Let's deal with the last example of machine learning problem with text  sentiment analysis  So  what is a sentiment? A sentiment  apart from the real life connotation  is an emotion behind a piece of text  Posts in Twitter and Facebook are a good example of a possible application of sentiment analysis for social media monitoring  Many big companies do social media monitoring to find out consumers opinion about a new product  You have to make three simple steps  simple for me certainly  but after this lecture  they will be pretty simple for you  too  First  collect the posts containing the name of the product  Second  train a classifier for predicting the sentiment  And third  apply this classifier  The most natural answer to the question  how can I do it? Is via machine learning  These problems are particular examples of supervised machine learning  In supervised machine learning  you have a set of objects  which denoted by capital Z  You have n objects in total  whereas each object has a label  The label is often denoted by Y  The goal in supervised machine learning is to predict the label Y_i of object Z_i basing on a vector of some features  which we will denote X_i  In supervised machine learning  you are going to build the function which maps from the space of features  capital X  to the space of labels  capital Y  Thus  for each vector of features  you can calculate their prediction  the label Y  So  our task here is how to generate a p_dimensional vector of features X_i for object Z_i  In our particular case  Z_i is a test  How to generate a vector of real valid features  a vector of numbers for some text  Consider a simple document of two sentences  This is an apple  An apple is a fruit not a vegetable  Transformation of this document into a meaningful feature vector will take several steps  First  converting this document to the lower case  The slide will give you the full idea of such transformation  Loss of some information is only natural here  Because there is often no difference when the word is the first in the sentence and it starts with the capital letter or it is somewhere in the middle of the sentence  Relax  With lots of information comes simplification  as the difference between these two apples is not really crucial here  Okay  The second step is punctuation removal and tokenization  Check the slide to get the idea of the text  deprived of all the punctuation marks  and split into tokens  Okay  The third step is lemmatization  This step is optional for the English language and later on  I will tell you why  The idea of lemmatization is transformation of each word to its dictionary form  For example  the word sits is transformed into sit  swimming into swim  and so on  You remove from the word information which is related to such grammatical categories as person  number  plural or singular  and so on  Lemmatization is important in such fusional languages like Russian  French  Italian  German  and so on  There  the word adds prefixes  suffixes  changes its grammatical aspects like person  and case  depending on the context  Whereas  with modern English  an example of analytical language  lemmatization is an optional step 
rgf5vvhw0Vo,"Feature Engineering for Texts  part 2  Okay  Now we are dealing with stop words removal  And your first natural question is  \""What is a stop word?\"" Stop words are extremely common words like \""is\""  \""a\""  \""an\""  \""not\""  Want to protect the text? Remove all the stop words from it completely  These words are frequent in the text  contain no information about this text  and are useless for a text classification  Removing all the stop words from our small text  you will get the following list of words  this  apple  apple  fruit  vegetable  High time to create a dictionary by denoting Dict  A set of distinct words in all the documents  The typical size of such a dictionary is from 10 000 to 100 000  Here is a fictional example of such a dictionary  In a real dictionary  each word has a unique number  Okay  Now we are going to calculate term frequencies and finally do their vectorization  In our example  word \""this\"" occurs once  word \""apple\"" two times  \""fruit\"" once and finally  \""vegetable\"" once  Let's create a vector  The size of this vector equals the size of the dictionary  And the elements of this vector are term frequencies  Term frequencies are words counts  This vector is typically sparse and components of this vector correspond to the words which occur in this document  Here you can see an example of such vector on the slide  Okay  And finally  let us understand the essence of TFIDF  which stands for Term Frequency Inverse Document Frequency Model  TF terms are word counts  Let's mark d  a document  capital D  all the documents under analysis  So TFIDF is the multiplication of TF function and IDF function  TF function is a term frequency  The document frequency is a number of documents where a term t appears  Inverse document frequency as you can see is a logarithm of a number of all the documents plus one  divided by the document frequency  plus one  So  here  the capital D with two vertical lines  denote a number of all documents which we are analyzing  Let us understand what this formula means  Imagine it was quite frequent in many docs  but not a stop word  as we have already removed all of them  It can be a quite common word like these for example  And for such word  their denominator DF will be large and then the value IDF will be small  Such words will be of very low priority contrasting with rare words of very high priority in our model  Okay  So finally we can obtain the TFIDF vector  And in general  this vector is very similar to the vector with term frequencies  The elements of this vector are multiplications of TF and IDF and the size of such vector is still the size of a dictionary  And this vector of course will be sparse in most of the cases  Here is a summary of this lesson  You have learned how to do feature engineering for texts  There are seven main steps  First of all  you should convert text to lowercase  Next  remove punctuation and do tokenization  Then make lemmatization or stemming (this step is optional)  Stop words should be removed and you should create a dictionary  And finally  calculate term frequencies  inverse document frequencies and obtain TFIDF vector  which is a vector representation of a text "
VSA_zJVDuWk,N-grams  Okay  and now we are going to speak about another way of representing a text as a vector of features  And this approach is called N-grams  So  what is N-gram? N-gram  it is just N consecutive words  Thus one gram is just one word  It is also called a unigram  Two gram is called a bigram and three gram is often called a trigram  To get a better idea of what I'm talking about  I suggest you to consider a simple text  This is an apple  an apple is a fruit not a vegetable  And which N-grams you can generate from this text? Unigrams are just separate words which we studied in the previous lesson  And here are some 2-grams or bigrams  this is  is an  an apple  an apple  apple is  and so on  So  here are some trigrams  this is an  is an apple  and so on  So  the difference from the previous approach is that you consider a token to be a sequence of words  Not one word anymore  How to create feature vectors using N-grams? Here is an example  Your actions are very similar to word's transformation  just you are creating a dictionary not for the words but for all the possible N-grams  It should be noted that while working with N-grams  you also generate all N-grams with degree less or equal to N  Thus working with bigrams  you also generate unigrams corresponding to separate words  Again  you create a dictionary  Here is a fictional example how this dictionary may look and it contains all the unigrams and all the bigrams which we have inferred from all the documents in our collection  Considering the number of unique bigrams is much  much larger than the number of distinct words  the size of dictionary may be huge if you work with N-grams of a large order N  And  again  you can calculate your frequencies for bigrams and you can calculate inverse document frequencies for bigrams  And exactly in the same way as you work with words  you can create a vector  a feature vector which represents a document  So  as I have already said  the problem here is that the dimensionality of such vector  which equal the size of dictionary  may be very large  And my next lesson is going to be dedicated to solving this issue  Here is a summary of this lesson  You have learned how to use N-grams for creating vector representations of documents  N-gram  by definition  is N consecutive words in a text  While working with N-grams  all N-grams with degree less or equal N should be generated  Number of distinct N-grams when N is greater than one  bigrams  trigrams is large 
gmoC8xC4MGw,Hashing trick  Hi again  and happy to see you all at our finish video of this lesson  Okay  during this lesson we are working to get answers for the following three questions  What is a hashing trick? How a hashing trick can help us to work with very high dimensional features spaces? And what is a hash table? In computer science hash table is a special data structure  which contains a hash function and array of fixed size  If we have at list of keys  for example  from key 1 to 4  then for each key we can calculate the hash function  And after applying the hash function each key is mapped to the specific cell of the hash table  Okay  a hash function is just a function which maps from keys to integers  In our case  of text processing  key is a string  Here is an example of how some fictional hash function may be applied  For example f apple = 10  f fruit = 5 and so on  The common hash functions which are often used for doing the hashing trick or fast hashing of the text are MurMur3 hash  Jenkins hash  CityHash  and md5 hash  Okay  let's consider a hash table in more details  Here I used an example from the previous lesson when we generated feature representations for a simple document  And after applying the hashing trick  the word this maps through the second cell  and the word apple maps in the same cell with the word vegetable  Whereas the word fruit is mapped to the fifth cell  So these cell in the array store turn frequencies which are the word counts  Please note the collision marked red here  Collisions happen when the hash function of two distinct keys are equal  Despite the fact that it is not desirable  you should not avoid it because it helps you to maintain good performance  Okay  when a collision happens you just calculate your frequencies of both words  Okay  again what is going on here? And here I'm trying to find an index  Of the word in a feature vector  See the word this here in this slide? Well  the rate of the word this is stored in the component of the vector  Working with dictionaries you have to look it up and find their corresponding index of the word  it is boring  And in case of hashing trick  you just calculate the hash function of the word  Voila  it is very simple  And the value of these hash functions means it's index and it's position in the feature vector  In this slide you can see a typical pattern how the model quality changes when the size of the hash table increases  The larger the hash table size is  the better is your model  However  you can pick an optimal point  which guarantees almost perfect model  while the hash table size is not to be very large  Okay  finally let us compare the approach based on dictionary and the hashing trick  Working with a dictionary has only one benefit  no collisions as each word or each B gram  3 gram and and any talking in general has a a distinct index in the dictionary  Whereas working with a hashing trick  you may face collisions  You need to store a possibly large dictionary during the learning procedure and the production  Because you're implementing a machine learning model into the production environment you will have to maintain this dictionary in production  And the hashing trick allows you to make all calculations on the fly  it is very fast  The lookup time in dictionary is logarithmic in terms of size of the dictionary  though not being very slow  The hashing trick can easily give each a hot start  Thus  this lookup maybe done in the constant size time  Okay  also as I have said earlier  when you work with dictionaries you add with a feature vector of possibly high dimensionality  You do understand it may slow down machine learning algorithms  don't you? Whereas with the hashing trick the size of the feature vector is fixed  And by adjusting the size of the feature vector you will have a very good stability and performance of your software  Having all this done  you can guarantee that your software  your learning will have a fixed memory footprint  and the out of memory error will not happen 
EUzsy3W4I0g,Categorical Features  Hello  everyone  This lesson will be about two important issues  studying categorical features in machine learning and creating feature vectors when object has categorical features  So  what are categorical features? In the machine learning  there are two most common types of features  The first type is a real-valued feature  a number  a quantitative property of an object  Categorical feature  opposing to the real-valued feature  is an element of an unordered set  which is also considered to be a qualitative property of an object  Okay  Let's consider some examples of categorical features  various identifiers like user identifier  ItemID  ShopID  of course  category and some other properties like region  city  color  IP address and so on  As for machine learning applications and Internet data analysis  in particular  there are a lot of categorical features there  A few detailed examples may help you to understand this task better  A city may be an element of a finite set containing several variants  for example  Beijing  New York  Moscow  Paris  By definition  a categorical feature is an element of an unordered set and these elements are called levels of a categorical feature  Despite the fact that categorical features are often stored in database as numbers  it makes completely no sense to add  multiply or compare categorical features  I would like to emphasize now that categorical features often have a hierarchy  for example  a category of an item in a webshop  where they are typically organized in a tree-like structure  For example  a category of Books has three subcategories  History  Romance and Sci-Fi  And a Sci-Fi category has two subcategories  Space and Alternate History  Of course  there are more other subcategories  as it is a simplified example  And other categorical features like IP address or region also have natural hierarchy  Okay  So  the problem is how to transform categorical features  which are qualitative properties of an object  to the vector of real-valued features  as it's exactly the vector machine-learning algorithms can work with  Let's consider a simple example  a categorical-feature city which has four levels  Beijing  New York  Moscow and Paris  And we can transform it to the real-valued vector by the one-hot encoding technique  So this feature is transformed to the vector with four components  The number of components equals number of levels of categorical feature  and all of the components of this vector equals zero but for one  which corresponds to the number of categorical feature  That is why this technique is called one-hot encoding  Okay  Another example  a category of an item with four levels again  Book  Movies  Clothing and Electronics  And you can transform this category to a real-valued vector in exactly the same way  What happens if you have two categorical features  for example  the city and the category? If you concatenate to real-valued features from the previous slides  then the total length of this vector equals eight  Here in this slide  the first four components of Vector X correspond to their categorical feature  city  and the last four components of Vector X correspond to their categorical feature  category  Okay  The summary of this lesson  you have learned how to work with categorical features in machine learning  Categorical feature is an element of an unordered set  a qualitative property of an object  It makes no sense to add  multiply and compare categorical features  Categorical feature can be transformed to a vector of real-valued features using one-hot encoding 
CNu5RDittrE,Feature Interactions  What is the mechanism of feature interactions? Feature interactions is a way of making a feature representation of some object more diverse  And with a diverse representation of an object  your predictive model will be more accurate  Consider an object having two vectors of real value features Z and T  where Z is a p-dimensional vector and T is an m-dimensional vector  So  by definition  the interaction of two feature vectors is a new feature vector which we will denote by X  And the dimensionality of this new feature vector is M by B  So  it is a multiplication of two original dimensionalities where the resulting vector contains all the pairwise products of vectors Z and T  Consider a simple example  an object with two vectors of features Z and T  Their interaction of two dimensional vector Z and four dimensional vector T gives a vector X which has eight dimensions and  its components  as you can see  are all pairwise products of Z and T components  I want to emphasize here the differences between the feature interactions and the concatenation of feature vectors  Feature interaction is a vector which dimensionality is B by M and the concatenation of features is a new vector with dimensionality B plus M  As you can see in this slide  the contents of vector X is different  So  these are two different ways of making our feature space more rich  Now you know two different ways how to merge information of two features 
pnunzdvezto,Spark ML  Feature Engineering for Texts  part 1  Hi there  Today  you will learn how to do feature engineering for texts and how to work with categorical features in Spark ML  Here is an example of a Python notebook which is very handy when you work with Spark in Python interface  After importing some libraries  you have to create Spark context to see some basic properties of your system  For instance  to see how Spark is executed on top of a Hadoop cluster  And then you should create a Spark session  It is necessary for manipulating Spark DataFrames which are the part of Spark ML library  Okay  here is a sample text consisting of three documents  To do some basic pre-processing of these three documents  you have to create a DataFrame  do a tokenization with a tokenizer class and the transformation to the lowercase  search the input column and output column with words  and finally make transformation and visualize the result  Let's run this cell  Let me give you a detailed explanation  We see that all the initial sentences firstly  transforms to the lowercase and secondly  these sentences are split into words  But as you can notice here  the dots and commas are not removed  This is the feature of the tokenizer object  It is quite simple as it only makes splitting by white spaces  In most cases it is not enough  And now  let's try to use the regexTokenizer  This kind of tokenizer uses regular expressions for splitting and that's the portion I used before  It may split either by white space character  by a comma  or by a dot  by any of them  And of course  you can extend this pattern for your particular application  Please check the result of the transformation of the initial DataFrame which is called inputDF  As you can see  this result is better because there are no commas or dots anymore and all your sentences are split into separate words  You get a vector of separate words  Okay  The next operation is removal of the StopWords  You import an object which is called StopWordsRemover  You create this object again and you specify columns to manipulate with  By calling the method  loadDefaultStopWords  you can see the list of frequently used English words  Okay  I'm scrolling down and now you call the transform method for this object and visualize the result  Actually  it is hard to understand what's going on here because there are four columns now and some of them are very large  You call the select method and manually specify which columns you want to visualize  Okay  As a result of this method  you have a new removedDF DataFrame  Okay  What's happened here? The initially a long document  'This is an apple  An apple is a fruit  not a vegetable'  was transformed to quite a short vector with only four elements as you have removed StopWords and left only the essential words  I want to note here that the list of StopWords was a bit different from the one I showed you in the lecture because this object has also removed the word this  And in the lecture  I didn't remove it  Okay  Now  let us see how to generate NGrams  You import the NGram object  create the instance of this object  specify the degree of NGrams  In this case  your object will generate only bigrams  Again  you specify input and output column and make the transformation  So  input argument of this transformation is a DataFrame and the result is a DataFrame too  Now  your DataFrame ngramDF has a lot of columns and you're going to explore it in another way  So  in this line  you are picking the first row of the DataFrame  And this cell prints two columns  the column called document and the column with ngrams  You see that the library has generated the ngrams properly and can do the same thing for the next row  Fruits are tasty and here are two distinct bigrams  Fruits are and are tasty  What result do you have trying trigrams? The vector which is the result of this operation contains all trigrams from your document  This is an apple  An apple is  Apple is a fruit  and so on  You can do the same for the second sentence and see that you get one trigram since the initial sentence has only three words  Okay  summary  in this lesson  you have learned how to do the basic text processing in Spark ML  you have learned how to transform text to lowercase  remove stop words  split it to tokens and generate ngrams 
TrlqZ_cIKhw,Spark ML  Feature Engineering for Texts  part 2  Okay  The next topic is generating real valued vectors of features for documents  And it is commonly done by the method which is called TF-IDF  which stands for Term Frequency Inverse Document Frequency  First of all  you need to import a class which is called CountVectorizer and then you create an instance of this class  You specify the input column and the output column  Finally  you apply the method fit to the dataset  And the result  it is not a data frame but a model  It happens because in Spark ML  CountVectorizer is implemented as an estimator not a transformer like previous classes  It is implemented in the way similar to machine learning algorithm and produces a model  Okay  you did it  And after this step  you see the vocabulary containing all the distinct words  Okay  I want to show here that you can specify the vocabulary using this argument and in this case  the CountVectorizer will select two most frequent words  So  if vocab size equals two  this function selects two of the most frequent words  Okay  And in this case  there are two words  apple and vegetables  Okay  Let's return to the initial run where the vocabulary was large enough  You have a model and using this model  you must do a transformation of the initial dataset which is called removedDF and the result is called countDF  And again  you pick the first row and see its columns  The initial sentence was  This is an Apple  An Apple is a fruit  not a vegetable  The second line  it is a bag of words after stop words removal  and the third line here is the vector of features  So  you see that it consists of three elements  Let us see the type  The feature vector being a sparse vector object in Spark stores only nonzero entries  Let us pick this one and transform it to the NumPy array  You can see that this vector has seven components because the size of our vocabulary is seven  Still  most of these components are zero and you can see here that sparse vector stores actually two vectors  a vector of indices and a vector of values  So  it means that the component number one equals two and the component number three equals one  the component number six equals to one and so on  And it is exactly the same as the vector V  There are seven elements in total  and this number is stored in the first component of the vector  Okay  Now  you're going to calculate the second part  document frequency  Now  you're importing caching TF and IDF objects  IDF  it is also an estimator  Thus  you use the dataset which counts and fit this IDF model  Okay  The input column here is a vector with 12 frequencies and you train this model because we need to apply this model to the original dataset  Okay  let's see what's going on here  Due to the great number of columns  it is hard to understand what's going on  You pick up the first row and get the result in the last line  It is TF-IDF vector representation of your document  finally  And again  it is stored as a sparse vector  What is interesting here is that the biggest component is the first one corresponding to the word Apple  Exploring the vocabulary  you see that Apple was the first word with a component zero  Okay  summary  In this lesson  you have learned how to generate TF-IDF vector representations of documents in Spark ML  This is done by means of CountVectorizer and IDF classes  Also  you have learned how to explore a dictionary generated while text processing 
bpB2rLqiiJU,Spark ML  Categorical Features  Okay  now you will learn how to work with categorical features in Spark and ML  First of all  you are going to import one hot and colder or/and strength indexer classes  And then create a toy data frame  CatDF  Let's do the one hot encoding  Firstly  you create a StringIndexer class  which will enumerate all the distinct values of categorical features city  This class also uses an implementation of estimator  That is why at first you fit it using the original data frame  and then you transform the original data frame using the model used by this class  By running this cell you see that Spark ML calculates indices of all the categorical variables  Okay  the next step is to produce vector with one-hot encodings by means of the one-hot encoder class  You create its instance and You're going to transform the cityIndex column to the cityVec column  Running this aisle  you get the result where cityVec is a sparse vector of one quote encodings of cities  Let us explore the first row by picking it from the final data frame  and transforming to the dense array  This array is an example of one hot encoding  which I showed you in the presentation in the lecture  And the second one is very similar  I want to emphasize one point here that I used an option set drop false  and without this option by default  the behavior of OneHotEncoder is quite different  The difference is that OneHotEncoder emits one categorical feature  So the vectors now have size not 4 but 3  And for one value of the categorical feature  the vector will contain only 0  From there  machine learning point of view if you have in your modal the global bias term  This approaches are very similar  but I prefer the first one where the size of web chart equals number of distinct categorical features  Finally  let us see how to do interactions of features  Consider a dataset  with two categorical features  city and category  Okay you have a trolls  only in Java and Scala interfaces  In this lesson you have learned how to work with categorical features in Spark ML  The main classes for doing it are one OneHotEncoder  string indexer and vector assembler 
#NAME?,Topic Modeling  LDA  Hi there  In this lesson  you will study topic modeling  What is topic modeling? It is a way of unsupervised learning  which means that our documents do not have any labels  Topic modeling is dedicated to discovering latent topics in a collection of documents  In opposite to supervised learning  we are going to infer an internal structure of our documents  There are several main applications of topic modeling  It is used for document classification  document clustering  visualization  and for building text recommender systems  The input of topic modeling is  for example  a set of web pages in Internet with news  And one possible problem which we are going to solve is doing automatic clustering of these web pages  For example  here are some fictional clusters like entertainment  tech  sport  and so on  And I want to emphasize here that topic modeling allows us to do this clusterization without labels in completely unsupervised fashion  Here are some basic assumptions and definitions of topic modeling  Let's make w a word  d a document  t a topic  We assume that the number of topics is fixed  and we will denote it by capital T  And we assume here that each topic t may generate a word w with some probability  We assume that each document d has a topic t with some other probability  These are very basic probabilistic assumptions of topic modeling  And I will explain these assumptions a bit later using some examples  Two of the most common methods for topic modeling are LDA and PLSA  Only LDA is implemented in spark ml  Here is an example from the research paper of the Professor David Blei  which is one of the inventors of topic modeling  In this research  David Blei used the document collection which contained 17 000 articles from the journal Science  and he applied to these articles topic modeling with a hundred of topics  Let's see what happens here in the journal's article taken as an example  This document have a list of latent topics and each word is generated from some topic  For example  this document is called Seeking Lives Bear Genetic Necessities  and the topics of this article are shown in this slide on the left  The first topic is about genetics  The second topic is about life and evolution  This third topic is about brain and neural system  and the last topic  which is shown in blue color  is about computers  I want to emphasize here that this method does not produce interpretable names of topics  because this method is completely unsupervised  Okay  one more example from this research  Here are four topics and  for each topic we have selected  five words with maximum probability of being generated from this topic  Again  the names of topics are not created by this algorithm  And you can see the first topic depicts human genome  the second topic is about evolution  the third topic is about diseases  host  and bacteria  and the last topic is about computers  Okay  what are document topics? Document topics are the distribution of topics inside this document  We assume that each document is a mix of some basic ideas  And here is a distribution for the particular research paper  Finally  I want to say a couple of words about topic model learning  It is a quite complex  and here is a sketch of the learning procedure  This procedure involves two matrices  phi and theta  The phi matrix is a distribution of words with respect to topics  And the theta matrix is a distribution of topics with respect to the documents  By n_sub_dw we denote a number of times which word w occurs in the document d  And by n_sub_d we will denote the lengths of the document d  It means that n_sub_dw divided by n_sub_d is a frequency of a particular word w in the document d  We approximate this frequency with the product of two matrices  phi and theta  You may think about topic modeling as a method for decomposing matrix  This is a very basic explanation of how topic modeling is done  In this lesson  you have learned what topic modeling is  You can identify the applied problems where topic modeling may be useful  Topic modeling discovers latent topics in collections of documents  Document labels are not required  Two main algorithms of topic modeling are PLSA and LDA  Each topic has the list of most typical words  and each document has a list of its topics 
9LHTQsDspjE,Word2Vec  Hi guys  This presentation is about the machine learning model which is called Word2Vec  The goal here is to find a vector representation of each word which captures a meaning  a sense of this word  These vectors are called word embeddings  This machine learning model word to vector is based on the distributional hypothesis in linguistics  The distributional hypothesis has used that words  which occur in the same contexts have similar meanings  And informally speaking  that underlying idea is that a word is characterized by the company it keeps like people  Here is a simple example  consider three sentences which you can easily find in a lot of documents  My name is John  my name is Mary  and my name is Jack  It is very easy to infer from these sentences that John  Mary and Jack are given names  And even if you find in the same context a new word  you may suppose with high probability that the new word is also a given name  The Work2Vec model was invented by a group of researchers from Google  The main inventor is Tomas Mikolov and in this presentation  I will use some results and pictures from their research papers  Consider the beginning of a sentence  the forest of Oak tress on the mountain  The word2vec model analyzes texts in a sliding window  Here is a sliding window of size five  In this window  there are always five consecutive words  And what you are doing is called a word prediction by supervised machine learning  In more details  there are two statistical models inside  the word2vec algorithm  The first model is called continuous bag-of-words CBOW  This model predicts a word which is in the center of the sliding window given all that surrounding words  The second statistical model is called a skip gram model  In opposite to the previous case  the model predicts surrounding words based on the word in the center  Here are some technical details of the word2vec model  The model assumes that each word is associated with two vectors  The first vector is called the input vector  and the second is called the output vector  Word2vec fits these vectors by the principle of maximum likelihood  Here are the function you are maximizing and I'm skipping some technical details  The main idea that you are fitting these two sets of vectors which are associated with words using the large text corpus  So you are going to find such vectors where the probability of absorbing  given the text corpus  is maximal  Okay  how to use the word2vec in practice? You should take a large corpus of documents for example from Wikipedia or a large set of used articles  And the second step  you fit the word2vec model  It is essential to take a really large sample of documents because in this case the quality of trained model will be significantly better  Sometimes training of the word2vec model may take a lot of time and it may require using distribute systems like Spark ML  And what happens next if you have trained this model? You pick one of the vectors  input or output ones and this vector will capture the meaning of this word  These vectors are so good that then you even can make simple algebraic calculations using them  With the word2vec model you can do that Question answering  If Paris is related to France? What work is related to Italy? Everyone can solve this simple quiz  the answer is Rome  It turns out that there were direct model can solve it too  You can take the vector associated with the word Paris  Subtract the vector associated with the word France  and at the vector associated with the word Italy  Let us denote these obtained vector vx  This vector is very similar to the vector which is associated with the word Rome  Of course  vx is not exactly Rome  but it is the closest vector among the others  And here is a visualization of such idea  Some more examples of the word2vec model capabilities  What's going on here? You take two words from the column word pair and one word from the first column of the word pair group  And in the last column  the answer of the word2vec model is exactly as in example with Italy  France  Paris and Rome  The Word2vec model can guess associations  If you ask Word2vec model Athens  Greece and Oslo Then  the answer is correct  It is Norway because Oslo is the capital of Norway  And it can capture even more complex associations  rules like currency  city  state  man or woman adjective to verb  and so on  It means that vectors which are learned by the word2vec model are very accurate  Okay  one more example with relationships  For example  if you say to the Word2Vec modal  France  Paris and Italy  the answer is Rome  And if you ask it  what are the words associated with Japan? It is Tokyo  For Florida  it is Tallahassee  You see it here in the slide In the second line you can see how the word2vec model tries to guess the degrees of comparison  So input here is big  bigger  small  oops  In this case word2vec model has an incorrect answer  larger  Should be smaller of course  But in the second line it is okay  Cold  colder  quick  quicker  In the first line there are interesting relationships  It is profession  Einstein is a scientist who is messy  Messi is a midfielder  it's correct  Mozart is a violinist which I think is close to the truth and Picasso is a painter  Okay  where interesting line about chemistry is the short hand for chemical elements  CU stands for copper Zn stands for Zinc  Au stand for gold and in the case Uranium it is not the right answer  Here should be capital U but we see a word plutonium  And you can explore this table and find many interesting co relationships  Okay  there are a lot of applications of the word2vec model  First to though  it can be used for doing text classification  And instead of using bag-of-words model and tfâ€“idf representation  you can use vectors of words produced by word2vec  For example  vector of features of some document might be a sum of all the word vectors in the document  Word2vec is used a lot in machine translation and even image captioning  Image captioning is an algorithm for producing text captions of an image  Okay  the summary of this lesson  you have learned that word2vec algorithm and now you can identify the situations where it is useful  Word2vec algorithm learns vector representations of words which capture the meanings of words  Similar words have similar vectors  One can perform algebraic manipulations with these vectors 
wi7T8rP2zMM,Decision Trees Basics  Hi guys  so happy to see that you're all here again  No one got scared of the content of the previous lessons  This lesson will be about Decision Trees  First of all  what is a Tree in computer science? By definition a Tree is an acyclic connected graph  Graph is a data structure which consists of nodes represented by circles in this slide  and edges represented by the blue lines  We assume that nodes are connected with themselves by means of edges  This graph is not a tree because as you can see  it contains a cycle  You can see a red triangle here  Another example of graph you can see in this slide  this graph contains two disconnected components  That is why it is not a tree  as there is no edge connecting the left component with the right component  Okay  By definition a tree is an acyclic connected graph  The selected node in a tree is called the root and it is also the top node  Nodes at the bottom are called leaves and all the nodes between them are called internal nodes  You may have noticed that the tree in computer science is different from a regular tree from the forest  It is like a regular tree but upside down because the root is typically placed at the top  Okay  Now in machine learning  the Decision tree is a binary tree  and it is a graphical representation of a function  According to the definition  there are only two splits per each node in a binary tree  Actually  other kinds of trees having more than two splits were also studied in machine learning  but it turned out that such data structure  such complex trees are of no use  In this lesson  we will consider only the binary trees which have at most two splits  In a decision tree  in roots  and internal nodes  there are splitting conditions  Splitting conditions are inequalities for some variables  You want to check whether a variable is less than a threshold  So  there are predictions in leaves in the bottom of the tree  Let us see how the function can be calculated given the representation of a tree  Consider a vector x = 0 5  10  0  It is a three-dimensional vector  And let us see how you can calculate f(x)  You start initially from the root  and check the splitting condition in this node  You check rather Xâ‚ is less than one or not  And it is true  Xâ‚ is less than one  And then  you go to the left  Then you go to the second internal node  Then you check whether it's less than two or not  In this case  it is false  And you go to the right  And finally  you will reach the last node  and get a prediction 1 8  You conclude that f(x) = 1 8  You see that the calculation of a function given by a decision tree is quite simple  You can easily make predictions using decision trees 
SO-oXQgvJt4,Decision Trees for Regression  Okay  You have learned that doing predictions with decision trees is quite simple  And the main problem is how to grow a decision tree so that it will make accurate predictions on the test set  The most popular approach is to build a tree greedily  layer by a layer of from top to bottom  Each split is selected to maximize an information gain IG  Here is a definition of the information gain  And informally speaking  the information gain is a decrease of error  Let us see in more details what it means  The information gain is the difference of the error without split and an error with split  First of all  let's consider a decision tree for regression  In the regression problem  you have a training set which consists of x_i  y_i  where x_i is a vector of features  And y_i are labels which are real values  Your goal here is to find a function f(x) which is a tree of course  Where the error at the test dataset will be low  This is done by building a function f which minimizes the error at the training set  How to grow decision tree for regression  As I said before  a tree is built from top to bottom  And at each step  you should find the best split in the internal node  You get a test dataset Z  And here is a splitting criteria x_k less than t or x_k is greater or equal than a threshold t  The problem is how to find k  the number of feature and the threshold t  And also you need to find values in leaves a_L and a_R  What happens without a split? Without a split  the best you can do is to predict one number  (a)  It makes sense to select such a number (a) to minimize the squared error  It is very easy to prove that such variable a equals an average of all y_i  Thus you need to calculate the average value of all the targets  In this slide  a-hat denotes the average value  Impurity z equals the mean squared error if we use a-hat as a prediction  What happens if you make some split by a condition x_k less t? For some part of training objects Z_L  you have x_k less t  For other part Z_R holds x_k greater or equal t  I want to show in this slide  that the error consists of two parts for Z_L and Z_R respectively  So here you need to find simultaneously k  t  a_L and a_R  But in fact you can calculate the optimal a_L and a_R exactly the same way as you did for the case without a split  You can easily prove that the optimal a_L equals an average of all targets which are the targets of objects which get to the leaf  And I will denote these values by a-hat L and a-hat R respectively  Okay  After this step we only need to find optimal k and t  You have formulas for impurity  for objects which get to the left branch of our splitting condition and to the right  Then you can find the best splitting criteria which maximizes the information gain  This procedure is done iteratively from top to bottom  You also need to specify the stopping rule and there are three commonly used stopping rules  The first rule is defined in terms of maximum depths of tree  The second stopping rule is defined in terms of minimum information again  It means you do not make any further split if the maximum information gain is less than some threshold  And finally  the last stopping rule is defined in terms of minimum number of objects which should be in leaves  So it means that we cannot create leaves which get only few objects  As we concluded before  the optimal predictions in leaves are just averages of all labels y_i  And if only few objects get to the leaf  then this average is unreliable  It is calculated based on very few examples and the predictions with such average will be poor and inaccurate 
d0OxV2rA398,Decision Trees for Classification  Okay  Decision trees  of course  are also used for doing classification  And the difference between decision trees  for classification and regression  is the following  Class labels are stored in leaves  in case of classification  This way the label of the class is our prediction  How to find the best split in case of classification problem? It is done in the same way as the regression  You want to maximize the information gain which equals the difference of impurity before split and impurity after split  The widely used formula for the classification problem is the so called Gini impurity  Gini impurity is the sum of fi by 1-fi over all the classes  Where fi  by definition  is a frequency of label i in node  And the capital C is the number of classes  What does it mean? Have a look at this slide and consider a split with capital ZL  ZL is a set of objects which go to the left leaf  For this set of objects  fi is just the number of examples in capital ZL having a label i divided by all number of examples in capital ZL  So  it is really simple  just a frequency of a label in a node  Maybe it is a bit easier to understand what Gini impurity means in case of two classes  You can easily show that Gini impurity equals 2 by f1  by 1-f1  where f1 is a frequency of the first class  And here is a plot of Gini impurity  You see in this plot that Gini impurity reaches its maximum where f1 equals 0 5  It means that when a number of objects having first class and a number of objects having zero class are equal  Gini impurity reaches its maximum  The other way round  if frequency of the first class equals 0 or 1 Gini impurity reaches its minimum  This is a desirable situation  since in a leaf you have a pure set of objects  objects of one class  That is why this function is called impurity 
krsvQHIGhvc,Bootstrap & Bagging  Hi there  Let's get started  This lesson is about the random forest algorithm  Random forest is a machine learning algorithm which produces and ensemble of decision trees  And this ensemble as a whole is more powerful than a single decision tree  First of all  let us understand what the bootstrap is  Bootstrapping is an algorithm which produces replicas of a data set by doing random sampling with replacement  This idea is essential for the random forest algorithm  Consider a data set  Z  for example  with five objects  How does bootstrapping works? At the first step  you pick at random an object  for example  the object number three  Then you repeat it and pick the object number one  Then the object number five  And so on  Then possibly you can pick again the object number five  because at each iteration you pick an object at random  and there is no correlation with the previous step  And finally  in our two example  you pick the object number two  After bootstrappingexam you have a new data set  The size of this data set is the number of elements in the original data set  But its content  as you can see  is slightly different  Some objects may be missing and other objects may be present several times  more than once  Okay  The next method is bagging  It was the second idea essential for understanding of the random forest algorithm  Bagging is a shorthand for bootstrap aggregation and it is a general method for averaging predictions of other algorithms  not decision trees  but any other algorithm in general  Bagging works because it reduces the variance of the prediction  Let me explain it to you  Here is a formal definition of the bagging algorithm  You have a training set Z which consists of pairs XI  YI  B is a number of iterations  and you have a machine learning method M  which can be of any nature  For example  decision trees  linear regression  neural networks  support vector machine  anything  What do you do? You repeat capital B iterations  At each iteration  you draw a bootstrap sample  G star B of size N  from the training data  Before we have already discussed the algorithm that allows to produce the bootstrapped replicas  Then you apply the machine learning method to this data set  G star B  And finally  you obtain the model  You will denote this model by f(x) star B  This algorithm returns an ensemble of B models  This is how training with bagging works  The next question is how to do predictions with this ensemble  In case of regression  the prediction of the whole aggregate model is just an average of all the single models F star B  And in case of classification  it is a majority vote of all their predictions  What is the majority vote? Consider an ensemble with five models  and if three models vote for the class number one  and two other models vote for the class number zero  then the majority vote will be the class number one  Surprisingly  but the bagging algorithm really works  In most of situations with any machine learning method in the core  the quality of such aggregated predictions will be better than of any single prediction  Why does bagging works? This phenomenon is based on a very general principle which is called the bias variance trade off  This topic is out of scope of our short course  but I will explain it to you informally  You can consider the training data set to be random by itself  Why is it so? What is the training data set? In the real situation  the training data set may be a user behavior in Internet  for example  web browsing  using search engine  doing clicks on advertisement  and so on  Other examples of training data sets are physical measurements  For example  temperature  locations  date  time  and so on  And all these measurements are essentially stochastic  If you can repeat the same experiment in the same conditions  the measurements actually will be different because of the noise in measurements  and since user behavior is essentially stochastic and not exactly predictable  Now  you understand that the training data set itself is random  In ideal situation  you should collect as much data as possible to reduce this noise component of the data set  But in fact  you cannot run the same experiment in the same situation as it is impossible to invent a time machine  but you can do a bootstrapping  So bootstrapping is a method for generating very similar replicas of the original data set  What happens here if you train a machine learning model on the bootstrapped replicas and average them? Then the aggregated machine learning model will be more stable  This happens because of averaging  After averaging  the noisy parts of machine learning model will vanish out  whereas stable and reliable parts will remain  The quality of the average model will be better than any single model  Okay  The summary of this lesson  you have learned what the bootstrap algorithm and bagging algorithm are  Now  you can improve the quality of almost any machine learning method by doing bagging  However  it might be very time consuming for large data sets 
qAIpx8vKqPw,Random Forest  Finally  you have all the building blocks to define the Random Forest Algorithm  And the Random Forest Algorithm is a bugging of de-correlated decision trees  Here is a formal definition of this algorithm  In the input  you have a training set z  And the capital b is a number of iterations  At each iteration  you draw a bootstrap sample  z star of size m from the original training set  and then you grow a de-correlated 3tb or these Bootstrapped data set  Finally  you'll return an assemble of all de-correlated decision trees  Prediction is done exactly the same way as we described before in case of bugging  went for regression there algorithm returns an average of all predictions of all decision trees  And in case of calcification  it returns a majority vote of all single tree predictions  I want to emphasize here that algorithm generated not irregular decision tree  but a de-correlated decision tree  What is a de-correlated decision tree? What is the difference with the regular one? Here is an algorithm from the previous lesson when you learned decision trees  and the only difference is in the step number two  In decision tree  you iteratively do splits  And at each iteration  you select the best split among all the possible ones  And in the Random Forest decision tree  you do split slightly different  You select not the best split overall  but the best split among m variables  Initially  you pick a random m possible variables  and select the best split only among these m variables  The criteria for selecting the best split is exactly the same  it is the maximization of information game  The same as reducing the error after split  The only difference is that you are finding the best split  not among the variables  but the among the subset of these variables  Here are some recommendations from the inventors of the Random Forest  There are commands to use m = square root of p for classification  however  p is the total number of variables  And in case of regression  they recommend to use m = p divided by 3  And also minimum instances per node in the decision tree should be 1 in case of classification  and 5 in case of regression  I want to show you this diagram  Here are the results of training of two random force  The first variant is marked with green here  and either the variant were at each step m equals speed  It means that at each step  you grow a regular decision tree and you find the best split among all the variables  And the blue line  it is a de-correlated decision tree  In this situation  you randomly pick m equals square root of b  And all the trees can be built using different subsets of variables  As you can see at this diagram  at the initial stage  the variant is m equals square root b is worse before 20 iterations  But eventually  this variant of the Random Forest algorithm converges to the better solution  This idea of introducing extra variants to each decision tree  it really works  Okay  the summary of this lesson  Random Forest is a good method for solving general purpose classification and regression problems  But typically  it is slightly worse than the gradient boosted decision trees algorithm  Of course  this algorithm can automatically handle interactions of features because this could be done by a single decision tree  Other important feature is computational scalability  Each decision tree could be effectively grown on a computer or a cluster  In the Random Forest algorithm  each tree can be built independently on other trees  It is an important feature and I would like to emphasize it  That is why the Random Forest algorithm east is essentially parallel  The Random Forest could be trained in the distributed environment with the high degree of parallelization  Okay  as far as predictive power of the Random Forest is  on the one hand better than a single decision tree  but it is slightly worse than gradient boosted decision trees  And here you'll lose the interpretability because their composition of hundreds or thousands Random Forest decision trees cannot be analyzed by human expert 
c1Ou_yqXjoo,Gradient Boosted Decision Trees  Intro & Regression  Hi guys  this lesson will be about gradient boosted decision trees  This algorithm is considered to be the best general purpose algorithm for solving classification and regression problems  Of course  in some particular areas like computer vision or speech recognition  there are algorithms which are more suited to solving some particular problems  But in many real world applications which you will face while working as a data engineer or data scientist  gradient boosted decision trees is the method of choice  First of all  what is boosting? Boosting is a method for combining outputs of many weak classifiers to produce a powerful ensemble  There are several variants of boosting algorithms  AdaBoost  BrownBoost  LogitBoost  and finally  Gradient Boosting  which you will learn during this lesson  When you work with big data  with big training data sets  they have on the one hand  a large number of training examples  And on the other hand  a large number of features describing objects  In this situation  It is very natural to assume that you would like to train a really complex model  even having such a great amount of data  And hopefully  this model will be accurate  There are two basic ways  in machine learning  to build complex models  The first way is to start with a complex model from the very beginning  and fit its parameters  This is exactly the way how neural network operates  And the second way is to build a complex model iteratively  You can build a complex model iteratively  where each step requires training of a simple model  In context of boosting  these models are called weak classifiers  or base classifiers  We will proceed the latter way  Consider for simplicity  a regression problem where you have a training set which consists of pairs xi  yi  Where xi is a vector of real-valued features and yi are labels or targets  And our goal here is to find the function f(x)  such that the square root loss on the test set will be as low as possible  So you train the model using only the training data  but you expect that the error in the test data set will be low  How to build such f(x)? In boosting  your goal is to build the function f(x) iteratively  You suppose that this function f(x) is just a sum of other simple functions  hm(x)  And particular  you assume that each function hm(x) is a decision tree  Okay  here is an algorithm for training gradient boosted trees for regression  In the input you have training set  and M is the maximum number of iterations  The first step  you calculate an average of all the targets yi  and it is your initial approximation to the function f(x)  And then you repeat line from three to five for M iterations  What do you do at the step number 3? You calculate the residual  what is it? Residual is a difference between the target yi and our current prediction  f m- 1 xi  And at each step  you are going to minimize this residual and train a new decision tree  such that it will fit this residual as close as possible  Informally speaking  you create an auxiliary training set  which consists of pairs xi y hat i  It is equivalent to the original training set  but you replaced the original labels yi with residuals  At the step 4  you feed the decision tree  hm(x)  to this new target of residuals y hat i  And finally at the step number 5  you add this decision tree to the composition  But you multiply it by a constant  nu  which is called the regularization or learning rate  This constant is recommended to be small  and particular less than 0 1  Okay  if you are familiar with the optimization theory  you may have noticed that gradient boosting is somewhat similar to the gradient descent in the optimization theory  If you want to minimize a function in the optimization theory using the gradient descent  you make a small step in the direction opposite to the gradient  Gradient of the function  by definition  is a vector which points to the direction with the fastest increase  Since you want to minimize the function  you must move to the direction opposite to the gradient  To ensure convergence  you must make very small steps  So you're multiplying each gradient by small constant  which is called step size  It is very similar to what you do in gradient boosting  And gradient boosting is considered to be a minimization in the functional space  Okay  the summary of this lesson  You have learned what the gradient boosting algorithm is  Boosting is a method for combining outputs of many weak classifiers or regressors to produce a powerful ensemble  Gradient Boosting is a gradient descent minimization of the target function in the functional space  Gradient Boosting with Decision Trees is considered to be the best algorithm for general purpose classification or regression problems 
kubJIJMOS8I,Gradient Boosted Decision Trees  Classification  Okay  In this lesson  you will study gradient boosted decision trees for solving the classification problem  It is a bit harder than gradient boosted decision trees for solving the regression problem  I think this blog can be considered as advanced material  The main difference here is that  in training set  y_i  which are the targets you're going to predict  now they act as class labels  but not real values in case of regression problems  And your goal here is to find the function f_x using training set where the misclassification error at the testing set will be as small as possible  How you're going to build such the function  f_x? You use a probabilistic model by using the following expression  one by one plus exp  you model the probability of belonging of an object to the first class  And here inside the exp  there is the sum of h_m_x  and each h_m_x is a decision tree  You can easily check that such expression for probability will be always between zero and one  so it is normal regular probability  This function one by one plus exp minus x is called the sigmoid function  which maps all the real values into the range between zero and one  Let us denote the sum of all h_m_x by f_x  It is an ensemble of decision trees  Then you can write the probability of belonging to the first class in a simple way using f_x  And the main idea which is used here is called the principle of maximum likelihood  What is it? First of all  what is the likelihood? Likelihood is a probability of absorbing some data given a statistical model  If you have a data set with an objects from one to n  then the probability of absorbing such data set is the multiplication of probabilities for all single objects  This multiplication is called the likelihood  The principle of maximum likelihood states the following  you should find a function f_x  where this function maximizes the likelihood  thus  you want to find the underlying hidden statistical model which maximizes the likelihood of absorbing the data set  It is easy to see that it is equivalent to find a function f_x  which maximizes the logarithm of the likelihood because logarithm is a monotone function  It is easier from the computational point of view to deal with logarithms  Okay  We will denote by q_f  the logarithm of the likelihood  and now  it is sum of all logarithms of probabilities and you are going to maximize this function  We will use shorthand for this logarithm l_y_i  f_x_i  It is the logarithm of probability  And here  I emphasize that this logarithms depend actually on the true label  y_i and your prediction  f_x_i  Now  q_f is a sum of l_y_i  f_x_i  In this slide  you can see an algorithm for training gradient boosted decision trees for classification  You have the training data set Z and capital M is the number of iterations  And what's going on here? At the first step  the initial approximation is f_zero_x  and it equals log_p_one by one minus p_one  p_one is the part of objects of the first class  One can show that it is an optimal initial approximation for the sigmoid function  Then you repeat capital M iterations  At the step number three  you calculate the gradient of L  What does it mean? You want to maximize the q_f and q_f is the sum of l_y_i  f_x_i  In order to maximize some function  you should move by the direction of its gradient  Thus  you calculate this gradient for all the objects in your data set  Then at step number four  you create an auxiliary training set x_i_g_i  The only difference of this data set with the original one is that original labels y_i are replaced with gradients g_i  and you fit a decision tree  h_m_x_i to this new target  g_i  After fitting  you calculate their optimal step size  rho_m  It is such as step size that their addition of new decision tree  h_m  into the composition maximizes the likelihood  And finally  at the step number six  you add this new decision tree  h_m_x to the ensemble with a small multiplier  mu  which improves convergence  Doing such small steps  you also guarantee the good predictive power of your model  Your model will not overfit in this case 
1qOMlqE6LhU,Stochastic Boosting  It turns out that Gradient Boosting can be improved further by borrowing one idea from the Random Forest algorithm  In this slide  you can see  in the red color  the difference of Stochastic Boosting with the regular boosting  The only difference is in the step number four  where you fit a decision tree  Instead of using old dataset  X_I_G_I  you create a new dataset  which size is only one half of the original one  You do it by a random sampling with replacement exactly as in the Random Forest algorithm  Here is an example how it may look like  If our original dataset has eight elements  n=8  then a new dataset will have four elements  which are created by random sampling with replacement from the original one  It turns out that this tree has two main benefits  On the other hand  it improves performance  Since the dataset for fitting decision tree is twice less  then the algorithm for fitting decision tree will be two times faster  On the other hand  such algorithm is better in terms of predictive power  The Stochastic Boosting is a mix of two ideas  The first idea is gradient boosting and the second idea is begging 
wfvg4rE7WpM,Gradient Boosted Decision Trees  Usage Tips & Summary  Okay  now I want to share with you some useful tips for gradient boosted decision trees algorithm  First of all  it is important to understand how the regularization parameter works  In this slide  you can see the behavior of the gradient boosted decision trees algorithm with two variants of this parameter  0 1 and 0 05  What happens here  at the initial stage of learning the variant with parameter 0 1 is better because it has lower testing error  At each iteration  you measure a testing error of our ensemble on the hold out data set  But eventually  the variant with lower regularization  0 05  reach lower testing error  Finally  this variant turns out to be superior  It is a very typical behavior  and you should not stop your algorithm after several dozen iterations  You should proceed until convergence  Convergence happens when your testing error doesn't change a lot  The variant with lower regularization converges more slowly  but eventually it builds a better model  The recommended learningRate should be less or equal than 0 1  The bigger your data set is  the larger number of iterations should be  The recommended number of iterations ranges from several hundred to several thousand  Also  the more features you have in your data set  the deeper your decision tree should be  These are very general rules because the bigger your data set is  the more features you have  the more complex model you can build without overfitting  Okay  it is a very general recipe  Here is the summary of this lesson  You have learned the gradient boosted decision trees algorithm  It is a best method for a general purpose classification and regression problems  It automatically handles interactions of features  because in the core  it is based on decision trees  which can combine several features in a single tree  But also  this algorithm is computationally scalable  It can be effectively executed in the distributed environment  for example  in Spark and Owl  So it can be executed at the top of the Spark cluster  But also  this algorithm has a very good predictive power  But unfortunately  the models are not interpretable  The final ensemble effects is very large and cannot be analyzed by a human expert  There is always a tradeoff in machine learning  between predictive power and interpretability  because the more complex and accurate your model is  the harder is the analysis of this model by a human  Do not be afraid of complex and accurate models 
ajdPLgNSAgk,"Spark ML  Decision Trees & Ensembles  Okay  hi guys  let's get started  And this lesson will be about using Spark ML for doing classification and regression with decision trees  and in samples of decision trees  First of all  you are going to create SparkContext and SparkSession  and here it is  Now you are downloading a dataset which is about breast cancer diagnosis  here it is  Let's explore this dataset a little bit  The first column is ID of observation  The second column is the diagnosis  and other columns are features which are comma separated  So these features are results of some analysis and measurements  There are total 569 examples in this dataset  And if the second column is M  it means that it is cancer  If B  means that there is no cancer in a particular woman  Okay  so let's get started with the dataset  First of all you need to transform the label  which is either M or B from the second column  You should transform it from a string to a number  You use a StringIndexer object for this purpose  and first of all you need to load all these datasets  Then you create a Spark DataFrame  which is stored in the distributed manner on our cluster  here it is  inputDF DataFrame has two columns  label and features  Okay  you use an object vector for creating a vector column in this dataset  and then you can do string indexing  So Spark now enumerates all the possible labels in a string form  and transforms them to the label indexes  And now label M is equivalent 1  and B label is equivalent 0  You can start doing the machine learning right now  First of all  you make training test splitting in the proportion 70% to 30%  And the first model you are going to evaluate is one single decision tree  You are making import DecisionTreeClassifier object  You create a class which is responsible for training  and you call the method fit to the training data  and obtain a decision tree model  So the training was quite fast  and here are some results  Number of nodes and depths of decision tree  feature importances  total number of features used in this decision tree  and so on  We can even visualize this decision tree and explore it  and here is a structure of this decision tree  Here are splitting conditions If and Else  which predict values and leaves of our decision trees  Here are the predictions  Okay  now you are applying a decision tree model to the test data  and obtain predictions  And you can explore these predictions  The predictions are in the last column  And in this particular case  our model always predicts zero class  Now we can evaluate the accuracy of our model  For this purpose  you use a MulticlassClassificationEvaluator with a metric named accuracy  The testing error is 3%  the model is quite accurate  Okay  the next method is Gradient Boosted Decision Trees classifier  First of all you import it  you create an object which will do this classification  I want to emphasize here that you are specifying labelCol = \""labelIndexed\""  and featuresCol = \""features\""  Actually  it is not mandatory to specify feature column  because its default name is features  You can do it either with this argument or without this argument  You are going to do 100 iterations  and the default stepSize is 0 1  Okay  you are starting this process  and it takes some time  Okay  the model is ready  and you can explore featureImportances  You can even visualize the sample of decision trees  and it is quite large and long  It is not interpretable by a human expert  Now you are doing predictions at testing data  And finally  we evaluate the accuracy  In this case  accuracy is a bit lower than the one of the single decision tree  Now let's try the random forest  You are importing the classes which are required for evaluating random forest  You are creating an object  and you are fitting this method  And here are featureImportances  and here is an example  Again  it is quite large  Here are the quality of our model predictions and testing accuracy  You can see in this example  testing accuracy of random forest was the best  But with the other dataset  the situation may be quite different  And as a general rule  the bigger your dataset is  the more features it has  then the quality of complex algorithms like gradient boosted decision trees or random forest will be better "
03fhijH6e2w,Spark ML  Cross-validation  Okay  Now  I will show you how to do a very important algorithm which is called Cross-validation in Spark ML  Cross-validation helps you to assess the quality of a machine learning model and to find the best model among a family of models  First of all  you start SparkContext and you use the same dataset which you use for evaluating different decision trees  So  I will skip the cells because it is exactly what you did before  Okay  finally you have the dataset which is called inputDF2  I will show its contents  What steps are required for doing cross-validation with this dataset? For example  you want to select the best parameters of a single decision tree  You create an object decision tree then you should create a pipeline  Pipeline  in general  may contain many stages including feature pre-processing  string indexing  and machine learning  and so on  But in this case  pipeline contains only one step  this training of decision tree  Then you import their cross-validator and ParamGridBuilder class and you are creating a ParamGridBuilder class  For example  you want to select the best maximum depths of a decision tree in the range from 1-8  Okay  you have now the ParamGridBuilder class  you create an evaluator so you want to select the model which has the best accuracy among others  You create a cross-validator class and pass a pipeline into this class  a ParamGrid and evaluator  And finally  you select the number of folds and I think that the number of folds should not be less than 5 or 10  Okay  you create cvModel and it takes some time because Spark needs to make training and evaluating the quality 10 times  we skip it  Okay  finally this cell is finished  You can see the average accuracy  amount of folds  for each failure of decision tree depths  The first stage of our pipeline was a decision tree and you can get the best model and the best model has depth 6 and it has 47 nodes  Then you can view this model to make predictions at any other dataset  I want to show one more thing here  In ParamGridBuilder  you can use several parameters  For example  maximum depths and some other parameters of a decision tree  for example  minInstancesPerNode and select some other grid here  but in this simple example  you did not do it for the simplicity  And of course  if you evaluate only one parameter  the training is much faster 
Xk-MTgB68rQ,Recommender Systems  Introduction  Part I  Hi there  In the previous weeks  you have learned how to predict future and how to estimate the quality of your predictions  Feel free to throw your glass ball away  because it's done with the help of big data  It is time to build your own big data product  Hey  we have a question from the audience  No  buddy  you don't need the magic wand anymore  turn it back to Hogwarts  I personally have been working in the field of recommender system developments for several years at and  To be honest  I'm really fond of recommender systems or RS for short and let me share my passion with you  In the up coming videos  I will give you a brief overview of recommender systems and then you will build your own scalable recommender system  It's time to do fancy things  let's go  The first question  what is a recommender system? If you read news on the Internet then you have definitely seen personalized news recommendations  When you buy items online  you usually see the recommended items  The most frequent recommendation in e-commerce websites is with item A  people usually buy X  Y and Z  Movie recommender system is another popular example  You can rate movies with likes and dislikes  or can provide a more granular feedback in the form of n star rating system  The system will automatically find out what you like most  It can be a movie genre  a favorite actor or actress  film location and so on  You might even not understand why you like these or those movie  but recommender system will generate hypothesis with high probability of trustworthiness  Music recommender system makes your running exercise more enjoyable  It makes your day more active or passive and even can change your mood  It depends on the type of music you listen  active  sad or dancing  I think each of you remember how a song intensifies your day-to-day activities  Works like magic  right? Imagine what can a well crafted playlist do  practically everything but for doing the dishes  And it is achievable with the help of recommender systems  There are too many different domains  Most common of them you see on the screen  And to make your life even more complicated  there are cross domain recommender systems  Nowadays one e-commerce website can sell books  movies  clothes  and even furniture  I speak about big e-commerce aggregators  What is interesting  recommender system can build a user model based on users browsing history  The history of his or her clicks and purchases in the movie domain  and then apply these findings to the users behavior in another domain  for example  books  People usually define several categories of recommender systems which you see on the screen  Non-personalized recommender systems are the default choice when you don't have any information about the user  It is usually called top items  most popular items or trending items over some period of time such as a day  week or overall  There are recommender systems which use content based algorithms  Items which you buy  books which you read  music which you listen to  all have attributes which identify them  For example  a book has authors  a publisher  a year of publication  a title  contents and so on  Based on this information  you can build a classification or regression model to predict users like or dislike of the arbitrary book item  Content based algorithms recommend items that are similar to those the user liked in the past  If you don't have user history  you still can benefit from content based algorithms  A user can provide some information about his or her preferences such as  favorite movie genre  Therefore  you will be able to match his or her interest with the relevant items  Neighborhood models cover the second class of recommender systems  It is a well know paradigm  people who buy A item also buy B item  There is a common problem with e-commerce websites following this paradigm  If you have the top selling item such as bananas in the grocery store  then recommender system is tucked into bananas problem  Whatever you buy  the recommender systems suggested to buy bananas in addition  In a movie domain it gets another name  Harry Potter problem  As you can understand the reasons are the same  It is a minor problem and we will discuss how to solve it in future videos  The main point of neighborhood models can be easily described in the following way  The recommender system will find a person who's behavior is similar to yours and then suggest you the items highly rated by another person  Of course  it take into consideration the items which you are already aware of and will not suggest you to buy an item which you have already bought  And breathtaking collaborative filtering algorithms  why are they so breathtaking? Simply because if you had known them in 2009  that you could have had a real chance to win $1 million  It is one more reason to invest in your education and to be precisely correct  neighborhood models are representatives of a wider family of collaborative filtering  The main idea of collaborative filtering algorithms is that people with similar tastes behave the same way  CF algorithms construct a latent user and item representations  We will talk more about CF algorithms in the upcoming lessons  So stay tuned  Knowledge based algorithms play an important role in high risk domains and domains with limited information  The good example is realty business  On average  a person doesn't usually buy a house frequently  Most of your service customers can have no buying history at all  so how do you identify which house to suggest? The only way you can do is bring in users domain knowledge to build a model  The recommender system can provide an interface to specify constraints such as number of bedrooms in the house  Then each page is constructed in a way to help users navigate their relevant content  For example  if you're looking for a house or an apartment in the city center  then the system shouldn't recommend you houses in the suburbs  But if you're looking for a house in the suburbs  in the west  then it is possible that you can see the houses in the suburbs in the South  It is exactly human domain knowledge built into the system  Each type of recommender system has its own strength and weaknesses  In real life  people use hybrid recommender systems which gives you power to combine the best functionality from all of them  In the slide  you see a common choice for different domains  Where you have several types of algorithms listed for one domain  hybrid recommender system will be a natural choice 
myVbSSyZtr4,Recommender Systems  Introduction  Part II  The question of Hybrid Recommender system quality evaluation is complicated  I can see the Recommender systems from rate and production perspective  If you have a five star rating system  where zero stars means the worst possible experience and five stars means the best possible experience ever  Then  some people can argue that it doesn't make sense to have the same prediction of resolution between zero and one stars and between four and five stars  People are usually more interested in highly rated items  Moreover  it may not be so important to predict the exact rating well  but the order of items suggested to you is of great help  In Recommender systems  this task is called top and recommendation where the previous task is called Rating prediction  Top and recommendation task from Recommender's system is related to more general concept learning to rank  in the field of information retrieval  And there are a number of different metrics from the field of information retrieval such as mean reciprocal rate MRR  discounted cumulative gain DCG and so on  To cap it all  it is not that simple in real life  Recommendation algorithms will not be working without extra tricks  When you're looking for a new movie to watch  do you want to see the sequels of the same movie in top five recommended items? I really doubt it  We have three dimensions  which we should carefully consider while doing cross validation  They are users  items and time  The last component has a dramatic influence on the quality of recommendations and offline metrics  It is really easy to build a good Recommender algorithm when you know in the future  but the over optimized algorithm will have a drastic negative outcome when published online  In addition to complex cross-validation procedure  we would like to simultaneously optimize several other metrics specific to recommender systems  For example  diversity is a measure which helps us to control diversity in the output to prevent the aforementioned recommendation of sequels  I am going to highlight a few challenges to close the loop on Recommender system basic concept  Their first well-known challenge  is the cold start problem  What should you do to provide recommendations to a users you haven't seen before? Tell fortunes or who would like to construe a new item without any existing feedback  shopping addicts? For instance  the last problem can be solved with the help of the content based Recommender algorithms  As soon as you have an item description  The correlation between online and offline methods is one of the hardest challenges  So a proper off line evaluations and user case studies are essential before online experiments  Another common problem  is an interface design  It doesn't matter how good your recommender system is  if you are not able to visualize or position your recommendations  It includes explanations to increase users trust in your system  It also includes your system's response time to update recommendations with regard to their recent user's activities  Exploration exploitation dilemma well know in reinforcement learning base its own place in Recommender systems  Do you want to satisfy your user needs just now? Or do you want to explore your item database and user status more deeply? A brief summary of what is awaiting for you in the upcoming lessons  You will build components of personalized Recommender systems  by implementing content based and collaborative filtering algorithms  You will learn how to evaluate Recommender systems  You will learn how to overcome the cold start problem and other challenges and everything is on a big scale 
LiSEdc9RpLU,Non-Personalized Recommender Systems  Hello  The subject of this lesson is Non-Personalized Recommender Systems  Since now  I will give you only the basic implementations  All the optimization is left for you as an assignment  And I bet you are already comfortable with it as you have elaborated all the necessary skills over the courses over this specialization  Even if it sounds silly  there are a number of obstacles on the way of implementation of a non-personalized recommender system  As an example  I took publicly available for educational purposes movie data set called MovieLens  Without much of hesitation  let me build the top rated movies recommendation list  We have here rating csv which contains a comma separated list of userId  movieId  rating  and timestamp  Rating is a value between zero and five  The higher the better  Movies csv which contains a comma separated list of movieId  title  and genres  We can easily find out the average score by aggregating by movieId  Here is the snippet of the spark code to do the calculation  You write ratings csv  skip the header column  pass lines into rating data structures  Extract movieId and rating value  group by movieID and finally calculate the average rating  When they code these codes several times  you will see different output each time  Moreover  far from all the items you see on the screen are really popular  Any ideas why? The reason is that some movies have only very few rating  By very few I mean one  The Damped Means technique is a solution to this problem  From a mathematical point of view  previously we had the following formula for the movie score  The nominator is a sum of all the ratings for a movie  The denominator is a quantity of ratings for the movie  The Damped Means' assumption is that each movie was rated by K hidden users with the value Âµ  From the mathematical point of view  it looks the way you see on the screen  In simple words  if you have only a few ratings for a movie then movie errors rating should be close to the data set average rating  the value Âµ  The more ratings per movie you have  the more confident you are about the movie average rating value provided by users  Let us fix our known personalized recommender with the help of Damped Means  You see? Much better output  and it is now stable  When people see the same popular items recommendation every day is becoming boring and the current situation becomes tense but as a good recommender service provider  you can take into consideration temporal effects to get the most popular items for the last week  for the last months  for the previous year and so on  MovieLens data set provides a timestamp for each rating event  So you can easily do it with as part fielder statement  Trending recommendations can be gathered with the help of mathematical sugar called exponential decay  From user's point of view ratings you provided in the past are becoming obsolete over time  In other words  all ratings may not reflect correctly the current user preferences  If we go to extremes  try to remember yourself at the age of five or 10 years  Do you still like all the movies from your childhood? Do you really remember them in all the details? Compare your feelings and memories about movies that you have seen today  with those you saw one a week ago  one month ago or a year ago and so on  Compared to the old one  recent experience has way more influence on you and your memories  You can experiment with a decay factor to see the difference in trending movie recommendations  As a side note  here we expect the data set to be steady but in reality you just provide feedback every day  every hour  and every minute  You can build your recommender service to consume these data simultaneously and automatically update recommendations  How to work with streaming data and what frameworks are available for you  you will learn in the next course  There will be a lot of practical exercises  Get ready  Moving forward with non-personalized recommendations  I bet you know a lot of examples of websites which provide recommendations in the following way  People who like A item also like B item  or customers who bought A item also bought B item  Let me reveal the secret how it works  In the literature it is known as basket recommendations or market basket analysis  First  we need to create baskets of light items  For example  movies for each user  In MovieLens data set we can binarize ratings into two categories  light items  heavy rating value bigger or equal to four  and all other items  The value of four is chosen empirically and it is not always the best choice  You can experiment with this value to see how recommendations change  So you have baskets  You choose an arbitrary item A and your task is to generate the recommendation in the form  People who like an item A also like an item B  Any ideas? And of course naive implementation that naturally comes into mind  will not work  Do you remember the story about bananas from the previous video? It is exactly what I'm talking about  If you take a look at each item B and count how many times it occurs in the same basket with A  then likely you get just the most popular items and disregard the information about A  There are several approaches to overcome the issue  First  you can adjust the value by overall popularity of B  This metric that you see on the screen is called the lift metric  Second  you can use the following metric which means that having an item A in the basket makes an item B more likeable compared to the baskets where we don't have the item A  This type of recommendations requires you to calculate a current metrics  The definition of the occurrence metric is pretty straightforward  It is a square matrix where both rows and columns correspond to items in the same order  Row-column intersection represents the number of times the corresponding pair of items occur or could occur together  For example  it can be the number of times the approaches together  If you have thousands of items then you will be able to do it within a second  But if you have millions of items then it is becoming a real challenge  Millions of items is not something difficult to imagine  I personally worked as a senior software development engineer for the biggest e-commerce website in Russia in 2014  and we had 60 million of distinct items  How to make these calculations efficient is a job for a data engineer  IT companies require your knowledge and your skills to complete these kinds of tasks  And I am sure that you are already confident to do it by yourself  Summary of this video  You can build a non-personalized recommender system and adjust it for rare occur items with the help of Damped Means  You can build a non-personalized recommender system taken into consideration temporal effect with the help of exponential decay  You can explain the concept of basket recommendations and how to overcome bananas problem  Do you remember with metrics for it? Though non-personalized recommender system sounds easy for big e-commerce website you need to think about efficient calculations  More on this is waiting for you in the assignments section 
fJaPlsVIzio,Content-Based Recommender Systems  Hi  let me walk you through the process of building a content based recommender system  Watch you steps  the work is still in progress  With the help of content based approaches  you will be able to generate quite good recommendations  So it is something that you will be able to share with your friends and to use personally  It's up to you how much you want to share  As I have already told you during the introductory video  content is not always a textual representation of items such as a movie plot  title and description  Content in this environment means fixed size feature space for an item  If an item is a book then it can have attributes such as book's author and publisher  If an item is a movie  then the list of attributes will likely include the movie director  film location  and budget  To build a content based recommender system  we need to answer three question  How to generate Items' representation  How to build users' representation in the same feature space  And how to measure the distance between a user and an item  Let us discuss these questions from top to bottom  If you have a numerical or categorical item feature then you don't need to do anything with it  except maybe one [INAUDIBLE] coding or a similar trick  use it as it is  If you have a textual item description or users' feedback  then you can take a look at the back of model  Do you remember TFIDF and from the previous lessons? Yes you do  but there is a better way in case you have a lot of textual information  Multimedia data processing  including text data processing  is where deep learning embedded tool kits show their best advantage  Spark provides Word2Vec algorithm implementation to generate fixed size vectors for words in a text corpus  Doc2Vec or Paragraph2Vec algorithm is a bit more complicated  Never the less  the relevant issue is still open at the time of shooting this video  We can use the following simple approximation  You're either trained or used already pre-trained Word2Vec model to generate vixed size vectors for each word  And then you aggregate words for each item with for example  idea the weight  As a side note  to outperform the existing pre-trained Word2Vec models you might have to collect a huge dataset and spend a lot of resources to optimize the model  For example the Google news dataset consists of 100 billion words  Do you have a bigger collection? Then the next question is user's representation  In a standalone content-based model  user is just an aggregate of items he or she has rated  Aggregation can have different forms  You can simply average of all the available item ratings  You can use ratings as weights and additionally scale these ratings  Ratings could have time stamps  So you can decay item ratings the way I described to you in non-personalized recommender system video  In real life  you usually start with the simplest implementation  And then tune your approach according to your service requirements  The benefit of a content based approach is that you can easily explain user's vector unless you use complex embeddings to generate feature space  For example  if you have coordinates representing general action then users with high values in this coordinate like action movies  More than users with low values in this field  Vice versa  you can have users' representation in some feature space and build item representation by aggregating user feature vectors  You can also have mixed user and item feature space and build the so called model based recommender systems  but it goes beyond of our discussion  The last and the most interesting question is how to measure the distance between a user and an item  This distance  or similarity  influences a rating prediction for item i by user u  The whole set of distance metrics is available for your experiments  My personal point of view is that Python Scikit library which you've seen in the first week of this course  provides a really good classification  There are metrics intended for real-value vector spaces such as Euclidean  Manhattan and Chebyshev distances  Metrics intended for integer-value vector spaces  For instance  Hamming distance which counts the number of known equal vector components  Metrics intended for boolean-valued vector spaces  my favorite are dice and Jaccard distances  Of course  if you are not able to find something that suits your needs you can implement your old distance metric for your application  Overall you can use the following diagram as a reference to make sure that you forget nothing important while building a content based reccommender system  What are the benefits and drawbacks of this approach? Compared to non personalized recommender systems  these types of recommendations is already personal  If you have user or item representation  then you overcome the cold start problem  But please bear in mind  if you have an item description and build a user's representation by averaging item feature vectors  then you only overcome the item cold start problem  If you have a new user then you don't have annual ratings  Therefore you are not able to average anything and build a user's representation  If you don't use complex embeddings  then by looking at a user's profile  you can describe his or her interests  Content based recommender system cannot easily tackle interdependencies  Imagine you have several attributes such as movie language is English  movie language is A  which is your native language  Movie genre is cartoons and movie genre is documentary  If you have started to learn the English language you can like watch cartoons in the English language  You're also a fan of documentary movies  But due to language complexity you prefer to watch them in your native language  You have high scores for all of these attributes  And this stand alone content-based recommender system will recommend to you cartoons in language A  and documentary in the English language  It is not something you expect  There are several ways to deal with these kind of problems  The first approach is to use more complex matrix which account for interdependencies  But your future space can expand quadratically  The second and more generic approach  is to use collaborative filtering algorithms  How to build them  watch the next videos  You are warmly welcome  And don't forget to provide your feedback to make this videos better  Overall in this video you have learned how to build users representation in items feature space or vice versa  What distance metrics are available for different scenarios  And how you can reason about benefits and drawbacks of the content based recommender system and analyze it if it is appropriate for your use case 
CnCFj4o_e9Q,Recommender System Evaluation  Hello friends  from the previous videos  you got knowledge in the field of recommender systems  In particular  what types of recommender systems exist? Where you can apply them  and how to construct non-personalized and quantum-based algorithms  We were talking a little bit on the subject of Recommender System comparison  Is one Recommender System better than another? How do you prove it? Who has more questions? It is exactly the subject of this video  I'm going to explain to you how to evaluate Recommender Systems  When you build recommender service you have some objectives in mind  For example  you would like to positively surprise your users with interesting recommendations  Or you may want to always recommend relevant new items  To be more precise  if you are a big data engineer then you will likely want to know about new books about Spark as soon as they are published  Even more general  you would like your users to generate their revenue  or to spend as much time as possible on your website  and so on  This is called a long-term objective  Take a look at this slide for the example of the metric that you can use to track customer lifetime value  Here  GC stands for Gross Contribution per customer  d is a discount rate per basic period of time you evaluate lifetime value  and r is a retention rate  Retention rate is the percentage of customers who continue to use your service after a fixed period of time  Okay  You have a long-term objective  How does it help to choose between several recommended system algorithms? You can have two versions of your service and compare them over a period of time  I hope that you remember from the first week of this course what AB testing stands for  Because it will help you to compare algorithms online  If you count monthly lifetime value  then you will spend the whole month to compare your algorithms  Compare it to the pace of technology change nowadays  it is like writing websites on pure HTML language  I bet you know better ways of time killing  That is why people usually use short-term process  For example  you can use click through rate method  or CTR for short  You can count how many times customers saw an item and how many times they clicked on it  It is called a short-term process because first  it is supposed to have a high correlation with the long term objective  LTV in our case  The higher the value of CTR  the more likely a customer enjoys your service and will be using it in the future  But bear in mind that optimization of a short term objective doesn't necessarily lead to optimization of a long-term objective  For instance  if you recommend books with nice kitties on the cover  it can increase the value of CTR  But it is not something that can be relevant for your target audience in the long term  And second  you can compare your algorithms way more faster  Experimenting on real service customers is risky  Consider a music streaming recommender service  If a user likes classical music and this service suggests rock  pop  or dance music for quite a while  then you risk to lose a customer  Saves your time and efforts and attract good customers  So running an experiment is not for free  How can you limit or overcome probable negative impressions? The answer is  use your studies  You setup an experimental service  Carefully prepare a questionnaire and the least of hypotheses you would like to validate  Then bring your target audience in a room virtually or physically and run your experiments  If you manage to do it physically  drop me a line how you did it  The experiments at top has to be as close to the real world conditions as possible  After you have made a decision  the chosen algorithm has to be validated online  But is there even a faster way to run experiments? Yes  you can compare the whole set of different algorithms offline  You can not get user's feedback on real time but you can use historical events to design experiments  The choice of offline metrics has a dramatic influence on the algorithms chosen for user case studies and online experiments  That is why it is important to find an appropriate offline metric which will have high correlation with online experiments  This question was highlighted by several speakers on the last ACF RecSys conference in 2016  In my personal working experience  we even built offline metrics by the correlation optimization during experiments for some recommender services  The field of offline metrics is well studied in academia  Therefore  before inventing your own metric  please look through the available literature  Time to go back to the math and other black magic  Get on your broomsticks  let's consider offline prediction metrics such as MAE and RMSE  The idea behind these metrics is that it is important to estimate users' preference with high precision and confidence  Given a user item metrics with actual ratings and predictions  there are several ways to measure these metrics  The obvious one is to use global metrics  for example GMAE  You count the difference between rating and prediction  and average over all the available values  The other one  which is also widely used in production  is CMAE  Which stands for Customer MAE  You calculate MAE per each customer  and only then average those values  This version of metrics try to account for diversity in the amount of feedback provided by different users  Similar to CMAE  you can calculate item-oriented MAE  But I personally haven't seen this variant of metric being used a lot  Let us imagine that we figure out what metrics is the most appropriate for us  and how to calculate it  Given a user item metrics  how do you run cross validation? The first approach  you can assign each user item right into a cross validation split at random  In this case  you don't need to think about anything  But it is not as precise in generalization to unseen data  as other cross validation approaches  The second approach  You can split data set by users or items  Remember CMIE  here we go  In the validation split  a part of user's ratings  also called query  is used to build a user's profile if necessary  Other ratings in validation split are used to evaluate the quality of our commander algorithm  The third approach is the most precise one  The problem with dividing datasets into train query and test sets at random is that we have intentionally used the information from the future  How does that happen? In the introductory video I told you that a user item metrics has three dimensions  The third dimension is time  All the events from our service can be ordered by time stamps  We split our data set by the time  Train algorithms on all the available data before this time  And evaluate them on the data after it  From the efficiency perspective  it has become important for algorithms to be incremental instead of being trained or also called retrained from scratch on every cross validation iteration  See Wikipedia to get more information about incremental learning  As you could have already guessed  we are not going to cover all the peculiarities of the recommender system evaluation  There are books and online courses dedicated to this topic  For the sake of simplicity  I provide you with a few examples of the metrics that we are going to use in assignments  Mean absolute error  and root mean squared error  Please bear in mind that these methods I would further rate in production task  But  are not so relevant for the top and recommendation task  The reason is simple  You can get the optimum value in terms of RMSE  which doesn't correspond to the optimum in terms of MRR and DCG-like methods  Summary of this video is the following  First  you can reason about usage and correlation of user studies offline and online experiments  You can explain where and when to use them  And you can explain why it is so important to properly choose offline metric  And second  you can list three ways to run offline experiments and should be able to write PiSpark code to evaluate the quality of the recommended algorithm on MIE and RMSE for each of these scenarios  Enjoy your lessons  and see you soon 
QnyLvrVbpTE,Collaborative Filtering RecSys  User-User and Item-Item  Hi there  time to discuss more advanced topics in the field of Recommender Systems  In the upcoming videos  we will learn about the neighborhood models  matrix dimensionality reduction  SVD-like algorithms  different types of user's feedback  and iterative procedures to optimize a recommendation model  In short  I'm going to teach how to build collaborative filtering recommender systems  In the neighborhood models  we are looking for similar users or items to make a prediction for an unknown user item preference  Disregarding the type a neighborhood model  user-user or item-item  there are three main components  Normalization  similarity measure  and neighborhood selection  It is easy to explain why we need to normalize users' ratings  Consider two users  One is optimistic by nature as myself and the other is pessimistic  Even if they have the same tastes  they provide feedback on a different scale  An optimistic user provides a rating 2 for the movies which he or she doesn't really like  While a pessimistic user or critic will provide a rating value 1  On the other hand  an optimistic person provides a rating of 5 for all the movies he or she likes  While a critic provides 5 only to outstanding movies  and good movies will get score 4  So in reality  the question of an item likability is more the question of deviation from the mean  That is why it is common to subtract the mean rating value for each user during the pre-processing step  Moreover  it is also common to normalize user scores to have a standard deviation of 1  Therefore  all our users' ratings still be on the same scale  To be mathematically correct  it is called Z-score  Transforming the scale back is pretty straightforward  You multiply the score by the standard deviation and then shift back to the mean user's rating value  The next component is a similarity measure  All the metrics that you saw in the previous lesson when we worked on content-based recommender systems can be applied here as well  Common choice for a similarity measure in neighborhood models is a correlation metric  As usual  life is not as easy as it sounds  There are different ways to calculate correlation or correlation like metric  Which one is most appropriate for your service  you'll be able to find out only during experiments  When we calculate cosine similarity  we use the following formula  If we use mean normalized ratings then the actual formula will look a bit more complicated  In comparison  take a look at the formula for Pearson correlation and try to spot the difference  I think you gotta try it  In Pearson correlation  you only use the items that were rated by both users  You don't try to extrapolate your knowledge to the unknown ratings  Where a cosine similarity expects it to be close to the mean value  Another example is Spearman's rank correlation  It doesn't take into consideration the actual rating value at all  It just sorts all the ratings per user  and assign them the ranks starting from one  and then uses the Pearson correlation formula  Of course  it is more expensive from the computational point of view  As you need to have one extra step to sort values for each user  Is it worth it? Depends on your service and data  There are even more different variations of correlation which take into consideration  support  number of items written by a user or by two of them  Bayesian damps  see damping means in non-personalized recommender systems  And other forms of shrinkage  Intellectually curious  already know where to go  And we are moving on  The last question we need to answer to build neighborhood-based collaborative filtering recommender system is how to choose friends  Neighbors  how to choose neighbors  You can say the more data  the better  but it's not the case  There will be more noise than information  How does help to build a recommendation list for you? On the one hand  you can filter top and most similar neighbors for each user  You can even do it during the cost effective preprocessing step  Therefore  during the prediction or recommendation request  we will be able to find the answer quickly  On the other hand  you can filter neighbors by the similarity threshold  For example  you can take into account only the neighbors with correlation bigger than 0 75  Even better  you can mix these approaches with the previous one  During the step where you select neighbors  you disregard everything below similarity threshold  and then filter the top and most similar neighbors  There is another interesting question  Do we need to take into account the users we have a negative correlation with? For example  you like comedies and your neighbor dislikes that  your neighbor like thrillers but you don't  and so on  In literature  there are a lot of examples where people exclude users with negative correlation from a neighborhood to improve prediction quality  So this technique also can be considered as a part of the strategy  A strategy number four  choosing a random subset of neighbors is not a joke  If you have a dense user item matrix then the choice of a user neighborhood is not so important  A rule to get prediction in user-user collaborative filtering model is the following  It is the weighted average of rating values from the user neighborhood where weights are the similarities between users  It is important to mention that this neighborhood is built of users who rated the item i  Therefore  for each item this neighborhood can be right  And consequently  if you use top and filtering to build a neighborhood during the preprocessing step  then the item neighborhood will be a subset of precalculated neighbors  So we need to make sure to choose big enough N  On the other side  there is an item-item collaborative filtering  It is a common choice for high load services  The reason is that usually item's neighborhood changes much slower compared to user's neighborhood  You calculate item similarity once and use it for ages  Another benefit of item-item collaborative filtering is that it is much easier to explain recommendations to the end user  The benefit of user-user recommendation system is that it provides more serendipitous recommendations in comparison to the item-item algorithm  The cornerstone of recommender system is a cold-start problem  When you have the only one rating for a user  and especially if it is a negative score  then standalone neighborhood collaborative filtering models will not help you to find good recommendations  But a variety of decomposition algorithms which build a latent space are here to help  My colleague  Evgeny Frolov  showed how to benefit even from a single negative user feedback with the help of tensor decomposition during the recent ACM RecSys conference in 2016  I'm glad Evgeny accepted the offer to work with me on these videos and assignments  so you will see him shortly  Of course  we are not going to show you how to work with tensors on Spark  But simple matrix factorization approaches will be covered during the lessons  Nevertheless  all this machine learning is helpful  it might not be of a great interest for you as a big data engineer  Let us take a look at the complexity of the models  So you will be able to help data scientists in your team to make a better choice of a recommender system algorithm from the efficiency point of view  Let me define n as a quantity of users  and m as a quantity of items  When you use user-user collaborative filtering  you can assume n squared space size for all pairwise similarities  Correspondantly  item-item CF will consume m squared space size  During a prediction phase  both of these models consumes the same amount of resources  User-item prediction is a weighted average of a k neighbors  which can be either users or items  Preprocessing step to calculate all pairwise similarities will be n squared multiplied by p in user-user CF  Where p is the maximum number of ratings provided by one user  And m squared multiplied by q in item-item CF  As you can see  during the system exploitation  you will use the same amount of computational resources  But the space consumed and the complexity of the preprocessing step can change dramatically when you switch from one algorithm to another  If you ask how the complexity of these algorithms changes  you can provide the following diagram  In one column  you see the average number of neighbors with at least one rated item or rating user in common  In the other  how many ratings I used on average to calculate pairwise similarity  Based on these numbers  you can make a deliberate choice  Summing up  after watching this video  you now know how to build user-user and item-item recommender systems  And you can list three components to make it working  You also can reason about the choice of a recommender system  taking into consideration accuracy-efficiency trade off 
hfmK6Oc0QlU,RecSys  SVD I  Hi there  Now  as you understand how basic collaborative filtering models work  I hope you are ready to move to more advance topics  In the following few lessons  you are going to learn about some of the state of the art dimensionality reduction algorithms using many real life applications  But first  let's look why would we need them at all? You have already seen a few examples of simple item-based and user-based collaborative filtering models  These models are easy to implement  allow for intuitive interpretation of recommendations  and provide good accuracy  This also makes them a good baseline for evaluation of other recommendation models  Maybe you want to add that another benefit of the neighborhood-based models is the ability to generate new recommendations almost instantly  However  as we will see soon  it's not the unique property of these models  Finally  there are several challenges which you might face when working with these models  First is scalability  In the worst case  the complexity of the algorithms  both in terms of the required storage and module precomputation time  depends quadratically on the number of users or items  However  in many real applications the complexity can be significantly decreased due to data sparsity  And it can be improved even further with additional heuristics and incremental techniques  On the other hand  sparse data may lead to another kind of problems known as limited coverage  For example  if there are too few items rated by users in common  the correlation measure between these users becomes unreliable  More than that  even if items with close characteristics are consumed by like-minded users but never actually intersect  then these items will never be recommended together  This results in a weak generalization of the methods  Which finally leads us to our new topic  dimensionality reduction  that to some extent helps to mitigate the problems I've described  Dimensionality reduction evolves to describe any user preferences and any item characteristics in terms of a small set of hidden parameters  also called latent features  Along with a compact representation  it also helps to uncover non-trivial patterns within data and use them to generate meaningful recommendations  There are various ways to perform this task  such as neural networks  mark-of-decision processes  latent digital allocation  and some other algorithms  We will focus primarily on a matrix factorization approach  It's one of the most popular in the field of recommender systems  As you have already seen  interactions between users and items can be represented by the so-called utility matrix  The type of interactions doesn't matter for now  For example  in the case of an explicit feedback  it can be the rating value assigned by a user  In the implicit case  it can be simply a binary value representing the fact that the user has somehow interacted with an item  Search for it  view this page  purchased  and so on  The goal of a matrix factorization task is to approximately represent sparse utility matrix in the form of a product of two other matrices  P and Q  These matrices are typically dense  Each row Pi  of the matrix P  would reflect preferences of user i described in terms of some latent features  Similarly  each row Qj of the matrix Q describes the association of an item j with those latent features  Vectors Pi and Qj are also called an embedding of users and items onto the latent features space  The size of the second dimension of this matrix is P and Q  a response to the number of latent features denoted as r  This number is typically much smaller than the number of items or users  Such a representation of a matrix as a product of two other matrices of smaller sizes is also called a low rank  or rank r approximation  From here  the utility of any item j for any user i can be estimated simply by a scalar product between the latent representations according to the matrix multiplication rule  In many cases  you will be interested not in the exact prediction of a utility but rather in a correct ranking of a list of top and recommended items  The simplest way to do this is to just sort your recommendations according to the predicted score  So  what are those r latent features? Here's an oversimplified but useful example based of movie genres  Imagine that every user's taste is perfectly described by a combination of some genre preferences  Likewise  every movie can be decomposed into a mix of distinct genres  In this perfect scenario  each latent feature of the movie would just correspond to a particular movie genre  In other words  every item vector Qj simply describes the level of association of each genre with the item  In a similar fashion  the user vector Pi would represent the level of interest of user i in every genre  In the real world  human behavior is  of course  much more complicated  And the learned latent representation will not necessarily correspond to any real conceivable features  This  however  is not a problem  as it doesn't prevent us from making predictions  And you can infer relations between any user item pair including previously unobserved ones  just by the virtue of this scalar product  Also note that once you compute your matrix factorization model  you can apply some of the neighborhood-based algorithms in the new  lower dimensional latent feature space  instead of the original latent space  This can often produce more meaningful and reliable results comparing to the straight forward approach  You can also apply some clustering techniques there  or even feed your latent feature vectors into another machine learning algorithm  Before we proceed  let me summarize what you have learned so far  You can now explain the advantages of dimensionality reduction approach over the standard neighborhood-based models  You are now familiar with the concept of latent features and have some intuition behind it  You have seen one particular example of a dimensionality reduction technique called matrix factorization  And you know that it is even possible to build a neighborhood-based model on top of a latent presentation 
tnOOiSjTI8I,RecSys  SVD II  All right  So now  when you understand what matrix factorization is  the next question is how to actually compute it  There are various ways to decompose a matrix into a product of several other matrices  and we will cover only some of them in the course  We will start from one of the most famous matrix decomposition methods  namely singular value decomposition  or SVD for short  SVD has long history in information retrieval  Latent Semantic Indexing  LSI  Latent Semantic Analysis  LSA  Principle Component Analysis  PCA  all of these methods have a straightforward connection to SVD  Here is a very important fact from linear algebra  Any real matrix A  can be exactly represented in the form of a product of three matrices  U  sigma and V transpose  Matrices U and V are orthogonal  which means that their columns called left and right singular vectors are also orthogonal and have unit length  Or equivalently  the product of the transpose of the matrix with itself gives an identity matrix  This property is very handy  and soon you will see why  In turn  sigma is a diagonal matrix  Its diagonal elements are called singular values  and by convention are sorted in the descending order  Now  you know how singular value decomposition looks like  Recall that you actually do not aim to ideally reconstruct the matrix A  otherwise  our model would simply fail to generalize  As we have previously discussed  what you aim for is to find just a good enough low-rank approximation that will help us to generate recommendations  For these purposes  you can use the so-called truncated SVD  which contains only a small number of the largest singular values  and corresponding singular vectors  This number denoted as r  is called the rank of SVD  The largest singular values ensure that you get the most descriptive latent features  It also turns out that among all the possible solutions  truncated SVD gives the best rank r approximation  in terms of the Frobenius norm  In other words  it gives an optimal solution to the following minimization task  The only problem that I haven't told you about yet  is that SVD is undefined for incomplete matrices like in our case  Luckily  you can do a little trick  Simply impute unknown entries with some predefined values  Possible choices are raw average  golden average  some combination of these  or even simpler just zeros  In the latter case  you get the so-called pure SVD model  Sure  this way the result values are going to be biased towards zero  as you replace approximately 99% of the data or even more with zeros  This is not good for rating prediction  However  it's not a big problem for top rank recommendations task  In fact  in some cases  it works quite well  and can even beat other state of the art methods based on the matrix factorization  or the neighborhood models  Here  you can see such an example computed on the Netflix data set  Different methods were evaluated in two different set ups corresponding to regular recommendations  and recommendations from the long-tail  In both setups  the authors were able to beat all the other methods with pure SVD  by tuning just a single parameter  the rank of decomposition  What a remarkable result! I have to make an important note here  As you can see  there are some other factorization methods having SVD letters in their names  Please do not be confused  Strictly speaking  most of these methods like Funk SVD  SVD++ and other SVD something  are variants of a matrix factorization approach  and have nothing in common with the mathematical formulation of the singular value decomposition  Unlike SVD  these methods do not build a space of singular vectors  and do not compute singular values  Most of them do not preserve the columns orthogonality property  These methods are designed to work with incomplete matrices  often ignoring unknown entries  They form a broader family of methods with different optimization objectives  specifically tuned to provide a better ranking or accurate rating prediction  But due to historical reasons  they are still sometimes called SVD  Speaking about the orthogonality  do you remember I told you that this is a convenient property? And in the introduction  I promised that not only neighborhood based models allow to generate new recommendations very quickly  It's time to connect these two propositions  Once you have computed SVD  it is easy to generate recommendations for the known users  But what if you have a new or unrecognized user  who is not present in the training data  but have already provided at least some ratings? Computing the SVD each time you have a small update in your data would be prohibitively expensive  Likely  you can work this with minimal efforts  Imagine that a new user ratings are encoded into a new row of the matrix A  This row update in A would most likely change all three matrices  U  sigma and V  However  if the number of users is sufficiently large  you can assume that this change is approximately accumulated just in a single row of matrix U  while all the other values stay unchanged  From here  you can approximately find the latent vector q  for the new user  That's where the column orthogonality of the vector matrices plays a crucial role  This approach is known in the literature as folding-in  It allows you to quickly find a new user embedding in the latent feature space  As previously  you can use this latent vector to find closest neighbors and do some other analytics  However  you can go in further  If now you make a reverse separation and try to restore actual matrix values  after a few simple linear algebra operations  you will get the following expression  This formula is very convenient  It states that we can compute our recommendations just by a sequence of two vector multiplications  Even more  if you look carefully  you can see that in the case when our new user vector is equal to a particular row of the original matrix  which means that this user is similar to one of the known users  then the prediction is not approximate but exact  In other words  we can always use this formula for both known and new users and get correct results  Those of you who are familiar with linear algebra  have already recognized that this formula is just a projection of a new user preferences onto the latent feature space  The complexity of this calculation is proportional to a product of the number of items and the number of latent features  which can be used to generate recommendations almost instantly  Note that once you have a lot of new users  it might be necessary to recompute the SVD  in order to build a more accurate model  However  this can be done in a separate offline pipeline  and doesn't affect generation of recommendations  Let me summarize  You are now familiar with the single value decomposition  and features that make it so special  You now know how to use it to build a recommended system based on the pure SVD model  Finally  you've got some intuition behind the folding-in technique  and know how it can be used to generate recommendations almost instantly  without the need to recompute your model too often 
VyeLZQ7cZmI,RecSys  SVD III  We have discussed many details about the SVD  Now  it's time to find out how to actually compute PureSVD model with it  One way would be to find a complete SVD of a dense matrix and then truncate it to a smaller size  This however  is a heavy computational task  impractical even for modest sized datasets  Instead  you can use another technique based on the efficient Lanczos method  The algorithm is iterative  and the only thing it requires is the rule  how to multiply your matrix by an arbitrary dense vector  and this can be done really fast  For example  with the help of efficient sparse matrix formats such as compressed sparse row storage  or CSR  And the overall complexity for this approach is linear in the number of non-zero values of the original matrix and linear with respect to matrix dimensions  And you don't have to write this algorithm yourself  Its highly optimized implementations are available out of the box in many programming languages  like MATLAB or Python  There is an implementation of the truncated SVD in Spark as well  However  its current version doesn't support custom matrix vector multiplication rules  Let me show you why this feature is also important  Recall that in order to deal with the unknown values  we have replaced them all with zeros  You probably might think that this imputation technique is too restrictive and  at least in some cases  it could be more beneficial to use a different strategy  for example  with average values instead of zeros  as I have mentioned previously  One common misconception is that in this case  the utility matrix becomes dense and impractical to work with  In practice  however  this is not a problem at all and you can easily avoid it  thanks again to the Lanczos method  Let's see it on the following example with average user ratings  You have your sparse utility matrix  A  First  let's compute its average values for every row  taking into account only its known elements  This produces a dense vector  Let's call it b  Now  your new matrix with the average values in place of the unknowns can be represented by the sum of two other matrices  The first one is just a dense matrix of the average values repeated across the columns  It can be constructed by another product of your vector  b  with a vector of all ones  denoted by e  The second matrix  let's call it D  has the same sparsity pattern as the matrix A  and it's non-zero values are equal to the difference between the non-zero values of A and the corresponding values of b  as shown in the formula below  When you sum up these two matrices  you get your new matrix A-hat  Now  I hope you remember that for the Lanczos method  all you need to know is how to multiply your matrix by an arbitrary vector b  With a little bit of linear algebra  you can show that your matrix vector product consists of two terms  First one  D multiplied by v  has the same computational complexity as the product of your original matrix A with this vector v  It doesn't add anything new  The second term involves only vector multiplications and its complexity is proportional to the number of items  In a similar way  you can show that the added complexity for the multiplication with the transposed matrix is proportional to the number of users  The overall computational complexity is then increased by a small increment  linear with respect to the dimensions of the original matrix  As you can see  this procedure avoids formation of a dense matrix along the way  and it's almost as efficient as PureSVD with zero imputation strategy  Similar trick can be applied to column averages or whatever combination of row and column averages you like  It is also easy to implement  Here  you can see the code for the previous example written in Python  The left block of the code is just a computation of matrix D and the vector b  On the right hand side  the key part is the matrix vector multiplication  which is handled by an instance of a special linear operator class provided by a psi-phi package  This instance is needed to imitate the behavior of your new matrix when multiplied by a vector  It avoids forming any dense matrices  This operator is then used as an input to the standard SVD routing  instead of the matrix itself  This is it  I hope you feel the beauty and the power of linear algebra  If you have a single machine with enough RAM to fit your data  this can be a great way to build your recommendation model  As long as you have your data in the appropriate sparse format  the computation is very straightforward and blazingly fast  If you don't need to implement any fancy imputation strategy  and often times it is not required  and especially if your data does not fit a single machine memory  you can also use Spark and its distributed implementation of the truncated SVD  To summarize this lesson  now you know how to make imputation of the PureSVD model very efficient with the Lanczos method  You also know how to use various missing value imputation strategies and avoid constructing dense matrices  Due to its simplicity  PureSVD is suitable for a quick prototyping of your recommendation models  especially if you have enough RAM to run it on a single machine  Distributed implementations of the truncated SVD is a difficult task and it's currently supports only the core functionality 
hjVNKeVdKcs,RecSys  MF I  Hi again  During the previous lessons  you have learnt about the simple yet pretty powerful factorization method based on the single value decomposition  However  as you remember  you had to find the way to deal with the missing values of the utility metrics  You have considered several different strategies with the missing values imputation and work through a few concrete examples with zero-based on average-based techniques  Despite some flexibility of the method  it depends on your decision what a good replacement for the unknown values is  but you can actually avoid making this decision  Let's go back to the general matrix factorization  You have already seen this picture during the past lessons  Recall that SVD gives us an optimal solution to the quadratic minimization task with complete matrices  You can equivalently rewrite it for the case of a general matrix factorization approach  This formulation assumes that all the elements  A  I  J  are known  which is a significant simplification as you have substituted probably more than 99% of all the data points  Now  what you can do here is to learn your model  not from the entire matrix but only from the observed data  This way  you don't have to make any assumptions about the missing elements  The only concern is whether there is enough data for your model to learn any viable relations  As the number of users and items is typically very large  there are almost no chances that only by observing a tiny fraction of the data  the model will uncover the true hidden relationships within it  In fact  while the model is likely to fit the training data well  its predictions on the unobserved data can be off by a significant margin  This problem is known as overfeeding  In order to minimize this problem  you regularize your model by an additional penalty term with the sum of squared Euclidean norms of the model's parameters  Lambda here is a regularization coefficient that is typically determined by cross-validation  I have also introduced a factor of one-half for convenience  Regularization will help to avoid uncontrollable growth of the parameters and  therefore  find a more reliable solution  So  how do we minimize this loss function? The more straightforward approach is to apply a technique called gradient descent  also known as a badge gradient method  Defining the meaning of your loss function L  you'll start by a random guess  selecting some initial point  Then  you find the direction of the steepest descent at that point  which is opposite as the direction of the gradient of your loss function and make a step  The step size is controlled by an additional variable called learning rate  In the simplest case  it is just a constant  Repeat steps until convergence  Very simple  Note that each iteration step or epoch  you can update latent features of users and items separately  In order to fully update matrices P and Q  the algorithm has to run over the entire dataset  and literally update each row PI and QJ of these matrices  This task can be quite computationally demanding due to the way the corresponding partial derivatives of the loss function are computed  At large scale  this may lead to a slow convergence and high memory load  The better alternative would be to use a stochastic variant of this method  a stochastic gradient descent or SGD  which is more computationally efficient at the cost of a less straightforward convergence  The idea of this method is the following  Instead of making updates with respect to all the observations  at each step  make a smaller update by considering only a single interaction between the randomly selected user item pair  Mathematically speaking  in the update formula  you substitute the full gradient with its stochastic approximation  In this case  the direction of the update step will not perfectly align with the true gradient direction  However  it is still hoped to approximately point towards the minimum  The algorithm simply makes more frequent but less expensive updates of the parameters at every iteration  It is very easy to implement  After you have initialized the factor matrices  P and Q  at each epoch  you sequentially compute approximate updates to the rows of the matrices by iterating over the observations in a shuffled order  Most notably  to update all the rows  PI and QJ  the algorithm sweeps through the entire dataset in a single pass  Optimization procedure terminates when either the prediction error teases to decrease or a maximum number of epochs is reached  The algorithm scales linearly with respect to the number of observations and in contrast to badge gradient method  It works with a small amount of memory  Stochastic gradient descent can be also used in the online settings when you need to quickly respond new users or new items  For example  in case of a new user  the previous algorithm can be simplified  At each epoch  you only need to iterate over the ratings of a new user to get an updated latent feature vector  The complexity of such an update is proportional to the number of ratings provided by a new user  This is basically the gradient-based version of the folding-in  Actually  you can also apply here a conventional folding-in approach  Firstly  recall how the folding-in formula looks like in the SVD case  how concise it is  Following the same ideas as previously  let's write the approximate definition of the row update  This time  the matrix Q does not have any special features like the alter gonality of columns  And you have to take special care of the inverses in the folding-in procedure  In fact  what you have here is simply the least squares problem  and its solution is known  The size of the square matrix Q transposed by Q  is defined by the number of latent features  which is typically small  An overall complexity is then increased only by the relatively small cubic term related to matrix inverse  From the numerical perspective  it is also a good idea to add an additional regularization within the inverse  As the matrix Q is arbitrary  Q transposed by Q can be close to degenerate  and the regularization will help to improve the numerical stability of the computation of its inverse  Remember this formula  In the upcoming lessons  you will see another optimization approach named alternating least squares  which uses the same technique to drive the entire optimization procedure  To sum up the lesson  you are now familiar with the basic matrix factorization technique  You know what a gradient descent is and how this is computed  You can explain the difference between the gradient descent approach and its stochastic alternative  Finally  you know how to implement simple SGD algorithm and perform incremental updates for online 
ACTFY2k4Fhk,RecSys  MF II  Previously you have seen how to effectively deal with the missing values within the basic matrix factorization approach  Now  you can do something even more interesting  include user and item biases into the model  Let me briefly remind you what are those biases  If for example a user has assigned rating 3 to some movie  can you tell if the movie is actually not so good? Well  if all other ratings provided by this user are higher  then probably yes  But what if the user almost never rates movies with five stars  waiting for something really exceptional? Such a demanding user may have a different rating pattern  shifted towards the lower ratings  Similar intuition works for items  as well  For example  popular items are likely to receive higher ratings  You can conclude that the same rating value may contain different information  depending on various biases  In fact  most of the signal is contained within user and item biases  and taking it into account may improve the quality of recommender systems  One of the greatest benefits of the factorization approach is the ability to flexibly modify the way utility function is defined  while staying in the same computational framework  As you remember  in the SVD model  you estimated LH values manually  and then used them as a substitution for the unknowns  But now  within your matrix factorization approach  you can directly add all the needed biases to your utility function and let your model figure out on its own what those biases actually are  In other words  in this new model  you assume that using bias bi and item biases  bj  are unknown  And they become new model parameters along with factors p and q  The global average mu is pre-computed in order to make the effect of other biases more pronounced  The new optimization objective becomes a bit more variables but still very intuitive  It has more parameters to estimate but this doesn't change the general optimization procedure  As in the previous case  you can use the approximate partial derivatives to make computations more efficient  employing the stochastic gradient descent to optimize their objective  After differentiating with respect to different types of the parameters  you obtain a new system of equations  As you can see  the only difference is that two additional dumps related to user and item biases are added  Although this algorithm is not implemented in Spark out of the box  there are a few open source-projects devoted to that  There is also an efficient parallel implementation of the algorithm called Hogwild  This matrix factorization approach became popular after it was published by Simon Funk when he attended the Netflix Prize competition  Because of that  sometimes this algorithm is also called Funk SVD  I hope you won't be confused by the name and can easily explain the main differences between this approach and actual SVD method  I also hope that you have enough confidence to make your own exploration of the field of matrix factorization algorithms  It might be good exercise to check your understanding  for example  by looking into famous SVD++ and timeSVD models  Now  as you have seen  there is one aspect common for all previously discussed models  All of them minimize the squared error  In other words  these algorithms are designed to optimize RMSE metric  which is suitable for the rating prediction task  However  more often  you are interested not in the specific rating values  but in a good list of top-N recommendations  And for this task  the metrics like NDCG and MAP are more appropriate  On the other hand  it turns out that the optimum value in terms of an error-based metric like RMSE  doesn't necessarily correspond to the optimum value in terms of the ranking-based metrics  Of course  as you have previously seen  it is possible simply to sort items by their predicted ratings  However  this is not what you initially optimized for  You can actually divide all those functions into three different groups  The first one is called pointwise  and you are already familiar with it  The second one is pairwise  Can you spot the main difference of this loss function from the pointwise loss? In the pairwise case  the function within summation operates solely on this course predicted by the model  while actual values are only used to enforce the ordering  The main goal of the approach is to minimize the number of inversions when the predicted order of any two items doesn't correspond to their natural order  Finally  the listwise loss operates over the list or sets of items  This one is especially suitable for optimizing ranking metrics  such as MAP and NDCG  Both pointwise and listwise approaches are members of an important family of methods called learning to rank  However  a bad optimization objective in terms of top-N recommendations task comes at the cost of a high computational complexity  Depending on the actual model  one may have to use additional computational fix and simplifications in order to avoid exhaustive shorts and deal with non-smooth nature of the metrics  Which is why pointwise methods are still popular  To summarize  you know how to extend basic matrix factorization and include user and item biases into it  You know about optimization objectives and can explain the key differences between them 
2y17L618vWs,RecSys  iALS I  Hi  friends  Are you tired of starcasting read and descend in SVD like algorithms? Yes  me too  So no more of this stuff  Let me teach you how to find matrix factorization with alternating least squares or ALS approach  In 2007  Belle and Koren  presented the implementation of ALS for Matrix factorization in recommender systems on International Conference on Data Mining  ICDM  You can find out all the details in the article Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights  Some good news  this algorithm is already implemented in Sparc and Melk library so you'll be able to use it out of the box  In addition  I have decided to pursue several goals in a few videos  I'm going to explain to you first  the difference between explicit and implicit feedback  complexities involved while processing and making assumptions on implicit feedback  And second  show you a scalable algorithm called iALS  where I in the acronym stands for implicit  This work was published by Yehuda  Koren and Volinsky  one year after the ALS article publication  The full text of the whole article  you can find by the name  Collaborative Filtering for Implicit Feedback Datasets  and it is exactly what Sparc and Melk library provides you with  Considering extra optimizations for distributed computations  Previously  we have been working with a user item matrix based only on explicit feedback  When you watch a movie  you provide the rating from one to five  If you listen to a song  you can provide feedback in the form of likes and dislikes  This type of feedback is explicit  A user is telling you if he or she likes or dislikes an item and what the level of his or her attitude to the item is  All other data in user item matrix is supposed to be missing  but is it the case in reality? Do we know at least something about some of user item interactions which are marked as unknowns ? And the answer is yes  We have some information about the user item indirection  For example  if you are watching a movie without giving any explicit feedback in the form of a rating  the movie service provider already has the information for how long you have been watching this or that movie  Of course  in case you have given the approval to collect usage data  Have you been watching 10% or 90% of the movie makes a difference in the interpretation of implicit feedback  The first case  10% of the movie watched directly shows that the filmmaker should have taken up cross teaching instead  Whereas  in the second case  a user is likely to enjoy them more  The other types of implicit feed backs are browsing approaches history  mouse movements on the screen and browsing patterns  The authors of the article define several major differences between implicit and explicit feedback  There is no positive or negative feedback  You have some feedback and it is a question to you as a recommender systems engineer to define what is positive or negative  The implicit feedback is noisy by definition  When a user is watching a TV show  are you sure if he or she is watching a TV at all? He or she might be cooking in another room or fall asleep  The evaluation of implicit feedback recommendation systems is not straightforward  Prediction based metrics such as My or RMSC  which we captioned previously can be applied here  but it doesn't make a lot of sense  It is not important to predict that you will watch 11% of a movie  It is important to predict if you like an item or not  So here we come back to the first statement  There is also the fourth statement which the authors define  RUI is a numeric value which a user gives to an item I in explicit scenario datasets  But it is a measure of likeness in implicit scenario datasets  For example  if you are listening to a music  the measure of likeness can be defined as a fraction of minutes you listen to the whole song  Bear in mind  that this measure can be greater than one  For instance  if you listen to your favorite song five times during a day  then RUI will be a numeric value of five for this day  In short  RUI is a preference for the explicit feedback scenario or confidence for the implicit feedback scenario  To align with the article notation  let me redefine the concepts of RUI and PUI  RUI is an observation value  For example  described by the ratio of movie watched  And since now  you have a dense metrics consisting of zero values  which were previously marked missing  PUI indicates the preference of a user U  which is binarization of RUI  To account for a confidence  you need to add CUI  There are different ways to define confidence in the article the authors presented for most that you see on the screen  where parameters alpha and epsom can be find with the help of cross validation  The idea behind these formulas is simple  The more value of user item observation  the more confident you are about this value  There will be a little bit of algebra in the next video  You can take a pen or pencil and grab a piece of paper to follow my conclusions more easily 
QEtWL4lWlL4,RecSys  iALS II  I see  not all of you are prepared  suit yourself  at the quiz time  you will need these notes badly  Let's see how good you are at making math in memory  SVD-like or matrix factorization algorithms  usually perform better than the neighborhood models on modest-sized data sets  Just to remind you  I used to learn the latent feature space for users and items  Therefore a user u is represented by the feature vector xu in the f dimensional feature space  And an item i is represented by the feature vector yi in the same f dimensional feature space  The prediction of a regression RUI is a p-hat ui  which is a product of xu and yi  See  piece of cake  Moving on  in the previous videos you had seen how to optimize SVD-like algorithms with the stochastic gradient descent  And it is working perfectly well for sparse matrices  For the implicit big data set  we have a dense matrix and the following objective to optimize  This cost function contains n x m terms  which is not cost effective and sometimes even impossible to calculate  That is why the authors provided the highly efficient optimization algorithm  which scales linearly with the quantity of non-zero values in the matrix  And this number is usually several magnitudes smaller than the size of the matrix  Take a closer look at the objective  if you fix user vectors then it's a quadratic equation with regard to each item i  Therefore  you can analytically find precise value for the item vector which minimize the objective  Paraphrasing that famous expression by Louis Armstrong  Small steps in the stochastic gradient descent  giant leaps in the direction in the minimal value  Vice versa  you can fix the item vectors and then solve the quadratic equation with regard to the user vectors  then repeat  From practice several dozens of interactions are enough to converse to their minimum  And you can easily see why it's called alternating least squares  You alternate between the different stages where you solve the least squares problem  Typically  ALS requires smaller amount of iterations compared to the stochastic gradient descent algorithms  and is likely to convert faster  But how to overcome m times m complexity problem? Linear algebra comes into play  These course is not only for people who like mathematics as much as I do  Therefore I only provide the answers to the previously mentioned equations and we will discuss in detail the complexity of the solution  We have the objective  We differentiate our objective on the subscript y and equate it to 0  Then the solution looks the way that you see on the screen  Mu xu is an outcome of matrix multiplications and inversion  Let us define n as the number of users and m as the number of items in our data set  Then matrix Y will be an m x f matrix  Each row will represent an item in f-dimensional feature space  i is an identity matrix of size f x f  C superscript u is a diagonal matrix of elements Cui  p(u) is a vector of pui elements of the length m  We have the formula  now let us calculate the complexity of this evaluation  If you forget the complexity of matrix multiplication or inversions  don't worry  I will remind you everything on the way  The first approach to calculate it for each user will have the complexity  multiple of m times m  Which is too big  if you remember  Let me break down it for you  For each user we have the computation of the left side of this formula  the computation of the right side of this formula  and the matrix to vector multiplication  From left to right  YT multiplied by Cu multiplied by Y  Y is an m by f matrix  therefore  YT is f by m matrix  Cu is a diagonal matrix m by m  It is a user perspective how two items are similar to each other  Brute force matrix multiplication here is f by m by m plus f by m by f  which it has a major coefficient fm squared  Okay  Cu is a diagonal matrix  so multiplication to a diagonal matrix can be elevated more efficiently  So  we will get here (f x m) + (m squared by m)  which has a major coefficient f squared m  The resulting matrix is [f x f] size  Then we add the diagonal identity matrix with a complexity O(f) and get the resulting matrix  [f x f] size  then we calculate the inverse matrix  There are efficient implementation  but even the simplest will have a complexity of f in a power of 3  Which is not too big  as f is usually between dozens and hundreds  Next is the right side of this formula  YT multiplied by Cu is of f x m complexity  P(u) is a vector  therefore  we will get the same big O(f x m) complexity  The resulting value is of size (f x m)  which is a vector  Finally  we need to multiply the left-hand side [f x f] matrix  and the right-hand side  [f x 1] vector  The complexity is [f x f] and we get our new [INAUDIBLE] vector  Summing up all these complexities we get  big O(f squared m + f in the power of 3 + f squared + fm)  Removing minor coefficients  it is O(f squared m + f in the power of 3)  As soon as we do all these calculations for each user  then the whole ILS step will take O(f squared nm + f in the power of 3 multiplied by n)  See this annoying n x m term? And it is possible to get rid of it  Let us take a closer look at what we have inside the parentheses  (YTCuY) which gives complexity f squared m  If you write it the following way  you get the term YTY which can be calculated once for all the users  And YT CU minus identify matrix Y  And the benefit of this representation is that CuI is a value of 1 plus something which is equivalent to 0  if rui = 0  Therefore  the middle diagonal matrix will have only mu known 0 elements  where mu is a quantity of observations provided by the user  And this number is too small compared to the value of m  So the complexity of this calculation is big O(f squared mu)  Sum it up for all the users we get big O(f squared small m) for YTY multiplication  Plus f squared big M  where big M is a quantity of all the non-zero observations in user item matrix  Plus f in a power of 3 n  Big M is supposed to be bigger than small m  so we get big O(f squared big M + f in the power of 3 n)  And this is way more efficient  no m by n term  When we perform similar matrix magic for the item vector computation  we will get big O(f squared multiplied by big M  + f to the power of 3 multiplied by n + m)  It is a complexity for 1 iALS iteration  Several dozens of iterations are enough for data sets  such as Netflix  to converge to the minimum value  In addition to efficiency and scalability  you can even provide explanations to the calculated recommendations similar to the item  item collaborative filter  But it is already going beyond our discussion  Look through the article  if you are interested  A lot of math magic and a few lines of code in Python to execute iALS algorithm on your data set  you only need to call ALS train or trainImplicit and then ALS model predict  You can play with the rank which is f  the dimensionality of the feature space  the number of iterations  and the regularization parameter lambda  There are a few more parameters which you can look up in the documentation  Let me wrap up this video  You now know what the implicit feedback is and can at least four vectors that differentiate it from the explicit feedback  [NOISE] You can explain the key differences between ALS and stochastic gradient descent  [NOISE] You can break down the complexity of iALS algorithm and explain how to make it work faster than big O(mn)  [NOISE] Finally  you can write Spark code to execute iALS algorithm for your recommender service  Have fun  build recommenders system services and make history 
sCmsrSBJT8A,"RecSys  Hybrid I  Hi  So far  you have learned about various techniques to produce both personalized and non-personalized recommendations  As you have seen  different recommendation models solves specific problems and have their own set of strengths and weaknesses  For example  collaborative filtering models won't be able to generate any recommendations for a newly introduced user until the user provides at least one rating  Even in the warm start scenario  when a few user ratings are already known  the model is likely to provide inaccurate or biased recommendations  On the other hand  content-based models are less affected by the cold start problems  but they are not as accurate as collaborative filtering models in standard scenarios  One obvious solution is to combine different recommendation techniques in order to compensate their individual limitations and generate more accurate results  This is called a hybrid approach  It allows to improve the quality of recommender systems not only in the specific scenarios like the cold start  but also in more general cases  Hybrid recommender systems are closely related to the field of ensemble analysis in standard classification tasks  For example  you can treat collaborative filtering models as a generalization of classification models  All ensemble systems in that respect  are hybrid models  The opposite however  is not necessarily true  so this is a broader concept  There are three top-level design patterns who build in hybrid recommender systems  The first one is the ensemble  In this setup  the existing recommender systems I used in the true black-box or off-the-shelf fashion  Every model in the ensemble produces scores in a unified way so that all the models are interchangeable and their outputs can be easily combined  As an example  one could combine predictions of the latent factors and the neighborhood-based models  Monolithic systems are designed for heterogeneous set ups with different sources of data or different classes of recommender models fused together  You cannot use them in a purely black-box mode as they typically require additional force to process the input data and most of their different parts  In some cases  it may not be even possible to have a clear distinction between those parts  Mixed systems simultaneously present the outputs of several recommender models  As an example  consider an online shop with several blocks of recommendations displayed in different locations of the web page  One block may be responsible for the current shopping trends while the other one for more tailored recommendations based on their recent purchase history  This classification provides a clear distinction between various designs  However  it doesn't tell us how those different systems are actually created  First of all  you can further divide all ensemble methods into parallel and sequential  The names are self-descriptive  In the sequential design  an output of one model becomes an input for another  In the parallel design  all the models operate independently of each other and their outputs are combined only in the end  Now  let's go into more details  There are two types of parallel ensemble methods and two types of sequential ones  Parallel methods can be weighted and switching  In the weighted approach  you simply combine all the models with some weights  These weights can be derived egoistically or by atomization approach  You can probably guess what's the simplest technique is  It is just a sum of the predictive scores of every model in the ensemble with the uniform weights  We will come back to this example soon  In this recent approach  you select the output of one or another model depending on the situation  For example  you can have one content-based model and one collaborative filtering model  In the case of a known user  you would prefer to select the latter one as it's likely to provide a superior quality of recommendations  In the case of a new user  you would opt for a content-based model as the only suitable  In fact  the choice can be even between several instances of the same model tuned in different weights  This is called a \""bucket of models \"" It is closely related to parameter tuning and model selection  Let's move on  Another type of ensembles is sequential methods which include cascade models and feature augmentation  In the cascade approach  intermediate recommendations of every consequent model are immediately evaluated and the next model in the queue tries to improve them  This is basically a boosting technique known from the classification field  In the feature augmentation approach  an output of one model is used as an input for another model without any intermediate refinements  For example  some quantum-based models can be used to generate predictions for some of the unknown values and then a standard collaborative filtering technique is applied on top of it  or you could go the other way around  First  build a collaborative filtering model and then describe every item and every user in terms of their closest neighborhoods  These descriptions can then be used to generate new feature vectors in a content-based approach  To draw an analogy with classification  once again  this is similar to the so-called second approach  Note  there is a little uncertainty related to the feature augmentation  In some cases  it is impossible to avoid modification of the models within it  Then by definition  it becomes a member of the monolithic family  Nevertheless  the main members of this family are feature combination and meta-level systems  In the feature combination approach  several heterogeneous data sources are combined into a unified representation and then used within the single recommendation model  As no other monolithic techniques  the key aspect here is that the process of building the recommendation model  requires a certain level of modifications to standard algorithms or additional data pre-processing steps  In the metal-level approach  not the data  but the models are glued together  As previously  due to the heterogeneous nature  you have to modify the models in order to tie them together  The typical example of such a system is a combination of a collaborative filtering and content-based algorithms  The former has to be modified in order to use the content features  For example  you may have to introduce special weights in the latent factor optimization objective or tuned the correlation measure in the neighborhood models  If you feel confused by now don't worry  it takes some time to wrap your head around this taxonomy  Generally speaking  the four techniques on the left can be used off-the-shelf  And for this reason  they are categorized as ensemble methods  The other two techniques on the right  aim to build a single model in the heterogeneous environment which is typically impossible without additional customization of its components  Here's the summary of the lesson  You now understand what the hybrid recommenders are and how they can help you in building a better recommendation models  You know about three top-level categories of the hybrid recommender systems  mixed  monolithic and ensembles  You're also familiar with six different types of hybridization techniques and can explain their key differences "
ikSbPwwjpKw,RecSys  Hybrid II  The previous lesson was full of new concepts  They  however  give a good sense of how diverse and flexible hybridization methods are  It is time to get to more practical aspects of hybridization  Let's look into the simplest ensemble model based on the weighted scheme  Let A-hat be the approximation of our original utility matrix A  By the definition of the ensemble  it is expressed as a weighted sum of the outputs of the S distinct recommendation models  Omega-k here are the ensemble weights  So how to determine these weights? How do make your ensemble out perform any of its models individually? You have different choices here  In the simplest case  you can choose all the weights to be uniform and equal to one over S  In practice  it is possible to get an improvement even with these values  You can also try to incorporate some domain or expert knowledge and based on that  figure out what the reasonable values are  For example  if it is known that the original matrix is extremely sparse  you can manually lower the weights of the collaborative filtering models as less reliable and increase the weights of the quantum-based models as more consistent  The next important question is  how do you actually verify that the result is improved with the selected weight values? You can use your training data and evaluate the model on the small subset of it  It means that you need to speed your training data set  You randomly select 25% of the node observations and one of them as a holdout set  I denote it by H  The remaining 75% of the observations are used to actually train your model  Once complete  you evaluate the ensemble-based prediction against the actual holdout values  Depending on the problem  the evaluation measure for this task may be right  In the simplest case of the raging prediction task  you can go with MAE or MSE  When you are satisfied with the weight values  it is important to retrain your model on the entire training set not just 75% of it  More formal techniques can also be applied to find more accurate values of ensemble weights  For example  you could do a grid search  Another option is a linear regression that fits MSE matrix  This matrix  however  is sensitive to noisy outliers  A better approach would be to use a gradient-based methods or the MAE or any other variation of the robust regression methods  Let's focus on the gradient method as you're already familiar with it  As always  you first need to take care of the gradient of your Lewis function  The partial derivatives of the MAE with respect to the ensemble weights are easy to find as they are proportional to the sine function  Note  it is also might be a good idea to add a regularization term  It could be for example  the Euclidean norm of the vector Omega  As now  you already know how the full gradient looks like  everything is ready to sketch the optimization algorithm  At the starting point of the gradient method  the weights should be initialized with a uniform value in order to avoid favoring any particular ensemble model  The other iterations continue until the error stops decreasing or maximum number of iterations is reached  As the problem is non-complex  the algorithm is only guaranteed to converge to local minima  And again  once the new weight values are computed  don't forget to retrain the model on the entire training data  There is another important class of the hybridization methods namely randomness injection  It has many principles in common with the famous random forest classification  The key idea of the method is simply to bring randomness into the model behavior  Then  by incorporating several randomized models  one can achieve greater quality  Two notable classes of the models suitable for this method are neighborhood-based models and matrix factorizations  Randomness in the neighborhood models is achieved by a considerable extension of the neighbor source space in which k neighbors are selected randomly  Randomness in the matrix factorization methods directly falls from the random initialization  As a side note  randomness plays an important role in many machine learning algorithms  For instance  random projections are used to quickly approximate kernel functions  And more relevant to our course example is the randomized SVG algorithm  Unlike SVG++  this is the true singular value decomposition  Unlike SVG though  it is computed by random projections rather than by the Lancer's algorithm  On the other hand  it operates with matrix vector multiplications similarly to the SVG  which makes it very convenient and computationally efficient  Time to get back to our hybrid recommenders  Instead of building an ensemble of several different models computed for a single data set  you could also perform a vice versa task  Build an ensemble of a single model computed over several subsamples of the original data  The latter technique is called Bagging  There are four different bagging methods that could be adapted for collaborative filtering models  Without going into details  it is important to emphasize that in these methods  you are required to build a number of subsamples of the original data  And therefore  additional storage space is needed  In the first three of these methods  the subsamples are similar to the original data in terms of the number of elements  which makes the applicability of this methods in the large scale settings questionable  Only the last method  Entry-wise subsampling  a lost to generate samples with the lower number of non-zero elements than in the original data  In this method  a collaborative filtering model is applied to each subsample and the final prediction is simply average across all them  The first three methods follow similar ideas with some nuances in how the averages are computed  As an important remark  the equation of the balance between the quality of recommendations and the computation of feasibility of a particular approach is not easy  There is often a trade off between them  This was a high-level description of the basic hybridization methods  And I encourage you to check the materials referenced at the bottom of the slide to get more details and see other examples  Let me summarize the lesson  You understand how weighted hybridization is performed  You know at least several techniques for tuning ensemble weights  You can justify a certain choice of the weight values in your model  And you are familiar with randomization techniques 
FKA794QD2fI,What is a neural network? The term  Deep Learning  refers to training Neural Networks  sometimes very large Neural Networks  So what exactly is a Neural Network? In this video  let's try to give you some of the basic intuitions  Let's start to the Housing Price Prediction example  Let's say you have a data sets with six houses  so you know the size of the houses in square feet or square meters and you know the price of the house and you want to fit a function to predict the price of the houses  the function of the size  So if you are familiar with linear regression you might say  well let's put a straight line to these data so and we get a straight line like that  But to be Pathans you might say well we know that prices can never be negative  right  So instead of the straight line fit which eventually will become negative  let's bend the curve here  So it just ends up zero here  So this thick blue line ends up being your function for predicting the price of the house as a function of this size  Whereas zero here and then there's a straight line fit to the right  So you can think of this function that you've just fit the housing prices as a very simple neural network  It's almost as simple as possible neural network  Let me draw it here  We have as the input to the neural network the size of a house which one we call x  It goes into this node  this little circle and then it outputs the price which we call y  So this little circle  which is a single neuron in a neural network  implements this function that we drew on the left  And all the neuron does is it inputs the size  computes this linear function  takes a max of zero  and then outputs the estimated price  And by the way in the neural network literature  you see this function a lot  This function which goes to zero sometimes and then it'll takes of as a straight line  This function is called a ReLU function which stands for rectified linear units  So R-E-L-U  And rectify just means taking a max of 0 which is why you get a function shape like this  You don't need to worry about ReLU units for now but it's just something you see again later in this course  So if this is a single neuron  neural network  really a tiny little neural network  a larger neural network is then formed by taking many of the single neurons and stacking them together  So  if you think of this neuron that's being like a single Lego brick  you then get a bigger neural network by stacking together many of these Lego bricks  Let's see an example  Letâ€™s say that instead of predicting the price of a house just from the size  you now have other features  You know other things about the host  such as the number of bedrooms  I should have wrote [INAUDIBLE] bedrooms  and you might think that one of the things that really affects the price of a house is family size  right? So can this house fit your family of three  or family of four  or family of five? And it's really based on the size in square feet or square meters  and the number of bedrooms that determines whether or not a house can fit your family's family size  And then maybe you know the zip codes  in different countries it's called a postal code of a house  And the zip code maybe as a future to tells you  walkability? So is this neighborhood highly walkable? Thing just walks the grocery store? Walk the school? Do you need to drive? And some people prefer highly walkable neighborhoods  And then the zip code as well as the wealth maybe tells you  right  Certainly in the United States but some other countries as well  Tells you how good is the school quality  So each of these little circles I'm drawing  can be one of those ReLU  rectified linear units or some other slightly non linear function  So that based on the size and number of bedrooms  you can estimate the family size  their zip code  based on walkability  based on zip code and wealth can estimate the school quality  And then finally you might think that well the way people decide how much they're will to pay for a house  is they look at the things that really matter to them  In this case family size  walkability  and school quality and that helps you predict the price  So in the example x is all of these four inputs  And y is the price you're trying to predict  And so by stacking together a few of the single neurons or the simple predictors we have from the previous slide  we now have a slightly larger neural network  How you manage neural network is that when you implement it  you need to give it just the input x and the output y for a number of examples in your training set and all this things in the middle  they will figure out by itself  So what you actually implement is this  Where  here  you have a neural network with four inputs  So the input features might be the size  number of bedrooms  the zip code or postal code  and the wealth of the neighborhood  And so given these input features  the job of the neural network will be to predict the price y  And notice also that each of these circle  these are called hidden units in the neural network  that each of them takes its inputs all four input features  So for example  rather than saying these first nodes represent family size and family size depends only on the features X1 and X2  Instead  we're going to say  well neural network  you decide whatever you want this known to be  And we'll give you all four of the features to complete whatever you want  So we say that layers that this is input layer and this layer in the middle of the neural network are density connected  Because every input feature is connected to every one of these circles in the middle  And the remarkable thing about neural networks is that  given enough data about x and y  given enough training examples with both x and y  neural networks are remarkably good at figuring out functions that accurately map from x to y  So  that's a basic neural network  In turns out that as you build out your own neural networks  you probably find them to be most useful  most powerful in supervised learning incentives  meaning that you're trying to take an input x and map it to some output y  like we just saw in the housing price prediction example  In the next video let's go over some more examples of supervised learning and some examples of where you might find your networks to be incredibly helpful for your applications as well 
GH8yBBC9n3Y,Supervised Learning with Neural Networks  There's been a lot of hype about neural networks  And perhaps some of that hype is justified  given how well they're working  But it turns out that so far  almost all the economic value created by neural networks has been through one type of machine learning  called supervised learning  Let's see what that means  and let's go over some examples  In supervised learning  you have some input x  and you want to learn a function mapping to some output y  So for example  just now we saw the housing price prediction application where you input some features of a home and try to output or estimate the price y  Here are some other examples that neural networks have been applied to very effectively  Possibly the single most lucrative application of deep learning today is online advertising  maybe not the most inspiring  but certainly very lucrative  in which  by inputting information about an ad to the website it's thinking of showing you  and some information about the user  neural networks have gotten very good at predicting whether or not you click on an ad  And by showing you and showing users the ads that you are most likely to click on  this has been an incredibly lucrative application of neural networks at multiple companies  Because the ability to show you ads that you're more likely to click on has a direct impact on the bottom line of some of the very large online advertising companies  Computer vision has also made huge strides in the last several years  mostly due to deep learning  So you might input an image and want to output an index  say from 1 to 1 000 trying to tell you if this picture  it might be any one of  say a 1000 different images  So  you might us that for photo tagging  I think the recent progress in speech recognition has also been very exciting  where you can now input an audio clip to a neural network  and have it output a text transcript  Machine translation has also made huge strides thanks to deep learning where now you can have a neural network input an English sentence and directly output say  a Chinese sentence  And in autonomous driving  you might input an image  say a picture of what's in front of your car as well as some information from a radar  and based on that  maybe a neural network can be trained to tell you the position of the other cars on the road  So this becomes a key component in autonomous driving systems  So a lot of the value creation through neural networks has been through cleverly selecting what should be x and what should be y for your particular problem  and then fitting this supervised learning component into often a bigger system such as an autonomous vehicle  It turns out that slightly different types of neural networks are useful for different applications  For example  in the real estate application that we saw in the previous video  we use a universally standard neural network architecture  right? Maybe for real estate and online advertising might be a relatively standard neural network  like the one that we saw  For image applications we'll often use convolution on neural networks  often abbreviated CNN  And for sequence data  So for example  audio has a temporal component  right? Audio is played out over time  so audio is most naturally represented as a one-dimensional time series or as a one-dimensional temporal sequence  And so for sequence data  you often use an RNN  a recurrent neural network  Language  English and Chinese  the alphabets or the words come one at a time  So language is also most naturally represented as sequence data  And so more complex versions of RNNs are often used for these applications  And then  for more complex applications  like autonomous driving  where you have an image  that might suggest more of a CNN convolution neural network structure and radar info which is something quite different  You might end up with a more custom  or some more complex  hybrid neural network architecture  So  just to be a bit more concrete about what are the standard CNN and RNN architectures  So in the literature you might have seen pictures like this  So that's a standard neural net  You might have seen pictures like this  Well this is an example of a Convolutional Neural Network  and we'll see in a later course exactly what this picture means and how can you implement this  But convolutional networks are often use for image data  And you might also have seen pictures like this  And you'll learn how to implement this in a later course  Recurrent neural networks are very good for this type of one-dimensional sequence data that has maybe a temporal component  You might also have heard about applications of machine learning to both Structured Data and Unstructured Data  Here's what the terms mean  Structured Data means basically databases of data  So  for example  in housing price prediction  you might have a database or the column that tells you the size and the number of bedrooms  So  this is structured data  or in predicting whether or not a user will click on an ad  you might have information about the user  such as the age  some information about the ad  and then labels why that you're trying to predict  So that's structured data  meaning that each of the features  such as size of the house  the number of bedrooms  or the age of a user  has a very well defined meaning  In contrast  unstructured data refers to things like audio  raw audio  or images where you might want to recognize what's in the image or text  Here the features might be the pixel values in an image or the individual words in a piece of text  Historically  it has been much harder for computers to make sense of unstructured data compared to structured data  And the fact the human race has evolved to be very good at understanding audio cues as well as images  And then text was a more recent invention  but people are just really good at interpreting unstructured data  And so one of the most exciting things about the rise of neural networks is that  thanks to deep learning  thanks to neural networks  computers are now much better at interpreting unstructured data as well compared to just a few years ago  And this creates opportunities for many new exciting applications that use speech recognition  image recognition  natural language processing on text  much more than was possible even just two or three years ago  I think because people have a natural empathy to understanding unstructured data  you might hear about neural network successes on unstructured data more in the media because it's just cool when the neural network recognizes a cat  We all like that  and we all know what that means  But it turns out that a lot of short term economic value that neural networks are creating has also been on structured data  such as much better advertising systems  much better profit recommendations  and just a much better ability to process the giant databases that many companies have to make accurate predictions from them  So in this course  a lot of the techniques we'll go over will apply to both structured data and to unstructured data  For the purposes of explaining the algorithms  we will draw a little bit more on examples that use unstructured data  But as you think through applications of neural networks within your own team I hope you find both uses for them in both structured and unstructured data  So neural networks have transformed supervised learning and are creating tremendous economic value  It turns out though  that the basic technical ideas behind neural networks have mostly been around  sometimes for many decades  So why is it  then  that they're only just now taking off and working so well? In the next video  we'll talk about why it's only quite recently that neural networks have become this incredibly powerful tool that you can use 
ceuXLsuZhLE,Why is Deep Learning taking off? if the basic technical idea is behind deep learning behind your networks have been around for decades why are they only just now taking off in this video let's go over some of the main drivers behind the rise of deep learning because I think this will help you that the spot the best opportunities within your own organization to apply these to over the last few years a lot of people have asked me Andrew why is deep learning certainly working so well and when a marsan question this is usually the picture I draw for them let's say we plot a figure where on the horizontal axis we plot the amount of data we have for a task and let's say on the vertical axis we plot the performance on above learning algorithms such as the accuracy of our spam classifier or our ad click predictor or the accuracy of our neural net for figuring out the position of other calls for our self-driving car it turns out if you plot the performance of a traditional learning algorithm like support vector machine or logistic regression as a function of the amount of data you have you might get a curve that looks like this where the performance improves for a while as you add more data but after a while the performance you know pretty much plateaus right suppose your horizontal lines enjoy that very well you know was it they didn't know what to do with huge amounts of data and what happened in our society over the last 10 years maybe is that for a lot of problems we went from having a relatively small amount of data to having you know often a fairly large amount of data and all of this was thanks to the digitization of a society where so much human activity is now in the digital realm we spend so much time on the computers on websites on mobile apps and activities on digital devices creates data and thanks to the rise of inexpensive cameras built into our cell phones accelerometers all sorts of sensors in the Internet of Things we also just have been collecting one more and more data so over the last 20 years for a lot of applications we just accumulate a lot more data more than traditional learning algorithms were able to effectively take advantage of and what new network lead turns out that if you train a small neural net then this performance maybe looks like that if you train a somewhat larger Internet that's called as a medium-sized internet to fall in something a little bit better and if you train a very large neural net then it's the form and often just keeps getting better and better so couple observations one is if you want to hit this very high level of performance then you need two things first often you need to be able to train a big enough neural network in order to take advantage of the huge amount of data and second you need to be out here on the x axes you do need a lot of data so we often say that scale has been driving deep learning progress and by scale I mean both the size of the neural network we need just a new network a lot of hidden units a lot of parameters a lot of connections as well as scale of the data in fact today one of the most reliable ways to get better performance in the neural network is often to either train a bigger network or throw more data at it and that only works up to a point because eventually you run out of data or eventually then your network is so big that it takes too long to train but just improving scale has actually taken us a long way in the world of learning in order to make this diagram a bit more technically precise and just add a few more things I wrote the amount of data on the x-axis technically this is amount of labeled data where by label data I mean training examples we have both the input X and the label Y I went to introduce a little bit of notation that we'll use later in this course we're going to use lowercase alphabet to denote the size of my training sets or the number of training examples this lowercase M so that's the horizontal axis couple other details to this Tigger in this regime of smaller training sets the relative ordering of the algorithms is actually not very well defined so if you don't have a lot of training data is often up to your skill at hand engineering features that determines the foreman so it's quite possible that if someone training an SVM is more motivated to hand engineer features and someone training even large their own that may be in this small training set regime the SEM could do better so you know in this region to the left of the figure the relative ordering between gene algorithms is not that well defined and performance depends much more on your skill at engine features and other mobile details of the algorithms and there's only in this some big data regime very large training sets very large M regime in the right that we more consistently see largely Ronettes dominating the other approaches and so if any of your friends ask you why are known as you know taking off I would encourage you to draw this picture for them as well so I will say that in the early days in their modern rise of deep learning it was scaled data and scale of computation just our ability to Train very large dinner networks either on a CPU or GPU that enabled us to make a lot of progress but increasingly especially in the last several years we've seen tremendous algorithmic innovation as well so I also don't want to understate that interestingly many of the algorithmic innovations have been about trying to make neural networks run much faster so as a concrete example one of the huge breakthroughs in your networks has been switching from a sigmoid function which looks like this to a railer function which we talked about briefly in an early video that looks like this if you don't understand the details of one about the state don't worry about it but it turns out that one of the problems of using sigmoid functions and machine learning is that there these regions here where the slope of the function would gradient is nearly zero and so learning becomes really slow because when you implement gradient descent and gradient is zero the parameters just change very slowly and so learning is very slow whereas by changing the what's called the activation function the neural network to use this function called the value function of the rectified linear unit our elu the gradient is equal to one for all positive values of input right and so the gradient is much less likely to gradually shrink to zero and the gradient here the slope of this line is zero on the left but it turns out that just by switching to the sigmoid function to the rayleigh function has made an algorithm called gradient descent work much faster and so this is an example of maybe relatively simple algorithm in Bayesian but ultimately the impact of this algorithmic innovation was it really hope computation so the regimen quite a lot of examples like this of where we change the algorithm because it allows that code to run much faster and this allows us to train bigger neural networks or to do so the reason or multi-client even when we have a large network roam all the data the other reason that fast computation is important is that it turns out the process of training your network this is very intuitive often you have an idea for a neural network architecture and so you implement your idea and code implementing your idea then lets you run an experiment which tells you how well your neural network does and then by looking at it you go back to change the details of your new network and then you go around this circle over and over and when your new network takes a long time to Train it just takes a long time to go around this cycle and there's a huge difference in your productivity building effective neural networks when you can have an idea and try it and see the work in ten minutes or maybe ammos a day versus if you've to train your neural network for a month which sometimes does happened because you get a result back you know in ten minutes or maybe in a day you should just try a lot more ideas and be much more likely to discover in your network and it works well for your application and so faster computation has really helped in terms of speeding up the rate at which you can get an experimental result back and this has really helped both practitioners of neuro networks as well as researchers working and deep learning iterate much faster and improve your ideas much faster and so all this has also been a huge boon to the entire deep learning research community which has been incredible with just you know inventing new algorithms and making nonstop progress on that front so these are some of the forces powering the rise of deep learning but the good news is that these forces are still working powerfully to make deep learning even better Tech Data society is still throwing up one more digital data or take computation with the rise of specialized hardware like GPUs and faster networking many types of hardware I'm actually quite confident that our ability to do very large neural networks or should a computation point of view will keep on getting better and take algorithms relative learning research communities though continuously phenomenal at innovating on the algorithms front so because of this I think that we can be optimistic answer the optimistic the deep learning will keep on getting better for many years to come so that let's go on to the last video of the section where we'll talk a little bit more about what you learn from this course
L2oWDL3Msaw,Binary Classification  Hello  and welcome back  In this week we're going to go over the basics of neural network programming  It turns out that when you implement a neural network there are some techniques that are going to be really important  For example  if you have a training set of m training examples  you might be used to processing the training set by having a four loop step through your m training examples  But it turns out that when you're implementing a neural network  you usually want to process your entire training set without using an explicit four loop to loop over your entire training set  So  you'll see how to do that in this week's materials  Another idea  when you organize the computation of  in your network  usually you have what's called a forward pause or forward propagation step  followed by a backward pause or what's called a backward propagation step  And so in this week's materials  you also get an introduction about why the computations  in learning an neural network can be organized in this for propagation and a separate backward propagation  For this week's materials I want to convey these ideas using logistic regression in order to make the ideas easier to understand  But even if you've seen logistic regression before  I think that there'll be some new and interesting ideas for you to pick up in this week's materials  So with that  let's get started  Logistic regression is an algorithm for binary classification  So let's start by setting up the problem  Here's an example of a binary classification problem  You might have an input of an image  like that  and want to output a label to recognize this image as either being a cat  in which case you output 1  or not-cat in which case you output 0  and we're going to use y to denote the output label  Let's look at how an image is represented in a computer  To store an image your computer stores three separate matrices corresponding to the red  green  and blue color channels of this image  So if your input image is 64 pixels by 64 pixels  then you would have 3 64 by 64 matrices corresponding to the red  green and blue pixel intensity values for your images  Although to make this little slide I drew these as much smaller matrices  so these are actually 5 by 4 matrices rather than 64 by 64  So to turn these pixel intensity values- Into a feature vector  what we're going to do is unroll all of these pixel values into an input feature vector x  So to unroll all these pixel intensity values into Feature vector  what we're going to do is define a feature vector x corresponding to this image as follows  We're just going to take all the pixel values 255  231  and so on  255  231  and so on until we've listed all the red pixels  And then eventually 255 134 255  134 and so on until we get a long feature vector listing out all the red  green and blue pixel intensity values of this image  If this image is a 64 by 64 image  the total dimension of this vector x will be 64 by 64 by 3 because that's the total numbers we have in all of these matrixes  Which in this case  turns out to be 12 288  that's what you get if you multiply all those numbers  And so we're going to use nx=12288 to represent the dimension of the input features x  And sometimes for brevity  I will also just use lowercase n to represent the dimension of this input feature vector  So in binary classification  our goal is to learn a classifier that can input an image represented by this feature vector x  And predict whether the corresponding label y is 1 or 0  that is  whether this is a cat image or a non-cat image  Let's now lay out some of the notation that we'll use throughout the rest of this course  A single training example is represented by a pair  (x y) where x is an x-dimensional feature vector and y  the label  is either 0 or 1  Your training sets will comprise lower-case m training examples  And so your training sets will be written (x1  y1) which is the input and output for your first training example (x(2)  y(2)) for the second training example up to <xm  ym) which is your last training example  And then that altogether is your entire training set  So I'm going to use lowercase m to denote the number of training samples  And sometimes to emphasize that this is the number of train examples  I might write this as M = M train  And when we talk about a test set  we might sometimes use m subscript test to denote the number of test examples  So that's the number of test examples  Finally  to output all of the training examples into a more compact notation  we're going to define a matrix  capital X  As defined by taking you training set inputs x1  x2 and so on and stacking them in columns  So we take X1 and put that as a first column of this matrix  X2  put that as a second column and so on down to Xm  then this is the matrix capital X  So this matrix X will have M columns  where M is the number of train examples and the number of railroads  or the height of this matrix is NX  Notice that in other causes  you might see the matrix capital X defined by stacking up the train examples in rows like so  X1 transpose down to Xm transpose  It turns out that when you're implementing neural networks using this convention I have on the left  will make the implementation much easier  So just to recap  x is a nx by m dimensional matrix  and when you implement this in Python  you see that x shape  that's the python command for finding the shape of the matrix  that this an nx  m  That just means it is an nx by m dimensional matrix  So that's how you group the training examples  input x into matrix  How about the output labels Y? It turns out that to make your implementation of a neural network easier  it would be convenient to also stack Y In columns  So we're going to define capital Y to be equal to Y 1  Y 2  up to Y m like so  So Y here will be a 1 by m dimensional matrix  And again  to use the notation without the shape of Y will be 1  m  Which just means this is a 1 by m matrix  And as you influence your new network  mtrain discourse  you find that a useful convention would be to take the data associated with different training examples  and by data I mean either x or y  or other quantities you see later  But to take the stuff or the data associated with different training examples and to stack them in different columns  like we've done here for both x and y  So  that's a notation we we'll use e for a regression and for neural networks networks later in this course  If you ever forget what a piece of notation means  like what is M or what is N or what is something else  we've also posted on the course website a notation guide that you can use to quickly look up what any particular piece of notation means  So with that  let's go on to the next video where we'll start to fetch out logistic regression using this notation 
mRGQylRWAsI,Logistic Regression  In this video  we'll go over logistic regression  This is a learning algorithm that you use when the output labels Y in a supervised learning problem are all either zero or one  so for binary classification problems  Given an input feature vector X maybe corresponding to an image that you want to recognize as either a cat picture or not a cat picture  you want an algorithm that can output a prediction  which we'll call Y hat  which is your estimate of Y  More formally  you want Y hat to be the probability of the chance that  Y is equal to one given the input features X  So in other words  if X is a picture  as we saw in the last video  you want Y hat to tell you  what is the chance that this is a cat picture? So X  as we said in the previous video  is an X dimensional vector  given that the parameters of logistic regression will be W which is also an X dimensional vector  together with b which is just a real number  So given an input X and the parameters W and b  how do we generate the output Y hat? Well  one thing you could try  that doesn't work  would be to have Y hat be w transpose X plus B  kind of a linear function of the input X  And in fact  this is what you use if you were doing linear regression  But this isn't a very good algorithm for binary classification because you want Y hat to be the chance that Y is equal to one  So Y hat should really be between zero and one  and it's difficult to enforce that because W transpose X plus B can be much bigger than one or it can even be negative  which doesn't make sense for probability  That you want it to be between zero and one  So in logistic regression  our output is instead going to be Y hat equals the sigmoid function applied to this quantity  This is what the sigmoid function looks like  If on the horizontal axis I plot Z  then the function sigmoid of Z looks like this  So it goes smoothly from zero up to one  Let me label my axes here  this is zero and it crosses the vertical axis as 0 5  So this is what sigmoid of Z looks like  And we're going to use Z to denote this quantity  W transpose X plus B  Here's the formula for the sigmoid function  Sigmoid of Z  where Z is a real number  is one over one plus E to the negative Z  So notice a couple of things  If Z is very large  then E to the negative Z will be close to zero  So then sigmoid of Z will be approximately one over one plus something very close to zero  because E to the negative of very large number will be close to zero  So this is close to 1  And indeed  if you look in the plot on the left  if Z is very large the sigmoid of Z is very close to one  Conversely  if Z is very small  or it is a very large negative number  then sigmoid of Z becomes one over one plus E to the negative Z  and this becomes  it's a huge number  So this becomes  think of it as one over one plus a number that is very  very big  and so  that's close to zero  And indeed  you see that as Z becomes a very large negative number  sigmoid of Z goes very close to zero  So when you implement logistic regression  your job is to try to learn parameters W and B so that Y hat becomes a good estimate of the chance of Y being equal to one  Before moving on  just another note on the notation  When we programmed neural networks  we'll usually keep the parameter W and parameter B separate  where here  B corresponds to an inter-spectrum  In some other courses  you might have seen a notation that handles this differently  In some conventions you define an extra feature called X0 and that equals a one  So that now X is in R of NX plus one  And then you define Y hat to be equal to sigma of theta transpose X  In this alternative notational convention  you have vector parameters theta  theta zero  theta one  theta two  down to theta NX And so  theta zero  place a row a B  that's just a real number  and theta one down to theta NX play the role of W  It turns out  when you implement your neural network  it will be easier to just keep B and W as separate parameters  And so  in this class  we will not use any of this notational convention that I just wrote in red  If you've not seen this notation before in other courses  don't worry about it  It's just that for those of you that have seen this notation I wanted to mention explicitly that we're not using that notation in this course  But if you've not seen this before  it's not important and you don't need to worry about it  So you have now seen what the logistic regression model looks like  Next to change the parameters W and B you need to define a cost function  Let's do that in the next video 
_i4wxceXNsI,Logistic Regression Cost Function  In a previous video  you saw the logistic regression model  To train the parameters W and B of the logistic regression model  you need to define a cost function  Let's take a look at the cost function you can use to train logistic regression  To recap  this is what we had defined from the previous slide  So your output y-hat is sigmoid of w transpose x plus b where a sigmoid of Z is as defined here  So to learn parameters for your model you're given a training set of m training examples and it seems natural that you want to find parameters W and B so that at least on the training set  the outputs you have  The predictions you have on the training set  which we only write as y-hat (i) that that will be close to the ground truth labels y_i that you got in the training set  So to throw in a little bit more detail for the equation on top  we had said that y-hat is as defined at the top for a training example x and of course for each training example  we're using these superscripts with round brackets with parentheses to index and to different training examples  Your prediction on training sample (i) which is y-hat (i) is going to be obtained by taking the sigmoid function and applying it to W transpose X  (i) the input that the training example plus B and you can also define Z (i) as follows  Z (i) is equal to the W transpose x (i) plus b  So throughout this course  we're going to use this notational convention  that the superscript parentheses i refers to data  be it  X or Y or Z or something else associated with the i-th training example  associated with the i-th example  That's what the superscript i in parentheses means  Now  let's see what loss function or error function we can use to measure how well our algorithm is doing  One thing you could do is define the loss when your algorithm outputs y-hat and the true label as Y to be maybe the square error or one half a square error  It turns out that you could do this  but in logistic regression people don't usually do this because when you come to learn the parameters  you find that the optimization problem which we talk about later becomes non-convex  So you end up with optimization problem with multiple local optima  So gradient descent may not find the global optimum  If you didn't understand the last couple of comments  Don't worry about it  we'll get to it in a later video  But the intuition to take away is that this function L called the loss function is a function you'll need to define to measure how good our output y-hat is when the true label is y  As square error seems like it might be a reasonable choice except that it makes gradient descent not work well  So in logistic regression  we will actually define a different loss function that plays a similar role as squared error  that will give us an optimization problem that is convex and so we'll see in that later video becomes much easier to optimize  So  what we use in logistic regression is actually the following loss function which I'm just gonna write out here  is negative y log y-hat plus one minus y log  one line is y-hat  Here's some intuition for why this loss function makes sense  Keep in mind that if we're using squared error then you want the squared error to be as small as possible  And with this logistic regression loss function  we'll also want this to be as small as possible  To understand why this makes sense  let's look at the two cases  In the first case  let's say Y is equal to one then the loss function y-hat comma y is just this first term  this negative sign  So this negative log y-hat  If y is equal to one  Because if y equals one then the second term one minus Y is equal to zero  So this says if y equals one you want negative log y-hat to be as big as possible  So that means you want log y-hat to be large  to be as big as possible and that means you want y-hat to be large  But because y-hat is you know  the sigmoid function  it can never be bigger than one  So this is saying that if y is equal to one  you want y-hat to be as big as possible  But it can't ever be bigger than one so saying you want y-hat to be close to one as well  The other case is if y equals zero  If y equals zero then this first term in the loss function is equal to zero because y zero and then the second term defines the loss function  So the loss becomes negative log one minus y-hat  And so if in your learning procedure you try to make the loss function small  what this means is that you want log one minus y-hat to be large  And because it's a negative sign there and then through a similar piece of reason you can conclude that this loss function is trying to make y-hat as small as possible  And again because y-hat has to be between zero and one  This is saying that if y is equal to zero then your loss function will push the parameters to make y-hat as close to zero as possible  Now  there are a lot of functions with roughly this effect that if y is equal to one we try to make y-hat large and if Y is equal to zero we try to make y-hat small  We just gave here in green a somewhat informal justification for this particular loss function will provide an optional video later to give a more formal justification for why in logistic regression we like to use the loss function with this particular form  Finally  the loss function was defined with respect to a single training example  It measures how well you're doing on a single training example  I'm now going to define something called the cost function  which measures how well you're doing an entire training set  So the cost function J which is applied to your parameters W and B is going to be the average with one over the m of the sum of the loss function applied to each of the training examples and turn  While here y-hat is of course the prediction output by your logistic regression algorithm using you know  a particular set of parameters W and B  And so just to expand this out  this is equal to negative one over m sum from i equals one through m of the definition of the loss function  So this is y (i) Log y-hat (i) plus one minus y (i) log one line is y-hat (i)  I guess I could put square brackets here  So the minus sign is outside everything else  So the terminology I'm going to use is that the loss function is applied to just a single training example like so  And the cost function is the cost of your parameters  So in training your logistic regression model  we're going to try to find parameters W and B that minimize the overall costs function J written at the bottom  So  you've just seen the set up for the logistic regression algorithm  the loss function for training example and the overall cost function for the parameters of your algorithm  It turns out that logistic regression can be viewed as a very very small neural network  In the next video we'll go over that so you can start gaining intuition about what neural networks do  So with that let's go onto the next video about how to view logistic regression as a very small neural network 
MKrhoMK7Mn0,Gradient Descent  You've seen the logistic regression model  You've seen the loss function that measures how well you're doing on the single training example  You've also seen the cost function that measures how well your parameters w and b are doing on your entire training set  Now let's talk about how you can use the gradient descent algorithm to train  or to learn  the parameters w and b on your training set  To recap  here is the familiar logistic regression algorithm  And we have on the second line the cost function  J  which is a function of your parameters w and b  And that's defined as the average  So it's 1 over m times the sum of this loss function  And so the loss function measures how well your algorithms outputs y-hat(i) on each of the training examples stacks up or compares to the ground true label y(i) on each of the training examples  And the full formula is expanded out on the right  So the cost function measures how well your parameters w and b are doing on the training set  So in order to learn the set of parameters w and b it seems natural that we want to find w and b that make the cost function J(w  b) as small as possible  So here's an illustration of gradient descent  In this diagram the horizontal axes represent your spatial parameters  w and b  In practice  w can be much higher dimensional  but for the purposes of plotting  let's illustrate w as a single real number and b as a single real number  The cost function J(w b ) is  then  some surface above these horizontal axes w and b  So the height of the surface represents the value of J(w b) at a certain point  And what we want to do is really to find the value of w and b that corresponds to the minimum of the cost function J  It turns out that this cost function J is a convex function  So it's just a single big bowl  so this is a convex function and this is opposed to functions that look like this  which are non-convex and has lots of different local  So the fact that our cost function J(w b) as defined here is convex is one of the huge reasons why we use this particular cost function  J  for logistic regression  So to find a good value for the parameters  what we'll do is initialize w and b to some initial value  maybe denoted by that little red dot  And for logistic regression almost any initialization method works  usually you initialize the value to zero  Random initialization also works  but people don't usually do that for logistic regression  But because this function is convex  no matter where you initialize  you should get to the same point or roughly the same point  And what gradient descent does is it starts at that initial point and then takes a step in the steepest downhill direction  So after one step of gradient descent you might end up there  because it's trying to take a step downhill in the direction of steepest descent or as quickly downhill as possible  So that's one iteration of gradient descent  And after two iterations of gradient descent you might step there  three iterations and so on  I guess this is now hidden by the back of the plot until eventually  hopefully you converge to this global optimum or get to something close to the global optimum  So this picture illustrates the gradient descent algorithm  Let's write a bit more of the details  For the purpose of illustration  let's say that there's some function  J(w)  that you want to minimize  and maybe that function looks like this  To make this easier to draw  I'm going to ignore b for now  just to make this a one-dimensional plot instead of a high-dimensional plot  So gradient descent does this  we're going to repeatedly carry out the following update  Were going to take the value of w and update it  going to use colon equals to represent updating w  So set w to w minus alpha  times  and this is a derivative dJ(w)/dw  I will repeatedly do that until the algorithm converges  So couple of points in the notation  alpha here  is the learning rate  and controls how big a step we take on each iteration or gradient descent  We'll talk later about some ways by choosing the learning rate alpha  And second  this quantity here  this is a derivative  This is basically the update or the change you want to make to the parameters w  When we start to write code to implement gradient descent  we're going to use the convention that the variable name in our code dw will be used to represent this derivative term  So when you write code you write something like w colon equals w minus alpha times dw  And so we use dw to be the variable name to represent this derivative term  Now let's just make sure that this gradient descent update makes sense  Let's say that w was over here  So you're at this point on the cost function J(w)  Remember that the definition of a derivative is the slope of a function at the point  So the slope of the function is really the height divided by the width  right  of a low triangle here at this tangent to J(w) at that point  And so  here the derivative is positive  W gets updated as w minus a learning rate times the derivative  The derivative is positive and so you end up subtracting from w  so you end up taking a step to the left  And so gradient descent will make your algorithm slowly decrease the parameter if you have started off with this large value of w  As another example  if w was over here  then at this point the slope here of dJ/dw will be negative and so the gradient descent update would subtract alpha times a negative number  And so end up slowly increasing w  so you end up making w bigger and bigger with successive iterations and gradient descent  So that hopefully whether you initialize on the left or on the right gradient descent will move you towards this global minimum here  If you're not familiar with derivates or with calculus and what this term dJ(w)/dw means  don't worry too much about it  We'll talk some more about derivatives in the next video  If you have a deep knowledge of calculus  you might be able to have a deeper intuitions about how neural networks work  But even if you're not that familiar with calculus  in the next few videos we'll give you enough intuitions about derivatives and about calculus that you'll be able to effectively use neural networks  But the overall intuition for now is that this term represents the slope of the function  and we want to know the slope of the function at the current setting of the parameters so that we can take these steps of steepest descent  so that we know what direction to step in in order to go downhill on the cost function J  So we wrote our gradient descent for J(s) if only w was your parameter  In logistic regression  your cost function is a function of both w and b  So in that case  the inner loop of gradient descent  that is this thing here  this thing you have to repeat becomes as follows  You end up updating w as w minus the learning rate times the derivative of J(w b) respect to w  And you update b as b minus the learning rate times the derivative of the cost function in respect to b  So these two equations at the bottom are the actual update you implement  As an aside I just want to mention one notational convention in calculus that is a bit confusing to some people  I don't think it's super important that you understand calculus  but in case you see this I want to make sure that you don't think too much of this  Which is that in calculus  this term here  we actually write as fallows  of that funny squiggle symbol  So this symbol  this is actually just a lower case d in a fancy font  in a stylized font for when you see this expression all this means is this isn't [INAUDIBLE] J(w b) or really the slope of the function J(w b)  how much that function slopes in the w direction  And the rule of the notation in calculus  which I think isn't totally logical  but the rule in the notation for calculus  which I think just makes things much more complicated than you need to be is that if J is a function of two or more variables  then instead of using lowercase d you use this funny symbol  This is called a partial derivative symbol  But don't worry about this  and if J is a function of only one variable  then you use lowercase d  So the only difference between whether you use this funny partial derivative symbol or lowercase d as we did on top  is whether J is a function of two or more variables  In which case  you use this symbol  the partial derivative symbol  or if J is only a function of one variable then you use lower case d  This is one of those funny rules of notation in calculus that I think just make things more complicated than they need to be  But if you see this partial derivative symbol all it means is you're measure the slope of the function  with respect to one of the variables  And similarly to adhere to the formerly correct mathematical notation in calculus  because here J has two inputs not just one  This thing at the bottom should be written with this partial derivative simple  But it really means the same thing as  almost the same thing as lower case d  Finally  when you implement this in code  we're going to use the convention that this quantity  really the amount by which you update w  will denote as the variable dw in your code  And this quantity  right? The amount by which you want to update b will denote by the variable db in your code  All right  so  that's how you can implement gradient descent  Now if you haven't seen calculus for a few years  I know that that might seem like a lot more derivatives in calculus than you might be comfortable with so far  But if you're feeling that way  don't worry about it  In the next video  we'll give you better intuition about derivatives  And even without the deep mathematical understanding of calculus  with just an intuitive understanding of calculus you will be able to make neural networks work effectively  So that  let's go onto the next video where we'll talk a little bit more about derivatives 
8PKHtRE2um0,Derivatives  In this video  I want to help you gain an intuitive understanding  of calculus and the derivatives  Now  maybe you're thinking that you haven't seen calculus since your college days  and depending on when you graduated  maybe that was quite some time back  Now  if that's what you're thinking  don't worry  you don't need a deep understanding of calculus in order to apply new networks and deep learning very effectively  So  if you're watching this video or some of the later videos and you're wondering  well  is this stuff really for me  this calculus looks really complicated  My advice to you is the following  which is that  watch the videos and then if you could do the homeworks and complete the programming homeworks successfully  then you can apply deep learning  In fact  when you see later is that in week four  we'll define a couple of types of functions that will enable you to encapsulate everything that needs to be done with respect to calculus  that these functions called forward functions and backward functions that you learn about  That lets you put everything you need to know about calculus into these functions  so that you don't need to worry about them anymore beyond that  But I thought that in this foray into deep learning that this week  we should open up the box and peer a little bit further into the details of calculus  But really  all you need is an intuitive understanding of this in order to build and successfully apply these algorithms  Finally  if you are among that maybe smaller group of people that are expert in calculus  if you are very familiar with calculus derivatives  it's probably okay for you to skip this video  But for everyone else  let's dive in  and try to gain an intuitive understanding of derivatives  I plotted here the function f(a) equals 3a  So  it's just a straight line  To get intuition about derivatives  let's look at a few points on this function  Let say that a is equal to two  In that case  f of a  which is equal to three times a is equal to six  So  if a is equal to two  then f of a will be equal to six  Let's say we give the value of a just a little bit of a nudge  I'm going to just bump up a  a little bit  so that it is now 2 001  So  I'm going to give a like a tiny little nudge  to the right  So now  let's say 2 001  just plot this into scale  2 001  this 0 001 difference is too small to show on this plot  just give a little nudge to that right  Now  f(a)  is equal to three times that  So  it's 6 003  so we plot this over here  This is not to scale  this is 6 003  So  if you look at this little triangle here that I'm highlighting in green  what we see is that if I nudge a 0 001 to the right  then f of a goes up by 0 003  The amounts that f of a  went up is three times as big as the amount that I nudge the a to the right  So  we're going to say that  the slope or the derivative of the function f of a  at a equals to or when a is equals two to the slope is three  The term derivative basically means slope  it's just that derivative sounds like a scary and more intimidating word  whereas a slope is a friendlier way to describe the concept of derivative  So  whenever you hear derivative  just think slope of the function  More formally  the slope is defined as the height divided by the width of this little triangle that we have in green  So  this is 0 003 over 0 001  and the fact that the slope is equal to three or the derivative is equal to three  just represents the fact that when you nudge a to the right by 0 001  by tiny amount  the amount at f of a goes up is three times as big as the amount that you nudged it  that you nudged a in the horizontal direction  So  that's all that the slope of a line is  Now  let's look at this function at a different point  Let's say that a is now equal to five  In that case  f of a  three times a is equal to 15  So  let's see that again  give a  a nudge to the right  A tiny little nudge  it's now bumped up to 5 001  f of a is three times that  So  f of a is equal to 15 003  So  once again  when I bump a to the right  nudg a to the right by 0 001  f of a goes up three times as much  So the slope  again  at a = 5  is also three  So  the way we write this  that the slope of the function f is equal to three  We say  d f(a) da and this just means  the slope of the function f(a) when you nudge the variable a  a tiny little amount  this is equal to three  An alternative way to write this derivative formula is as follows  You can also write this as  d da of f(a)  So  whether you put f(a) on top or whether you write it down here  it doesn't matter  But all this equation means is that  if I nudge a to the right a little bit  I expect f(a) to go up by three times as much as I nudged the value of little a  Now  for this video I explained derivatives  talking about what happens if we nudged the variable a by 0 001  If you want a formal mathematical definition of the derivatives  Derivatives are defined with an even smaller value of how much you nudge a to the right  So  it's not 0 001  It's not 0 000001  It's not 0 00000000 and so on 1  It's even smaller than that  and the formal definition of derivative says  whenever you nudge a to the right by an infinitesimal amount  basically an infinitely tiny  tiny amount  If you do that  this f(a) go up three times as much as whatever was the tiny  tiny  tiny amount that you nudged a to the right  So  that's actually the formal definition of a derivative  But for the purposes of our intuitive understanding  which I'll talk about nudging a to the right by this small amount 0 001  Even if it's 0 001 isn't exactly tiny  tiny infinitesimal  Now  one property of the derivative is that  no matter where you take the slope of this function  it is equal to three  whether a is equal to two or a is equal to five  The slope of this function is equal to three  meaning that whatever is the value of a  if you increase it by 0 001  the value of f of a goes up by three times as much  So  this function has a safe slope everywhere  One way to see that is that  wherever you draw this little triangle  The height  divided by the width  always has a ratio of three to one  So  I hope this gives you a sense of what the slope or the derivative of a function means for a straight line  where in this example the slope of the function was three everywhere  In the next video  let's take a look at a slightly more complex example  where the slope to the function can be different at different points on the function 
BnIlVgZbqV0,More Derivative Examples  In this video  I'll show you a slightly more complex example where the slope of the function can be different to different points in the function  Let's start with an example  You have plotted the function f(a) = aÂ²  Let's take a look at the point a=2  So aÂ² or f(a) = 4  Let's nudge a slightly to the right  so now a=2 001  f(a) which is aÂ² is going to be approximately 4 004  It turns out that the exact value  you call the calculator and figured this out is actually 4 004001  I'm just going to say 4 004 is close enough  So what this means is that when a=2  let's draw this on the plot  So what we're saying is that if a=2  then f(a) = 4 and here is the x and y axis are not drawn to scale  Technically  does vertical height should be much higher than this horizontal height so the x and y axis are not on the same scale  But if I now nudge a to 2 001 then f(a) becomes roughly 4 004  So if we draw this little triangle again  what this means is that if I nudge a to the right by 0 001  f(a) goes up four times as much by 0 004  So in the language of calculus  we say that a slope that is the derivative of f(a) at a=2 is 4 or to write this out of our calculus notation  we say that d/da of f(a) = 4 when a=2  Now one thing about this function f(a) = aÂ² is that the slope is different for different values of a  This is different than the example we saw on the previous slide  So let's look at a different point  If a=5  so instead of a=2  and now a=5 then aÂ²=25  so that's f(a)  If I nudge a to the right again  it's tiny little nudge to a  so now a=5 001 then f(a) will be approximately 25 010  So what we see is that by nudging a up by  001  f(a) goes up ten times as much  So we have that d/da f(a) = 10 when a=5 because f(a) goes up ten times as much as a does when I make a tiny little nudge to a  So one way to see why did derivatives is different at different points is that if you draw that little triangle right at different locations on this  you'll see that the ratio of the height of the triangle over the width of the triangle is very different at different points on the curve  So here  the slope=4 when a=2  a=10  when a=5  Now if you pull up a calculus textbook  a calculus textbook will tell you that d/da of f(a)  so f(a) = aÂ²  so that's d/da of aÂ²  One of the formulas you find are the calculus textbook is that this thing  the slope of the function aÂ²  is equal to 2a  Not going to prove this  but the way you find this out is that you open up a calculus textbook to the table formulas and they'll tell you that derivative of 2 of aÂ² is 2a  And indeed  this is consistent with what we've worked out  Namely  when a=2  the slope of function to a is 2x2=4  And when a=5 then the slope of the function 2xa is 2x5=10  So  if you ever pull up a calculus textbook and you see this formula  that the derivative of aÂ²=2a  all that means is that for any given value of a  if you nudge upward by 0 001 already your tiny little value  you will expect f(a) to go up by 2a  That is the slope or the derivative times other much you had nudged to the right the value of a  Now one tiny little detail  I use these approximate symbols here and this wasn't exactly 4 004  there's an extra  001 hanging out there  It turns out that this extra  001  this little thing here is because we were nudging a to the right by 0 001  if we're instead nudging it to the right by this infinitesimally small value then this extra every term will go away and you find that the amount that f(a) goes out is exactly equal to the derivative times the amount that you nudge a to the right  And the reason why is not 4 004 exactly is because derivatives are defined using this infinitesimally small nudges to a rather than 0 001 which is not  And while 0 001 is small  it's not infinitesimally small  So that's why the amount that f(a) went up isn't exactly given by the formula but it's only a kind of approximately given by the derivative  To wrap up this video  let's just go through a few more quick examples  The example you've already seen is that if f(a) = aÂ² then the calculus textbooks formula table will tell you that the derivative is equal to 2a  And so the example we went through was it if (a) = 2  f(a) = 4  and we nudge a  since it's a little bit bigger than f(a) is about 4 004 and so f(a) went up four times as much and indeed when a=2  the derivatives is equal to 4  Let's look at some other examples  Let's say  instead the f(a) = aÂ³  If you go to a calculus textbook and look up the table of formulas  you see that the slope of this function  again  the derivative of this function is equal to 3aÂ²  So you can get this formula out of the calculus textbook  So what this means? So the way to interpret this is as follows  Let's take a=2 as an example again  So f(a) or aÂ³=8  that's two to the power of three  So we give a a tiny little nudge  you find that f(a) is about 8 012 and feel free to check this  Take 2 001 to the power of three  you find this is very close to 8 012  And indeed  when a=2 that's 3x2Â² does equal to 3x4  you see that's 12  So the derivative formula predicts that if you nudge a to the right by tiny little bit  f(a) should go up 12 times as much  And indeed  this is true when a went up by  001  f(a) went up 12 times as much by  012  Just one last example and then we'll wrap up  Let's say that f(a) is equal to the log function  So on the right log of a  I'm going to use this as the base e logarithm  So some people write that as log(a)  So if you go to calculus textbook  you find that when you take the derivative of log(a)  So this is a function that just looks like that  the slope of this function is given by 1/a  So the way to interpret this is that if a has any value then let's just keep using a=2 as an example and you nudge a to the right by  001  you would expect f(a) to go up by 1/a that is by the derivative times the amount that you increase a  So in fact  if you pull up a calculator  you find that if a=2  f(a) is about 0 69315 and if you increase f and if you increase a to 2 001 then f(a) is about 0 69365  this has gone up by 0 0005  And indeed  if you look at the formula for the derivative when a=2  d/da f(a) = 1/2  So this derivative formula predicts that if you pump up a by  001  you would expect f(a) to go up by only 1/2 as much and 1/2 of  001 is 0 0005 which is exactly what we got  Then when a goes up by  001  going from a=2 to a=2 001  f(a) goes up by half as much  So  the answers are going up by approximately  0005  So if we draw that little triangle if you will is that if on the horizontal axis just goes up by  001 on the vertical axis  log(a) goes up by half of that so  0005  And so that 1/a or 1/2 in this case  1a=2 that's just the slope of this line when a=2  So that's it for derivatives  There are just two take home messages from this video  First is that the derivative of the function just means the slope of a function and the slope of a function can be different at different points on the function  In our first example where f(a) = 3a those a straight line  The derivative was the same everywhere  it was three everywhere  For other functions like f(a) = aÂ² or f(a) = log(a)  the slope of the line varies  So  the slope or the derivative can be different at different points on the curve  So that's a first take away  Derivative just means slope of a line  Second takeaway is that if you want to look up the derivative of a function  you can flip open your calculus textbook or look up Wikipedia and often get a formula for the slope of these functions at different points  So that  I hope you have an intuitive understanding of derivatives or slopes of lines  Let's go into the next video  We'll start to talk about the computation graph and how to use that to compute derivatives of more complex functions 
mMK-TlvH5c4,Computation graph  You've heard me say that the computations of a neural network are organized in terms of a forward pass or a forward propagation step  in which we compute the output of the neural network  followed by a backward pass or back propagation step  which we use to compute gradients or compute derivatives  The computation graph explains why it is organized this way  In this video  we'll go through an example  In order to illustrate the computation graph  let's use a simpler example than logistic regression or a full blown neural network  Let's say that we're trying to compute a function  J  which is a function of three variables a  b  and c and let's say that function is 3(a+bc)  Computing this function actually has three distinct steps  The first is you need to compute what is bc and let's say we store that in the variable call u  So u=bc and then you my compute V=a *u  So let's say this is V  And then finally  your output J is 3V  So this is your final function J that you're trying to compute  We can take these three steps and draw them in a computation graph as follows  Let's say  I draw your three variables a  b  and c here  So the first thing we did was compute u=bc  So I'm going to put a rectangular box around that  And so the input to that are b and c  And then  you might have V=a+u  So the inputs to that are V  So the inputs to that are u with just computed together with a  And then finally  we have J=3V  So as a concrete example  if a=5  b=3 and c=2 then u=bc would be six because a+u would be 5+6 is 11   J is three times that  so J=33  And indeed  hopefully you can verify that this is three times five plus three times two  And if you expand that out  you actually get 33 as the value of J  So  the computation graph comes in handy when there is some distinguished or some special output variable  such as J in this case  that you want to optimize  And in the case of a logistic regression  J is of course the cost function that we're trying to minimize  And what we're seeing in this little example is that  through a left-to-right pass  you can compute the value of J and what we'll see in the next couple of slides is that in order to compute derivatives there'll be a right-to-left pass like this  kind of going in the opposite direction as the blue arrows  That would be most natural for computing the derivatives  So to recap  the computation graph organizes a computation with this blue arrow  left-to-right computation  Let's refer to the next video how you can do the backward red arrow right-to-left computation of the derivatives  Let's go on to the next video 
eskFflL7Ny0,Derivatives with a Computation Graph  In the last video  we worked through an example of using a computation graph to compute a function J  Now  let's take a clean diversion of that computation graph  And show how you can use it to figure out derivative calculations for that function J  So here's a computation graph  Let's say you want to compute the derivative of J with respect to v  So what is that? Well  this says  if we were to take this value of v and change it a little bit  how would the value of J change? Well  J is defined as 3 times v  And right now  v = 11  So if we're to bump up v by a little bit to 11 001  then J  which is 3v  so currently 33  will get bumped up to 33 003  So here  we've increased v by 0 001  And the net result of that is that J goes out 3 times as much  So the derivative of J with respect to v is equal to 3  Because the increase in J is 3 times the increase in v  And in fact  this is very analogous to the example we had in the previous video  where we had f(a) = 3a  And so we then derived that df/da  which with slightly simplified  a slightly sloppy notation  you can write as df/da = 3  So instead  here we have J = 3v  and so dJ/dv = 3  With here  J playing the role of f  and v playing the role of a in this previous example that we had from an earlier video  So indeed  terminology of backpropagation  what we're seeing is that if you want to compute the derivative of this final output variable  which usually is a variable you care most about  with respect to v  then we've done one step of backpropagation  So we call it one step backwards in this graph  Now let's look at another example  What is dJ/da? In other words  if we bump up the value of a  how does that affect the value of J? Well  let's go through the example  where now a = 5  So let's bump it up to 5 001  The net impact of that is that v  which was a + u  so that was previously 11  This would get increased to 11 001  And then we've already seen as above that J now gets bumped up to 33 003  So what we're seeing is that if you increase a by 0 001  J increases by 0 003  And by increase a  I mean  you have to take this value of 5 and just plug in a new value  Then the change to a will propagate to the right of the computation graph so that J ends up being 33 003  And so the increase to J is 3 times the increase to a  So that means this derivative is equal to 3  And one way to break this down is to say that if you change a  then that will change v  And through changing v  that would change J  And so the net change to the value of J when you bump up the value  when you nudge the value of a up a little bit  is that  First  by changing a  you end up increasing v  Well  how much does v increase? It is increased by an amount that's determined by dv/da  And then the change in v will cause the value of J to also increase  So in calculus  this is actually called the chain rule that if a affects v  affects J  then the amounts that J changes when you nudge a is the product of how much v changes when you nudge a times how much J changes when you nudge v  So in calculus  again  this is called the chain rule  And what we saw from this calculation is that if you increase a by 0 001  v changes by the same amount  So dv/da = 1  So in fact  if you plug in what we have wrapped up previously  dv/dJ = 3 and dv/da = 1  So the product of these 3 times 1  that actually gives you the correct value that dJ/da = 3  So this little illustration shows hows by having computed dJ/dv  that is  derivative with respect to this variable  it can then help you to compute dJ/da  And so that's another step of this backward calculation  I just want to introduce one more new notational convention  Which is that when you're witting codes to implement backpropagation  there will usually be some final output variable that you really care about  So a final output variable that you really care about or that you want to optimize  And in this case  this final output variable is J  It's really the last node in your computation graph  And so a lot of computations will be trying to compute the derivative of that final output variable  So d of this final output variable with respect to some other variable  Then we just call that dvar  So a lot of the computations you have will be to compute the derivative of the final output variable  J in this case  with various intermediate variables  such as a  b  c  u or v  And when you implement this in software  what do you call this variable name? One thing you could do is in Python  you could give us a very long variable name like dFinalOurputVar/dvar  But that's a very long variable name  You could call this  I guess  dJdvar  But because you're always taking derivatives with respect to dJ  with respect to this final output variable  I'm going to introduce a new notation  Where  in code  when you're computing this thing in the code you write  we're just going to use the variable name dvar in order to represent that quantity  So dvar in a code you write will represent the derivative of the final output variable you care about such as J  Well  sometimes  the last l with respect to the various intermediate quantities you're computing in your code  So this thing here in your code  you use dv to denote this value  So dv would be equal to 3  And your code  you represent this as da  which is we also figured out to be equal to 3  So we've done backpropagation partially through this computation graph  Let's go through the rest of this example on the next slide  So let's go to a cleaned up copy of the computation graph  And just to recap  what we've done so far is go backward here and figured out that dv = 3  And again  the definition of dv  that's just a variable name  where the code is really dJ/dv  We've figured out that da = 3  And again  da is the variable name in your code and that's really the value dJ/da  And we hand wave how we've gone backwards on these two edges like so  Now let's keep computing derivatives  Now let's look at the value u  So what is dJ/du? Well  through a similar calculation as what we did before and then we start off with u = 6  If you bump up u to 6 001  then v  which is previously 11  goes up to 11 001  And so J goes from 33 to 33 003  And so the increase in J is 3x  so this is equal  And the analysis for u is very similar to the analysis we did for a  This is actually computed as dJ/dv times dv/du  where this we had already figured out was 3  And this turns out to be equal to 1  So we've gone up one more step of backpropagation  We end up computing that du is also equal to 3  And du is  of course  just this dJ/du  Now we just step through one last example in detail  So what is dJ/db? So here  imagine if you are allowed to change the value of b  And you want to tweak b a little bit in order to minimize or maximize the value of J  So what is the derivative or what's the slope of this function J when you change the value of b a little bit? It turns out that using the chain rule for calculus  this can be written as the product of two things  This dJ/du times du/db  And the reasoning is if you change b a little bit  so b = 3 to  say  3 001  The way that it will affect J is it will first affect u  So how much does it affect u? Well  u is defined as b times c  So this will go from 6  when b = 3  to now 6 002 because c = 2 in our example here  And so this tells us that du/db = 2  Because when you bump up b by 0 001  u increases twice as much  So du/db  this is equal to 2  And now  we know that u has gone up twice as much as b has gone up  Well  what is dJ/du? We've already figured out that this is equal to 3  And so by multiplying these two out  we find that dJ/db = 6  And again  here's the reasoning for the second part of the argument  Which is we want to know when u goes up by 0 002  how does that affect J? The fact that dJ/du = 3  that tells us that when u goes up by 0 002  J goes up 3 times as much  So J should go up by 0 006  So this comes from the fact that dJ/du = 3  And if you check the math in detail  you will find that if b becomes 3 001  then u becomes 6 002  v becomes 11 002  So that's a + u  so that's 5 + u  And then J  which is equal to 3 times v  that ends up being equal to 33 006  And so that's how you get that dJ/db = 6  And to fill that in  this is if we go backwards  so this is db = 6  And db really is the Python code variable name for dJ/db  And I won't go through the last example in great detail  But it turns out that if you also compute out dJ  this turns out to be dJ/du times du  And this turns out to be 9  this turns out to be 3 times 3  I won't go through that example in detail  So through this last step  it is possible to derive that dc is equal to  So the key takeaway from this video  from this example  is that when computing derivatives and computing all of these derivatives  the most efficient way to do so is through a right to left computation following the direction of the red arrows  And in particular  we'll first compute the derivative with respect to v  And then that becomes useful for computing the derivative with respect to a and the derivative with respect to u  And then the derivative with respect to u  for example  this term over here and this term over here  Those in turn become useful for computing the derivative with respect to b and the derivative with respect to c  So that was the computation graph and how does a forward or left to right calculation to compute the cost function such as J that you might want to optimize  And a backwards or a right to left calculation to compute derivatives  If you're not familiar with calculus or the chain rule  I know some of those details  but they've gone by really quickly  But if you didn't follow all the details  don't worry about it  In the next video  we'll go over this again in the context of logistic regression  And show you exactly what you need to do in order to implement the computations you need to compute the derivatives of the logistic regression model 
z2yqmHClVO8,Logistic Regression Gradient Descent  Welcome back  In this video  we'll talk about how to compute derivatives for you to implement gradient descent for logistic regression  The key takeaways will be what you need to implement  That is  the key equations you need in order to implement gradient descent for logistic regression  In this video  I want to do this computation using the computation graph  I have to admit  using the computation graph is a little bit of an overkill for deriving gradient descent for logistic regression  but I want to start explaining things this way to get you familiar with these ideas so that  hopefully  it will make a bit more sense when we talk about full-fledged neural networks  To that  let's dive into gradient descent for logistic regression  To recap  we had set up logistic regression as follows  your predictions  Y_hat  is defined as follows  where z is that  If we focus on just one example for now  then the loss  or respect to that one example  is defined as follows  where A is the output of logistic regression  and Y is the ground truth label  Let's write this out as a computation graph and for this example  let's say we have only two features  X1 and X2  In order to compute Z  we'll need to input W1  W2  and B  in addition to the feature values X1  X2  These things  in a computational graph  get used to compute Z  which is W1  X1 + W2 X2 + B  rectangular box around that  Then  we compute Y_hat  or A = Sigma_of_Z  that's the next step in the computation graph  and then  finally  we compute L  AY  and I won't copy the formula again  In logistic regression  what we want to do is to modify the parameters  W and B  in order to reduce this loss  We've described the four propagation steps of how you actually compute the loss on a single training example  now let's talk about how you can go backwards to compute the derivatives  Here's a cleaned-up version of the diagram  Because what we want to do is compute derivatives with respect to this loss  the first thing we want to do when going backwards is to compute the derivative of this loss with respect to  the script over there  with respect to this variable A  So  in the code  you just use DA to denote this variable  It turns out that if you are familiar with calculus  you could show that this ends up being -Y_over_A+1-Y_over_1-A  And the way you do that is you take the formula for the loss and  if you're familiar with calculus  you can compute the derivative with respect to the variable  lowercase A  and you get this formula  But if you're not familiar with calculus  don't worry about it  We'll provide the derivative form  what else you need  throughout this course  If you are an expert in calculus  I encourage you to look up the formula for the loss from their previous slide and try taking derivative with respect to A using calculus  but if you don't know enough calculus to do that  don't worry about it  Now  having computed this quantity of DA and the derivative or your final alpha variable with respect to A  you can then go backwards  It turns out that you can show DZ which  this is the part called variable name  this is going to be the derivative of the loss  versus back to Z  or for L  you could really write the loss including A and Y explicitly as parameters or not  right? Either type of notation is equally acceptable  We can show that this is equal to A-Y  Just a couple of comments only for those of you experts in calculus  if you're not expert in calculus  don't worry about it  But it turns out that this  DL DZ  this can be expressed as DL_DA_times_DA_DZ  and it turns out that DA DZ  this turns out to be A_times_1-A  and DL DA we have previously worked out over here  if you take these two quantities  DL DA  which is this term  together with DA DZ  which is this term  and just take these two things and multiply them  You can show that the equation simplifies to A-Y  That's how you derive it  and that this is really the chain rule that have briefly eluded to the form  Feel free to go through that calculation yourself if you are knowledgeable in calculus  but if you aren't  all you need to know is that you can compute DZ as A-Y and we've already done that calculus for you  Then  the final step in that computation is to go back to compute how much you need to change W and B  In particular  you can show that the derivative with respect to W1 and in quotes  call this DW1  that this is equal to X1_times_DZ  Then  similarly  DW2  which is how much you want to change W2  is X2_times_DZ and B  excuse me  DB is equal to DZ  If you want to do gradient descent with respect to just this one example  what you would do is the following  you would use this formula to compute DZ  and then use these formulas to compute DW1  DW2  and DB  and then you perform these updates  W1 gets updated as W1 minus  learning rate alpha  times DW1  W2 gets updated similarly  and B gets set as B minus the learning rate times DB  And so  this will be one step of grade with respect to a single example  You see in how to compute derivatives and implement gradient descent for logistic regression with respect to a single training example  But training logistic regression model  you have not just one training example given training sets of M training examples  In the next video  let's see how you can take these ideas and apply them to learning  not just from one example  but from an entire training set 
frDyLjZva9M,Gradient Descent on m Examples  In a previous video  you saw how to compute derivatives and implement gradient descent with respect to just one training example for logistic regression  Now  we want to do it for m training examples  To get started  let's remind ourselves of the definition of the cost function J  Cost- function w b which you care about is this average  one over m sum from i equals one through m of the loss when you algorithm output a_i on the example y  where a_i is the prediction on the ith training example which is sigma of z_i  which is equal to sigma of w transpose x_i plus b  So  what we show in the previous slide is for any single training example  how to compute the derivatives when you have just one training example  So dw_1  dw_2 and d_b  with now the superscript i to denote the corresponding values you get if you were doing what we did on the previous slide  but just using the one training example  x_i y_i  excuse me  missing an i there as well  So  now you notice the overall cost functions as a sum was really average  because the one over m term of the individual losses  So  it turns out that the derivative  respect to w_1 of the overall cost function is also going to be the average of derivatives respect to w_1 of the individual lost terms  But previously  we have already shown how to compute this term as dw_1_i  which we  on the previous slide  show how to compute this on a single training example  So  what you need to do is really compute these derivatives as we showed on the previous training example and average them  and this will give you the overall gradient that you can use to implement the gradient descent  So  I know that was a lot of details  but let's take all of this up and wrap this up into a concrete algorithm until when you should implement logistic regression with gradient descent working  So  here's what you can do  let's initialize j equals zero  dw_1 equals zero  dw_2 equals zero  d_b equals zero  What we're going to do is use a for loop over the training set  and compute the derivative with respect to each training example and then add them up  So  here's how we do it  for i equals one through m  so m is the number of training examples  we compute z_i equals w transpose x_i plus b  The prediction a_i is equal to sigma of z_i  and then let's add up J  J plus equals y_i log a_i plus one minus y_i log one minus a_i  and then put the negative sign in front of the whole thing  and then as we saw earlier  we have dz_i  that's equal to a_i minus y_i  and d_w gets plus equals x1_i dz_i  dw_2 plus equals xi_2 dz_i  and I'm doing this calculation assuming that you have just two features  so that n equals to two otherwise  you do this for dw_1  dw_2  dw_3 and so on  and then db plus equals dz_i  and I guess that's the end of the for loop  Then finally  having done this for all m training examples  you will still need to divide by m because we're computing averages  So  dw_1 divide equals m  dw_2 divides equals m  db divide equals m  in order to compute averages  So  with all of these calculations  you've just computed the derivatives of the cost function J with respect to each your parameters w_1  w_2 and b  Just a couple of details about what we're doing  we're using dw_1 and dw_2 and db as accumulators  so that after this computation  dw_1 is equal to the derivative of your overall cost function with respect to w_1 and similarly for dw_2 and db  So  notice that dw_1 and dw_2 do not have a superscript i  because we're using them in this code as accumulators to sum over the entire training set  Whereas in contrast  dz_i here  this was dz with respect to just one single training example  So  that's why that had a superscript i to refer to the one training example  i that is computerised  So  having finished all these calculations  to implement one step of gradient descent  you will implement w_1  gets updated as w_1 minus the learning rate times dw_1  w_2  ends up this as w_2 minus learning rate times dw_2  and b gets updated as b minus the learning rate times db  where dw_1  dw_2 and db were as computed  Finally  J here will also be a correct value for your cost function  So  everything on the slide implements just one single step of gradient descent  and so you have to repeat everything on this slide multiple times in order to take multiple steps of gradient descent  In case these details seem too complicated  again  don't worry too much about it for now  hopefully all this will be clearer when you go and implement this in the programming assignments  But it turns out there are two weaknesses with the calculation as we've implemented it here  which is that  to implement logistic regression this way  you need to write two for loops  The first for loop is this for loop over the m training examples  and the second for loop is a for loop over all the features over here  So  in this example  we just had two features  so  n is equal to two and x equals two  but maybe we have more features  you end up writing here dw_1 dw_2  and you similar computations for dw_t  and so on delta dw_n  So  it seems like you need to have a for loop over the features  over n features  When you're implementing deep learning algorithms  you find that having explicit for loops in your code makes your algorithm run less efficiency  So  in the deep learning era  we would move to a bigger and bigger datasets  and so being able to implement your algorithms without using explicit for loops is really important and will help you to scale to much bigger datasets  So  it turns out that there are a set of techniques called vectorization techniques that allow you to get rid of these explicit for-loops in your code  I think in the pre-deep learning era  that's before the rise of deep learning  vectorization was a nice to have  so you could sometimes do it to speed up your code and sometimes not  But in the deep learning era  vectorization  that is getting rid of for loops  like this and like this  has become really important  because we're more and more training on very large datasets  and so you really need your code to be very efficient  So  in the next few videos  we'll talk about vectorization and how to implement all this without using even a single for loop  So  with this  I hope you have a sense of how to implement logistic regression or gradient descent for logistic regression  Things will be clearer when you implement the programming exercise  But before actually doing the programming exercise  let's first talk about vectorization so that you can implement this whole thing  implement a single iteration of gradient descent without using any for loops 
Rzl13ESnVWY,"Vectorization  >> Welcome back  Vectorization is basically the art of getting rid of explicit folders in your code  In the deep learning era safety in deep learning in practice  you often find yourself training on relatively large data sets  because that's when deep learning algorithms tend to shine  And so  it's important that your code very quickly because otherwise  if it's running on a big data set  your code might take a long time to run then you just find yourself waiting a very long time to get the result  So in the deep learning era  I think the ability to perform vectorization has become a key skill  Let's start with an example  So  what is Vectorization? In logistic regression you need to compute Z equals W transpose X plus B  where W was this column vector and X is also this vector  Maybe there are very large vectors if you have a lot of features  So  W and X were both these R and no R  NX dimensional vectors  So  to compute W transpose X  if you had a non-vectorized implementation  you would do something like Z equals zero  And then for I in range of X  So  for I equals 1  2 NX  Z plus equals W I times XI  And then maybe you do Z plus equal B at the end  So  that's a non-vectorized implementation  Then you find that that's going to be really slow  In contrast  a vectorized implementation would just compute W transpose X directly  In Python or a numpy  the command you use for that is Z equals np W  X  so this computes W transpose X  And you can also just add B to that directly  And you find that this is much faster  Let's actually illustrate this with a little demo  So  here's my Jupiter notebook in which I'm going to write some Python code  So  first  let me import the numpy library to import  Send P  And so  for example  I can create A as an array as follows  Let's say print A  Now  having written this chunk of code  if I hit shift enter  then it executes the code  So  it created the array A and it prints it out  Now  let's do the Vectorization demo  I'm going to import the time libraries  since we use that  in order to time how long different operations take  Can they create an array A? Those random thought round  This creates a million dimensional array with random values  b = np random rand  Another million dimensional array  And  now  tic=time time  so this measure the current time  c = np dot (a  b)  toc = time time  And this print  it is the vectorized version  It's a vectorize version  And so  let's print out  Let's see the last time  so there's toc - tic x 1000  so that we can express this in milliseconds  So  ms is milliseconds  I'm going to hit Shift Enter  So  that code took about three milliseconds or this time 1 5  maybe about 1 5 or 3 5 milliseconds at a time  It varies a little bit as I run it  but looks like maybe on average it's taking like 1 5 milliseconds  maybe two milliseconds as I run this  All right  Let's keep adding to this block of code  That's not implementing non-vectorize version  Let's see  c = 0  then tic = time time  Now  let's implement a folder  For I in range of 1 million  I'll pick out the number of zeros right  C += (a i) x (b  i)  and then toc = time time  Finally  print more than explicit full loop  The time it takes is this 1000 x toc - tic + \""ms\"" to know that we're doing this in milliseconds  Let's do one more thing  Let's just print out the value of C we compute it to make sure that it's the same value in both cases  I'm going to hit shift enter to run this and check that out  In both cases  the vectorize version and the non-vectorize version computed the same values  as you know  2 50 to 6 99  so on  The vectorize version took 1 5 milliseconds  The explicit for loop and non-vectorize version took about 400  almost 500 milliseconds  The non-vectorize version took something like 300 times longer than the vectorize version  With this example you see that if only you remember to vectorize your code  your code actually runs over 300 times faster  Let's just run it again  Just run it again  Yeah  Vectorize version 1 5 milliseconds seconds and the four loop  So 481 milliseconds  again  about 300 times slower to do the explicit four loop  If the engine x slows down  it's the difference between your code taking maybe one minute to run versus taking say five hours to run  And when you are implementing deep learning algorithms  you can really get a result back faster  It will be much faster if you vectorize your code  Some of you might have heard that a lot of scaleable deep learning implementations are done on a GPU or a graphics processing unit  But all the demos I did just now in the Jupiter notebook where actually on the CPU  And it turns out that both GPU and CPU have parallelization instructions  They're sometimes called SIMD instructions  This stands for a single instruction multiple data  But what this basically means is that  if you use built-in functions such as this np function or other functions that don't require you explicitly implementing a for loop  It enables Phyton Pi to take much better advantage of parallelism to do your computations much faster  And this is true both computations on CPUs and computations on GPUs  It's just that GPUs are remarkably good at these SIMD calculations but CPU is actually also not too bad at that  Maybe just not as good as GPUs  You're seeing how vectorization can significantly speed up your code  The rule of thumb to remember is whenever possible  avoid using explicit four loops  Let's go onto the next video to see some more examples of vectorization and also start to vectorize logistic regression "
cJ7hmwHVwtc,More Vectorization Examples  In the previous video you saw a few examples of how vectorization  by using built in functions and by avoiding explicit for loops  allows you to speed up your code significantly  Let's look at a few more examples  The rule of thumb to keep in mind is  when you're programming your new networks  or when you're programming just a regression  whenever possible avoid explicit for-loops  And it's not always possible to never use a for-loop  but when you can use a built in function or find some other way to compute whatever you need  you'll often go faster than if you have an explicit for-loop  Let's look at another example  If ever you want to compute a vector u as the product of the matrix A  and another vector v  then the definition of our matrix multiply is that your Ui is equal to sum over j   Aij  Vj  That's how you define Ui  And so the non-vectorized implementation of this would be to set u equals NP zeros  it would be n by 1  For i  and so on  For j  and so on   And then u[i] plus equals a[i][j] times v[j]  So now  this is two for-loops  looping over both i and j  So  that's a non-vectorized version  the vectorized implementation which is to say u equals np dot (A v)  And the implementation on the right  the vectorized version  now eliminates two different for-loops  and it's going to be way faster  Let's go through one more example  Let's say you already have a vector  v  in memory and you want to apply the exponential operation on every element of this vector v  So you can put u equals the vector  that's e to the v1  e to the v2  and so on  down to e to the vn  So this would be a non-vectorized implementation  which is at first you initialize u to the vector of zeros  And then you have a for-loop that computes the elements one at a time  But it turns out that Python and NumPy have many built-in functions that allow you to compute these vectors with just a single call to a single function  So what I would do to implement this is import numpy as np  and then what you just call u = np exp(v)  And so  notice that  whereas previously you had that explicit for-loop  with just one line of code here  just v as an input vector u as an output vector  you've gotten rid of the explicit for-loop  and the implementation on the right will be much faster that the one needing an explicit for-loop  In fact  the NumPy library has many of the vector value functions  So np log (v) will compute the element-wise log  np abs computes the absolute value  np maximum computes the element-wise maximum to take the max of every element of v with 0  v**2 just takes the element-wise square of each element of v  One over v takes the element-wise inverse  and so on  So  whenever you are tempted to write a for-loop take a look  and see if there's a way to call a NumPy built-in function to do it without that for-loop  So  let's take all of these learnings and apply it to our logisti regression gradient descent implementation  and see if we can at least get rid of one of the two for-loops we had  So here's our code for computing the derivatives for logistic regression  and we had two for-loops  One was this one up here  and the second one was this one  So in our example we had nx equals 2  but if you had more features than just 2 features then you'd need have a for-loop over dw1  dw2  dw3  and so on  So its as if there's actually a 4j equals 1  2  and x  dWj gets updated  So we'd like to eliminate this second for-loop  That's what we'll do on this slide  So the way we'll do so is that instead of explicitly initializing dw1  dw2  and so on to zeros  we're going to get rid of this and instead make dw a vector  So we're going to set dw equals np zeros  and let's make this a nx by 1  dimensional vector  Then  here  instead of this for loop over the individual components  we'll just use this vector value operation  dw plus equals xi times dz(i)  And then finally  instead of this  we will just have dw divides equals m  So now we've gone from having two for-loops to just one for-loop  We still have this one for-loop that loops over the individual training examples  So I hope this video gave you a sense of vectorization  And by getting rid of one for-loop your code will already run faster  But it turns out we could do even better  So the next video will talk about how to vectorize logistic aggression even further  And you see a pretty surprising result  that without using any for-loops  without needing a for-loop over the training examples  you could write code to process the entire training sets  So  pretty much all at the same time  So  let's see that in the next video 
wkYnrTADrWI,Vectorizing Logistic Regression  We have talked about how vectorization lets you speed up your code significantly  In this video  we'll talk about how you can vectorize the implementation of logistic regression  so they can process an entire training set  that is implement a single elevation of grading descent with respect to an entire training set without using even a single explicit for loop  I'm super excited about this technique  and when we talk about neural networks later without using even a single explicit for loop  Let's get started  Let's first examine the four propagation steps of logistic regression  So  if you have M training examples  then to make a prediction on the first example  you need to compute that  compute Z  I'm using this familiar formula  then compute the activations  you compute [inaudible] in the first example  Then to make a prediction on the second training example  you need to compute that  Then  to make a prediction on the third example  you need to compute that  and so on  And you might need to do this M times  if you have M training examples  So  it turns out  that in order to carry out the four propagation step  that is to compute these predictions on our M training examples  there is a way to do so  without needing an explicit for loop  Let's see how you can do it  First  remember that we defined a matrix capital X to be your training inputs  stacked together in different columns like this  So  this is a matrix  that is a NX by M matrix  So  I'm writing this as a Python draw pie shape  this just means that X is a NX by M dimensional matrix  Now  the first thing I want to do is show how you can compute Z1  Z2  Z3 and so on  all in one step  in fact  with one line of code  So  I'm going to construct a 1 by M matrix that's really a row vector while I'm going to compute Z1  Z2  and so on  down to ZM  all at the same time  It turns out that this can be expressed as W transpose to capital matrix X plus and then this vector B  B and so on  B  where this thing  this B  B  B  B  B thing is a 1xM vector or 1xM matrix or that is as a M dimensional row vector  So hopefully there you are with matrix multiplication  You might see that W transpose X1  X2 and so on to XM  that W transpose can be a row vector  So this W transpose will be a row vector like that  And so this first term will evaluate to W transpose X1  W transpose X2 and so on  dot  dot  dot  W transpose XM  and then we add this second term B  B  B  and so on  you end up adding B to each element  So you end up with another 1xM vector  Well that's the first element  that's the second element and so on  and that's the nth element  And if you refer to the definitions above  this first element is exactly the definition of Z1  The second element is exactly the definition of Z2 and so on  So just as X was once obtained  when you took your training examples and stacked them next to each other  stacked them horizontally  I'm going to define capital Z to be this where you take the lowercase Z's and stack them horizontally  So when you stack the lower case X's corresponding to a different training examples  horizontally you get this variable capital X and the same way when you take these lowercase Z variables  and stack them horizontally  you get this variable capital Z  And it turns out  that in order to implement this  the non-pie command is capital Z equals NP dot W dot T  that's W transpose X and then plus B  Now there is a subtlety in Python  which is at here B is a real number or if you want to say you know 1x1 matrix  is just a normal real number  But  when you add this vector to this real number  Python automatically takes this real number B and expands it out to this 1XM row vector  So in case this operation seems a little bit mysterious  this is called broadcasting in Python  and you don't have to worry about it for now  we'll talk about it some more in the next video  But the takeaway is that with just one line of code  with this line of code  you can calculate capital Z and capital Z is going to be a 1XM matrix that contains all of the lower cases Z's  Lowercase Z1 through lower case ZM  So that was Z  how about these values A  What we like to do next  is find a way to compute A1  A2 and so on to AM  all at the same time  and just as stacking lowercase X's resulted in capital X and stacking horizontally lowercase Z's resulted in capital Z  stacking lower case A  is going to result in a new variable  which we are going to define as capital A  And in the program assignment  you see how to implement a vector valued sigmoid function  so that the sigmoid function  inputs this capital Z as a variable and very efficiently outputs capital A  So you see the details of that in the programming assignment  So just to recap  what we've seen on this slide is that instead of needing to loop over M training examples to compute lowercase Z and lowercase A  one of the time  you can implement this one line of code  to compute all these Z's at the same time  And then  this one line of code  with appropriate implementation of lowercase Sigma to compute all the lowercase A's all at the same time  So this is how you implement a vectorize implementation of the four propagation for all M training examples at the same time  So to summarize  you've just seen how you can use vectorization to very efficiently compute all of the activations  all the lowercase A's at the same time  Next  it turns out  you can also use vectorization very efficiently to compute the backward propagation  to compute the gradients  Let's see how you can do that  in the next video 
CeOV_tVo970,Vectorizing Logistic Regression's Gradient Output  In the previous video  you saw how you can use vectorization to compute their predictions  The lowercase a's for an entire training set all at the same time  In this video  you see how you can use vectorization to also perform the gradient computations for all M training samples  Again  all sort of at the same time  And then at the end of this video  we'll put it all together and show how you can derive a very efficient implementation of logistic regression  So  you may remember that for the gradient computation  what we did was we computed dz1 for the first example  which could be a1 minus y1 and then dz2 equals a2 minus y2 and so on  And so on for all M training examples  So  what we're going to do is define a new variable  dZ is going to be dz1  dz2  dzm  Again  all the D lowercase z variables stacked horizontally  So  this would be 1 by m matrix or alternatively a m dimensional row vector  Now recall that from the previous slide  we'd already figured out how to compute capital A which was this  a1 through am and we had defined capital Y as y1 through ym  Also you know  stacked horizontally  So  based on these definitions  maybe you can see for yourself that dz can be computed as just A minus Y because it's going to be equal to a1 - y1  So  the first element  a2 - y2  so in the second element and so on  And  so this first element a1 - y1 is exactly the definition of dz1  The second element is exactly the definition of dz2 and so on  So  with just one line of code  you can compute all of this at the same time  Now  in the previous implementation  we've gotten rid of one for loop already but we still had this second for loop over training examples  So we initialize dw to zero to a vector of zeroes  But then we still have to loop over 20 examples where we have dw plus equals x1 times dz1  for the first training example dw plus equals x2 dz2 and so on  So we do the M times and then dw divide equals by M and similarly for B  right? db was initialized as 0 and db plus equals dz1  db plus equals dz2 down to you know dz(m) and db divide equals M  So that's what we had in the previous implementation  We'd already got rid of one for loop  So  at least now dw is a vector and we went separately updating dw1  dw2 and so on  So  we got rid of that already but we still had the for loop over the M examples in the training set  So  let's take these operations and vectorize them  Here's what we can do  for the vectorized implementation of db  what it's doing is basically summing up  all of these dzs and then dividing by m  So  db is basically one over m  sum from I equals one through m of dzi and well all the dzs are in that row vector and so in Python  what you do is implement you know  1 over a m times np  sum of dz  So  you just take this variable and call the np  sum function on it and that would give you db  How about dw? I'll just write out the correct equations who can verify is the right thing to do  DW turns out to be one over M  times the matrix X times dz transpose  And  so kind of see why that's the case  This is equal to one over m then the matrix X's  x1 through xm stacked up in columns like that and dz transpose is going to be dz1 down to dz(m) like so  And so  if you figure out what this matrix times this vector works out to be  it is turns out to be one over m times x1 dz1 plus    plus xm dzm  And so  this is a n/1 vector and this is what you actually end up with  with dw because dw was taking these you know  xi dzi and adding them up and so that's what exactly this matrix vector multiplication is doing and so again  with one line of code you can compute dw  So  the vectorized implementation of the derivative calculations is just this  you use this line to implement db and use this line to implement dw and notice that without a for loop over the training set  you can now compute the updates you want to your parameters  So now  let's put all together into how you would actually implement logistic regression  So  this is our original  highly inefficient non vectorize implementation  So  the first thing we've done in the previous video was get rid of this volume  right? So  instead of looping over dw1  dw2 and so on  we have replaced this with a vector value dw which is dw+= xi  which is now a vector times dz(i)  But now  we will see that we can also get rid of not just a for loop below but also get rid of this for loop  So  here is how you do it  So  using what we have from the previous slides  you would say  capitalZ  Z equal to w transpose X + B and the code you is write capital Z equals np  w transpose X + B and then a equals sigmoid of capital Z  So  you have now computed all of this and all of this for all the values of I  Next on the previous slide  we said you would compute dz equals A - Y  So  now you computed all of this for all the values of i  Then  finally dw equals 1/m x dz transpose and db equals 1/m of you know  np  sum dz  So  you've just done forward propagation and back propagation  really computing the predictions and computing the derivatives on all M training examples without using a for loop  And so the gradient descent update then would be you know W gets updated as w minus the learning rate times dw which was just computed above and B is update as B minus the learning rate times db  Sometimes is putting colons to that to denote that as is an assignment  but I guess I haven't been totally consistent with that notation  But with this  you have just implemented a single iteration of gradient descent for logistic regression  Now  I know I said that we should get rid of explicit full loops whenever you can but if you want to implement multiple iterations as a gradient descent then you still need a full loop over the number of iterations  So  if you want to have a thousand iterations of gradient descent  you might still need a full loop over the iteration number  There is an outermost full loop like that then I don't think there is any way to get rid of that full loop  But I do think it's incredibly cool that you can implement at least one iteration of gradient descent without needing to use a full loop  So  that's it you now have a highly vectorize and highly efficient implementation of gradient descent for logistic regression  There is just one more detail that I want to talk about in the next video  which is in our description here I briefly alluded to this technique called broadcasting  Broadcasting turns out to be a technique that Python and numpy allows you to use to make certain parts of your code also much more efficient  So  let's see some more details of broadcasting in the next video 
AKOWGChr9iA,Broadcasting in Python  In the previous video  I mentioned that broadcasting is another technique that you can use to make your Python code run faster  In this video  let's delve into how broadcasting in Python actually works  Let's suppose today broadcasting with an example  In this matrix  I've shown the number of calories from carbohydrates  proteins  and fats in 100 grams of four different foods  So for example  a 100 grams of apples turns out  has 56 calories from carbs  and much less from proteins and fats  Whereas  in contrast  a 100 grams of beef has 104 calories from protein and 135 calories from fat  Now  let's say your goal is to calculate the percentage of calories from carbs  proteins and fats for each of the four foods  So  for example  if you look at this column and add up the numbers in that column you get that 100 grams of apple has 56 plus 1 2 plus 1 8 so that's 59 calories  And so as a percentage the percentage of calories from carbohydrates in an apple would be 56 over 59  that's about 94 9%  So most of the calories in an apple come from carbs  whereas in contrast  most of the calories of beef come from protein and fat and so on  So the calculation you want is really to sum up each of the four columns of this matrix to get the total number of calories in 100 grams of apples  beef  eggs  and potatoes  And then to divide throughout the matrix  so as to get the percentage of calories from carbs  proteins and fats for each of the four foods  So the question is  can you do this without an explicit for-loop? Let's take a look at how you could do that  What I'm going to do is show you how you can set  say this matrix equal to three by four matrix A  And then with one line of Python code we're going to sum down the columns  So we're going to get four numbers corresponding to the total number of calories in these four different types of foods  100 grams of these four different types of foods  And I'm going to use a second line of Python code to divide each of the four columns by their corresponding sum  If that verbal description wasn't very clearly  hopefully it will be clearer in a second when we look in the Python code  So here we are in the Jupiter notebook  I've already written this first piece of code to prepopulate the matrix A with the numbers we had just now  so we'll hit shift enter and just run that  so there's the matrix A  And now here are the two lines of Python code  First  we're going to compute tau equals a  that sum  And x is equals 0 means to sum vertically  We'll say more about that in a little bit  And then print cal  So we'll sum vertically  Now 59 is the total number of calories in the apple  239 was the total number of calories in the beef and the eggs and potato and so on  And then with a compute percentage equals A/cal reshape 1 4  Actually we want percentages  so multiply by 100 here  And then let's print percentage  Let's run that  And so that command we've taken the matrix A and divided it by this one by four matrix  And this gives us the matrix of percentages  So as we worked out kind of by hand just now in the apple there was a first column 94 9% of the calories are from carbs  Let's go back to the slides  So just to repeat the two lines of code we had  this is what have written out in the Jupiter notebook  To add a bit of detail this parameter  (axis = 0)  means that you want Python to sum vertically  So if this is axis 0 this means to sum vertically  where as the horizontal axis is axis 1  So be able to write axis 1 or sum horizontally instead of sum vertically  And then this command here  this is an example of Python broadcasting where you take a matrix A  So this is a three by four matrix and you divide it by a one by four matrix  And technically  after this first line of codes cal  the variable cal  is already a one by four matrix  So technically you don't need to call reshape here again  so that's actually a little bit redundant  But when I'm writing Python codes if I'm not entirely sure what matrix  whether the dimensions of a matrix I often would just call a reshape command just to make sure that it's the right column vector or the row vector or whatever you want it to be  The reshape command is a constant time  It's a order one operation that's very cheap to call  So don't be shy about using the reshape command to make sure that your matrices are the size you need it to be  Now  let's explain in greater detail how this type of operation works  right? We had a three by four matrix and we divided it by a one by four matrix  So  how can you divide a three by four matrix by a one by four matrix? Or by one by four vector? Let's go through a few more examples of broadcasting  If you take a 4 by 1 vector and add it to a number  what Python will do is take this number and auto-expand it into a four by one vector as well  as follows  And so the vector [1  2  3  4] plus the number 100 ends up with that vector on the right  You're adding a 100 to every element  and in fact we use this form of broadcasting where that constant was the parameter b in an earlier video  And this type of broadcasting works with both column vectors and row vectors  and in fact we use a similar form of broadcasting earlier with the constant we're adding to a vector being the parameter b in logistic regression  Here's another example  Let's say you have a two by three matrix and you add it to this one by n matrix  So the general case would be if you have some (m n) matrix here and you add it to a (1 n) matrix  What Python will do is copy the matrix m  times to turn this into m by n matrix  so instead of this one by three matrix it'll copy it twice in this example to turn it into this  Also  two by three matrix and we'll add these so you'll end up with the sum on the right  okay? So you taken  you added 100 to the first column  added 200 to second column  added 300 to the third column  And this is basically what we did on the previous slide  except that we use a division operation instead of an addition operation  So one last example  whether you have a (m n) matrix and you add this to a (m 1) vector  (m 1) matrix  Then just copy this n times horizontally  So you end up with an (m n) matrix  So as you can imagine you copy it horizontally three times  And you add those  So when you add them you end up with this  So we've added 100 to the first row and added 200 to the second row  Here's the more general principle of broadcasting in Python  If you have an (m n) matrix and you add or subtract or multiply or divide with a (1 n) matrix  then this will copy it n times into an (m n) matrix  And then apply the addition  subtraction  and multiplication of division element wise  If conversely  you were to take the (m n) matrix and add  subtract  multiply  divide by an (m 1) matrix  then also this would copy it now n times  And turn that into an (m n) matrix and then apply the operation element wise  Just one of the broadcasting  which is if you have an (m 1) matrix  so that's really a column vector like [1 2 3]  and you add  subtract  multiply or divide by a row number  So maybe a (1 1) matrix  So such as that plus 100  then you end up copying this real number n times until you'll also get another (n 1) matrix  And then you perform the operation such as addition on this example element-wise  And something similar also works for row vectors  The fully general version of broadcasting can do even a little bit more than this  If you're interested you can read the documentation for NumPy  and look at broadcasting in that documentation  That gives an even slightly more general definition of broadcasting  But the ones on the slide are the main forms of broadcasting that you end up needing to use when you implement a neural network  Before we wrap up  just one last comment  which is for those of you that are used to programming in either MATLAB or Octave  if you've ever used the MATLAB or Octave function bsxfun in neural network programming bsxfun does something similar  not quite the same  But it is often used for similar purpose as what we use broadcasting in Python for  But this is really only for very advanced MATLAB and Octave users  if you've not heard of this  don't worry about it  You don't need to know it when you're coding up neural networks in Python  So  that was broadcasting in Python  I hope that when you do the programming homework that broadcasting will allow you to not only make a code run faster  but also help you get what you want done with fewer lines of code  Before you dive into the programming excercise  I want to share with you just one more set of ideas  which is that there's some tips and tricks that I've found reduces the number of bugs in my Python code and that I hope will help you too  So with that  let's talk about that in the next video 
ky17zbYo4Rw,A note on python/numpy vectors  The ability of python to allow you to use broadcasting operations and more generally  the great flexibility of the python numpy program language is  I think  both a strength as well as a weakness of the programming language  I think it's a strength because they create expressivity of the language  A great flexibility of the language lets you get a lot done even with just a single line of code  But there's also weakness because with broadcasting and this great amount of flexibility  sometimes it's possible you can introduce very subtle bugs or very strange looking bugs  if you're not familiar with all of the intricacies of how broadcasting and how features like broadcasting work  For example  if you take a column vector and add it to a row vector  you would expect it to throw up a dimension mismatch or type error or something  But you might actually get back a matrix as a sum of a row vector and a column vector  So there is an internal logic to these strange effects of Python  But if you're not familiar with Python  I've seen some students have very strange  very hard to find bugs  So what I want to do in this video is share with you some couple tips and tricks that have been very useful for me to eliminate or simplify and eliminate all the strange looking bugs in my own code  And I hope that with these tips and tricks  you'll also be able to much more easily write bug-free  python and numpy code  To illustrate one of the less intuitive effects of Python-Numpy  especially how you construct vectors in Python-Numpy  let me do a quick demo  Let's set a = np random randn(5)  so this creates five random Gaussian variables stored in array a  And so let's print(a) and now it turns out that the shape of a when you do this is this five color structure  And so this is called a rank 1 array in Python and it's neither a row vector nor a column vector  And this leads it to have some slightly non-intuitive effects  So for example  if I print a transpose  it ends up looking the same as a  So a and a transpose end up looking the same  And if I print the inner product between a and a transpose  you might think a times a transpose is maybe the outer product should give you matrix maybe  But if I do that  you instead get back a number  So what I would recommend is that when you're coding new networks  that you just not use data structures where the shape is 5  or n  rank 1 array  Instead  if you set a to be this  (5 1)  then this commits a to be (5 1) column vector  And whereas previously  a and a transpose looked the same  it becomes now a transpose  now a transpose is a row vector  Notice one subtle difference  In this data structure  there are two square brackets when we print a transpose  Whereas previously  there was one square bracket  So that's the difference between this is really a 1 by 5 matrix versus one of these rank 1 arrays  And if you print  say  the product between a and a transpose  then this gives you the outer product of a vector  right? And so  the outer product of a vector gives you a matrix  So  let's look in greater detail at what we just saw here  The first command that we ran  just now  was this  And this created a data structure with a shape was this funny thing (5 ) so this is called a rank 1 array  And this is a very funny data structure  It doesn't behave consistently as either a row vector nor a column vector  which makes some of its effects nonintuitive  So what I'm going to recommend is that when you're doing your programing exercises  or in fact when you're implementing logistic regression or neural networks that you just do not use these rank 1 arrays  Instead  if every time you create an array  you commit to making it either a column vector  so this creates a (5 1) vector  or commit to making it a row vector  then the behavior of your vectors may be easier to understand  So in this case  a shape is going to be equal to 5 1  And so this behaves a lot like a  but in fact  this is a column vector  And that's why you can think of this as (5 1) matrix  where it's a column vector  And here a shape is going to be 1 5  and this behaves consistently as a row vector  So when you need a vector  I would say either use this or this  but not a rank 1 array  One more thing that I do a lot in my code is if I'm not entirely sure what's the dimension of one of my vectors  I'll often throw in an assertion statement like this  to make sure  in this case  that this is a (5 1) vector  So this is a column vector  These assertions are really Set to execute  and they also help to serve as documentation for your code  So don't hesitate to throw in assertion statements like this whenever you feel like  And then finally  if for some reason you do end up with a rank 1 array  You can reshape this  a equals a reshape into say a (5 1) array or a (1 5) array so that it behaves more consistently as either column vector or row vector  So I've sometimes seen students end up with very hard to track because those are the nonintuitive effects of rank 1 arrays  By eliminating rank 1 arrays in my old code  I think my code became simpler  And I did not actually find it restrictive in terms of things I could express in code  I just never used a rank 1 array  And so takeaways are to simplify your code  don't use rank 1 arrays  Always use either n by one matrices  basically column vectors  or one by n matrices  or basically row vectors  Feel free to toss a lot of insertion statements  so double-check the dimensions of your matrices and arrays  And also  don't be shy about calling the reshape operation to make sure that your matrices or your vectors are the dimension that you need it to be  So that  I hope that this set of suggestions helps you to eliminate a cause of bugs from Python code  and makes the problem exercise easier for you to complete 
Q7pb3EectLo,Quick tour of Jupyter/iPython Notebooks  With everything you've learned  you're just about ready to tackle your first programming assignment  Before you do that  let me just give you a quick tour of iPython notebooks in Coursera  Here you see Jupiter iPython notebook that you can get to on Coursera  Let me just quickly show you a few features of this  The instructions are written right here in the text in the iPython notebook  And these long light gray blocks are blocks of code  So occasionally  you'll see in these blocks something that'll say this is the START CODE HERE and END CODE HERE  To do your exercise please make sure to write your code between the START CODE HERE and END CODE HERE  So  for example  print Hello world  And then to execute a code block  you can hit shift+enter and then execute this code block which  I guess  we just wrote print Hello world  So that prints it Hello World  To run a cell  you can also  to run one of these code blocks of cell  you can also click cell and then run cell  So that executes this  It's possible that on your computer  the keyboard shortcut for Cell  Run Cell might be different than shift+enter  But on both  my Mac as well as on my PC is shift+enter  so might be the same for you as well  Now when you're reading the instructions  if you accidentally double click on it  you might end up with this mark down language  If you end up with this funny looking text  to convert it back to the nice looking text just run this Cell  So you can go to Cell  Run Cell or I'm going to hit shift+enter and that basically executes the mark down and turns it back into this nice looking code  Just a couple more tips  When you execute code like this  it actually runs on a kernel  on a piece of code that runs on the server  If you're running an excessively large job or if you leave a computer for a very long time or something goes wrong  your internet connection or something  there is a small chance that a kernel on the back end might die  in which case  just click Kernel and then restart Kernel  And hopefully  that will reboot the kernel and make it work again  So that shouldn't happen if you're just running relatively small jobs and you're just starting up iPython notebook  If you see an error message that the Kernel has died or something  you can try Kernel  Restart  Finally  in iPython notebook  like this  there may be multiple blocks of code  So even if an earlier block of code doesn't have any create in code  make sure to execute this block of code because  in this example  it imports numpy as np and so on  and sets up some of the variables that you might need in order to execute the lower down blocks of code  So be sure to execute the ones on top even if you aren't asked to write any code in them  And finally  when you're done implementing your solutions  there's this blue submit assignment buttons here on the upper right and we click that to submit your solutions for grading  I've found that the interactive command shell nature of iPython notebooks to be very useful for learning quickly  implement a few lines of code  see an outcome  learn and add very quickly  And so I hope that from the exercises in Coursera  Jupyter iPython notebooks will help you quickly learn and experiment and see how to implement these algorithms  There's one more video after this  This is an optional video that talks about the cost function for logistic regression  You can watch that or not  Either way is perfectly fine  But either way  best of luck with the week 2 programming assignments  And I also look forward to seeing you at the start of the week three 
KwMB-JP74YE,Explanation of logistic regression cost function (optional)  In an earlier video  I've written down a form for the cost function for logistic regression  In this optional video  I want to give you a quick justification for why we like to use that cost function for logistic regression  To quickly recap  in logistic regression  we have that the prediction y hat is sigmoid of w transpose x + b  where sigmoid is this familiar function  And we said that we want to interpret y hat as the p( y = 1 | x)  So we want our algorithm to output y hat as the chance that y = 1 for a given set of input features x  So another way to say this is that if y is equal to 1 then the chance of y given x is equal to y hat  And conversely if y is equal to 0 then the chance that y was 0 was 1- y hat  right? So if y hat was a chance  that y = 1  then 1- y hat is the chance that y = 0  So  let me take these last two equations and just copy them to the next slide  So what I'm going to do is take these two equations which basically define p(y|x) for the two cases of y = 0 or y = 1  And then take these two equations and summarize them into a single equation  And just to point out y has to be either 0 or 1 because in binary cost equations  y = 0 or 1 are the only two possible cases  all right  When someone take these two equations and summarize them as follows  Let me just write out what it looks like  then we'll explain why it looks like that  So (1 â€“ y hat) to the power of (1 â€“ y)  So it turns out this one line summarizes the two equations on top  Let me explain why  So in the first case  suppose y = 1  right? So if y = 1 then this term ends up being y hat  because that's y hat to the power of 1  This term ends up being 1- y hat to the power of 1- 1  so that's the power of 0  But  anything to the power of 0 is equal to 1  so that goes away  And so  this equation  just as p(y|x) = y hat  when y = 1  So that's exactly what we wanted  Now how about the second case  what if y = 0? If y = 0  then this equation above is p(y|x) = y hat to the 0  but anything to the power of 0 is equal to 1  so that's just equal to 1 times 1- y hat to the power of 1- y  So 1- y is 1- 0  so this is just 1  And so this is equal to 1 times (1- y hat) = 1- y hat  And so here we have that the y = 0  p (y|x) = 1- y hat  which is exactly what we wanted above  So what we've just shown is that this equation is a correct definition for p(ylx)  Now  finally  because the log function is a strictly monotonically increasing function  your maximizing log p(y|x) should give you a similar result as optimizing p(y|x)  And if you compute log of p(y|x)  thatâ€™s equal to log of y hat to the power of y  1 - y hat to the power of 1 - y  And so that simplifies to y log y hat + 1- y times log 1- y hat  right? And so this is actually negative of the loss function that we had to find previously  And there's a negative sign there because usually if you're training a learning algorithm  you want to make probabilities large whereas in logistic regression we're expressing this  We want to minimize the loss function  So minimizing the loss corresponds to maximizing the log of the probability  So this is what the loss function on a single example looks like  How about the cost function  the overall cost function on the entire training set on m examples? Let's figure that out  So  the probability of all the labels In the training set  Writing this a little bit informally  If you assume that the training examples I've drawn independently or drawn IID  identically independently distributed  then the probability of the example is the product of probabilities  The product from i = 1 through m p(y(i) ) given x(i)  And so if you want to carry out maximum likelihood estimation  right  then you want to maximize the  find the parameters that maximizes the chance of your observations and training set  But maximizing this is the same as maximizing the log  so we just put logs on both sides  So log of the probability of the labels in the training set is equal to  log of a product is the sum of the log  So that's sum from i=1 through m of log p(y(i)) given x(i)  And we have previously figured out on the previous slide that this is negative L of y hat i  y i  And so in statistics  there's a principle called the principle of maximum likelihood estimation  which just means to choose the parameters that maximizes this thing  Or in other words  that maximizes this thing  Negative sum from i = 1 through m L(y hat  y) and just move the negative sign outside the summation  So this justifies the cost we had for logistic regression which is J(w b) of this  And because we now want to minimize the cost instead of maximizing likelihood  we've got to rid of the minus sign  And then finally for convenience  to make sure that our quantities are better scale  we just add a 1 over m extra scaling factor there  But so to summarize  by minimizing this cost function J(w b) we're really carrying out maximum likelihood estimation with the logistic regression model  Under the assumption that our training examples were IID  or identically independently distributed  So thank you for watching this video  even though this is optional  I hope this gives you a sense of why we use the cost function we do for logistic regression  And with that  I hope you go on to the programming exercises and the quiz questions of this week  And best of luck with both the quizzes  and the programming exercise 
HNtNeyOVh38,Neural Networks Overview  Welcome back  In this week  you learned to implement a neural network  Before diving into the technical details  I want in this video  to give you a quick overview of what you'll be seeing in this week's videos  So  if you don't follow all the details in this video  don't worry about it  we'll delve into the technical details in the next few videos  But for now  let's give a quick overview of how you implement in your network  Last week  we had talked about logistic regression  and we saw how this model corresponds to the following computation draft  where you didn't put the features x and parameters w and b that allows you to compute z which is then used to computes a  and we were using a interchangeably with this output y hat and then you can compute the loss function  L  A neural network looks like this  As I'd already previously alluded  you can form a neural network by stacking together a lot of little sigmoid units  Whereas previously  this node corresponds to two steps to calculations  The first is compute the z-value  second is it computes this a value  In this neural network  this stack of notes will correspond to a z-like calculation like this  as well as  an a-like calculation like that  Then  that node will correspond to another z and another a like calculation  So the notation which we will introduce later will look like this  First  we'll inputs the features  x  together with some parameters w and b  and this will allow you to compute z one  So  new notation that we'll introduce is that we'll use superscript square bracket one to refer to quantities associated with this stack of nodes  it's called a layer  Then later  we'll use superscript square bracket two to refer to quantities associated with that node  That's called another layer of the neural network  The superscript square brackets  like we have here  are not to be confused with the superscript round brackets which we use to refer to individual training examples  So  whereas x superscript round bracket I refer to the ith training example  superscript square bracket one and two refer to these different layers  layer one and layer two in this neural network  But so going on  after computing z_1 similar to logistic regression  there'll be a computation to compute a_1  and that's just sigmoid of z_1  and then you compute z_2 using another linear equation and then compute a_2  A_2 is the final output of the neural network and will also be used interchangeably with y-hat  So  I know that was a lot of details but the key intuition to take away is that whereas for logistic regression  we had this z followed by a calculation  In this neural network  here we just do it multiple times  as a z followed by a calculation  and a z followed by a calculation  and then you finally compute the loss at the end  You remember that for logistic regression  we had this backward calculation in order to compute derivatives or as you're computing your d a  d z and so on  So  in the same way  a neural network will end up doing a backward calculation that looks like this in which you end up computing da_2  dz_2  that allows you to compute dw_2  db_2  and so on  This right to left backward calculation that is denoting with the red arrows  So  that gives you a quick overview of what a neural network looks like  It's basically taken logistic regression and repeating it twice  I know there was a lot of new notation laws  new details  don't worry about saving them  follow everything  we'll go into the details most probably in the next few videos  So  let's go on to the next video  We'll start to talk about the neural network representation 
k2ajK9uM0Co,Neural Network Representation  You see me draw a few pictures of neural networks  In this video  we'll talk about exactly what those pictures means  In other words  exactly what those neural networks that we've been drawing represent  And we'll start with focusing on the case of neural networks with what was called a single hidden layer  Here's a picture of a neural network  Let's give different parts of these pictures some names  We have the input features  x1  x2  x3 stacked up vertically  And this is called the input layer of the neural network  So maybe not surprisingly  this contains the inputs to the neural network  Then there's another layer of circles  And this is called a hidden layer of the neural network  I'll come back in a second to say what the word hidden means  But the final layer here is formed by  in this case  just one node  And this single-node layer is called the output layer  and is responsible for generating the predicted value y hat  In a neural network that you train with supervised learning  the training set contains values of the inputs x as well as the target outputs y  So the term hidden layer refers to the fact that in the training set  the true values for these nodes in the middle are not observed  That is  you don't see what they should be in the training set  You see what the inputs are  You see what the output should be  But the things in the hidden layer are not seen in the training set  So that kind of explains the name hidden layer  just because you don't see it in the training set  Let's introduce a bit more notation  Whereas previously  we were using the vector X to denote the input features and alternative notation for the values of the input features will be A superscript square bracket 0  And the term A also stands for activations  and it refers to the values that different layers of the neural network are passing on to the subsequent layers  So the input layer passes on the value x to the hidden layer  so we're going to call that activations of the input layer A super script 0  The next layer  the hidden layer  will in turn generate some set of activations  which I'm going to write as A superscript square bracket 1  So in particular  this first unit or this first node  we generate a value A superscript square bracket 1 subscript 1  This second node we generate a value  Now we have a subscript 2 and so on  And so  A superscript square bracket 1  this is a four dimensional vector you want in Python because the 4x1 matrix  or a 4 column vector  which looks like this  And it's four dimensional  because in this case we have four nodes  or four units  or four hidden units in this hidden layer  And then finally  the open layer regenerates some value A2  which is just a real number  And so y hat is going to take on the value of A2  So this is analogous to how in logistic regression we have y hat equals a and in logistic regression which we only had that one output layer  so we don't use the superscript square brackets  But with our neural network  we now going to use the superscript square bracket to explicitly indicate which layer it came from  One funny thing about notational conventions in neural networks is that this network that you've seen here is called a two layer neural network  And the reason is that when we count layers in neural networks  we don't count the input layer  So the hidden layer is layer one and the output layer is layer two  In our notational convention  we're calling the input layer layer zero  so technically maybe there are three layers in this neural network  Because there's the input layer  the hidden layer  and the output layer  But in conventional usage  if you read research papers and elsewhere in the course  you see people refer to this particular neural network as a two layer neural network  because we don't count the input layer as an official layer  Finally  something that we'll get to later is that the hidden layer and the output layers will have parameters associated with them  So the hidden layer will have associated with it parameters w and b  And I'm going to write superscripts square bracket 1 to indicate that these are parameters associated with layer one with the hidden layer  We'll see later that w will be a 4 by 3 matrix and b will be a 4 by 1 vector in this example  Where the first coordinate four comes from the fact that we have four nodes of our hidden units and a layer  and three comes from the fact that we have three input features  We'll talk later about the dimensions of these matrices  And it might make more sense at that time  But in some of the output layers has associated with it also  parameters w superscript square bracket 2 and b superscript square bracket 2  And it turns out the dimensions of these are 1 by 4 and 1 by 1  And these 1 by 4 is because the hidden layer has four hidden units  the output layer has just one unit  But we will go over the dimension of these matrices and vectors in a later video  So you've just seen what a two layered neural network looks like  That is a neural network with one hidden layer  In the next video  let's go deeper into exactly what this neural network is computing  That is how this neural network inputs x and goes all the way to computing its output y hat 
LQHKz1jcARY,Computing a Neural Network's Output  In the last video  you saw what a single hidden layer neural network looks like  In this video  let's go through the details of exactly how this neural network computes these outputs  What you see is that is like logistic regression  the repeater a lot of times  Let's take a look  So  this is what a two-layer neural network looks  Let's go more deeply into exactly what this neural network computes  Now  we've said before that logistic regression  the circle in logistic regression  really represents two steps of computation rows  You compute z as follows  and a second  you compute the activation as a sigmoid function of z  So  a neural network just does this a lot more times  Let's start by focusing on just one of the nodes in the hidden layer  Let's look at the first node in the hidden layer  So  I've grayed out the other nodes for now  So  similar to logistic regression on the left  this nodes in the hidden layer does two steps of computation  The first step and think of as the left half of this node  it computes z equals w transpose x plus b  and the notation we'll use is  these are all quantities associated with the first hidden layer  So  that's why we have a bunch of square brackets there  This is the first node in the hidden layer  So  that's why we have the subscript one over there  So first  it does that  and then the second step  is it computes a_[1]_1 equals sigmoid of z_[1]_1  like so  So  for both z and a  the notational convention is that a  l  i  the l here in superscript square brackets  refers to the layer number  and the i subscript here  refers to the nodes in that layer  So  the node we'll be looking at is layer one  that is a hidden layer node one  So  that's why the superscripts and subscripts were both one  one  So  that little circle  that first node in the neural network  represents carrying out these two steps of computation  Now  let's look at the second node in the neural network  or the second node in the hidden layer of the neural network  Similar to the logistic regression unit on the left  this little circle represents two steps of computation  The first step is it computes z  This is still layer one  but now as a second node equals w transpose x  plus b_[1]_2  and then a_[1] two equals sigmoid of z_[1]_2  Again  feel free to pause the video if you want  but you can double-check that the superscript and subscript notation is consistent with what we have written here above in purple  So  we've talked through the first two hidden units in a neural network  having units three and four also represents some computations  So now  let me take this pair of equations  and this pair of equations  and let's copy them to the next slide  So  here's our neural network  and here's the first  and here's the second equations that we've worked out previously for the first and the second hidden units  If you then go through and write out the corresponding equations for the third and fourth hidden units  you get the following  So  let me show this notation is clear  this is the vector w_[1]_1  this is a vector transpose times x  So  that's what the superscript T there represents  It's a vector transpose  Now  as you might have guessed  if you're actually implementing a neural network  doing this with a for loop  seems really inefficient  So  what we're going to do  is take these four equations and vectorize  So  we're going to start by showing how to compute z as a vector  it turns out you could do it as follows  Let me take these w's and stack them into a matrix  then you have w_[1]_1 transpose  so that's a row vector  or this column vector transpose gives you a row vector  then w_[1]_2  transpose  w_[1]_3 transpose  w_[1]_4 transpose  So  by stacking those four w vectors together  you end up with a matrix  So  another way to think of this is that we have four logistic regression units there  and each of the logistic regression units  has a corresponding parameter vector  w  By stacking those four vectors together  you end up with this four by three matrix  So  if you then take this matrix and multiply it by your input features x1  x2  x3  you end up with by how matrix multiplication works  You end up with w_[1]_1 transpose x  w_2_[1] transpose x  w_3_[1] transpose x  w_4_[1] transpose x  Then  let's not figure the b's  So  we now add to this a vector b_[1]_1 one  b_[1]_2  b_[1]_3  b_[1]_4  So  that's basically this  then this is b_[1]_1  b_[1]_2  b_[1]_3  b_[1]_4  So  you see that each of the four rows of this outcome correspond exactly to each of these four rows  each of these four quantities that we had above  So  in other words  we've just shown that this thing is therefore equal to z_[1]_1  z_[1]_2  z_[1]_3  z_[1]_4  as defined here  Maybe not surprisingly  we're going to call this whole thing  the vector z_[1]  which is taken by stacking up these individuals of z's into a column vector  When we're vectorizing  one of the rules of thumb that might help you navigate this  is that while we have different nodes in the layer  we'll stack them vertically  So  that's why we have z_[1]_1 through z_[1]_4  those corresponded to four different nodes in the hidden layer  and so we stacked these four numbers vertically to form the vector z[1]  To use one more piece of notation  this four by three matrix here which we obtained by stacking the lowercase w_[1]_1  w_[1]_2  and so on  we're going to call this matrix W capital [1]  Similarly  this vector  we're going to call b superscript [1] square bracket  So  this is a four by one vector  So now  we've computed z using this vector matrix notation  the last thing we need to do is also compute these values of a  So  prior won't surprise you to see that we're going to define a_[1]  as just stacking together  those activation values  a [1]  1 through a [1]  4  So  just take these four values and stack them together in a vector called a[1]  This is going to be a sigmoid of z[1]  where this now has been implementation of the sigmoid function that takes in the four elements of z  and applies the sigmoid function element-wise to it  So  just a recap  we figured out that z_[1] is equal to w_[1] times the vector x plus the vector b_[1]  and a_[1] is sigmoid times z_[1]  Let's just copy this to the next slide  What we see is that for the first layer of the neural network given an input x  we have that z_[1] is equal to w_[1] times x plus b_[1]  and a_[1] is sigmoid of z_[1]  The dimensions of this are four by one equals  this was a four by three matrix times a three by one vector plus a four by one vector b  and this is four by one same dimension as end  Remember  that we said x is equal to a_[0]  Just say y hat is also equal to a two  If you want  you can actually take this x and replace it with a_[0]  since a_[0] is if you want as an alias for the vector of input features  x  Now  through a similar derivation  you can figure out that the representation for the next layer can also be written similarly where what the output layer does is  it has associated with it  so the parameters w_[2] and b_[2]  So  w_[2] in this case is going to be a one by four matrix  and b_[2] is just a real number as one by on  So  z_[2] is going to be a real number we'll write as a one by one matrix  Is going to be a one by four thing times a was four by one  plus b_[2] as one by one  so this gives you just a real number  If you think of this last upper unit as just being analogous to logistic regression which have parameters w and b  w really plays an analogous role to w_[2] transpose  or w_[2] is really W transpose and b is equal to b_[2]  I said we want to cover up the left of this network and ignore all that for now  then this last upper unit is a lot like logistic regression  except that instead of writing the parameters as w and b  we're writing them as w_[2] and b_[2]  with dimensions one by four and one by one  So  just a recap  For logistic regression  to implement the output or to implement prediction  you compute z equals w transpose x plus b  and a or y hat equals a  equals sigmoid of z  When you have a neural network with one hidden layer  what you need to implement  is to computer this output is just these four equations  You can think of this as a vectorized implementation of computing the output of first these for logistic regression units in the hidden layer  that's what this does  and then this logistic regression in the output layer which is what this does  I hope this description made sense  but the takeaway is to compute the output of this neural network  all you need is those four lines of code  So now  you've seen how given a single input feature  vector a  you can with four lines of code  compute the output of this neural network  Similar to what we did for logistic regression  we'll also want to vectorize across multiple training examples  We'll see that by stacking up training examples in different columns in the matrix  with just slight modification to this  you also  similar to what you saw in this regression  be able to compute the output of this neural network  not just a one example at a time  prolong your  say your entire training set at a time  So  let's see the details of that in the next video 
4N14CdtvzqM,Vectorizing across multiple examples  In the last video  you saw how to compute the prediction on a neural network  given a single training example  In this video  you see how to vectorize across multiple training examples  And the outcome will be quite similar to what you saw for logistic regression  Whereby stacking up different training examples in different columns of the matrix  you'd be able to take the equations you had from the previous video  And with very little modification  change them to make the neural network compute the outputs on all the examples on pretty much all at the same time  So let's see the details on how to do that  These were the four equations we have from the previous video of how you compute z1  a1  z2 and a2  And they tell you how  given an input feature back to x  you can use them to generate a2 =y hat for a single training example  Now if you have m training examples  you need to repeat this process for say  the first training example  x superscript (1) to compute y hat 1 does a prediction on your first training example  Then x(2) use that to generate prediction y hat (2)  And so on down to x(m) to generate a prediction y hat (m)  And so in all these activation function notation as well  I'm going to write this as a[2](1)  And this is a[2](2)  and a(2)(m)  so this notation a[2](i)  The round bracket i refers to training example i  and the square bracket 2 refers to layer 2  okay  So that's how the square bracket and the round bracket indices work  And so to suggest that if you have an unvectorized implementation and want to compute the predictions of all your training examples  you need to do for i = 1 to m  Then basically implement these four equations  right? You need to make a z[1](i) = W(1) x(i) + b[1]  a[1](i) = sigma of z[1](1)  z[2](i) = w[2]a[1](i) + b[2] andZ2i equals w2a1i plus b2 and a[2](i) = sigma point of z[2](i)  So it's basically these four equations on top by adding the superscript round bracket i to all the variables that depend on the training example  So adding this superscript round bracket i to x is z and a  if you want to compute all the outputs on your m training examples examples  What we like to do is vectorize this whole computation  so as to get rid of this for  And by the way  in case it seems like I'm getting a lot of nitty gritty linear algebra  it turns out that being able to implement this correctly is important in the deep learning era  And we actually chose notation very carefully for this course and make this vectorization steps as easy as possible  So I hope that going through this nitty gritty will actually help you to more quickly get correct implementations of these algorithms working  All right so let me just copy this whole block of code to the next slide and then we'll see how to vectorize this  So here's what we have from the previous slide with the for loop going over our m training examples  So recall that we defined the matrix x to be equal to our training examples stacked up in these columns like so  So take the training examples and stack them in columns  So this becomes a n  or maybe nx by m diminish the matrix  I'm just going to give away the punch line and tell you what you need to implement in order to have a vectorized implementation of this for loop  It turns out what you need to do is compute Z[1] = W[1] X + b[1]  A[1]= sig point of z[1]  Then Z[2] = w[2] A[1] + b[2] and then A[2] = sig point of Z[2]  So if you want the analogy is that we went from lower case vector xs to just capital case X matrix by stacking up the lower case xs in different columns  If you do the same thing for the zs  so for example  if you take z[1](i)  z[1](2)  and so on  and these are all column vectors  up to z[1](m)  right  So that's this first quantity that all m of them  and stack them in columns  Then just gives you the matrix z[1]  And similarly you look at say this quantity and take a[1](1)  a[1](2) and so on and a[1](m)  and stacked them up in columns  Then this  just as we went from lower case x to capital case X  and lower case z to capital case Z  This goes from the lower case a  which are vectors to this capital A[1]  that's over there and similarly  for z[2] and a[2]  Right they're also obtained by taking these vectors and stacking them horizontally  And taking these vectors and stacking them horizontally  in order to get Z[2]  and E[2]  One of the property of this notation that might help you to think about it is that this matrixes say Z and A  horizontally we're going to index across training examples  So that's why the horizontal index corresponds to different training example  when you sweep from left to right you're scanning through the training cells  And vertically this vertical index corresponds to different nodes in the neural network  So for example  this node  this value at the top most  top left most corner of the mean corresponds to the activation of the first heading unit on the first training example  One value down corresponds to the activation in the second hidden unit on the first training example  then the third heading unit on the first training sample and so on  So as you scan down this is your indexing to the hidden units number  Whereas if you move horizontally  then you're going from the first hidden unit  And the first training example to now the first hidden unit and the second training sample  the third training example  And so on until this node here corresponds to the activation of the first hidden unit on the final train example and the nth training example  Okay so the horizontally the matrix A goes over different training examples  And vertically the different indices in the matrix A corresponds to different hidden units  And a similar intuition holds true for the matrix Z as well as for X where horizontally corresponds to different training examples  And vertically it corresponds to different input features which are really different than those of the input layer of the neural network  So of these equations  you now know how to implement in your network with vectorization  that is vectorization across multiple examples  In the next video I want to show you a bit more justification about why this is a correct implementation of this type of vectorization  It turns out the justification would be similar to what you had seen [INAUDIBLE]  Let's go on to the next video 
lZJI_Fth66A,Explanation for Vectorized Implementation  In the previous video  we saw how with your training examples stacked up horizontally in the matrix x  you can derive a vectorized implementation for propagation through your neural network  Let's give a bit more justification for why the equations we wrote down is a correct implementation of vectorizing across multiple examples  So let's go through part of the propagation calculation for the few examples  Let's say that for the first training example  you end up computing this x1 plus b1 and then for the second training example  you end up computing this x2 plus b1 and then for the third training example  you end up computing this 3 plus b1  So  just to simplify the explanation on this slide  I'm going to ignore b  So let's just say  to simplify this justification a little bit that b is equal to zero  But the argument we're going to lay out will work with just a little bit of a change even when b is non-zero  It does just simplify the description on the slide a bit  Now  w1 is going to be some matrix  right? So I have some number of rows in this matrix  So if you look at this calculation x1  what you have is that w1 times x1 gives you some column vector which you must draw like this  And similarly  if you look at this vector x2  you have that w1 times x2 gives some other column vector  right? And that's gives you this z12  And finally  if you look at x3  you have w1 times x3  gives you some third column vector  that's this z13  So now  if you consider the training set capital X  which we form by stacking together all of our training examples  So the matrix capital X is formed by taking the vector x1 and stacking it vertically with x2 and then also x3  This is if we have only three training examples  If you have more  you know  they'll keep stacking horizontally like that  But if you now take this matrix x and multiply it by w then you end up with  if you think about how matrix multiplication works  you end up with the first column being these same values that I had drawn up there in purple  The second column will be those same four values  And the third column will be those orange values  what they turn out to be  But of course this is just equal to z11 expressed as a column vector followed by z12 expressed as a column vector followed by z13  also expressed as a column vector  And this is if you have three training examples  You get more examples then there'd be more columns  And so  this is just our matrix capital Z1  So I hope this gives a justification for why we had previously w1 times xi equals z1i when we're looking at single training example at the time  When you took the different training examples and stacked them up in different columns  then the corresponding result is that you end up with the z's also stacked at the columns  And I won't show but you can convince yourself if you want that with Python broadcasting  if you add back in  these values of b to the values are still correct  And what actually ends up happening is you end up with Python broadcasting  you end up having bi individually to each of the columns of this matrix  So on this slide  I've only justified that z1 equals w1x plus b1 is a correct vectorization of the first step of the four steps we have in the previous slide  but it turns out that a similar analysis allows you to show that the other steps also work on using a very similar logic where if you stack the inputs in columns then after the equation  you get the corresponding outputs also stacked up in columns  Finally  let's just recap everything we talked about in this video  If this is your neural network  we said that this is what you need to do if you were to implement for propagation  one training example at a time going from i equals 1 through m  And then we said  let's stack up the training examples in columns like so and for each of these values z1  a1  z2  a2  let's stack up the corresponding columns as follows  So this is an example for a1 but this is true for z1  a1  z2  and a2  Then what we show on the previous slide was that this line allows you to vectorize this across all m examples at the same time  And it turns out with the similar reasoning  you can show that all of the other lines are correct vectorizations of all four of these lines of code  And just as a reminder  because x is also equal to a0 because remember that the input feature vector x was equal to a0  so xi equals a0i  Then there's actually a certain symmetry to these equations where this first equation can also be written z1 equals w1 a0 plus b1  And so  you see that this pair of equations and this pair of equations actually look very similar but just of all of the indices advance by one  So this kind of shows that the different layers of a neural network are roughly doing the same thing or just doing the same computation over and over  And here we have two-layer neural network where we go to a much deeper neural network in next week's videos  You see that even deeper neural networks are basically taking these two steps and just doing them even more times than you're seeing here  So that's how you can vectorize your neural network across multiple training examples  Next  we've so far been using the sigmoid functions throughout our neural networks  It turns out that's actually not the best choice  In the next video  let's dive a little bit further into how you can use different  what's called  activation functions of which the sigmoid function is just one possible choice 
BF_ngWfgv3g,Activation functions  When you build your neural network  one of the choices you get to make is what activation function to use in the hidden layers  as well as what is the output units of your neural network  So far  we've just been using the sigmoid activation function  But sometimes other choices can work much better  Let's take a look at some of the options  In the fourth propagation steps for the neural network  we have these three steps where we use the sigmoid function here  So that sigmoid is called an activation function  And here's the familiar sigmoid function  a equals one over one plus e to the negative z  So in the more general case  we can have a different function  g of z  which I'm going to write here  where g could be a nonlinear function that may not be the sigmoid function  So for example  the sigmoid function goes within zero and one  and activation function that almost always works better than the sigmoid function is the tangent function or the hyperbolic tangent function  So this is z  this is a  this is a equals tanh(z)  and this goes between plus 1 and minus 1  The formula for the tanh function is e to the z minus e to the negative z over their sum  And is actually mathematically  a shifted version of the sigmoid function  So  as a sigmoid function just like that  but shifted so that it now crosses a zero zero point and v scale  so it goes 15 minus 1 and plus 1  And it turns out for hidden units  if you let the function g of z be equal to tanh(z)  this almost always works better than the sigmoid function because the values between plus 1 and minus 1  the mean of the activations that come out of your head  and they are closer to having a 0 mean  And so just as sometimes when you train a learning algorithm  you might center the data and have your data have 0 mean using a tanh instead of a sigmoid function  It kind of has the effect of centering your data so that the mean of your data is closer to 0 rather than  maybe 0 5  And this actually makes learning for the next layer a little bit easier  We'll say more about this in the second course when we talk about optimization algorithms as well  But one takeaway is that I pretty much never use the sigmoid activation function anymore  The tanh function is almost always strictly superior  The one exception is for the output layer because if y is either 0 or 1  then it makes sense for y hat to be a number  the one to output that's between 0 and 1 rather than between minus 1 and 1  So the one exception where I would use the sigmoid activation function is when you are using binary classification  in which case you might use the sigmoid activation function for the output layer  So g of z 2 here is equal to sigma of z 2  And so what you see in this example is where you might have a tanh activation function for the hidden layer  and sigmoid for the output layer  So deactivation functions can be different for different layers  And sometimes to note that activation functions are different for different layers  we might use these square bracket superscripts as well to indicate that g of square bracket one may be different than g of square bracket two  And again  square bracket one superscript refers to this layer  and superscript square bracket two refers to the output layer  Now  one of the downsides of both the sigmoid function and the tanh function is that if z is either very large or very small  then the gradient or the derivative or the slope of this function becomes very small  So if z is very large or z is very small  the slope of the function ends up being close to 0  And so this can slow down gradient descent  So one other choice that is very popular in machine learning is what's called the rectify linear unit  So the value function looks like this  And the formula is a = max(0 z)  So the derivative is 1  so long as z is positive  And the derivative or the slope is 0  when z is negative  If you're implementing this  technically the derivative when z is exactly 0 is not well defined  But when you implement this in the computer  the answer you get exactly is z equals 0000000000000  It's very small so you don't need to worry about it in practice  You could pretend the derivative  when z is equal to 0  you can pretend it's either 1 or 0 and then you kind of work just fine  So the fact that it's not differentiable  and the fact that  so here are some rules of thumb for choosing activation functions  If your output is 0  1 value  if you're using binary classification  then the sigmoid activation function is a very natural choice for the output layer  And then for all other unit's ReLU  or the rectified linear unit  Is increasingly the default choice of activation function  So if you're not sure what to use for your hidden layer  I would just use the ReLU activation function  It's what you see most people using most days  Although sometimes people also use the tanh activation function  One disadvantage of the ReLU is that the derivative is equal to zero  when z is negative  In practice  this works just fine  But there is another version of the ReLU called the leaky ReLU  I will give you the formula on the next slide  But instead of it being 0 when z is negative  it just takes a slight slope like so  so this is called the leaky ReLU  This usually works better than the ReLU activation function  although it's just not used as much in practice  Either one should be fine  although  if you had to pick one  I usually just use the ReLU  And the advantage of both the ReLU and the leaky ReLU is that for a lot of the space of Z  the derivative of the activation function  the slope of the activation function is very different from 0  And so in practice  using the ReLU activation function  your neural network will often learn much faster than when using the tanh or the sigmoid activation function  And the main reason is that there is less of these effects of the slope of the function going to 0  which slows down learning  And I know that for half of the range of z  the slope of ReLU is 0  but in practice  enough of your hidden units will have z greater than 0  So learning can still be quite fast for most training examples  So let's just quickly recap the pros and cons of different activation functions  Here's a sigmoid activation function  I will say never use this  except for the output layer  if you are doing binary classification  or maybe almost never use this  And the reason I almost never use this is because the tanh is pretty much strictly superior  So the tanh activation function is this  And then the default  the most commonly used activation function is the ReLU  which is this  So if you're not sure what else to use  use this one  and maybe feel free also to try the leaky ReLU  Where it might be (0 01 z  z)  Right? So a is the max of 0 01 times z and z  so that gives you these some bends in the function  And you might say  why is that constant 0 01? Well  you can also make that another parameter of the learning algorithm  And some people say that works even better  But i hardly see people do that  But if you feel like trying that in your application  please feel free to do so  And you can just see how it works  and how well it works  and stick with it if it gives you a good result  So I hope that gives you a sense of some of the choices of activation functions you can use in your neural network  One of the themes we'll see in deep learning is that you often have a lot of different choices in how you code your neural network  Ranging from number of hidden units  to the choice activation function  to how you initialize the ways which we'll see later  A lot of choices like that  And it turns out that it's sometimes difficult to get good guidelines for exactly what would work best for your problem  So throughout these courses I keep on giving you a sense of what I see in the industry in terms of what's more or less popular  But for your application  with your application's idiosyncrasies  it's actually very difficult to know in advance exactly what will work best  So a common piece of advice would be  if you're not sure which one of these activation functions work best  try them all  and evaluate on a holdout validation set  or a development set  which we'll talk about later  and see which one works better  and then go with that  And I think that by testing these different choices for your application  you'd be better at future-proofing your neural network architecture against the idiosyncracies of your problem  as well as evolutions of the algorithms  Rather than if I were to tell you always use a ReLU activation and don't use anything else  That just may or may not apply for whatever problem you end up working on either in the near future or in the distant future  All right  so that was the choice of activation functions and you've seen the most popular activation functions  There's one other question that sometimes you could ask  which is  why do you even need to use an activation function at all? Why not just do away with that? So let's talk about that in the next video  where you see why neural networks do need some sort of nonlinear activation function 
mb5wV4AqXso,Why do you need non-linear activation functions? Why does a neural network need a non-linear activation function? Turns out that your neural network to compute interesting functions  you do need to pick a non-linear activation function  let's see one  So  here's the four prop equations for the neural network  Why don't we just get rid of this? Get rid of the function g? And set a1 equals z1  Or alternatively  you can say that g of z is equal to z  all right? Sometimes this is called the linear activation function  Maybe a better name for it would be the identity activation function because it just outputs whatever was input  For the purpose of this  what if a(2) was just equal z(2)? It turns out if you do this  then this model is just computing y or y-hat as a linear function of your input features  x  to take the first two equations  If you have that a(1) = Z(1) = W(1)x + b  and then a(2) = z (2) = W(2)a(1) + b  Then if you take this definition of a1 and plug it in there  you find that a2 = w2(w1x + b1)  move that up a bit  Right? So this is a1 + b2  and so this simplifies to  (W2w1)x + (w2b1 + b2)  So this is just  let's call this w' b'  SO this is just equal to w' x + b'  If you were to use linear activation functions or we can also call them identity activation functions  then the neural network is just outputting a linear function of the input  And we'll talk about deep networks later  neural networks with many  many layers  many hidden layers  And it turns out that if you use a linear activation function or alternatively  if you don't have an activation function  then no matter how many layers your neural network has  all it's doing is just computing a linear activation function  So you might as well not have any hidden layers  Some of the cases that are briefly mentioned  it turns out that if you have a linear activation function here and a sigmoid function here  then this model is no more expressive than standard logistic regression without any hidden layer  So I won't bother to prove that  but you could try to do so if you want  But the take home is that a linear hidden layer is more or less useless because the composition of two linear functions is itself a linear function  So unless you throw a non-linear [INAUDIBLE] in there  then you're not computing more interesting functions even as you go deeper in the network  There is just one place where you might use a linear activation function  g(x) = z  And that's if you are doing machine learning on the regression problem  So if y is a real number  So for example  if you're trying to predict housing prices  So y is not 0  1  but is a real number  anywhere from - I don't know - $0 is the price of house up to however expensive  right  houses get  I guess  Maybe houses can be potentially millions of dollars  so however much houses cost in your data set  But if y takes on these real values  then it might be okay to have a linear activation function here so that your output y hat is also a real number going anywhere from minus infinity to plus infinity  But then the hidden units should not use the activation functions  They could use ReLU or tanh or Leaky ReLU or maybe something else  So the one place you might use a linear activation function is usually in the output layer  But other than that  using a linear activation function in the hidden layer except for some very special circumstances relating to compression that we're going to talk about using the linear activation function is extremely rare  And  of course  if we're actually predicting housing prices  as you saw in the week one video  because housing prices are all non-negative  Perhaps even then you can use a value activation function so that your output y-hats are all greater than or equal to 0  So I hope that gives you a sense of why having a non-linear activation function is a critical part of neural networks  Next we're going to start to talk about gradient descent and to do that to set up for our discussion for gradient descent  in the next video I want to show you how to estimate-how to compute-the slope or the derivatives of individual activation functions  So let's go on to the next video 
c59E5b0pcyw,Derivatives of activation functions  When you implement back propagation for your neural network  you need to either compute the slope or the derivative of the activation functions  So  let's take a look at our choices of activation functions and how you can compute the slope of these functions  Here's the familiar Sigmoid activation function  So  for any given value of z  maybe this value of z  This function will have some slope or some derivative corresponding to  if you draw a little line there  the height over width of this lower triangle here  So  if g of z is the sigmoid function  then the slope of the function is d  dz g of z  and so we know from calculus that it is the slope of g of x at z  If you are familiar with calculus and know how to take derivatives  if you take the derivative of the Sigmoid function  it is possible to show that it is equal to this formula  Again  I'm not going to do the calculus steps  but if you are familiar with calculus  feel free to post a video and try to prove this yourself  So  this is equal to just g of z  times 1 minus g of z  So  let's just sanity check that this expression make sense  First  if z is very large  so say z is equal to 10  then g of z will be close to 1  and so the formula we have on the left tells us that d dz g of z does be close to g of z  which is equal to 1 times 1 minus 1  which is therefore very close to 0  This isn't the correct because when z is very large  the slope is close to 0  Conversely  if z is equal to minus 10  so it says well there  then g of z is close to 0  So  the formula on the left tells us d dz g of z would be close to g of z  which is 0 times 1 minus 0  So it is also very close to 0  which is correct  Finally  if z is equal to 0  then g of z is equal to one-half  that's the sigmoid function right here  and so the derivative is equal to one-half times 1 minus one-half  which is equal to one-quarter  and that actually turns out to be the correct value of the derivative or the slope of this function when z is equal to 0  Finally  just to introduce one more piece of notation  sometimes instead of writing this thing  the shorthand for the derivative is g prime of z  So  g prime of z in calculus  the little dash on top is called prime  but so g prime of z is a shorthand for the calculus for the derivative of the function of g with respect to the input variable z  Then in a neural network  we have a equals g of z  equals this  then this formula also simplifies to a times 1 minus a  So  sometimes in implementation  you might see something like g prime of z equals a times 1 minus a  and that just refers to the observation that g prime  which just means the derivative  is equal to this over here  The advantage of this formula is that if you've already computed the value for a  then by using this expression  you can very quickly compute the value for the slope for g prime as well  All right  So  that was the sigmoid activation function  Let's now look at the Tanh activation function  Similar to what we had previously  the definition of d dz g of z is the slope of g of z at a particular point of z  and if you look at the formula for the hyperbolic tangent function  and if you know calculus  you can take derivatives and show that this simplifies to this formula and using the shorthand we have previously when we call this g prime of z again  So  if you want you can sanity check that this formula makes sense  So  for example  if z is equal to 10  Tanh of z will be very close to 1  This goes from plus 1 to minus 1  Then g prime of z  according to this formula  would be about 1 minus 1 squared  so there's very close to 0  So  that was if z is very large  the slope is close to 0  Conversely  if z is very small  say z is equal to minus 10  then Tanh of z will be close to minus 1  and so g prime of z will be close to 1 minus negative 1 squared  So  it's close to 1 minus 1  which is also close to 0  Then finally  if z is equal to 0  then Tanh of z is equal to 0  and then the slope is actually equal to 1  which is actually the slope when z is equal to 0  So  just to summarize  if a is equal to g of z  so if a is equal to this Tanh of z  then the derivative  g prime of z  is equal to 1 minus a squared  So  once again  if you've already computed the value of a  you can use this formula to very quickly compute the derivative as well  Finally  here's how you compute the derivatives for the ReLU and Leaky ReLU activation functions  For the value g of z is equal to max of 0 z  so the derivative is equal to  turns out to be 0   if z is less than 0 and 1 if z is greater than 0  It's actually undefined  technically undefined if z is equal to exactly 0  But if you're implementing this in software  it might not be a 100 percent mathematically correct  but it'll work just fine if z is exactly a 0  if you set the derivative to be equal to 1  It always had to be 0  it doesn't matter  If you're an expert in optimization  technically  g prime then becomes what's called a sub-gradient of the activation function g of z  which is why gradient descent still works  But you can think of it as that  the chance of z being exactly 0 000000  It's so small that it almost doesn't matter where you set the derivative to be equal to when z is equal to 0  So  in practice  this is what people implement for the derivative of z  Finally  if you are training a neural network with a Leaky ReLU activation function  then g of z is going to be max of say 0 01 z  z  and so  g prime of z is equal to 0 01 if z is less than 0 and 1 if z is greater than 0  Once again  the gradient is technically not defined when z is exactly equal to 0  but if you implement a piece of code that sets the derivative or that sets g prime to either 0 01 or or to 1  either way  it doesn't really matter  When z is exactly 0  your code will work just  So  under these formulas  you should either compute the slopes or the derivatives of your activation functions  Now  we have this building block  you're ready to see how to implement gradient descent for your neural network  Let's go on to the next video to see that 
SDllHfhXZmQ,Gradient descent for Neural Networks  All right  I think this'll be an exciting video  In this video  you'll see how to implement gradient descent for your neural network with one hidden layer  In this video  I'm going to just give you the equations you need to implement in order to get back-propagation or to get gradient descent working  and then in the video after this one  I'll give some more intuition about why these particular equations are the accurate equations  are the correct equations for computing the gradients you need for your neural network  So  your neural network  with a single hidden layer for now  will have parameters W1  B1  W2  and B2  So  as a reminder  if you have NX or alternatively N0 input features  and N1 hidden units  and N2 output units in our examples  So far I've only had N2 equals one  then the matrix W1 will be N1 by N0  B1 will be an N1 dimensional vector  so we can write that as N1 by one-dimensional matrix  really a column vector  The dimensions of W2 will be N2 by N1  and the dimension of B2 will be N2 by one  Right  so far we've only seen examples where N2 is equal to one  where you have just one single hidden unit  So  you also have a cost function for a neural network  For now  I'm just going to assume that you're doing binary classification  So  in that case  the cost of your parameters as follows is going to be one over M of the average of that loss function  So  L here is the loss when your neural network predicts Y hat  right  This is really A2 when the gradient label is equal to Y  If you're doing binary classification  the loss function can be exactly what you use for logistic regression earlier  So  to train the parameters of your algorithm  you need to perform gradient descent  When training a neural network  it is important to initialize the parameters randomly rather than to all zeros  We'll see later why that's the case  but after initializing the parameter to something  each loop or gradient descents with computed predictions  So  you basically compute your Y hat I  for I equals one through M  say  Then  you need to compute the derivative  So  you need to compute DW1  and that's the derivative of the cost function with respect to the parameter W1  you can compute another variable  shall I call DB1  which is the derivative or the slope of your cost function with respect to the variable B1 and so on  Similarly for the other parameters W2 and B2  Then finally  the gradient descent update would be to update W1 as W1 minus Alpha  The learning rate times D  W1  B1 gets updated as B1 minus the learning rate  times DB1  and similarly for W2 and B2  Sometimes  I use colon equals and sometimes equals  as either notation works fine  So  this would be one iteration of gradient descent  and then you repeat this some number of times until your parameters look like they're converging  So  in previous videos  we talked about how to compute the predictions  how to compute the outputs  and we saw how to do that in a vectorized way as well  So  the key is to know how to compute these partial derivative terms  the DW1  DB1 as well as the derivatives DW2 and DB2  So  what I'd like to do is just give you the equations you need in order to compute these derivatives  I'll defer to the next video  which is an optional video  to go greater into Jeff about how we came up with those formulas  So  let me just summarize again the equations for propagation  So  you have Z1 equals W1X plus B1  and then A1 equals the activation function in that layer applied element wise as Z1  and then Z2 equals W2  A1 plus V2  and then finally  just as all vectorized across your training set  right? A2 is equal to G2 of Z2  Again  for now  if we assume we're doing binary classification  then this activation function really should be the sigmoid function  same just for that end neural  So  that's the forward propagation or the left to right for computation for your neural network  Next  let's compute the derivatives  So  this is the back propagation step  Then I compute DZ2 equals A2 minus the gradient of Y  and just as a reminder  all this is vectorized across examples  So  the matrix Y is this one by M matrix that lists all of your M examples stacked horizontally  Then it turns out DW2 is equal to this  and in fact  these first three equations are very similar to gradient descents for logistic regression  X is equals one  comma  keep dims equals true  Just a little detail this np sum is a Python NumPy command for summing across one-dimension of a matrix  In this case  summing horizontally  and what keepdims does is  it prevents Python from outputting one of those funny rank one arrays  right? Where the dimensions was your N comma  So  by having keepdims equals true  this ensures that Python outputs for DB a vector that is N by one  In fact  technically this will be I guess N2 by one  In this case  it's just a one by one number  so maybe it doesn't matter  But later on  we'll see when it really matters  So  so far what we've done is very similar to logistic regression  But now as you continue to run back propagation  you will compute this  DZ2 times G1 prime of Z1  So  this quantity G1 prime is the derivative of whether it was the activation function you use for the hidden layer  and for the output layer  I assume that you are doing binary classification with the sigmoid function  So  that's already baked into that formula for DZ2  and his times is element-wise product  So  this here is going to be an N1 by M matrix  and this here  this element-wise derivative thing is also going to be an N1 by N matrix  and so this times there is an element-wise product of two matrices  Then finally  DW1 is equal to that  and DB1 is equal to this  and p sum DZ1 axis equals one  keepdims equals true  So  whereas previously the keepdims maybe matter less if N2 is equal to one  Result is just a one by one thing  is just a real number  Here  DB1 will be a N1 by one vector  and so you want Python  you want Np sons  I'll put something of this dimension rather than a funny rank one array of that dimension which could end up messing up some of your data calculations  The other way would be to not have to keep the parameters  but to explicitly reshape the output of NP sum into this dimension  which you would like DB to have  So  that was forward propagation in I guess four equations  and back-propagation in I guess six equations  I know I just wrote down these equations  but in the next optional video  let's go over some intuitions for how the six equations for the back propagation algorithm were derived  Please feel free to watch that or not  But either way  if you implement these algorithms  you will have a correct implementation of forward prop and back prop  You'll be able to compute the derivatives you need in order to apply gradient descent  to learn the parameters of your neural network  It is possible to implement this algorithm and get it to work without deeply understanding the calculus  A lot of successful deep learning practitioners do so  But  if you want  you can also watch the next video  just to get a bit more intuition of what the derivation of these equations 
piuWSMLNeco,Backpropagation intuition (optional)  In the last video you saw the equations for backpropagation  In this video let's go over some intuition  using the computation graph for how those equations were derived  This video is completely optional so if you free to watch it or not  you should be able to do the whole work either way  So recall that when we talked about logistic regression we had this forward pass  where we'll compute z then a and then a loss  and entity derivatives we had this backward pass where we could first compute da  and then go on to compute dz  and then go on to compute dw and db  So  the definition for the loss was l of a comma y equals negative y log a minus one minus y times log 1 minus a  So if you're familiar with calculus and you take the derivative of this with respect to a  that will give you the formula for da  So da is equal to that  If you actually figure out the calculus you could show that this is negative y over a plus one minus y over one minus a  Just derived at from calculus by taking derivatives of this  It turns out when you take another step backwards to compute dz  we then worked out that dz is equal to a minus y  I did explain why previously  but it turns out that from the chain rule of calculus  dz is equal to da times g prime of z  Where here  g of z equals sigmoid of z  is activation function for this output unit in logistic regression  So just remember this is still logistic regression  We have x_1  x_2  x_3 and then just one sigmoid unit and then that gives us a  gives us y hat  So here the activation function was a sigmoid function  As in aside only for those of you familiar with the chain rule of calculus  the reason for this is because a is equal to sigmoid of z and so partial of L with respect to z is equal to partial of L with respect to a times da  dz but since a is equal to sigmoid of z  this is equal to d  dz g of z which is equal to g prime of z  So that's why this expression  which is dz in our codes  is equal to this expression which is da in our codes times g prime of z  and so this is just that  So that last deviation will make sense only if you're familiar with calculus and specifically the chain rule from calculus  but if not don't worry about it  I'll try to explain the intuition wherever is needed  Then finally having computed dz for logistic regression we will compute dw  which it turned out was dz times x and db which is just dz where you have a single chain example  So that was logistic regression  So what we're going to do when computing backpropagation for a neural network is a calculation a lot like this but only we'll do it twice because now we have not x going to an output unit but x going to a hidden layer and then going to an output unit  So instead of instead of this computation being one step as we have here  we'll have two steps here in this kind of neural network with two layers  So in this two layer neural network that is with the input layer  a hidden layer  and an output layer remember the steps of our computation  First  you compute z_1 using this equation  and then compute a_1  and then you compute z_2  Notice z_2 also depends on the parameters w_2 and b_2  Then based on z_2 compute a_2 and finally that gives you the loss  So what backpropagation does is  it will go backwards to compute da_2 and then dz_2  Looking to go back to compute dw_2 and db_2  go backwards to compute da_1  dz_1 and so on  We don't need to take derivatives respect to the input x since the input x for supervised learning is fixed  So we're not trying to optimize x  So we won't bother to take derivatives at least for supervised learning with respect to x  So I'm going to skip explicitly computing da_2  If you want you can actually compute da_2 and then use that to compute dz_2 but in practice you could collapse both of these steps into one step so you end up at dz_2 is equal to a_2 minus y  same as before  You also going to write dw_2 and db_2 down here below  You have that dW_2 is equal to dz_2 times a_1 transpose and db_2 equals dz_2  So this step is quite similar for logistic regression where we had that dw was equal to dz times x  Except that now  a_1 plays the role of x and there's an extra transpose there because the relationship between the capital matrix W and our individual parameters w there's a transpose there  Because w is equal to a row vector  in the case of logistic regression with a single output dw_2 is like that  whereas w here was a column vector  So that's why there's an extra transpose for a_1 whereas we didn't for x here for logistic regression  So this completes half of backpropagation  Then again you can compute da_1 if you wish although in practice the computation for da_1 and dz_1 usually collapse into one step and so what you could actually implement is that dz_1 is equal to w_2 transpose times dz_2 and then times an element-wise product of g_1 prime of z_1  Just to do a check on the dimensions  if you have a neural network that looks like this  outputs y like so  If you have n_0 and x equals n_0 input features and one hidden unit  and n_2  So far  and n_2 In our case  just one output unit then the matrix W_2 is n_2 by n_1 dimensional  z_2 and therefore  dz_2 are going to be n_2 by one-dimensional  There's really going to be a one-by-one when we're doing binary classification  Z_1  and therefore also  dz_1 are going to be n_1 by one-dimensional  Note that for any variable foo  foo and dfoo always have the same dimensions  So that's why w and dw always have the same dimension  Similarly for b  and db  and z  and dz  and so on  So to make sure that the dimensions of this all match up  we have that dz_1 is equal to w_2 transpose times dz_2  Then this is an element-wise product times g_1 prime of z_1  So matching the dimensions from above  this is going to be n_1 by one  is equal to w_2 transpose  we transpose of this  So it's just going to be n_1 by n_2 dimensional  dz_2 is going to be n_2 by one-dimensional  Then this  this the same dimension as z_1  So this is also n_1 by one dimensional  so element-wise product  So the dimensions too make sense  n_1 one by 1 dimensional vector can be obtained by an n_1 by n_2 dimensional matrix times n_2 by n_1  because the product of these two things gives you an n_1 by one-dimensional matrix  So this becomes the element-wise product of two and one by one dimensional vectors and so dimensions do match up  One tip when implementing a backdrop  If you just make sure that the dimensions of your matrices match up  so if you think through what are the dimensions of your various matrices including w_1  w_2  z_1  z_2  a_1  a_2  and so on and just make sure that the dimensions of these matrix operations may match up  Sometimes  that will already eliminate quite a lot of bugs and back-prop  All right  So this gives us dz_1  Then finally  just to wrap up dw_1 and db_1  we should write them here I guess  But since I'm running out of space  I'll write them on the right of the slide  dw_1 and db_1 are given by the following formulas  This is going to be equal to dz_1 times x transpose  This is going to be equal to dz  and you might notice a similarity between these equations and these equations  which is really no coincidence because x plays the role of a_0  So x transpose is a_0 transpose  So those equations are actually very similar  All right  So that gives a sense for how back-propagation is derived  We have six key equations here for dz_2  dw_2  db_2  dz_1  dw_1  and db_1  So let me just take these six equations and copy them over to the next slide  Here they are  So far we have derived back-propagation for ABA training on a single training example at the time  But it should come as no surprise that rather than working on a single example at a time  we would like to vectorize across different training examples  So you remember that for propagation  when we're operating on one example at a time  we had equations like this  as let us say  a_1 equals g_1 of z_1  In order to vectorize  we took say the z's and stack them up in columns like this  z_1_m  and call this capital Z  Then we found that by stacking things up in columns and defining the capital uppercase version of this  we then just had z_1 equals w_1_x plus b and a_1 equals g_1 of z_1  We define the notation very carefully in this course to make sure that stacking examples into different columns of a matrix makes all this workout  So it turns out that if you go through the math carefully  the same trick also works for back-propagation  So the vectorized equations are as follows  First  if you take these d_z's for different training examples and stack them as the different columns of the matrix and same for this and same for this  then this is the vectorized implementation  Then here's the definition or here's how you can compute dW_2  There is this extra one over m because the cost function J is this one over m of sum from I equals one through m of the losses  So when computing derivatives  we have that extra one over m term just as we did when we were computing the weight updates for logistic regression  Then that's the update you get for db_2  Again  some of the d_z's and then we have a one over m  Then dz_1 is computed as follows  Once again  this is an element-wise product  only whereas previously  we saw on the previous slide that this was an n_1 by one dimensional vector  Now  this is a n_1 by m dimensional matrix  Both of these are also n_1 by m dimensional  So that's why that asterisk is a element-wise product  All right  Then finally  the remaining two updates  Perhaps  it shouldn't look too surprising  So I hope that gives you some intuition for how the back-propagation algorithm is derived  In all of machine learning  I think the derivation of the back-propagation algorithm is actually one of the most complicated pieces of math I've seen  It requires knowing both linear algebra as well as the derivative of matrices to really derive it from scratch from first principles  If you are an expert in matrix calculus  using this process  you might want to derive the algorithm yourself  But I think that there are actually plenty of deep learning practitioners that have seen the derivation at about the level you've seen in this video  and are already able to have all the right intuitions and be able to implement this algorithm very effectively  So if you are an expert in calculus  do see if you can divide the whole thing from scratch  It is one of the very hardest pieces of map on the very hardest derivations that I've seen in all of machine learning  But either way  if you implement this  this will work and I think you have enough intuitions to tune in and get it to work  So there's just one last detail I will share of you before you implement your neural network  which is how to initialize the weights of your neural network  It turns out that initializing your parameters not to zero but randomly turns out to be very important for training video network  In the next video  you'll see why 
nc1yivH1Yac,Random Initialization  When you change your neural network  it's important to initialize the weights randomly  For logistic regression  it was okay to initialize the weights to zero  But for a neural network of initialize the weights to parameters to all zero and then applied gradient descent  it won't work  Let's see why  So you have here two input features  so n0=2  and two hidden units  so n1=2  And so the matrix associated with the hidden layer  w 1  is going to be two-by-two  Let's say that you initialize it to all 0s  so 0 0 0 0  two-by-two matrix  And let's say B1 is also equal to 0 0  It turns out initializing the bias terms b to 0 is actually okay  but initializing w to all 0s is a problem  So the problem with this formalization is that for any example you give it  you'll have that a1 1 and a1 2  will be equal  right? So this activation and this activation will be the same  because both of these hidden units are computing exactly the same function  And then  when you compute backpropagation  it turns out that dz11 and dz12 will also be the same colored by symmetry  right? Both of these hidden units will initialize the same way  Technically  for what I'm saying  I'm assuming that the outgoing weights or also identical  So that's w2 is equal to 0 0  But if you initialize the neural network this way  then this hidden unit and this hidden unit are completely identical  Sometimes you say they're completely symmetric  which just means that they're completing exactly the same function  And by kind of a proof by induction  it turns out that after every single iteration of training your two hidden units are still computing exactly the same function  Since [INAUDIBLE] show that dw will be a matrix that looks like this  Where every row takes on the same value  So we perform a weight update  So when you perform a weight update  w1 gets updated as w1- alpha times dw  You find that w1  after every iteration  will have the first row equal to the second row  So it's possible to construct a proof by induction that if you initialize all the ways  all the values of w to 0  then because both hidden units start off computing the same function  And both hidden the units have the same influence on the output unit  then after one iteration  that same statement is still true  the two hidden units are still symmetric  And therefore  by induction  after two iterations  three iterations and so on  no matter how long you train your neural network  both hidden units are still computing exactly the same function  And so in this case  there's really no point to having more than one hidden unit  Because they are all computing the same thing  And of course  for larger neural networks  let's say of three features and maybe a very large number of hidden units  a similar argument works to show that with a neural network like this  [INAUDIBLE] drawing all the edges  if you initialize the weights to zero  then all of your hidden units are symmetric  And no matter how long you're upgrading the center  all continue to compute exactly the same function  So that's not helpful  because you want the different hidden units to compute different functions  The solution to this is to initialize your parameters randomly  So here's what you do  You can set w1 = np random randn  This generates a gaussian random variable (2 2)  And then usually  you multiply this by very small number  such as 0 01  So you initialize it to very small random values  And then b  it turns out that b does not have the symmetry problem  what's called the symmetry breaking problem  So it's okay to initialize b to just zeros  Because so long as w is initialized randomly  you start off with the different hidden units computing different things  And so you no longer have this symmetry breaking problem  And then similarly  for w2  you're going to initialize that randomly  And b2  you can initialize that to 0  So you might be wondering  where did this constant come from and why is it 0 01? Why not put the number 100 or 1000? Turns out that we usually prefer to initialize the weights to very small random values  Because if you are using a tanh or sigmoid activation function  or the other sigmoid  even just at the output layer  If the weights are too large  then when you compute the activation values  remember that z[1]=w1 x + b  And then a1 is the activation function applied to z1  So if w is very big  z will be very  or at least some values of z will be either very large or very small  And so in that case  you're more likely to end up at these fat parts of the tanh function or the sigmoid function  where the slope or the gradient is very small  Meaning that gradient descent will be very slow  So learning was very slow  So just a recap  if w is too large  you're more likely to end up even at the very start of training  with very large values of z  Which causes your tanh or your sigmoid activation function to be saturated  thus slowing down learning  If you don't have any sigmoid or tanh activation functions throughout your neural network  this is less of an issue  But if you're doing binary classification  and your output unit is a sigmoid function  then you just don't want the initial parameters to be too large  So that's why multiplying by 0 01 would be something reasonable to try  or any other small number  And same for w2  right? This can be random random  I guess this would be 1 by 2 in this example  times 0 01  Missing an s there  So finally  it turns out that sometimes they can be better constants than 0 01  When you're training a neural network with just one hidden layer  it is a relatively shallow neural network  without too many hidden layers  Set it to 0 01 will probably work okay  But when you're training a very very deep neural network  then you might want to pick a different constant than 0 01  And in next week's material  we'll talk a little bit about how and when you might want to choose a different constant than 0 01  But either way  it will usually end up being a relatively small number  So that's it for this week's videos  You now know how to set up a neural network of a hidden layer  initialize the parameters  make predictions using  As well as compute derivatives and implement gradient descent  using backprop  So that  you should be able to do the quizzes  as well as this week's programming exercises  Best of luck with that  I hope you have fun with the problem exercise  and look forward to seeing you in the week four materials 
RlQYb3FZtQ8,"Deep L-layer neural network  Welcome to the fourth week of this course  By now  you've seen forward propagation and back propagation in the context of a neural network  with a single hidden layer  as well as logistic regression  and you've learned about vectorization  and when it's important to initialize the weights randomly  If you've done the past couple weeks homework  you've also implemented and seen some of these ideas work for yourself  So by now  you've actually seen most of the ideas you need to implement a deep neural network  What we're going to do this week  is take those ideas and put them together so that you'll be able to implement your own deep neural network  Because this week's problem exercise is longer  it just has been more work  I'm going to keep the videos for this week shorter as you can get through the videos a little bit more quickly  and then have more time to do a significant problem exercise at then end  which I hope will leave you having thoughts deep in neural network  that if you feel proud of  So what is a deep neural network? You've seen this picture for logistic regression and you've also seen neural networks with a single hidden layer  So here's an example of a neural network with two hidden layers and a neural network with 5 hidden layers  We say that logistic regression is a very \""shallow\"" model  whereas this model here is a much deeper model  and shallow versus depth is a matter of degree  So neural network of a single hidden layer  this would be a 2 layer neural network  Remember when we count layers in a neural network  we don't count the input layer  we just count the hidden layers as was the output layer  So  this would be a 2 layer neural network is still quite shallow  but not as shallow as logistic regression  Technically logistic regression is a one layer neural network  we could then  but over the last several years the AI  on the machine learning community  has realized that there are functions that very deep neural networks can learn that shallower models are often unable to  Although for any given problem  it might be hard to predict in advance exactly how deep in your network you would want  So it would be reasonable to try logistic regression  try one and then two hidden layers  and view the number of hidden layers as another hyper parameter that you could try a variety of values of  and evaluate on all that across validation data  or on your development set  See more about that later as well  Let's now go through the notation we used to describe deep neural networks  Here's is a one  two  three  four layer neural network  With three hidden layers  and the number of units in these hidden layers are I guess 5  5  3  and then there's one one upper unit  So the notation we're going to use  is going to use capital L  to denote the number of layers in the network  So in this case  L = 4  and so does the number of layers  and we're going to use N superscript [l] to denote the number of nodes  or the number of units in layer lowercase l  So if we index this  the input as layer \""0\""  This is layer 1  this is layer 2  this is layer 3  and this is layer 4  Then we have that  for example  n[1]  that would be this  the first is in there will equal 5  because we have 5 hidden units there  For this one  we have the n[2]  the number of units in the second hidden layer is also equal to 5  n[3] = 3  and n[4] = n[L] this number of upper units is 01  because your capital L is equal to four  and we're also going to have here that for the input layer n[0] = nx = 3  So that's the notation we use to describe the number of nodes we have in different layers  For each layer L  we're also going to use a[l] to denote the activations in layer l  So we'll see later that in for propagation  you end up computing a[l] as the activation g(z[l]) and perhaps the activation is indexed by the layer l as well  and then we'll use W[l ]to denote  the weights for computing the value z[l] in layer l  and similarly  b[l] is used to compute z [l]  Finally  just to wrap up on the notation  the input features are called x  but x is also the activations of layer zero  so a[0] = x  and the activation of the final layer  a[L] = y-hat  So a[L] is equal to the predicted output to prediction y-hat to the neural network  So you now know what a deep neural network looks like  as was the notation we'll use to describe and to compute with deep networks  I know we've introduced a lot of notation in this video  but if you ever forget what some symbol means  we've also posted on the course website  a notation sheet or a notation guide  that you can use to look up what these different symbols mean  Next  I'd like to describe what forward propagation in this type of network looks like  Let's go into the next video "
RkAE4zE4uSE,Forward Propagation in a Deep Network  In the last video  we described what is a deep L-layer neural network and also talked about the notation we use to describe such networks  In this video  you see how you can perform forward propagation  in a deep network  As usual  let's first go over what forward propagation will look like for a single training example x  and then later on we'll talk about the vectorized version  where you want to carry out forward propagation on the entire training set at the same time  But given a single training example x  here's how you compute the activations of the first layer  So for this first layer  you compute z1 equals w1 times x plus b1  So w1 and b1 are the parameters that affect the activations in layer one  This is layer one of the neural network  and then you compute the activations for that layer to be equal to g of z1  The activation function g depends on what layer you're at and maybe what index set as the activation function from layer one  So if you do that  you've now computed the activations for layer one  How about layer two? Say that layer  Well  you would then compute z2 equals w2 a1 plus b2  Then  so the activation of layer two is the y matrix times the outputs of layer one  So  it's that value  plus the bias vector for layer two  Then a2 equals the activation function applied to z2  Okay? So that's it for layer two  and so on and so forth  Until you get to the upper layer  that's layer four  Where you would have that z4 is equal to the parameters for that layer times the activations from the previous layer  plus that bias vector  Then similarly  a4 equals g of z4  So  that's how you compute your estimated output  y hat  So  just one thing to notice  x here is also equal to a0  because the input feature vector x is also the activations of layer zero  So we scratch out x  When I cross out x and put a0 here  then all of these equations basically look the same  The general rule is that zl is equal to wl times a of l minus 1 plus bl  It's one there  And then  the activations for that layer is the activation function applied to the values of z  So  that's the general forward propagation equation  So  we've done all this for a single training example  How about for doing it in a vectorized way for the whole training set at the same time? The equations look quite similar as before  For the first layer  you would have capital Z1 equals w1 times capital X plus b1  Then  A1 equals g of Z1  Bear in mind that X is equal to A0  These are just the training examples stacked in different columns  You could take this  let me scratch out X  they can put A0 there  Then for the next layer  looks similar  Z2 equals w2 A1 plus b2 and A2 equals g of Z2  We're just taking these vectors z or a and so on  and stacking them up  This is z vector for the first training example  z vector for the second training example  and so on  down to the nth training example  stacking these and columns and calling this capital Z  Similarly  for capital A  just as capital X  All the training examples are column vectors stack left to right  In this process  you end up with y hat which is equal to g of Z4  this is also equal to A4  That's the predictions on all of your training examples stacked horizontally  So just to summarize on notation  I'm going to modify this up here  A notation allows us to replace lowercase z and a with the uppercase counterparts  is that already looks like a capital Z  That gives you the vectorized version of forward propagation that you carry out on the entire training set at a time  where A0 is X  Now  if you look at this implementation of vectorization  it looks like that there is going to be a For loop here  So therefore l equals 1-4  For L equals 1 through capital L  Then you have to compute the activations for layer one  then layer two  then for layer three  and then the layer four  So  seems that there is a For loop here  I know that when implementing neural networks  we usually want to get rid of explicit For loops  But this is one place where I don't think there's any way to implement this without an explicit For loop  So when implementing forward propagation  it is perfectly okay to have a For loop to compute the activations for layer one  then layer two  then layer three  then layer four  No one knows  and I don't think there is any way to do this without a For loop that goes from one to capital L  from one through the total number of layers in the neural network  So  in this place  it's perfectly okay to have an explicit For loop  So  that's it for the notation for deep neural networks  as well as how to do forward propagation in these networks  If the pieces we've seen so far looks a little bit familiar to you  that's because what we're seeing is taking a piece very similar to what you've seen in the neural network with a single hidden layer and just repeating that more times  Now  it turns out that we implement a deep neural network  one of the ways to increase your odds of having a bug-free implementation is to think very systematic and carefully about the matrix dimensions you're working with  So  when I'm trying to debug my own code  I'll often pull a piece of paper  and just think carefully through  so the dimensions of the matrix I'm working with  Let's see how you could do that in the next video 
_Pvu1599vFU,Getting your matrix dimensions right  When implementing a deep neural network  one of the debugging tools I often use to check the correctness of my code is to pull a piece of paper  and just work through the dimensions and matrix I'm working with  So let me show you how to do that  since I hope this will make it easier for you to implement your deep nets as well  Capital L is equal to 5  right  counting quickly  not counting the input layer  there are five layers here  so four hidden layers and one output layer  And so if you implement forward propagation  the first step will be z1 = w1x + b1  So let's ignore the bias terms b for now  and focus on the parameters w  Now this first hidden layer has three hidden units  so this is layer 0  layer 1  layer 2  layer 3  layer 4  and layer 5  So using the notation we had from the previous video  we have that n1  which is the number of hidden units in layer 1  is equal to 3  And here we would have the n2 is equal to 5  n3 is equal to 4  n4 is equal to 2  and n5 is equal to 1  And so far we've only seen neural networks with a single output unit  but in later courses  we'll talk about neutral networks with multiple output units as well  And finally  for the input layer  we also have n0 = nx = 2  So now  let's think about the dimensions of z  w  and x  z is the vector of activations for this first hidden layer  so z is going to be 3 by 1  it's going to be a 3-dimensional vector  So I'm going to write it a n1 by 1-dimensional vector  n1 by 1-dimensional matrix  all right  so 3 by 1 in this case  Now how about the input features x  x  we have two input features  So x is in this example 2 by 1  but more generally  it would be n0 by 1  So what we need is for the matrix w1 to be something that when we multiply an n0 by 1 vector to it  we get an n1 by 1 vector  right? So you have sort of a three dimensional vector equals something times a two dimensional vector  And so by the rules of matrix multiplication  this has got be a 3 by 2 matrix  Right  because a 3 by 2 matrix times a 2 by 1 matrix  or times the 2 by 1 vector  that gives you a 3 by 1 vector  And more generally  this is going to be an n1 by n0 dimensional matrix  So what we figured out here is that the dimensions of w1 has to be n1 by n0  And more generally  the dimensions of wL must be nL by nL minus 1  So for example  the dimensions of w2  for this  it would have to be 5 by 3  or it would be n2 by n1  Because we're going to compute z2 as w2 times a1  and again  let's ignore the bias for now  And so this is going to be 3 by 1  and we need this to be 5 by 1  and so this had better be 5 by 3  And similarly  w3 is really the dimension of the next layer  comma  the dimension of the previous layer  so this is going to be 4 by 5  w4 Is going to be 2 by 4  and w5 is going to be 1 by 2  okay? So the general formula to check is that when you're implementing the matrix for layer L  that the dimension of that matrix be nL by nL-1  Now let's think about the dimension of this vector b  This is going to be a 3 by 1 vector  so you have to add that to another 3 by 1 vector in order to get a 3 by 1 vector as the output  Or in this example  we need to add this  this is going to be 5 by 1  so there's going to be another 5 by 1 vector  In order for the sum of these two things I have in the boxes to be itself a 5 by 1 vector  So the more general rule is that in the example on the left  b1 is n1 by 1  right  that's 3 by 1  and in the second example  this is n2 by 1  And so the more general case is that bL should be nL by 1 dimensional  So hopefully these two equations help you to double check that the dimensions of your matrices w  as well as your vectors p  are the correct dimensions  And of course  if you're implementing back propagation  then the dimensions of dw should be the same as the dimension of w  So dw should be the same dimension as w  and db should be the same dimension as b  Now the other key set of quantities whose dimensions to check are these z  x  as well as a of L  which we didn't talk too much about here  But because z of L is equal to g of a of L  applied element wise  then z and a should have the same dimension in these types of networks  Now let's see what happens when you have a vectorized implementation that looks at multiple examples at a time  Even for a vectorized implementation  of course  the dimensions of wb  dw  and db will stay the same  But the dimensions of z  a  as well as x will change a bit in your vectorized implementation  So previously  we had z1 = w1x+b1 where this was n1 by 1  this was n1 by n0  x was n0 by 1  and b was n1 by 1  Now  in a vectorized implementation  you would have z1 = w1x + b1  Where now z1 is obtained by taking the z1 for the individual examples  so there's z11  z12  up to z1m  and stacking them as follows  and this gives you z1  So the dimension of z1 is that  instead of being n1 by 1  it ends up being n1 by m  and m is the size you're trying to set  The dimensions of w1 stays the same  so it's still n1 by n0  And x  instead of being n0 by 1 is now all your training examples stacked horizontally  So it's now n 0 by m  and so you notice that when you take a n1 by n0 matrix and multiply that by an n0 by m matrix  That together they actually give you an n1 by m dimensional matrix  as expected  Now  the final detail is that b1 is still n1 by 1  but when you take this and add it to b  then through Python broadcasting  this will get duplicated and turn n1 by m matrix  and then add the element wise  So on the previous slide  we talked about the dimensions of wb  dw  and db  Here  what we see is that whereas zL as well as aL are of dimension nL by 1  we have now instead that ZL as well AL are nL by m  And a special case of this is when L is equal to 0  in which case A0  which is equal to just your training set input features X  is going to be equal to n0 by m as expected  And of course when you're implementing this in backpropagation  we'll see later you  end up computing dZ as well as dA  And so these will of course have the same dimension as Z and A  So I hope the little exercise we went through helps clarify the dimensions that the various matrices you'd be working with  When you implement backpropagation for a deep neural network  so long as you work through your code and make sure that all the matrices' dimensions are consistent  That will usually help  it'll go some ways toward eliminating some cause of possible bugs  So I hope that exercise for figuring out the dimensions of various matrices you'll been working with is helpful  When you implement a deep neural network  if you keep straight the dimensions of these various matrices and vectors you're working with  Hopefully they'll help you eliminate some cause of possible bugs  it certainly helps me get my code right  So next  we've now seen some of the mechanics of how to do forward propagation in a neural network  But why are deep neural networks so effective  and why do they do better than shallow representations? Let's spend a few minutes in the next video to discuss that 
qpCvPS-WwGA,Why deep representations? We've all been hearing that deep neural networks work really well for a lot of problems  and it's not just that they need to be big neural networks  is that specifically  they need to be deep or to have a lot of hidden layers  So why is that? Let's go through a couple examples and try to gain some intuition for why deep networks might work well  So first  what is a deep network computing? If you're building a system for face recognition or face detection  here's what a deep neural network could be doing  Perhaps you input a picture of a face then the first layer of the neural network you can think of as maybe being a feature detector or an edge detector  In this example  I'm plotting what a neural network with maybe 20 hidden units  might be trying to compute on this image  So the 20 hidden units visualized by these little square boxes  So for example  this little visualization represents a hidden unit that's trying to figure out where the edges of that orientation are in the image  And maybe this hidden unit might be trying to figure out where are the horizontal edges in this image  And when we talk about convolutional networks in a later course  this particular visualization will make a bit more sense  But the form  you can think of the first layer of the neural network as looking at the picture and trying to figure out where are the edges in this picture  Now  let's think about where the edges in this picture by grouping together pixels to form edges  It can then detect the edges and group edges together to form parts of faces  So for example  you might have a low neuron trying to see if it's finding an eye  or a different neuron trying to find that part of the nose  And so by putting together lots of edges  it can start to detect different parts of faces  And then  finally  by putting together different parts of faces  like an eye or a nose or an ear or a chin  it can then try to recognize or detect different types of faces  So intuitively  you can think of the earlier layers of the neural network as detecting simple functions  like edges  And then composing them together in the later layers of a neural network so that it can learn more and more complex functions  These visualizations will make more sense when we talk about convolutional nets  And one technical detail of this visualization  the edge detectors are looking in relatively small areas of an image  maybe very small regions like that  And then the facial detectors you can look at maybe much larger areas of image  But the main intuition you take away from this is just finding simple things like edges and then building them up  Composing them together to detect more complex things like an eye or a nose then composing those together to find even more complex things  And this type of simple to complex hierarchical representation  or compositional representation  applies in other types of data than images and face recognition as well  For example  if you're trying to build a speech recognition system  it's hard to revisualize speech but if you input an audio clip then maybe the first level of a neural network might learn to detect low level audio wave form features  such as is this tone going up? Is it going down? Is it white noise or sniffling sound like [SOUND]  And what is the pitch? When it comes to that  detect low level wave form features like that  And then by composing low level wave forms  maybe you'll learn to detect basic units of sound  In linguistics they call phonemes  But  for example  in the word cat  the C is a phoneme  the A is a phoneme  the T is another phoneme  But learns to find maybe the basic units of sound and then composing that together maybe learn to recognize words in the audio  And then maybe compose those together  in order to recognize entire phrases or sentences  So deep neural network with multiple hidden layers might be able to have the earlier layers learn these lower level simple features and then have the later deeper layers then put together the simpler things it's detected in order to detect more complex things like recognize specific words or even phrases or sentences  The uttering in order to carry out speech recognition  And what we see is that whereas the other layers are computing  what seems like relatively simple functions of the input such as where the edge is  by the time you get deep in the network you can actually do surprisingly complex things  Such as detect faces or detect words or phrases or sentences  Some people like to make an analogy between deep neural networks and the human brain  where we believe  or neuroscientists believe  that the human brain also starts off detecting simple things like edges in what your eyes see then builds those up to detect more complex things like the faces that you see  I think analogies between deep learning and the human brain are sometimes a little bit dangerous  But there is a lot of truth to  this being how we think that human brain works and that the human brain probably detects simple things like edges first then put them together to from more and more complex objects and so that has served as a loose form of inspiration for some deep learning as well  We'll see a bit more about the human brain or about the biological brain in a later video this week  The other piece of intuition about why deep networks seem to work well is the following  So this result comes from circuit theory of which pertains the thinking about what types of functions you can compute with different AND gates  OR gates  NOT gates  basically logic gates  So informally  their functions compute with a relatively small but deep neural network and by small I mean the number of hidden units is relatively small  But if you try to compute the same function with a shallow network  so if there aren't enough hidden layers  then you might require exponentially more hidden units to compute  So let me just give you one example and illustrate this a bit informally  But let's say you're trying to compute the exclusive OR  or the parity of all your input features  So you're trying to compute X1  XOR  X2  XOR  X3  XOR  up to Xn if you have n or n X features  So if you build in XOR tree like this  so for us it computes the XOR of X1 and X2  then take X3 and X4 and compute their XOR  And technically  if you're just using AND or NOT gate  you might need a couple layers to compute the XOR function rather than just one layer  but with a relatively small circuit  you can compute the XOR  and so on  And then you can build  really  an XOR tree like so  until eventually  you have a circuit here that outputs  well  lets call this Y  The outputs of Y hat equals Y  The exclusive OR  the parity of all these input bits  So to compute XOR  the depth of the network will be on the order of log N  We'll just have an XOR tree  So the number of nodes or the number of circuit components or the number of gates in this network is not that large  You don't need that many gates in order to compute the exclusive OR  But now  if you are not allowed to use a neural network with multiple hidden layers with  in this case  order log and hidden layers  if you're forced to compute this function with just one hidden layer  so you have all these things going into the hidden units  And then these things then output Y  Then in order to compute this XOR function  this hidden layer will need to be exponentially large  because essentially  you need to exhaustively enumerate our 2 to the N possible configurations  So on the order of 2 to the N  possible configurations of the input bits that result in the exclusive OR being either 1 or 0  So you end up needing a hidden layer that is exponentially large in the number of bits  I think technically  you could do this with 2 to the N minus 1 hidden units  But that's the older 2 to the N  so it's going to be exponentially larger on the number of bits  So I hope this gives a sense that there are mathematical functions  that are much easier to compute with deep networks than with shallow networks  Actually  I personally found the result from circuit theory less useful for gaining intuitions  but this is one of the results that people often cite when explaining the value of having very deep representations  Now  in addition to this reasons for preferring deep neural networks  to be perfectly honest  I think the other reasons the term deep learning has taken off is just branding  This things just we call neural networks with a lot of hidden layers  but the phrase deep learning is just a great brand  it's just so deep  So I think that once that term caught on that really neural networks rebranded or neural networks with many hidden layers rebranded  help to capture the popular imagination as well  But regardless of the PR branding  deep networks do work well  Sometimes people go overboard and insist on using tons of hidden layers  But when I'm starting out a new problem  I'll often really start out with even logistic regression then try something with one or two hidden layers and use that as a hyper parameter  Use that as a parameter or hyper parameter that you tune in order to try to find the right depth for your neural network  But over the last several years there has been a trend toward people finding that for some applications  very  very deep neural networks here with maybe many dozens of layers sometimes  can sometimes be the best model for a problem  So that's it for the intuitions for why deep learning seems to work well  Let's now take a look at the mechanics of how to implement not just front propagation  but also back propagation 
iMJ1qWKmZXE,Building blocks of deep neural networks  In the earlier videos from this week  as well as from the videos from the past several weeks  you've already seen the basic building blocks of forward propagation and back propagation  the key components you need to implement a deep neural network  Let's see how you can put these components together to build your deep net  Here's a network of a few layers  Let's pick one layer  And look into the computations focusing on just that layer for now  So for layer L  you have some parameters wl and bl and for the forward prop  you will input the activations a l-1 from your previous layer and output a l  So the way we did this previously was you compute z l = w l times al - 1 + b l  And then al = g of z l  All right  So  that's how you go from the input al minus one to the output al  And  it turns out that for later use it'll be useful to also cache the value zl  So  let me include this on cache as well because storing the value zl would be useful for backward  for the back propagation step later  And then for the backward step or for the back propagation step  again  focusing on computation for this layer l  you're going to implement a function that inputs da(l)  And outputs da(l-1)  and just to flesh out the details  the input is actually da(l)  as well as the cache so you have available to you the value of zl that you computed and then in addition  outputing da(l) minus 1 you bring the output or the gradients you want in order to implement gradient descent for learning  okay? So this is the basic structure of how you implement this forward step  what we call the forward function as well as this backward step  which we'll call backward function  So just to summarize  in layer l  you're going to have the forward step or the forward prop of the forward function  Input al- 1 and output  al  and in order to make this computation you need to use wl and bl  And also output a cache  which contains zl  right? And then the backward function  using the back prop step  will be another function that now inputs da(l) and outputs da(l-1)  So it tells you  given the derivatives respect to these activations  that's da(l)  what are the derivatives? How much do I wish? You know  al- 1 changes the computed derivatives respect to deactivations from a previous layer  Within this box  right? You need to use wl and bl  and it turns out along the way you end up computing dzl  and then this box  this backward function can also output dwl and dbl  but I was sometimes using red arrows to denote the backward iteration  So if you prefer  we could draw these arrows in red  So if you can implement these two functions then the basic computation of the neural network will be as follows  You're going to take the input features a0  feed that in  and that would compute the activations of the first layer  let's call that a1 and to do that  you need a w1 and b1 and then will also  you know  cache away z1  right? Now having done that  you feed that to the second layer and then using w2 and b2  you're going to compute deactivations in the next layer a2 and so on  Until eventually  you end up outputting a l which is equal to y hat  And along the way  we cached all of these values z  So that's the forward propagation step  Now  for the back propagation step  what we're going to do will be a backward sequence of iterations in which you are going backwards and computing gradients like so  So what you're going to feed in here  da(l) and then this box will give us da(l- 1) and so on until we get da(2) da(1)  You could actually get one more output to compute da(0) but this is derivative with respect to your input features  which is not useful at least for training the weights of these supervised neural networks  So you could just stop it there  But along the way  back prop also ends up outputting dwl  dbl  I just used the prompt as wl and bl  This would output dw3  db3 and so on  So you end up computing all the derivatives you need  And so just to maybe fill in the structure of this a little bit more  these boxes will use those parameters as well  wl  bl and it turns out that we'll see later that inside these boxes we end up computing the dz's as well  So one iteration of training through a neural network involves  starting with a(0) which is x and going through forward prop as follows  Computing y hat and then using that to compute this and then back prop  right  doing that and now you have all these derivative terms and so  you know  w would get updated as w1 = the learning rate times dw  right? For each of the layers and similarly for b rate  Now the computed back prop have all these derivatives  So that's one iteration of gradient descent for your neural network  Now before moving on  just one more informational detail  Conceptually  it will be useful to think of the cache here as storing the value of z for the backward functions  But when you implement this  and you see this in the programming exercise  When you implement this  you find that the cache may be a convenient way to get to this value of the parameters of w1  b1  into the backward function as well  So for this exercise you actually store in your cache to z as well as w and b  So this stores z2  w2  b2  But from an implementation standpoint  I just find it a convenient way to just get the parameters  copy to where you need to use them later when you're computing back propagation  So that's just an implementational detail that you see when you do the programming exercise  So you've now seen what are the basic building blocks for implementing a deep neural network  In each layer there's a forward propagation step and there's a corresponding backward propagation step  And has a cache to pass information from one to the other  In the next video  we'll talk about how you can actually implement these building blocks  Let's go on to the next video 
ATsAl_t52xY,Forward and Backward Propagation  In the previous video  you saw the basic blocks of implementing a deep neural network  A forward propagation step for each layer  and a corresponding backward propagation step  Let's see how you can actually implement these steps  We'll start with forward propagation  Recall that what this will do is input a[l-1] and output a[l]  and the cache z[l]  And we just said that an implementational point of view  maybe where cache w[l] and b[l] as well  just to make the functions come a bit easier in the problem exercise  And so  the equations for this should already look familiar  The way to implement a forward function is just this equals w[l] * a[l-1] + b[l]  and then  a[l] equals deactivation function applied to z  And if you want to vectorize implementation  then it's just that times a[l-1] + b  adding b  being a hyper-broadcasting  and a[l] = g applied element-wise to z  And you remember  on the diagram for the forward step  remember we had this chain of boxes going forward  so you initialize that with feeding an a[0]  which is equal to X  So  you initialized this  Really  what is the input to the first one  right? It's really a[0] which is the input features to either for one training sample  if you're doing one example at a time  or A[0]  the entire training set  if you are processing the entire training set  So that's the input to the first four functions in the chain  and then just repeating this allows you to compute forward propagation from left to right  Next  let's talk about the backward propagation step  Here  your goal is to input da[l]  and output da[l-1] and dW[l] and db  Let me just right out the steps you need to compute these things  dz[l] = da[l]  element-wise product with g[l]` z[l]  and then  to compute the derivatives dW[l] = dz[l] * a[l - 1]  I didn't explicitly put that in the cache but it turns out  you need this as well  And then  db[l] = dz[l]  and finally  da[l-1] = w[l]_transpose * dz[l]  okay? And  I don't want to go through the detailed derivation for this  but it turns out that if you take this definition for da and plug it in here  then you get the same formula as we had in the previous week  for how you compute dz[l] as a function of the previous dz[l]  in fact  well  If I just plug that in here  you end up that dz[l] = w[l+1]_transpose dz[l+1] * g[l]` z[l]  I know this looks like a lot of algebra  You can actually double check for yourself that this is the equation we have written down for back propagation last week when we are doing a neural network with just a single hidden layer  And as reminder  this time  this element-wise product  and so all you need is those four equations to implement your backward function  And then finally  I'll just write out a vectorized version  So the first line becomes dz[l] = dA[l]  element-wise product with g[l]` of z[l]  Maybe no surprise there  dW[l] becomes 1/m  dz[l] * a[l-1]_transpose and then  db[l] becomes 1/m np sum dz[l]  then  axis = 1  keepdims = true  We talked about the use of np sum in the previous week to compute db  And then finally  dA[l-1] is W[l]_transpose * dz[l]  So this allows you to input this quantity  da  over here  and output dW[l]  db[l]  the derivatives you need  as well as dA[l-1]  right? As follows  So that's how you implement the backward function  So just to summarize  take the input X  you might have the first layer  maybe has a ReLU activation function  Then go to the second layer  maybe uses another ReLU activation function  goes to the third layer  maybe has a Sigmoid activation function if you're doing binary classification  and this outputs y_hat  And then  using y_hat  you can compute the loss  and this allows you to start your backward iteration  I'll draw the arrows first  okay? So I don't have to change pens too much  Where you will then have back-prop compute the derivatives  to compute dW[3]  db[3]  dW[2]  db[2]  dW[1]  db[1]  and along the way you would be computing  I guess  the cache would transfer z[1]  z[2]  z[3]  and here you pause backward da[2] and da[1]  This could compute da[0]  but we won't use that  So you can just discard that  right? And so  this is how you implement forward-prop and back-prop for a three layer neural network  Now  there's just one last detail that I didn't talk about which is for the forward recursion  we will initialize it with the input data X  How about the backward recursion? Well  it turns out that da[L]  when you're using logistic regression  when you're doing binary classification  is equal to y/a + 1-y/1-a  So it turns out that the derivative for the loss function  respect to the output  with respect to y_hat  can be shown to be what it is  If you're familiar with calculus  If you look up the loss function L  and take the riveters  respect to y_hat or respect to a  you can show that you get that formula  So this is the formula you should use for da for the final layer  capital L  And of course  if you were to have a vectorized implementation  then you initialize the backward recursion  not with this but with dA for the layer l  which is going to be the same thing for the different examples  over a  for the first training example  + 1-y  for the first training example  over 1-a  for the first training example     down to the end training example  so 1-a[m]  So that's how you implement the vectorized version  That's how you initialize the vectorized version of back propagation  So you've now seen the basic building blocks of both forward propagation as well as back propagation  Now  if you implement these equations  you will get a correct implementation of forward-prop and back-prop to get you the derivatives you need  You might be thinking  while there was a lot of equation  I'm slightly confused  I'm not quite sure I see how this works  And if you're feeling that way  my advice is  when you get to this week's programming assignment  you will be able to implement these for yourself  and they will be much more concrete  And I know there is lot of equations  and maybe some equations didn't make complete sense  but if you work through the calculus  and the linear algebra  which is not easy  so feel free to try  but that's actually one of the more difficult derivations in machine learning  It turns out the equations roll down  or just the calculus equations for computing the derivatives specially in back-prop  But once again  if this feels a little bit abstract  a little bit mysterious to you  my advice is  when you've done the primary exercise  it will feel a bit more concrete to you  Although I have to say  even today  when I implement a learning algorithm  sometimes  even I'm surprised when my learning algorithms implementation works and it's because a lot of complexity of machine learning comes from the data rather than from the lines of code  So sometimes  you feel like  you implement a few lines of code  not quite sure what it did  but this almost magically works  because a lot of magic is actually not in the piece of code you write  which is often not too long  It's not exactly simple  but it's not ten thousand  a hundred thousand lines of code  but your feeding so much data that sometimes  even though I've worked in machine learning for a long time  sometimes  it still surprises me a bit when my learning algorithm works because a lot of complexity of your learning algorithm comes from the data rather than necessarily from your writing thousands and thousands of lines of code  All right  So  that's how you implement deep neural networks  And again  this will become more concrete when you done the primary exercise  Before moving on  in the next video  I want to discuss hyper parameters and parameters  It turns out that when you're training deep nets  being able to organize your hyper parameters well will help you be more efficient in developing your networks  In the next video  let's talk about exactly what that means 
OkncKzflw8I,Parameters vs Hyperparameters  being effective in developing your deep neural Nets requires that you not only organize your parameters well but also your hyper parameters so what are hyper parameters let's take a look so the parameters your model are W and B and there are other things you need to tell your learning algorithm such as the learning rate alpha because on we need to set alpha and that in turn will determine how your parameters evolve or maybe the number of iterations of gradient descent you carry out your learning algorithm has other you know numbers that you need to set such as the number of hidden layers so we call that capital L or the number of hidden units right such as zero and one and two and so on and then you also have the choice of activation function do you want to use a rel you or ten age or a sigma little something especially in the hidden layers and so all of these things are things that you need to tell your learning algorithm and so these are parameters that control the ultimate parameters W and B and so we call all of these things below hyper parameters because these things like alpha the learning rate the number of iterations number of hidden layers and so on these are all parameters that control W and B so we call these things hyper parameters because it is the hyper parameters that you know somehow determine the final value of the parameters W and B that you end up with in fact deep learning has a lot of different hyper parameters later in the later course we'll see other hyper parameters as well such as the momentum term the mini batch size various forms of regularization parameters and so on and if none of these terms at the bottom make sense yet don't worry about it we'll talk about them in the second course because deep learning has so many hyper parameters in contrast to earlier errors of machine learning I'm going to try to be very consistent in calling the learning rate alpha a hyper parameter rather than calling the parameter I think in earlier eras of machine learning when we didn't have so many hyper parameters most of us used to be a bit slow up here and just call alpha a parameter and technically alpha is a parameter but is a parameter that determines the real parameters our childhood consistent in calling these things like alpha the number of iterations and so on hyper parameters so when you're training a deep net for your own application you find that there may be a lot of possible settings for the hyper parameters that you need to just try out so apply deep learning today is a very imperiled process where often you might have an idea for example you might have an idea for the best value for the learning rate you might say well maybe alpha equals 0 01 I want to try that then you implemented try it out and then see how that works and then based on that outcome you might say you know what I've changed online I want to increase the learning rate to 0 05 and so if you're not sure what's the best value for the learning ready-to-use you might try one value of the learning rate alpha and see their cost function j go down like this then you might try a larger value for the learning rate alpha and see the cost function blow up and diverge then you might try another version and see it go down really fast it's inverse to higher value you might try another version and see it you know see the cost function J do that then I'll be China so the values you might say okay looks like this the value of alpha gives me a pretty fast learning and allows me to converge to a lower cost function jennice I'm going to use this value of alpha you saw in a previous slide that there are a lot of different hybrid parameters and it turns out that when you're starting on the new application I should find it very difficult to know in advance exactly what's the best value of the hyper parameters so what often happen is you just have to try out many different values and go around this cycle your trial some value really try five hidden layers with this many number of hidden units implement that see if it works and then iterate so the title of this slide is that apply deep learning is very empirical process and empirical process is maybe a fancy way of saying you just have to try a lot of things and see what works another effect I've seen is that deep learning today is applied to so many problems ranging from computer vision to speech recognition to natural language processing to a lot of structured data applications such as maybe a online advertising or web search or product recommendations and so on and what I've seen is that first I've seen researchers from one discipline any one of these try to go to a different one and sometimes the intuitions about hyper parameters carries over and sometimes it doesn't so I often advise people especially when starting on a new problem to just try out a range of values and see what works and then mix course we'll see a systematic way we'll see some systematic ways for trying out a range of values all right and second even if you're working on one application for a long time you know maybe you're working on online advertising as you make progress on the problem is quite possible there the best value for the learning rate a number of hidden units and so on might change so even if you tune your system to the best value of hyper parameters to daily as possible you find that the best value might change a year from now maybe because the computer infrastructure I'd be it you know CPUs or the type of GPU running on or something has changed but so maybe one rule of thumb is you know every now and then maybe every few months if you're working on a problem for an extended period of time for many years just try a few values for the hyper parameters and double check if there's a better value for the hyper parameters and as you do so you slowly gain intuition as well about the hyper parameters that work best for your problems and I know that this might seem like an unsatisfying part of deep learning that you just have to try on all the values for these hyper parameters but maybe this is one area where deep learning research is still advancing and maybe over time we'll be able to give better guidance for the best hyper parameters to use but it's also possible that because CPUs and GPUs and networks and data says are all changing and it is possible that the guidance won't to converge for some time and you just need to keep trying out different values and evaluate them on a hold on cross-validation set or something and pick the value that works for your problems so that was a brief discussion of hyper parameters in the second course we'll also give some suggestions for how to systematically explore the space of hyper parameters but by now you actually have pretty much all the tools you need to do their programming exercise before you do that adjust or share view one more set of ideas which is I often ask what does deep learning have to do the human brain
tPcTenIPQm4,What does this have to do with the brain? So  what does deep learning have to do with the brain? At the risk of giving away the punchline  I would say not a whole lot  But let's take a quick look at why people keep making the analogy between deep learning and the human brain  When you implement a neural network  this is what you do  forward prop and back prop  I think because it's been difficult to convey intuitions about what these equations are doing really gradient descent on a very complex function  the analogy that is like the brain has become really an oversimplified explanation for what this is doing  but the simplicity of this makes it seductive for people to just say it publicly  as well as  for media to report it  and certainly caught the popular imagination  There is a very loose analogy between  let's say a logistic regression unit with a sigmoid activation function  and here's a cartoon of a single neuron in the brain  In this picture of a biological neuron  this neuron  which is a cell in your brain  receives electric signals from your other neurons  X_1  X_2  X_3  or maybe from other neurons A_1  A_2  A_3  does a simple thresholding computation  and then if this neuron fires  it sends a pulse of electricity down the axon  down this long wire perhaps to other neurons  So  there is a very simplistic analogy between a single neuron in a neural network and a biological neuron-like that shown on the right  but I think that today even neuroscientists have almost no idea what even a single neuron is doing  A single neuron appears to be much more complex than we are able to characterize with neuroscience  and while some of what is doing is a little bit like logistic regression  there's still a lot about what even a single neuron does that no human today understands  For example  exactly how neurons in the human brain learns  is still a very mysterious process  It's completely unclear today whether the human brain uses an algorithm  does anything like back propagation or gradient descent or if there's some fundamentally different learning principle that the human brain uses? So  when I think of deep learning  I think of it as being very good at learning very flexible functions  very complex functions to learn X to Y mappings  to learn input-output mappings in supervised learning  Whereas this is like the brain analogy  maybe that was useful ones  I think the field has moved to the point where that analogy is breaking down and I tend not to use that analogy much anymore  So  that's it for neural networks and the brain  I do think that maybe the few that computer vision has taken a bit more inspiration from the human brain than other disciplines that also apply deep learning  but I personally use the analogy to the human brain less than I used to  So  that's it for this video  You now know how to implement forward prop and back prop and gradient descent even for deep neural networks  Best of luck with the problem exercise  and I look forward to sharing more of these ideas with you in the second course 
x8s2jJGCT8M,Exercise 1  Introduction to Deep Learning  Hello everyone  This first exercise utilizes the neon framework to classify handwritten digits  While we will go into the specific detail to deep learning algorithms in the next lectures  this exercise will hopefully provide an introduction to the basic concepts that are common to all deep learning models  If you have any questions or issues during the exercise  please post in the forum corresponding to this exercise and someone will help you  All of the exercises run as IPython Jupyter Notebooks  which allow you to interactively create and run Python code  We use Python as it is not only quite popular  but also fast to prototype with  and has a rich ecosystem of existing packages especially  for data science  In this first exercise  we will utilize the computer vision dataset and MNIST  which contains 70 000 images of handwritten digits  Each image is a 28 by 28 pixel handwritten digit from 0-9 as shown here  To classify each image  we will generate a backend  load in this dataset  specify the model architecture  define the training parameters  and then train the model itself  Finally  we will evaluate our first deep learning model  To move between cells and this IPython Notebook  simply press shift and enter  You can always create new cells by selecting insert and insert cell below  and feel free to hack around and modify the code  If you would like to restart the kernel  press kernel and select whichever option you would like  Okay  Let's get started  Because each image is at 28 by 28 pixels  we have a total of 784 features for each image  This will be the input into our neural network  Our model will be a multi-layer perceptron  also called the softmax regression  and will tell us which digit from 0-9 in the image from MNIST corresponds to it  We'll start by setting up the compute backend  which we can do by simply importing gen_ backend from neon and setting the pot size  Remember titch shift and enter to move between the cells  The next step is to load the data  which in many cases can be non-trivial  Many times that will not fit into memory and normally  you would have to specify the training and validation sets  However  in this example  we have included an easy function that downloads the MNIST dataset and loads it into memory  As shown here  the data is partitioned into two datasets  a training dataset and a validation dataset  We will talk in depth about the importances of splitting the dataset  but in essence  we do this so that we can evaluate the model based on data that has not seen before to avoid overfitting  After loading the data  we can start constructing our model  We start by initializing our weights which we will do using a Gaussian distribution with zero mean and 0 01 standard deviation  The initialization function is actually very important to the success of a model and needs to be chosen very carefully  There are many other initializers in neon  many of which we will use in future exercises  Next  we will create the architecture of our model 
Qab0d7rwWhI,Deep Learning Basics  Welcome back  During the last lecture  I provided a brief introduction to deep learning and the neon framework  which will be used for all the exercises  In this lecture  we will learn the fundamentals of deep learning including activation functions  various ways to initialize the model  cost functions  optimization techniques  and the famous backpropagation algorithm  Let's get started  We will first have here some concepts from the previous lecture  We learned that deep learning is used across a variety of domains including video recognition  object detection  clustering  speech recognition  playing video games  and fault line detection in seismic data  Its recent success is attributed to larger datasets  faster hardware  and newer algorithms  We learned that deep learning is a subset of machine learning which is a subset of artificial intelligence  Machine learning can be described as a program that learns a function that maps features from the input data to some desired output and whose performance improves with more data  Deep learning can be described as a program that learns to extract features from the data with increased complexity at each layer  and also learns a function that maps these features to some desired output  We also discussed the neon framework and its ability to define a wide range of models  In this lecture  we will learn the fundamentals that are needed to train deep networks  While the exercises are in neon  these principles are applicable across all the frameworks  We'll start by introducing the multi-layer perceptrons or MLP  The most popular types of deep networks used in supervised learning are multi-layered perceptrons or MLP  also known as deep feed forward networks  convolutional networks or CNNs  and recurrent networks or RNNs  and there are many others such as auto encoders  generative adversarial networks  deconvolutional networks  etc  On the right  we show an MLP model with two hidden layers  This MLP can and has been used for the task of digit classification  The input are the 784 pixel values of 28 by 28 image  containing one handwritten digit  That is the input layer has 784 units  The first hidden layer has 128 units and the second hidden layer has 32 units  The last layer is the output layer and has 10 units  one for each digit  The output of each unit represents a probability that the input image corresponds to a particular digit  As a side note  some people refer to these units as neurons and to these models and networks as neural networks in an attempt to claim resemblance to actual neurons in the brain  However  given that I am a machine learning engineer and not a neuroscientist  I will not attempt to claim a resemblance to actual neurons  and will continue my nomenclature as units and deep networks instead of neurons and neural networks  Also in my nomenclature  X is the input  W is a matrix of weights  A is the activation or output of a hidden unit  Y hat is the output  and Y which is not shown here  is the ground truth  The superscripts represent the layer index and the subscripts represent the unit index  This small network already requires over 100 000 parameters  Reducing the number of parameters will be one of the main motivations for convolutional networks  which will be discussed in the following lecture  There is a weight from every unit in a layer to every other unit in the next layer and a bias  The bias is not drawn here  therefore  W zero has 784 times 128 values  B zero has 128 values  W one has 128 times 32 values  B one has 32 values  W two has 32 times 10 values  and B two has 10 values for a total of 104 938 weight  This network can be trained for the task of image classification  In this example  the inputs are the pixels in an image  The output is a probability distribution over the 10 digits  In this figure  I'm only showing three output units  but imagine there were 10 instead  Y hat 1 through Y hat 10  If the image corresponds to the number four  the vector y is a vector with all entries zero except for the fifth entry corresponding to the number four where there is a one  There are six steps required to train a network  First  the weights of the network are initialized often drawing samples from some distribution  Second  as small batch of data is fetched  Third  the data in the batch is passed through the network  This is known as forward propagation  Fourth  the cost C is computed  The cost is a metric of difference between the actual output Y hat and the expected output Y  Fifth  gradients of the cost with respect to the weights and activations are back propagated in order to know how to adjust the weights to reduce the cost  Six  the weights are updated  This completes one iteration of training and the cycle is repeated starting with step two until the cost no longer decreases  Note that  I am only showing one input in the batch  If the batch has  say 32 input  then both Y hat and Y would be matrices of size 10 by 32  That is  its input in the batch has its output with a distribution over 10 values and its own ground truth  Once the network is trained  it can be used to infer labels on new data it has not seen before  This is known an inference  In this example  an image of the number 4 is the input to the network and the output is a probability distribution over the 10 values  This network is 80 percent confident that the input data corresponds to the digit four  Note that inference is simply a forward propagation through the network  Let's dive into what happens at each layer and unit  The MLP network is composed of an Affine layer  An Affine layer is a layer of units that linearly combines the weighted output from the previous layer  adds a bias  and applies an activation function  Each unit linearly combines the weighted output from the previous layer  adds a bias and applies an activation function  For example  the input to this unit is the weighted sum x1  x2 and x3 plus a bias  that is  x1 times w1  plus x2 times w2  plus x3 times w3  plus the bias b  This weighted sum is denoted by z  Then an activation function denoted by g is applied with z  resulting in the activation or the output of the unit a  Note that we refer to g as the activation function and to a as the activation  The same applies for some layer i_plus_one  The activation a_i_plus_one is the weighted sum of the activations from layer i plus a bias passed through the activation function g  Many types of activation functions are commonly used  One such activation function is the hyperbolic tangent  which has an appealing interpretation  Suppose your i unit is responsible for detecting a particular feature  If your input z is near zero  then you're uncertain about the presence of that feature  In that case  your gradient or derivative is high  which encourages training  However  when your input z is too high or too low  your gradient is almost zero  causing the weight not to learn  The logistic activation function is similar except that its range is from zero to one  which can be useful for modeling probabilities  The rectified linear unit activation function or RELU has become quite popular as it was found to accelerate the train process compared to the sigmoid or hyperbolic tangent functions  The RELU can be implemented by simply thresholding a matrix of inputs at zero and does not require computing exponentials  While they are very popular  however  RELU units can be fragile during the training and can die  For example  a large gradient flowing through a RELU unit could cause the weights to update in such a way that the unit will never activate  There are variants of RELU that mitigate this at the expense of additional computations  such as noisy RELU  leaky RELU and parametric RELU  A recently proposed activation function is the scaled exponential linear unit or SELU  which self-normalizes the activations to a zero mean and unit variance which reduces the risk of diminishing gradients or exploding gradient for very deep networks  We will talk more about these risks in a moment  Finally  the Softmax activation function is common for classification as its output can be interpreted as a probability distribution  That is  the output values are all between zero and one  and sum to one  Next  let's talk about various methods to initialize the weights to start the training process  The initialization of the model weights can drastically affect the train speed and performance of deep networks  in particular  for deeper networks  A simple initialization approach is to sample from a Gaussian distribution  but there are better ways  Here's why  exploding and diminishing gradients are two challenges with deep networks  That is  as we backpropagate the gradients  they can exponentially increase or decrease  and for very deep networks  they can explode or diminish to zero  One way to mitigate this is to initialize the weight such that the activations and backpropagations have unit variance  GlorotUniform and Xavier approximates this units variance for sigmoid or logistic activations  and Kaiming for RELU activations using the distribution shown here  Each of these methods are named after its author  with Xavier and Glorot coming from the same author  Xavier Glorot  In these equations  d_in represents the number of input  also known as fan-in  and d_out the number of output  also known as fan-out  Logistics or sigmoid have linear activations near the origin  That is  the output a is approximately equal to the input z for values near the origin  and the z values are usually near zero for the first few training iterations  The variance of the activations is equal to the variance of the input times the variance of the weight  times the number of inputs d_in during the forward propagation  or times the number of outputs d_out during the backward propagation  Assuming the inputs have unit variance  to maintain unit variance in the forward propagation  the weights should be initialized to have a variance of one over d_in  To maintain unit variance in the backward propagation  the weights should be initialized to have a variance of one over d_out  Glorot makes a compromise between these two as two over d_in plus d_out  It can be shown  although that relation is not shown here  that a uniform distribution over the interval negative b to b has variance b_squared over three  We set that equal to the variance of the weight and solve for b  and find that b equal to square root of six over d_in plus d_out  This is the GlorotUniform initialization  the weighted sample from a uniform distribution over the interval negative b to b  Note that the distribution is the same for units within a layer  but it may be different for units across layers  The Kaiming initialization is useful for the RELU activation function  The variance of the activation is half of what it was for sigmoid because RELU zeroes out the negative inputs  Assuming the inputs have unit variance  to maintain unit variance in the activations  the weights should be initialized to have a variance of two over d_in  Kaiming's initialization allow the training of much deeper networks  For example  prior to this technique  the authors of the popular VGG paper had to meticulously initialize the layers of the larger VGG networks in various steps  And it's okay if you're not yet familiar with the VGG network  My point is that with Kaiming's initialization technique  this meticulous way to initialize layers is no longer needed  In this plot  we observe that Kaiming's initialization allows for better convergence  meaning a lower cost or error for a 22-layer network and enables the training of a 30-layer network that would otherwise not train  Next  we will discuss the metrics used to compute the cost  The cost is a measure of the difference between the output y_hat and the ground truth y  Training aims to minimize this cost or difference  Various cost functions or metrics can be used to train networks  including the cross-entropy loss  the misclassification rate  the L2 loss  also known as the mean squared error  and the L1 loss  also known as the mean absolute error  For regression tasks  the L2 loss is a popular metric  For classification tasks  the cross-entropy loss is a popular metric  The cross-entropy loss for one input sample is computed as the sum over all the outputs of the negative expected output times the log of the actual output  In this example  the expected output is zero for all but the fifth output which is one  The fifth value of the actual output is 0 1  Therefore  the cross-entropy error is negative one times the log of 0 1  which equals one  The cross entropy loss is a better metric than the misclassification error as we'll demonstrate by this example  Here we see the output of two different models  The outputs on the top come from the first model and the outputs on the bottom comes from the second model  Both of these models have the same misclassification rate  The first two outputs are correct and the third output is incorrect  However  upon closer inspection  the outputs of the second model are better because they have a higher confidence or probability on the correct class  and lower confidence on the incorrect class  The cross-entropy loss take this into account  The cross-entropy loss for the first model is 1 38  and for the second model it's only 0 64  Next  we'll learn about various optimization techniques that are used to reduce the cost  We will also take a short detour to review gradient descent and its variants  There are various optimization algorithms that can be used to minimize the loss function  such as gradient descent  stochastic gradient descent  AdaGrad  and others  Because most of these optimizers are variants of gradient descent  we will take a detour to review gradient descent  Gradient descent is a popular algorithm used to find a local minimum of an objective function  On the top right  you see the objective function computed at some value w_zero at iteration zero before starting grading descent  w represents a set of weight of the network  In this two example  w has only one dimension  but in practice  it can have millions or even billions of dimensions and the same theory applies  In supervised learning  the cost is a penalty of how far the network's output is from the true expected output  The user can choose a cost function such as the squared difference loss commonly used for regression tasks or the cross entropy loss commonly used for classification tasks  The objective function is the sum of the cost over all the N training samples  When training a network with no prior knowledge of the search space  the first guess w zero is usually sampled from some distribution defined by the user  For example  a Gaussian distribution with a K mean standard deviation  Next  we need to know which direction to move towards the minimum  In this toy example  we can visualize the object function and know that we have to move towards the left  But in practice  in a high dimensional space  we won't be able to visualize the object  One method to determine which direction to move  is to compute the derivative at w zero  If the derivative is positive  then you move to the left and vice versa  If the derivative is negative  then you move to the right  In other words  you move in the direction opposite to the gradient  As we can see in this figure  the derivative is positive  so we will want to move to the left  Note that the cost functions must have the property of differentiability  Both the cross entropy laws and the sum of square differences have this property  Next we update the weights by taking a step in the opposite direction of the gradient  That is w one equals w zero minus the gradient at w zero  Well  almost  The size of the step taken is controlled by a learning rate alpha  which is an important hyper parameter to tune by the user  Choosing a good learning rate is important for the training efficiency and effectiveness  If the learning rate is too small it will take too many steps to reach the minimum  And notice that as you approach the minimum  the gradient gets closer to zero and therefore the steps get even smaller  If the learning rate is too large and w one it's too far to the left  the objective function at wâ‚ is now worse than at w zero  That is you are diverging and moving farther away from the minimum  Here we have selected a good enough learning rate  The steps taken do not overshoot the local minimum  but are also not too small  After one step is taken and the weights are updated  another step can be taken by repeating the same process  This process can be repeated to calculate w three and w four and so on until the local minimum is reached  However in practice  we use variants of gradient decent  as gradient decent has some glaring issues for deep learning  One issue is that gradient descent requires summing overall the cost of the end trailing samples as shown on the top right  That is  you need to compute the cost for all the training samples before you can take one step  In deep learning N can be millions  making gradient descent very inefficient  Aside from the computational expense  gradient descent converges to our poor local minimum  There are some theories to explain this but these are still theories and this is an active area of research  One theory is that saddle points pose a problem for the algorithm  You can get stuck in a saddle point and the training stops at a sub optimal value  When the weight reaches a saddle point  the learning stops  A second theory  it appears that the optimisation space has many sharp minimums  Gradient descent does not explore the optimization space but moves towards the local minimal directly underneath its starting position  which is often a sharp minimum  Sharp minimums do not generalize well because the cost of the validation or test dataset may be much different than the cost on the training dataset  In this diagram  we see that the objective function with respect to the dataset is similar to that of the train dataset but is slightly shifted  This shifts result in models that converts to a sharp minimum having a high cost with respect to the test dataset  Meaning that the model does not generalize well for data outside of the training set  On the other hand  models that are converts to a flat minimum have a low cost with respect to a test dataset  Meaning that the model generalizes well for data outside of the training set  Note  that this is not the result of overfilling  Even though the symptoms are similar  Don't worry if you're not familiar with overfilling  We will cover overfilling in a later lecture  In summary  the three issues with gradient descent are as follows  It is computationally expensive  Training can stop at a saddle point and it often converges to a sharp minimum  which does not generalize well  Note that the last two points are theories and the exact reasons for the poor performance is an active area of research  Modified versions of gradient descend are used to mitigate these issues  One is a stochastic gradient descent or SGD  Note that the term is SGD was originally reserved as a special case of mini batch gradient descent with a batch size of one  But now  including it in this lecture  the term is SGD means meaning mini batch gradient descent  Apologies to the purist  In SGD with computed gradient  which I call the batch gradient  which is computed use in the aggregated cost over a small batch of training samples  This gradient will be different than the gradient computed in gradient descent  which I call the global gradient  which is computed using the aggregated cost of the entire dataset  SGD mitigates the issues previously discussed as follows  SGD allows us to make faster progress because we don't have to compute the error for the entire dataset  SGD mitigates the saddle point issue because while the global gradient at a saddle point is zero  the batch gradients will not be and the model can move away from the saddle points  SGD mitigates the sharp minima because it better explores the solution space  Empirically  SGD has been shown to optimize models much better than plain gradient descent  The objective function used in gradient descent is shown on the top left  The objective function used in stochastic gradient descent are shown on the top right  In SGD  the dataset is split into M different equal sized batches  For each data batch  the costs are aggregated  The ingredients are computed  and the weight are updated  The size of each batch is a hyper parameter that the user has to choose  A very large batch starts to suffer from the same issues as gradient descent  A very small batch size  may not be able to keep their hardware a high realization  and limits the number of nodes that can be used for distributed training  We will dedicate an entire lecture to distributed training and their relation to SGD  In this figure the red X represents the weight of the current model at the start of training  In this toy example  the model only has two values  That is  the weight space is only a two dimensional space  The dotted line represents the trajectory that the model would have taken with gradient descent  Note that the arrow shows the vector equal to a negative batch gradient  This is the direction of the first step  Over the next slide  we will take many steps in the direction of the batch gradients computed for each mini batch  Note that SGD makes progress towards the minimum  but then it has a hard thing converging and bounces around the minimum  During training  SGD computes the cost with respect to multiple data batches and actually cycles multiple times or the various data batches  We call an epoch every time all the boxes in the dataset are used  In this SGD example  One epoch equals m iterations since the dataset has m set of batches  Training a model may require anywhere from one epoch to a couple hundred epochs  Note that in gradient descent  a batch equals entirely that descent  Therefore in gradient descent  one epoch equals one iteration  Another training hyper parameter is momentum  In stochastic gradient descent  while each individual gradient may not be in the direction towards the minimum  on average the overall direction tends to be towards the minimum  We can take advantage of this by accumulating the overall direction of the gradient and taking steps in the direction of the accumulated gradient known as the velocity  In the bottom right  the velocity is updated as a way to sum between the previous value of the velocity and the current gradient  While the momentum parameter mu is another hyper parameter to potentially adjust  there is active research to develop robust techniques to better select this momentum parameter  For now  most practitioners use mu equals to 0 9 as the de facto momentum  Momentum can be thought of as similar to momentum in classical physics  As when we threw up or downward  it gathers momentum and its velocity keeps increasing  Here is a graphical total representation of momentum which may help  The red arrow is the gradient and the green arrow is the velocity  The bottom of the arrow is the current position in the weight domain  and the head of the arrow is the next position  With each parameter update  we can observe that the gradient modifies the velocity and the velocity determines the next position in the weight domain  On the next light  we observe that the gradient modifies the velocity  And the velocity determines the next position in the weights domain  Even when a particular gradient point in the direction opposite to the desired minimum  progress in the desired direction continues and the velocity only slows down  Another view of creating descent and SGD is as follows  Training constitutes feeding in the various data samples into a model for propagating  calculating the cost  and back-propagating to compute the gradient  With gradient descent  the weight update  delta W  is computed by aggregating all the gradients  For example  on the other hand with stochastic gradient descent  the data is splitting to many batches  In this two example  two batches  The weight update for each batch is computed by aggregating all the gradients for each sample in the batch  There are other optimizers that make slight modifications to stochastic gradient descent to improve it further  AdaGrad for example allows the learning rate to be normalized by the size of the gradient  This helps the model take larger steps than he would otherwise take when the gradients are small which can occur in a subtle point  And it helps the model take smaller steps than it would otherwise take when the gradients are large  Having these dynamic learning rate reduces the need for the user to manually search for a good learning rate  This can take time for the deep learning practitioner  Note that each waiting model will have its own dynamic learning rate  AdaGrad normalizes the learning rate using all the gradients for that particular weight  starting at iteration zero  RMS propagation is similar  only that it takes into account the previous few iterations only and are all as AdaGrad  Both RMS prop and AdaGrad are very popular optimization techniques  While some research shows that SGD can converge to a slightly better minimum AdaGrad and RMS prop reduce the need for the user to manually search for a good learning rate  Regardless  training and deep network requires an incredible amount to compute  For example  Alexnet on the image net dataset runs for 90 epochs  processing 115 million images over a seven layer deep network with around 60 million parameters  And that was in 2012  Now we have deep networks with hundreds of layers  We have covered the cost and optimizers  The optimizers require a simple gradients of the weights  In this section  we will explain how to compute the gradients of the cost with respect to each weight for all the weights in the model via the back-propagation algorithm  In this network  suppose that we want to compute the gradients or partial derivatives of the cost C with respect to weight four  five in layer one  We can compute these using the chain rule  Not that I will abbreviate partial derivative as just partial  To compute the partial of C with respect to W451  we need to back-propagate from C and compute all the partials that are circled in red  Working backwards  we started by computing the partial C with respect to ZK3  This equals the partial of C with respect to Y hat K times the partial of Y hat K with respect to ZK3  The result is backpropagated to compute partial C with respect to A42  Note that A42 is used in three inputs in layer three  and all those inputs affect the cost C  Therefore  the partial of C with respect to A42 is a summation over the three units in A3 as shown here  Similarly  the partials C with respect to A42 is back-propagated and multiplied by the partial A42 with respect to Z42 to compute the partial C with respect to Z42  This is back-propagated and multiplied by the partial Z42 with respect to W451 to compute the partial C with respect to W451  And we're done computing their partials  In practice  we don't compute the partials one by one but in groups  as we'll see in the next slides  On top the equations that were used in the previous slides are shown  Below them  the functions whose gradients need to be computed are also shown  Using these equations and high school calculus  all the gradients can be computed  I will leave that as an exercise to you  You can derive these equations and compute the partial C which relate to any weight in the network  Finally  note that in practice  all these operations can be computed more efficiently as matrix multiplications  Okay  now that we have gone through the derivation of back-propagation  let us quickly talk about controlling gradient flow  As networks get larger and deeper  controlling the flow of gradients is essential to prevent them from becoming too small and vanishing or becoming too large and exploding  There are a few techniques that we can employ to prevent this  Activation functions as we have seen  are important in controlling the gradients  For example  the RELU activation function is zero when the activation is negative  making degrading zero  no matter what else is going on  Choosing a weight initializer such as Kaiming and or learning rules such as AdaGrad is also important  Finally  layer architecture including the number of layers and layers types  also play a role in controlling the gradient flow  Once again  we will be using the neon framework to design and develop deep learning models in this course  Neon contains many of the elements discussed in this lecture including various initializers  optimizers  activations  layers  and costs  The general procedure for building a deep network in unit is as follows  First  the back-end must be generated  then the data is loaded  The model is defined and hyper parameters are chosen  Then the model is trained and simultaneously evaluated to ensure correct training  We have covered a lot of material in this lecture  We learned the fundamentals of deep learning including activation functions  various ways to initialize a model  cost functions  various optimization techniques  and the famous backpropagation algorithm  You should be ready to start training simple networks and understand the most important hyper parameter choices  In the next lecture  my colleague Kalnin will introduce you to convolutional networks 
VYqHjvWQRec,Exercise 2  Deep Learning Basics  Now that you have a solid understanding of how fully connected neural network works  You're going to put that knowledge into action by improving the previous exercise of recognizing handwritten digits by tuning hyperparameters  And then by creating your own activation function in custom layer  We'll start by returning to the MNIST exercise that we explored in the last lecture  When you went through the exercise last time  the hyperparameters were already set to their current values  Including the learning rate  batch size  and architecture of the model  In this exercise  you will go back and change these values in order to improve the accuracy of the model  Adjust the learning rate by making it either quite small  such as 0 001  or quite large  like 1  Similarly  change the batch size from around 16 to 10 000  Adjust the initialization to Xavier or and adjust the number of layers and number of units per layer  While making these changes  try to understand how each hyperparameter actually alters the training of the network  If you have any questions about how a certain neon function works  refer to the documentation on the neon website  neon nervanasys com  When tuning the network aim to get the misclassification accuracy under 3%  You will now construct your own model by writing a custom activation function in layer  in this next exercise  Activation functions  also called transforms  are non-linear functions that usually follow an path finder convolution layer  They even include the rectified linear softmax or hyperbolic tangent functions  As shown here  the call function is used in forward propagation  and bprop is used in backward propagation  which is used to compute the derivative  Finish the call function by implementing the softmax function  which is defined in this comment  When you complete the softmax implementation  test your code  By the end of this exercise  you should be comfortable with recognizing overfitting  using visualization tools  and applying dropout  In the next lecture  we will transition to discussing multi-node training  an important topic that allows us to minimize the time to training  [MUSIC]
zMv5QI-5bjo,Convolutional Neural Networks  My name is Hanlin Tang  I'm a senior algorithms engineer in Intel's AI Products Group  In this lecture  we will discuss convolutional neural networks  which is a category of neural networks that have proven very effective in area such as image recognition and classification  They are used to identify paces to power self-driving cars and particularly form an integral part in many of our robotic applications  We will start in this session by reviewing some key concepts from the previous lecture  and then provide an overview of Convolutional Neural Networks  And then we will look at some of the important component layers in a CNN such as convolution  pooling  dropout  and more  And then we'll end by providing an overview of many of the popular CNN architectures that are being used today  During the last lecture  we discussed the training procedure for deep neural networks  Take the example of digit classification  where the input to the model are the pixels of an image  and the output is a probability distribution over the 10 digits  That this figure shows only three output units  but imagine there are 10 instead  If the image corresponds to the number four  the vector y here will have all the entries zero except for the fourth  which contains a one  representing the confidence score of the model  To train a network  we slowly adjust the weights  such as the actual output of the model matches the expected output  thus causing our cost to decrease  During the last lecture  we also went over some of the key optimization algorithms used such as gradient descent  stochastic gradient descent  and other key concepts shows the momentum and back propagation  We also looked at initializers  optimizers  activations  costs and metrics used in deep learning models  These are many of the important components that form the deep learning models that we want to build  Today  we'll pay particular attention to convolution  pooling  deconvolution  dropout and local response normalization layers as these are many of the important components inside a typical convolutional neural network  In convolutional neural networks  we take this input image here  such as NxN  And then that image is passed through a sequence of convolution operations that ends up in a subsequent identification  This individual layers will learn complex features to extract  and these features become more and more sophisticated as you go deeper and deeper into the network itself  So for example  the early layer is here in Conv 1 may only be detecting simple edges in the image itself  whereas subsequent and deeper layers will begin to learn complex textures  object parts  or even finally object classes  In addition to classification  convolutional neural networks can also be used with great results in a number of applications  It is popular with any type of two dimensional image where there's a sense of space being an important consideration  So this will include not just videos and images  but also synthetic aperture radar for example  or even spectrograms where you have applaud of time and frequency  And understanding the spacial relationship between those two dimensions is important for the network to perform its task  Additionally  convolutional neural networks are also used in a variety of other applications beyond just image classification  And this example here  we have SegNet  which is a popular model to do image instance segmentation  So the input to this model is an image and the output is an image of exactly the same size  except that pixels are colored according to the particular category that that object belongs to  so building  road  sidewalk  cars or trees  Finally  convolutional networks are also often used to take an image and not just to identify where the individual objects are in that scene  but also a classified image of particular categories  So you can see some examples here on the right  where we have several images and the model learns to put bounding boxes around each individual image  but also attach a particular category to it  Here we want to motivate why you want to use convolutional neural networks  In the previous lecture  we had used a feed four network to classify images on the endless data set  So you may ask  why we need to use a different method to classify images in other data sets  If you recall in the endless data set  the input image had about 784 pixels  What that meant was that the first layer of weights had about 100 000 parameters  which is manageable for this particular model  But a lot of the natural images that we encounter are not so small  For example  a typical image would be 256 by 256 pixels or around 56 000 pixels total  That would mean that the first layer weights have about 8 million parameters  This method is unscalable to real images  You end up building networks that have so many parameters that is difficult to train with the mono data that you have  Additionally  we also want to take advantage of the spacial locality present in images  That's the intuition that nearby pixels are more correlated than pixels that are farther away  So we can use this prior knowledge to reduce the number of parameters  So what we're going to do in this example here  is take the affine layer  the linear layer that we had shown before the MNIST dataset and then we are going to delete connections  Importantly  we are going to delete connections that are consistent with the understanding that nearby elements are related together  So specifically in this case  we will delete all the connections such that each unit in the subsequent layer is connected to a local path on the input layer  This is taking advantage of our intuition of special accounting that are highly discuss previously  But the fundamental operations are exactly the same  We take those two input elements  We multiply it by a set of weights  We add a bias  passes through a long non-linear activation function to provide the output in the hidden layer  So we call these waves now filters  So why do we call them filters? Here's an example  As you recall the output Z is a weighted sum of the inputs according to our weights W1 and W2  Suppose we had a scenario where the two weights were one and minus one  So then the output value would be z  would be x1- x2  In that case  Z would be maximal when x1 and x2 are one and zero  What this means is that this filter is very good at it detecting edges in the input space where there's a large jump between x1 and x2  And that's how we mean by filters  is that this set of filters are very good at extracting certain features  And the exact filter values are learned during the training process  And so by doing this  we can take advantage of the spatial locality and reduce the number of parameters even further  What we do here is we replicate that exact same filter weight  Across the different input spaces  So we're essentially taking this filter and then tiling it across space  as you can see here  with the red and the blue colored weights here  So importantly the weights are exactly the same  They're being duplicated  And this is sort of taking advantage of our intuition that edges are important in an image regardless of where they are in the image itself  So then we have a set of filter detectors to extract important features from these images  So this is nothing more than one dimensional convolution with one filter  W1 and W2  And that filter has a size of two  meaning that there are two elements here  And we have a stride of 2  meaning that we take this filter and we move it two positions forward and duplicate it to represent the input filter for the next spatial location  Here's what one-dimensional convolution with a stride of 1 looks like  As you can see here now  the element takes in an overlapping section of the input space to form its output  Because now we're taking this filter  W1 and W2  and we're moving it up just by one position  When the number of filters is two  as with this network  we can see that the depth to the next layer is increased  If you focus on the hidden layer here  you will see that the first column of activations represents that output from the first filter  And then the second column moving out in a z direction represents the activations from applying the second filter  Additionally in this example we have a padding of one  meaning that we padded the image with zeros on its boundary  And padding allows us to control the spatial size of the output  Where generally for a one dimensional convolutional neural network  is if the input is a dimension Dx1 and if we apply C tilde filters  the depth of the next layer is equal to C tilde  the number of filters that we have  While importantly  the height depends on the stride and padding parameters that we provided here  Typically  inputs are not one dimensional  but rather two dimensional as with images  Here we use a very similar process and instead produce a three-dimensional output that has a depth equal to the number of filters  And then again  the height and the width are dependent on the stride and the padding in the input dimensions H and W  So now you can see here we have an input of an image that has a particular height and width  And then now we have two-dimensional filters  and we have a number of them  And that produces an output volume of H tilde  W tilde  and C tilde  Images are often not just grayscale  but have many channels such as RGB for each individual pixel  To accommodate this  we simply increase the depth of the filters to match that of the channel size  To imagine the convolutional network in action  consider that the filters are being applied to the pixels in the top left corner of the image  Each filter then computes a value that fills the upper left column in the output here  Here's a concrete example that runs through the exact computations that we apply when we use a two-dimensional convolutional neural network  In this example  let us apply two filters  W0 and W1  on an input that has three channels as shown here in the three different matrices  Because three-dimensional volumes are hard to visualize  the depth here is shown with the stacks of two-dimensional arrays  The blue here is the input volume and the red are the weight volumes and finally the green is the output volume  We start with a 3x3 filter  W0 here  which we apply with a stride of 2  Notice how we've applied a padding of one to the input volume  making the outer border of the input volume filled with zeroes  The output is calculated by multiplying in an element-wise fashion the highlighted input in blue  with the filter in red  Summing it up  offsetting the result by a bias can that then seize the upper left corner of the output volume  We then repeat this process with two pixels to the right to get the next value in the output  So you can see here  we're sliding the filter across the input volume  performing an element-wise multiplication  a sum and a bias to finally see the output volume  Similarly with a second filter W1  we then take this filter and we slide it through the input volume on a second pass  Now multiplying with this new set of filters to see the output volume for the second channel  The convolution operation can be implemented naively as a set of seven for loops as shown here  Of course  we develop much more efficient implementations using matrix multiplication that are not discussed in this particular lecture  As discussed earlier  in the convolutional layer  each kernel or filter shown here in red  searches for unique patterns  For example  in the lower layers  kernels may search for edges  and in higher layers  faces or other complex structures  The fascinating phenomenon in deep learning is that the kernel specialize on their own during the training phase to detect different types of features  And in many cases those features are generalizable across multiple datasets  because they capture the important statistics that exist in the natural world  The pooling layer is another important component to convolutional networks and serves to down sample the input of the model  For example  in this particular figure  each of the 2x2 region is pooled to one value by selecting the maximum value in that region  This also provides some invariance to translation  as small shifts in the network input may not affect the output of the pooling layer  given the max operation that is being used  This operation  common in the VTG network  takes as input in this case with dimensions 4x4xC and then applies a convolutional layer with a stride of 1 and a padding of 1  We then apply pooling using a 2x2 max pooling  with a stride of 2  And in this fashion we're able to maintain through convolution the size of the output space while increasing the depth of the number of filters  And then we rely on the pooling layers to reduce the size of the feature map  This is a common approach in designing convolutional neural networks such as VGG  The number of features increase as we go deeper into the network  So you can see here we start off with just 3 channels in the input space and we eventually increase to about 512 channels  But as we go deeper and deeper  the H and W decreases from 224x224 to  eventually  7x7  And we choose specific convolution stride  and padding  and filter sizes to retain the feature map size after a convolution has taken place  So essentially what we're doing here  as we go do deeper into the network  is that we are trading off spacial information  And pushing it into a more complex feature representation  that is reflected by the larger number of channels in many of these convolutional layers  Local response normalization  or LRM  consists of normalizing values with nearby features with in or across channels  While this was used in AlexNet  it is not commonly used anymore  and it has been replaced by dropout and batch normalization as important regularization techniques  Dropout reduces overfitting  by preventing co-adaptation on the training data  During training what dropout will do it will ignore a fraction of the units  and that selection is going to be randomized from mini-batch to mini batch  So what that does is that prevents the model from relying on specific features  of individual units to drive the output  Instead  it must rely on a distribution of units and the information there in order to perform its computation  The other way to think about drop out is that  because at each mini-batch a different model  slightly different model  is being trained because you're silencing different portions of a unit  In that way  you're training very different models from mini-batch to mini-batch  And also mirrors a lot of the ensemble methods that we use in machine learning  to combine information from multiple models together  Batch Normalization allows networks to converge faster and achieve lower error  At the end of every Batch Normalization layer  the output of all the neurons are normalized to have zero mean and unit variance  This allows the network to be significantly more robust to bad initializations  And it reduces what's called the internal covariance shift  Essentially  as the model continues to learn the output statistics on a particular layer may change drastically  That is may considers a challenge for future layers who receive an input to adjust to these statistics  Instead  by inserting batch normalization layers to ensure a normalized output  we reduce this issue that future layers have to dealt with  Here's an example of batch normalization network  What we're showing here is the accuracy over training time  And you can see that if you use batch normalization you can train significantly faster  But also to a higher accuracy compared to methods that do not use batch norm  An important question is to try to understand  what the rates are running after the training process has completed  So here is some more visualization from various layers over computational neural network  You can see the first layer here  the filter weights that are being run are principally edge detectors  or they are detecting contrast and color Here's an example of these edge detectors at work applied to a particular image  You can see that edge detectors of different orientations give rise to output feature maps  that highlight edges that lie along those particular angles  As we go deeper and deeper into the network  the filter weights become much more complex  and often difficult to interpret  So you can see here in Layer 2 and Layer 3  particular combinations of edges or lines are being learned  And the model is beginning to be able to extract more complex features  patterns from the image  Deeper layers  such as these  give rise to the more complex features that are ultimately used to classify an image  As you can see here with Layer 4 and Layer 5  we began to find patterns that are specific to particular objects in the scene  Interestingly the method by which convolution neural networks interpret images  are very similar to that of the human brain  In fact  the way we architect our convolutional neural networks together  are very inspired by experiments and compuational neuroscience over the past several decades  So on the left here  you can see a model that was published in 2007  based on neuroscience experiments  that also has an alternating misshare of convolutional layers and pooling layers  And in fact  if you were to go in to a monkey brain for example  and record the filters of the early visual cortex layers  You will see the exact same filters as were learned by our convolutional neural networks in deep learning  You will still find edge detectors that are very similar between these two different modalities  Here's a plot of classification error on ImageNet  which is a popular image classification benchmark over the years  You can see in blue the error rate of the model  and then in the orange are the number of layers in that network  And you can see a significant drop in the error rate as we move in 2011  to CNN and deep learning base approaches in 2012  But it is really with the ability to stack deeper and deeper layers here as you can see moving from eight layers with AlexNet to 19 layers with VGG  And finally more recent approaches with deep residual networks  which can have hundreds or in many cases thousands of layers  That will finally reach a near human level of performance with image recognition  In this last section  I will cover many of these popular architectures that are being used today  These include AlexNet  which was one the early networks that won the ImageNet competition  and then GoogLeNet  VGG  and a deep residual networks  AlexNet was very much one of the early neural networks that proved that these deep learning techniques  which have been around for many years  finally have relevance in a practical setting  You can see here we have alternation of convolution of pooling layers  finally ending with fully connected alpine layers tat do with the classification at the end  The GoogleNet  they decided to use instead all the different types of filter sizes in the context of a single layer  And here is what it looks like  On the left here  you have the image  and you have a number of filters being applied  and those filters are then stacked together in the channel dimension to represent the output  So then the output has some image size  H and W  but then filters where contributions are given by each individual set of parameters  5 by 5  3 by 3  and 1 by 1 filters  The advantage here is that you can maintain the same output volume  but dramatically reduce the number of parameters  Here's an example of an inception module highlighted in red here  The output size is 14 by 14 HW  with 512 channels  And  importantly  those 512 channels receive contributions from a variety of different input configurations  1x1 filters  3x3 filters  5x5 filters  and more  And this particular module has 437 000 parameters  But if we had just simply used a sort of dense 3x3 filters  as was present with other networks  Then that exact same layer would have something on the order of 2 4 million parameters  So you can see here we have not only dramatically reduced the number of parameters  But we have also incorporated different types of convolutions in the same layer  VGG demonstrated that the depth of the network was critical for good performance  and is also attractive because of its simplicity  All of the convolutions are 3x3  and there's only convolution and pooling layers that have the exact same kernel sizes  In practical experience  we found that the VGG network is much better for fine-tuning tasks compared to GoogleNet  And as I had mentioned before  the design philosophy here is that  as you go deeper into the network  the HW of the feature map decreases  But then we transform a lot of that spacial information into a more rich representation  Because the number of channels increases as we go deeper into the network  Residual networks introduced in 2015 feature skip connections that bypass certain layers  As an example  this 34-layer residual network takes the VGG 19 model  adds new layers  And then adds skip connections  which then allow the gradient to through without vanishing  an issue with extremely deep networks  Interestingly  the network does not feature pooling  But rather uses convolution with a stride of two to reduce the future amount  As networks get deeper and deeper  challenges  such as vanishing gradients  become pressing  Innovations  such as skip connections and normalization have allowed networks to progress hundreds of layers  But new techniques must be used in order to build convolutional networks that have thousands or eventually tens of thousands of layers  In neon  convolutional networks are quite easy to implement  By simply using the built in convolution pooling dropout and affine layers of neon  Alex then can be written with these lines of code 
IBL9MqvpPlM,Exercise 3  Convolutional Neural Networks  Hello everyone  for the first part of this lecture's exercises  you will return back to the MNIST dataset to implement a convolutional neural network to recognize handwritten digits  Afterwards  you will explore image classification on the CIFAR-10 dataset  In previous exercises  you classified MNIST images using a multilayer perceptron with fully connected layers  Here  you will instead implement a CNN  We will use the same code as before to set up our model  including setting up the compute backend and loading MNIST dataset  However  when defining the model architecture  you will instead use a convolutional neural network  To do so  add a convolutional layer to the list layers using the compound layer conv with 25 by 5 filters and a padding of 2  Then append a 2 by 2 max-pooling layer  followed by affine and dropout layers  and ultimately an output layer with 10 outputs corresponding to the 10 digits  Don't forget to import the necessary layers  and as always for more information on the various layers in Neon  you can go to the Neon documentation  Now that you've created and tested your first convolutional neural network on the MNIST dataset  you will now explore image classification using the conv net on the CIFAR-10 dataset  As you go through the exercise  you will notice that there are many kind of topologies that can be used in convolutional neural networks  From as simple as a model with one conv layer max Boolean and fully connected layer to a deep residual network  Try out different topologies and see how that affects the model  And also go through and fill in the missing components of the deep residual network implemented in the exercise  Again  if you run into issues  please post in the forum alongside this exercise and someone will help you  [MUSIC]
2idtvCcOna8,"Fine-Tuning and Detection  Last lecture  we discussed convolutional networks and their intricacies  In this lecture we will look at fine-tuning techniques as well as detection and segmentation algorithms  We will begin with a brief review  look at feature extractors  discuss fine-tuning and transferred learning and end with a discussion on detection algorithms  Deep networks are extremely versatile and can be used with many different input whether that be images  speech  or text  Let's begin with a brief review  We have learned that deep networks can be used across various applications and domains  We have learned the required steps to train the network  We start by randomly initializing the weights  We then fetch a batch of data and forward propagate through the network  Afterwards  we compute a cost  back propagate and update the weights  We learn that convolutional networks or CNNs used with images vastly reduces the number of parameters in the network by sharing the weights  Covenants transform the original image layer by layer to form more complex feature maps to ultimately classify the image  As shown here earlier kernels act as h and block detectors while later kernels identify more complex structures  such as object parts  We have learnt the bolder terms which are needed to train models in Neon or other frameworks  Let's discuss feature extractors  Convolutional neural networks can be thought of as functions that map pixel space to the feature space with extracted feature then being used to classify images  The activations from the second to last layer in convolutional networks is a popular feature representation circled in this image  Another popular feature representation are the activations from com 5 as well as a combination of the fully connected layers  A feature representation can be extremely useful  Images are often not linearly separable in the pixel space  but they may be linearly separable in the feature space  Shown here are pixel representations from images from different classes which are not linearly separable  When mapped to the feature space the features may become linearly separable  This is a sign that the network is properly learning good features  For example  images on the same row are very close to one another in the feature space  Their colliding distance between their feature vectors  is small  However  one can notice that the L2 distance in the pixel space  can be quite large as the images do not exactly correspond to one another  pixel by pixel  The images are similar in what they represent  A good feature representation can be used as an input to another machine learning classifier  or even another deep network  such as a recurrent network  Shown here  a CNN can be used to extract features from an image which in turn can be used to generate a caption using a language generated recurrent network  Another use of extracting features is to merge future representations from very censored modalities  For example  to combine sensory data from an RGB camera with a radar sensor in order to prove classification  Lets now discuss fine-tuning and transferred learning  Transferred learning via fine-tuning is another powerful tool that can be employed using the feature representation  While training a network with a very large data set can take days a pre-train model can be fine-tuned to fit the application at hand  A pre-train model learns a mapping from pixels to feature space  And as a result  the majority of the network does not have to be retrained  Instead  the last layer or layers can be modified and re-learned with a smaller data set in order to fine-tune the network to match your desired application  such as retraining layer FC8  the linear classifier in this example  Generally  the first few layers are similar within a given domain while the last layers are task specific  therefore a generic image classifier can be used as a basis for a task specific network which can be fine-tuned for a particular task  whether that be for classifying medical images as various types of nuclei or categorizing images as cats versus dogs  Depending on this similarity of the particular task we see a domain more or less layers may need to be fine-tuned  Typically  to fine-tune  one would freeze all the layers  as the earlier layers would not change very much  by setting their learning rate to 0  and then drop the initial learning rate by a factor of 10  The last layer  the linear classifier  can be replaced with a layer with the desired output size and the weights can be trained  So for example  if the desired output is 2  then the last layer would only have two units  Fine-tuning in Neon is quite simple  Start by defining the model  Load the trained weights to all but the modified layer and then assign \""default\"" and \""class_layer\"" optimization parameters in order to fine-tune the network  Finally let's discuss detection algorithms and recent advances in the area  Object detection is the task of finding many different objects in an image  and classifying them  as shown here  The input is an image  And the output are bounding boxes and labels for each object in the image  First around 2 000 region proposals are extracted from the image  using selective search and are passed through a pretrained AlexNet to compute the CNN features  On the final layer of the CNN  our CNN has a support vector machine that simply classifies whether the proposal is an object and if so  what object  However  while the method was effective  it was extremely competitionally expensive as each of the 2 000 regional proposals had to be propagated through network for each image  Additionally R-CNN contains three different models  R-CNN to generate image features  A classifier that predicts the class and a regression model that tightens the bounding box  As result  the model is hard to train  Fast R-CNN addresses many of these short comings by running the CNN just once per image  After doing so  using a technique called Rol Pool meaning region of interest pooling  the CNN features for each region are obtained by selecting a corresponding region from the CNN's feature map  Then the features in each region are pooled  usually using max pooling  Additionally  Fast R-CNN jointly trains the CNN classifier and bonnding box regressor in a single model  greatly simplifying the network  While Fast R-CNN made great improvements in R-CNN in terms of speed  one bottleneck remain in the process  the region proposal  which uses selective search  Faster R-CNN reuse the CNN results calculated with the four parts of the CNN for region proposals by passing this separate selective search algorithm  This Region Proposal Network or RPN takes an image of any size as input and output a set of rectangular object proposals  each with an opticness score  Using Faster R-CNN we're able to use CNN features to locate different objects in an image with bounding boxes  Mask R-CNN extends this technique by allowing for the location of exact pixels in an image a process known as image segmentation  In Mask R-CNN a fully conversional network  FCN  is added on top of the CNN features of Faster R-CNN to generate a mask  the segmentation output  Notice how this is in parallel to the classification and bounding box regression network of faster R-CNN as shown in these diagrams  Instead of our ROIPool  the image gets passed through RoiAlign so that the regions of the feature maps selected by ROIPool correspond more precisely to the regions of the original image  This is needed because pixel level segmentation requires more fine grain alignment than bounding boxes  You only look once or YOLO is unlike other detection systems which repurpose classifiers  or localizers to perform detection  Instead a single network is applied to the image which divides the image into regions and predicts bounding boxes and probabilities for each region  This method  has been shown to be many times faster  than Faster R-CNN  Single shot multi box detector or SSD does not use bounding box proposals but rather discretizes the arbour space of bounding boxes into a set of default boxes over different aspect of ratios and scales per feature map location  This greatly simplifies object detection as it eliminates proposal generation and subsequent pixel or feature sampling  MobileNets present a streamlined CNN for mobile/embedded vision applications  Using depth-wise separable convolutions MobileNets are comparable or slightly worse inaccuracy but many times more efficient than larger networks  Ultimately  object detection has improved remarkably  around 335 times in just a couple of years  Beginning with RCNN in 2014 which ran at 0 2 frames per second  recent models now run at about 60 frames per second  [MUSIC] In this lecture we have covered fine-tuning techniques  and looked at feature extraction and transfer learning  We also discussed detection and segmentation algorithms and recent advancement in these algorithms  During the next lecture we will explore recurrent networks "
#NAME?,Exercise 4  Fine-Tuning and Detection  In this exercise you're going to implement a custom iterator for a modified version of the Street View House Number or SVHN data set  And then using this dataset  you will design and train a neural network  The SVHN dataset contains around 70 000 images of house numbers collected from Google Street View  We have modified the data slightly so the images are 64 by 64 pixels and contain a single bounding box over all of the digits  You goal is to return the bounding box coordinates  four points  for the location of the digits given that image  Hit start by filling out the skeleton of the SBH and data iterator  Once you have done so  you can test your implementation with the code  After writing your custom iterator  you can move on to the model itself  which we'll use this iterator  Start by setting up the weight initializer  such as Gaussian distribution and then start making some layers  I'd recommend using a VDG style convolutional neuro network  which you can find outlined in the lecture slides  Experiment with different sizes  but make sure that the model does not take too long to train  Also make sure not to over fit the data  by comparing the training set cost to the validation set cost  Finally  try to reach a validation set loss of around 220  after ten [INAUDIBLE] In most cases  one will not create a model from scratch  but rather take an existing model and fine tune it so that it works on a different set of data  In this exercise  we'll load a pre-trained convolutional neural network  VGG  which is trained on ImageNet  a large corpus of natural images with 1 000 categories  and use this image to train on the CIFAR-10 dataset  a much smaller set of images with ten categories  After defining the VGG model and loading the pre-trained weights from the neon Model Zoo  you will then fine tune on VGG on the CIFAR-10 dataset as shown  Fill in the code to see the fine tuning in action and note that it is quite easy to adapt the content of this exercise the fine tuning of the dataset as discussed at the bottom of the exercise  [MUSIC]
ZUPE7YEaXbk,Recurrent Neural Networks  I'm Hanlin  from Intel  In this lecture  we will discuss  recurrent neural networks  We will start with a review of important topics  from previous lectures  And then provide an overview  of how our own ends operate  We will then discuss word in beddings  and then take a look  at some of the key  neural network  architectures out there  including Alice T M  and GRU networks  As we now know  training a neural network  consists of randomly  initializing the weights  fetching a batch of data  and then a forward  propagating it  through the network  We can then compute a cost  which will then be used  to back propagate  and determine our weight updates  How do we change our ways  to reduce the cost? In this example  the network is being trained  to recognize  handwritten digits  using the MNIS data set  Back propagation uses a chain rule  to compute the partial derivative  of the cost  with respect to all the ways in the network  Using grading descent  or a variant of gradient descent  we can take a step  towards a better set of weights  with the lower costs  All of these operations  can be computed  efficiently  as Matrix multiplications  So why do we need recurrent neo networks? Feed forward neural networks  make a couple of assumptions  For example  they Assume independence  in the training example  And that after  a training example has passed  through the network  the state is then discarded  All we care about  are the gradient networks  But in many cases  such as with the sequences of words  there is temporal dependence  and contextual dependence  that needs to be explicitly captured  by the model  Also  most feed forward  neuro networks  assume  an input vector  of a fixed length  For example  in all of our previous  slides  the images  were all fixed  in size  across a batch  However  text  or  speech  can vary greatly in size  across sometimes  in order of magnitude  And that is a variability that cannot be captured  by feed forward networks  Instead  our own ends  introduce  an explicit  modeling  of sequentiality  Which allows it to both  capture short range  and long range dependencies  and the data  Through training  such a model  can also learn  and adapt  to the time scales  and the data itself  So our own ends are often used  to handle  any type of data  where we have variable sequence lengths  and  where this  idea  of contextual dependence  and temporal dependencies  that have to be captured  in order  for the task to be accomplished successfully  The building block  of recurrent neuro networks  is a recurrent neuron  What I am showing here  is  a simple afine layer  similar  to what I had shown previously  where  the output of the unit  is the  input  multiply  by a weight matrix  To turn this  into a recurrent  neuron  we add a recurrent connection  such that  the activity  at  a particular time t  depends on  it's activity  at the previous time  T minus one  but multiply  by a recurrent matrix  W R  And so now we have our equations  which were  exactly the same  as what we had seen before  except we have this additional term  here  to represent  that the activity depends  on the activity of the previous time step  As determined  through the weight matrix  W R  Additionally  we add  an output  to this model  y of T  which takes  the  activity  of that recurring neuron  and passes it through  a matrix W Y  to provide the output  And so here you have  the fundamental  building block  of  a recurrent neuron  So how do we train  such a network? We can unroll  their current neural network  into a future forward one  So here's what I mean by that  Here is our same  recurring neuron before  where we have it  taking its input X of T  multiplying it  by a way matrix  to provide  h of t  this hidden representation  And we can read out  from this hidden representation  by passing it through a way matrix  W Y  to get the output W T  And importantly here  we have an arrow  W R  to represent the fact  that  the  subsequent  time steps activity  H of T plus one depends  on what came before it  And so here you can see  unrolling the network  into a feed forward Network  and Time  except where  the way  W R  that spans  these different layers  in the feed forward network  are tied together  But once we've  unfurled  this  recurrent neural network  into a feed forward network  we can apply  the same  back propagation  equations  that I had shown  previously  to train this network  Now that you understand  how an RNN works  we can look at how they're used  So here is an example  where we use an R N  to learn  a language model  Such that  it correctly  predicts  the next letter  in the word  based  on the previous letter  So you can imagine a world  where we have a vocabulary  of forward letters  so the input here  is encoded  So here we have the input  character age  And we have the values  in these gray  boxes  that represent  the hidden activations  And  at every time  we read out  a prediction  of the next character  So here  the model  is quite confident  that the next character is going to be e  And as we move on  and apply more and more characters  it can begin to  predict  additional  characters  in the next time step  So this is nothing more than a language model  Where a model is learning to predict  what happens next  based on  the sequence of characters  that it had seen before  Here's an example  of that same language model  but now apply  to words  where you have the input  cash  flow is high  and then a model  is predicting  the next word  in that sentence  based on what  has been seen  before  You will also notice in this prediction  that there is a level  of ambiguity  here  When the model  sees  it's cash  flow  is  depending on the state of the market  you can see that its output  predictions  in this green box  here  are sort of  split  between low  and high  Which makes sense  as both  are  syntactically  correct  The difference in probabilities  between the two  may be related to the context of the text  or previous data  that it has  been trained on  Recurrent neo networks  and also be merged  with convolutional neural networks  to produce an image capturing network  As seen here  after we provide  the image as the input  the network can learn  to generate  a caption  such as  a group of people  shopping at an outdoor  market  And here  the input to the RNN  could be their feature representation  in one of the last  few layers  of the convolutional Neo network  Training recurrent neo networks  is very  similar  to that  of a feed forward network  Using the chain rule  you can determine  the partial of the cost  with respect  to each weight  in the network  In contrast  to feed forward networks  now we have  costs  associated  with every single timestamp  And so  how we do  is  we combine  the gradients  across  these time steps  as shown here  The issue of vanishing  and exploding gradients  can be quite  problematic  for recurrent neo networks  With especially  deep networks  the weight  W R  is repetitively  multiplied  over  at each time step  So in that way  the magnitude  of your gradients  is proportional  to the magnitude  of W R  to the power  of t  What this means  is that  if the weight is greater than 1  the gradients  can explode  Whereas  if the gradients are less than one  they can possibly vanish  Thats going to also depend a lot  on the activation function  in unhidden node  Given  a rectified  linear unit  It is easier  to imagine  for example  an exploding gradient  Whereas  with the sigmoid  activation function  the vanishing gradient problem  becomes  much more pressing  It is actually  quite easy  to detect  Exploding gradients  during training  You simply  see  the cost  explode  And to counteract  that  one can simply  clip the gradients  inside a particular  threshold  Thus  force  the gradients  to be within a certain  bound  Or additionally  optimization methods  such as  RMSprop  can adaptively  adjust to learning rate  depending  on the size  of the gradient itself  Vanishing gradients  however  are more problematic to find  Because  it is not always  obvious  when they occur  or how to deal with them  One of the more popular methods  of addressing this issue  is actually  not to use  the RNNs  that I had introduced earlier  but to use  LSTM and GRU networks  And that's what we're going to discuss next  So  one way  to combat  the  Vanishing  or Exploding gradient problem  is to have  this very simple model here  where you have a unit  that's connected to itself  over time  with a weight of one  So now you can see  that  as you unroll  this network  the  activity  at every time step  is equal  to the activity  at the previous time step  So while  you no longer have  a Vanishing  or Exploding gradient  here  It's not a very interesting behavior  because it just repeating itself  over and over again  as if it were  sort of a memory unit  So  what we're going to do  is  we're going to manipulate  this memory unit  by adding  different operations  So adding  the ability  to  flush the memory  by rewriting it  The ability  to  add to the memory  and the ability  to read  from the memory  So what we do  is  we have  this  memory  as I had shown you before  where  each unit  was connected to the next one  with a weight of one  And then  we attach  what's shown here  as an output gate  So  when we want to  read  from a memory cell  we take the activity  of that memory cell  which is a vector  and we pass it through  a tanh function  and multiply  by a gate  as shown here  as O of t  So this  gate  is a vector of numbers  between zero  and one  And it controls  what exactly  is emitted from the network  The gate itself  is affine layer  with a sigmoid activation function  During training  the weights are learning to produce  a right output  from the model  from  the hidden state  of the network  You can see this math  being shown  in this equation here  where the  output of the model  is  an activation function  that wraps  an affine layer  And then we do  an element wise  multiplication  with the tanh  of the memory cell  To make this  easier  to express  we  represent  the output gate  as O of t  So now you just have  O of t  and then the Illinois operation  with a tanh  of the activity from the memory cell  And importantly  this output gate  is simply  an affine layer  very similar  to what we had introduced previously  The forget gate  follows  a similar  approach  to the output gate  We use an affine layer  with a different set of weights  whose outputs  are between  zero and one  And we  insert that gate  as a multiplication  between  the memory cell  at time t  and a t time t plus one  So if these values  are close to zero  the values  in the memory cell  are forgotten  from one time step to the next  Or  if they are close to one  they're being maintained  across time  The Input gate  is used to write  new data  to the memory cell  And it has two components an affine layer  with weights  Wc  and a Tanh  activation function  Which generates  a new  proposed  input  into the memory cell  And it also  has  contain in it  in input gate  as shown here  which modulates  to propose  input  and then writes it  to the memory cell  here  So we can think of the next  stage  of the LSTM  C of t  plus one  as  how much you want to  forget  from the previous time step  plus  a proposal  for the new time step input  multiply  by how much we want to accept  this new proposal  It is important to remember here  that  all the values in the network  are vectors  and not scalars  So here is  the LSTM model  with the forget gate  the input gate  and the output gate  And here's an example  of an LSTM  where  the value  will be recorded in the memory cell  is the gender identity  of the speaker  And you can imagine this  being a very important value  for conditioning  the predictions  of the model itself  So you can see here  that  when you encounter  the word Bob  you have learned  to  forget that  previous  activity  because  now you have a new gender  for the speaker  And you will learn to  overwrite  that value  with value of one  to represent  a male speaker  And then the model continues on  processing more data  So in this scenario  the forget gate  outputs zero  because you want to forget everything that came before  And you wanna  input one  into the model itself  So in this example  the forget gate  is zero  because we want to forget  what had come before  And then  the input gate  is one  to represent  the fact that we have a male speaker  in this particular sentence  And that is important  because when we reach a prediction phase  we can use that  to predict  his  instead of her  as the next possible word  in the sentence  Another  popular architecture  is Gated Recurring Units  or GRUs  And they're essentially  a simplified  LSTM  Here  all the gates  are compressed  into one update gate  And the input module  is an affine layer  that proposes the input  which is combined  with an update gate  to obtain  the representation  at the next time step  The remember gate  controls  how much  the previous  time representation  impacts your proposal  We've seen in many scenarios  where  that GRU  performed  similarly  to the LSTM  So in that way  it is somewhat attractive  because  of its more simplified  representation  Bidirectional RNNs  are also recurring neural networks  Except that  they connect  two hidden layers  of opposite directions  of the same output  With this structure  the upper layer  can get information  from both the past  and  future states  Additionally  you can stack  these bi RNNs  on top of each other  to obtain  more complex  abstractions  and features  of the input  These  architectures  are often  used  in speech applications  Where they transcription  from a speech  may depend  not just  on  the  sound  that had come before  but also  the audio afterwards  A great application of this  is the Deep Speech Two model  which is a state of the art  speech transcription model  that was  published  by two  several years ago  It is important to understand  what LSTM units  learn  after the training process is completed  So  this is work  by Andre Carrpathi  where he trained  a language model  Recurrent Neural Network  on  several important texts  such as  the war and peace  novel  and also  a corpus of Linux Kernels  And he has identified  individual  cells  that are sensitive to particular properties  So on the top here  you can see a cell that is sensitive  to position  in the line itself  Or a cell  that changes his activation  based on  whether it encounters  an inside quote  or not  So  in all of these examples  you have the text  and the caller  represents the activity  of this particular  identified  unit  You can see  in  the Linux Kernels  dataset  we have cells  that are robustly  activate  inside  if statements  Or even cells  that turn  on  comments  or quotes  in the code itself  It is exciting  to see  what we can build  with the current neural networks  and  the type of visualizations  that will become available  Try to understand  what  these recurrent Neural Networks  are learning  Isley  and just  is a large corpus  of natural language 
-4mLOLHSnMw,Exercise 5  Recurrent Neural Networks  For this exercise  you're going to analyze IMDb movie reviews to determine if they are positive or negative in sentiment  This is called sentiment analysis  Some example use cases of sentiment analysis include monitoring real-time reactions to policy announcements  the public opinion of campaign messages or recruitment campaigns  And understanding opinions and satisfaction  Here  you will use the IMDb dataset  which consists of 25 000 reviews that are labeled as either positive or negative  The dataset was reduced in complexity by limiting the vocabulary size and truncating each example  Reviews that were shorter than the maximum length were padded with white space  Begin by importing the data and generating a backend  your model should consist of the following layers  One  a lookup table which is a word embedding  second  an LSTM recurrent layer  third  a recurrent sum which collapses over the time dimension by summing the outputs from individual steps  Four  dropout which performs regularlization as we've seen before  and five an affine  a fully connected layer which classifies the review as either positive or negative  Add these layers and refer to the neon documentation if needed  For training you will use the autograd optomizer and the cross entropy cost function  as shown here  You will also use callbacks to allow the model to report its progress  as we have used before and we'll then train the model for 2 epochs to complete passes through the dataset  Finally  report the accuracy on both the training and validation sets  After doing so  you can run inference with this other Jupyter notebook  which after processing the data  forward propagates through the model and prints the estimated sentiment  After running the model  you should go back and tune the hyper parameters  Change the word embedings in the LSTM output size and add in another LSTM layer  You can also easily enter your own reviews and see how your model analyzes their sentiment using the second Jupyter notebook  as shown here  [MUSIC]
OBp8klRZdDw,Training Tips  In this lecture we will provide some training tips and tricks that are maybe useful when training deep learning models  We will start with a quick review then discuss overfitting  data augmentation  and end with a discussion on training validation development and testing  In classical machine learning  traditionally you have an image that is end by end  and you engineer a smaller set of K features  These features can be for example the ratio of the length to the height of the object  or the number of circle objects in the image  Then you apply your valid algorithm to learn to associate these patterns of features with an identity  In this case  vehicle  With supervised learning  we should learn algorithms used pre-labled training data to infer a function  Here imagine that you're trying to classify image into five categories  Vehicles  animals  faces  fruits  and chairs using two features  the two axises here  Supervised learning using the mission learning algorithms listed here allows for the determination of decision boundaries  shown here  using pre-labeled data  Here we can see that  while the decision boundaries are not perfect  they do a reasonable good job classifying the data  We'll now talk about overfitting  a common problem in machine learning  These decision boundaries  when looked at a training error preformed were near perfection  However  when we apply a new data set  we can see that the decision boundaries have memorized the training data set  And actually performed quite poorly when testing  as compared to the previous decision boundaries  This problem is called overfitting  and is common in machine learning  The graph on the left shows what underfitting would look like  when the machine learning algorithm can not capture the underlining trend of the data  In this case the testing error and training error will be both high  The middle graph shows a well trained model  the testing and validation error is at a minimum  And the training error is relatively low  On the other hand  the graph on the right shows overfitting  where the training error maybe lowerpertterns  but the test error is higher  Overfitting can be an issue with larger networks as they have lot of weights  These networks can memorize the weights that perform excellent on the training data set  But doe not generalize and has poor predictive performance  Suppose that we have this data set  and say that we have two morals which are being trained on this data set  We can see that the first model prefers with 100% accuracy  while the second does not but it's still close  When estimating the value of the position in red  we can see that the model on the far right is much closer to the grand truth  despite turning lower training accuracy  Let's look at another training trick  data augmentation  Data augmentation is often used when the training data is not sufficient to learn a generalisable model  This can be done by slightly altering the original image  whether that maybe by mirroring it  cropping it  or stretching it  There are dangerous with simulated data  as the data is not exactly what you would find if the data was collected  But if done correctly  data augmentation can be a powerful tool  One must be careful to only augment in realistic ways  For example  flipping a cat upside down will not be a good way to augment the data  as you would probably never expect to find an upside down cat in real life  We'll end this lecture by reviewing tips for minimizing training and testing errors among others  In order to reduce high training error and high validation error  you may use this techniques  For high training error  bigger models and longer training will help  For high validation error  more data  more regularization such as dropout and weight decay  and early stopping are effective techniques  Finally  it is generally best to use a large model to overfit the data  and then fix the overfitting afterwards  Additionally  the validation and test data  should be from the same distribution  and it can be easier to break a task into various pieces  For example  Baidu uses four different networks to convert text to speech  An autonomous driving model use numerous networks  In this lecture we have explored methods to prevent overfitting  data argumentation and other train tips and tricks that may be used for when training deep learning model  During the next lecture  we will discuss training in order to minimize the time to train 
2Xp1c-Zw1zk,Exercise 6  Training Tips  In this exercise  you will examine the concept of overfitting where model fails to generalize its performance to unseen data such as a separate validation set  You will also learn how to recognize overfitting using visualization tools  and how to apply Dropout to prevent overfitting  Here  you will use a simple convolutional network on the CIFAR-10 dataset  After running the model  you should notice that in the logs of the model  after around IPAC 15  the model begins to overfit  Even though that cost them the training set continues to decrease  the validation loss flattens  and even increases slightly  You can visualize these effects using the code provided  In order to correct this overfitting  you will introduce dropout layers into your model which randomly silences a subset of units for each minibatch which greatly helps overfitting  Add this to your model as indicated in the exercise  and compare the generated plot with the plots associated with the model without dropout  You should notice that the validation loss in blue  is not shifted downwards  compared to the previous figure  and the model reaches a better validation performance 
0XJPzek__nw,Multinode Distributed Training  Welcome back  in this lecture  we will look at multi-node distributed training  Which allows us to efficiently parallelize deep networks across multiple servers  in order to minimize the time to train  Specifically  we will look at scaling challenges  data and model parallelism  Allreduce approaches  and synchronous and asynchronous multi-node training and trade-offs  Before we do  let's review gradient descent and stochastic gradient descent  When using gradient descent to find a local minimum  we take steps proportional to the negative of the gradient of the cost  with respect to the weight  However  there are three main issues with gradient descent  First  in gradient descent  you have to run through all of the samples in your training set to do a single update for a parameter in a particular iteration  Second  gradient descent gets stuck at saddle point  which are very common in high-dimensional spaces  AlexNet  for example  has 60 million dimensions  making saddle point a large issue  Finally  gradient descent is thought to converge to sharp minimums more frequently  This is problematic  as even a slight variations between the training and test objective functions can cause dramatic shifts in their values  when in sharp minimum  On the other hand  a stochastic gradient descent solves many of these issues  By breaking the training data set into minibatches  each of which are then used to compute a parameter update  This algorithm is more explanatory than gradient descent  converges to flatter minimum  Does not get caught in saddle points points as easily  and is much more computationally efficient  As seen here  in gradient descent  to make a word update  one must go through entire training data set  which can be very slow  On the other hand  stochastic gradient descent makes an update after each minibatch  which has a dramatic effect on efficiency  Nearly all  if not all  deep learning models are trained with some variant of a stochastic gradient descent  for these reasons  We'll now transition to discussing scale challenges  When using SGD  choosing the right batch size is very important  For smaller batch sizes  one does not efficiently utilize the computational resources  Yet for larger batch sizes  one can run into similar issues that we explored with gradient descent  Very slow progress near saddle-like points  and getting stuck in sharp minima  With very deep networks  trained on large data set  efficiently parallelizing networks across multiple servers becomes essential  in order to minimize the time to train  To this end  we will explore two algorithms  data parallelism and model parallelism  In model parallelism  we split the model weights among end nodes  with each node working on the same minibatch  With data parallelism  we use the same model with every node  but feed it different parcels of data  Such a method is better for networks with few weights  like GoogLeNet  and is currently used in Intel Optimized Caffe  Visually  we can see that with data parallelism  the algorithm distributes the data between various nodes  And each node independently tries to estimate the same parameters  then they exchange their estimates with each layer  Using a parameter server or an AllReduce method  as we will discuss  to come up with the right estimate for the step  While the minibatch is distributed or mini nodes  three in this example  one can not simply increase the byte size by three times  As the time to train increases with larger minibatches  due to similar issues as with gradient descent  With model parallelism  the algorithm sends the same data to all nodes  but each node is responsible for estimating different parameters  These nodes then exchange their estimates with each other  to come up with the right estimates for all the parameters  Data parallelism is preferred when the number of weights is small  which is true for linear topologies  When updating weights on a single node using stochastic grading descent  we pass in x training examples  where x is a batch size  and forward-propagate them through the network  After computing the cost of all examples  we then compute the data gradient from layer to layer  and update the model weights using the weight gradients  When we have multiple nodes  32 in this example  and are using data parallelism  we partition the minibatch into 32 subsections  and distribute them to 32 workers  When back-propagating  each worker computes a sum of the weight gradient for each subset of their batch  The weight gradients are then summed across workers  producing identical numerical result as one would find with a single node  training with a large batch size  Deep learning practitioners have demonstrated a scaling across various nodes  Baidu distrubuted training to 40 GPU nodes  later that year  UC Berkeley scaled training to 120 GPU nodes  Their paper provided sufficient details for other practitioners to build upon their work  A few months later  Intel demonstrated scaling to 128 CPUs  Google to 120 GPUs  Amazon to 120 GPUs  Most recently  and not shown in this slide  Facebook demonstrated near-linear scaling to 256 GPUs  reducing the time to train from several days to just one hour  With very large batch sizes  the time to train becomes quite large  making training slow and not able to reach the same accuracy  Therefore  let's assume that we have a batch size of 1024  how can we distribute the data across nodes? One option is to have 1024 nodes  each node with a batch size 1  However  with this arrangement  the communication between the nodes becomes a bottleneck  and the computation itself on each node is too little  On the other hand  using 16 nodes  each with a batch size of 64  is more reasonable  as most of the communication is hidden in the computation  Multi-node training on IntelCaffe  which uses data parallelism  works in the following manner  First  the data on a given node is forward-propagated through the network  which in this case is composed of two layers  L1 and L2  Then the L2 gradients are sent to the parameter server after that layer has been propagated through  Similarly  the L1 gradients are sent subsequently to the server after L1 has been back-propagated through  When the server receives the L2 gradings from all nodes  it then applies an update  and broadcasts it to all the nodes  and likewise with the L1 gradients  Nodes wait for these updates before forward-propagating through the updated network  Now that we have discussed how data and model parallelism work  we will consider strategies for implementing gradient aggregation  Parameter server  reduction trees  rings  and butterfly  One strategy for communicating gradients is to appoint one node as the parameter server  Which computes the sum of the communicated gradients  and sends the updates to each of the workers  However  there is a bottleneck in sending and receiving all of the gradients with just one parameter server  Another strategy is an AllReduce tree  An AllReduce communication method is where each worker produces one or more data values that must be globally reduced  Generally  we'd have commutative  binary element-wise operator  to produce a single result value  And then this single value must be broadcast to all workers before they can continue  In an AllReduce tree  the local gradient information is distributed to the entire network using a tree-based algorithm  which is then broadcasted to each individual node  In this example  there are seven nodes  and each has a gradient value between one and seven  In this AllReduce tree example  where the goal is to sum all of its values  1  2  and 5 are summed to 8  and 3  4  and 6 are summed to 13  in the first step  Then these results  8 and 13  are summed with 7 to make 28  in the next step  Finally  28 is then broadcasted to the rest of the network in just two steps  In total  if N is the number of nodes  the time is a function of the log of n  which makes it ideal for power of two number of nodes- 1  All Reduce butterfly requires the least of number of steps  as it does not require separate reduce and broadcast steps  In this example  there are four nodes with three steps  starting from the top  and how the communication happens at each step  As shown here in step one  node 1 and 2 are first summed concurrently with 3 and 4  Resulting in the first two nodes with the values 3  and the next two nodes with value 7  In step two  the nodes with 3 and 7 communicate  resulting in all the nodes with a value of 10  The complexity of this algorithm is also log 2 of N  but requires half of the steps as AllReduce tree  A ring All Reduce algorithm is useful as the communication cost is constant  and independent of the number of nodes in the system  And is determined solely by the slowest connection  Each node will only ever send data to its right neighbor  and receive data from its left neighbor  The algorithm proceeds in two steps  First  a scalar reduce to exchange data  and then an all-gather to communicate the final result  While we have looked at synchronous multi-node training  we will now briefly discuss asynchronous multi-node training  and its trade-offs  In order to accelerate the convergence of SGD  and improve the training speed  asynchronous parallelization of stochastic gradient descent has been investigated  While asynchronous SGD overcomes communication delays  it comes with a myriad of problems  The algorithm requires more tuning of hyperparameters such as momentum and learning rate  and requires more epochs to train  Furthermore  it does not match single node performance  It's quite hard to debug  and has not been shown to scale and retain accuracy on large models  Therefore  while there do exist benefits of asynchronous SGD  there exist many issues that have not yet been fully addressed  In this lecture  we have covered multi-node distributed training  which is very useful to train deep networks on large data sets  To that end  we discussed scaling challenges  data parallelism and model parallelism  AllReduce algorithms  and asynchronous stochastic gradient descent  During the next lecture  we will look into some hot topics in deep learning research  [MUSIC]
YZ_z2mE_6Vs,"Hot Research  In this lecture  we will provide an overview of some hard research topics in deep learning  Specifically  we will look at a low bit precision generative adversarial networks  deep voice  and reinforcement learning  Keep in mind that much of the content in this lecture is very quickly evolving  Just as a quick review  deep neural networks are trained by a process of forward propagation  calculating a cost  and a backward propagation  to update the network's weights  These updates utilize gradients of the cost with respect to the weight  which can be very small and generally require a high level of precision  Inference refers to the first part of training which entails using up pre-training network on new data  Because the weights are not being updated  the level of precision needed for inference is much lower  Often training on a large scale or deep network is constrained by the available competition of resources  Recently leading to considerable interest in studying the effects of limited precision data representation and computation on neural networks training  That's what we will talk about next  Most deep learning practitioners use 32 bits of floating point precision to train and test deep networks  However  such high-precision is often not required especially in testing or deploying deep networks  There are five classes of numbers in deep networks  input numbers to a layer  output numbers of layer  model numbers  gradient numbers  and communication numbers  In here as shown in parentheses  the range of low-precision values that have been used for each class of numbers  and in brackets  are recommended tradeoff  Practitioners have shown that both the input and output vectors typically only require eight bits of precision  For the model weights  usually eight bit is also sufficient  The gradients are often very small and require the most precision  In turn  others have successfully trained various models with the gradients are only 16 bits of precision  So while the future will likely only use 16 bits for the gradients  the current standard is still 32 bits  in order to guarantee convergence across all models  Finally  in order to distributed the training across multiple nodes  the gradients are often communicated  and 16 bits should be sufficient  Microsoft has even shown results communicating with only one bit  We will now explore one of the hottest topics in deep learning  Generative adversarial networks  Ian Goodfellow created a detailed lecture on generative adversarial networks that I recommend you explore if you're interested in learning more about GANs  As we have seen  convolutional networks can take as input an image of a bus  and output the category label bus  CNN's have gotten very good at Image recognition and do not fail very often in real images  However  a group of researchers from Google  Facebook  and NYU  specifically designed noise and added it to real images to cause thinnings to classify them as ostriches across all the images shown here  The figure on the center is the noise added to the image magnified 10 times for visualization purposes  To the human eye  the image on the right looks exactly like the image on the left  But to this CNN the image was classified as an ostrich  This raised a lot of interest in the vision community  Some said that adversarial examples with noise were unnatural images and should therefore not be relevant  Others argue that it was important and many trade training neural networks with adversarial examples  It was found that training networks with adversarial examples made it more difficult for them  though it did have a minor impact on the classification performance  Generative adversarial networks take this a step further  Instead of adding noise to images  a network is used to generate adversarial examples  These two networks compete against each other  One trying to generate images to fool the CNN  and the other one trying to distinguish real images from fake images  A surprising result is that as you improve your CNN  your adversarial network generates real images not previously found in the dataset  A young pioneer  young LECun who oversees air research at Facebook has called GANs the most interesting idea in the last 10 years in machine learning  Most GANs today are at least loosely based on the DCGAN Architecture which stands for deep convolutional GAN  In the DCGAN Architecture  most deconvs are batch normalized  and the models does not contain either pulling or unpulling layers  Instead when the generator needs to increase the spatial dimensions of the representation  it uses transpose convolution with a stride greater than one  DCGANs have been able to generate high quality images when trained on restricted domains of images  such as images of bedrooms when trained on the LSUN dataset  DCGANs have also demonstrated that GANs can do vector space arithmetic  For example  if you take the vector associated with the image man with glasses  and subtract the vector from man and add the vector for women  the resulting vector maps two images that correspond to woman with glasses  Minibatch features which allowed the discriminator to compare an example to a minibatch of generated samples and a beanie batch of real samples  have been quite successful  Minibatch GANs train on the c14 dataset  obtain excellent results  as we can see that the produce samples on the right  can be generally classified into specifics c14 classes including some cars  horses  and planes  Minibatch GANs have also been trained on 128 by 128 ImageNet images  which also produces images that are somewhat recognizable  One application of GANs is text to image synthesis  where the input to the model is a caption for an image  and the output is an image matching that description  We can see that GAN generate images that correspond well to the description given  GANs have also been used for Single Image Super-Resolution and has been found to demonstrate excellent Super-Resolution result as shown here  Because GANs are able to recognize multiple correct answers  the network does not need to average or the many answers producing better results  Adobe created an application called interactive generative adversarial networks or iGAN which allows a user to draw a rough sketch of an image such as a black triangle and a few green lines to produce an image of a mountain with a grassy field  Ultimately  GANs have been showing many promising results and is a very hard topic in deep learning currently  We will now investigate deep voice  text to speech or TTS  which allows Human-Computer Interaction without requiring visual interfaces  By this recent deep voice  real time neuron text to speech consist of several building blocks each one using a different deep network  The first network is a grapheme-to-phoneme which converts grapheme or English text characters such as letters and punctuation to phonemes which are units of sound  For example  the input Hello  returns the phonemes \""HH\""  \""EH\""  \""L\""  \""OW\""  Second segmentation which is phoneme-audio-alignment  This identifies were in the audio each phoneme begins and ends  This is only using training when the ground truth audio is present and not during inference  The third network is a phoneme-duration and fundamental-frequency predictor  This predicts the duration of every phoneme in an utterance and the fundamental frequency or F0 throughout the phonemes duration  The fourth network is an audio-synthesis generator  This network combines the output of the grapheme-to-phoneme and segmentation models to synthesize audio at a high sampling rate  In many production qualities  TTS applications  real time inference is critical to avoid delays and provide a more natural user interaction  Inter-CPUs were found to offer faster than real time inference due to cut friendly memory access and by avoiding recomputations  We will end this lecture by discussing reinforcement learning  a branch of machine learning that allows machines and software agents to automatically determine the ideal behavior within a specific context in order to maximize its performance  Reinforcement learning has a wide range of applications  It has been used to control robotic arms  It's been used for navigations  for assembly and logic games  Because decision processors are quite numerous  reinforcement learning has a lot of potential to solve a large number of problems in artificial intelligence  The majority of this course focused on supervised learning which uses train examples that are labeled  Mission accuracy with this type of learning is quite easy and supervised learning is usually task-driven such as with classification  And supervised learning involves looking for patterns within data as the data is not labelled  This can include clustering and the evaluation of a model success is usually indirect or qualitative  Finally  reinforcement learning is reward driven  The environment provide rewards to actions that guide the learning  A deep reinforcement learning algorithm follows this process during training  An autonomous agent learns how to choose the optimal action in each state to achieve its goal  Every action taken by the agent in the environment  produces a positive or negative reward  The goal of training is to optimize the narrow parameters that maximizes the expected sum of rewards modulated by a discounted factor y  a value between zero and one  Note that a reward r_t is a weighted sum of all rewards afterwards whose values are exponentially less or more important depending if it's a positive or negative reward and by using a discount factor  For example  y=0 99  The figure below illustrates this concept  On the first row  we see the positive and negative awards after each game  However  note that the vast majority of the rewards are zero  On the second row of the bottom figure  we see there discounted values  The zero value reward in the first rule have non-zero values now as a result of a moving average over the original report values  The discounted reward r_t  now can be used to maximize the lock probability of actions that lead to good outcomes and minimize the probability of those that did not as shown here  From this  the more parameters theta are updated considering both the noise gradients computed in an environment with high uncertainty and driven by a range of positive or negative reports over time  During today's lecture  we have covered some hard deep learning research topics such as low-bit precision  GANs  deep voice  and reinforcement learning  In our next lecture  we will look at what Intel is doing for deep learning "
YSr5zmVqZLI,Exercise 8  Reinforcement Learning  In this reinforcement learning tutorial  we will train a reinforcement learning model to learn to play Pong  The goal is to train an agent  in green  to win a game of Pong against an opponent in orange as shown in this video  An action consists of moving a paddle up or down  eventually generating a positive reward  or +1 if the trained agent wins a game or -1  if an agent misses the ball  Before knowing the result of the game  the model gets a fake label via a stochastic process  similar to tossing a coin to decide whether to accept the log probabilities of a neural network  And the optimal set of actions will maximize the sum of rewards along the game  An important event is when the agent wins or loses a game  at which point the algorithm modulates its rewards by giving more preference to early actions  The underlying model is a two layer  fully connected neural network that receives an input image frame  samples an action from the output unit  probability of moving a tile up or down  and updates its weights with gradients  When the outcome of a game is known  a reward is available  +1 if we win and -1 if we lose  And this becomes the policy gradients that encourage the decisions made in winning games via back propagation  As a result  these actions have an increased probability of being present in future games  This algorithm is called policy gradients  and has been shown to perform better then DQN  even by its authors  as it allows end-to-end training in the deep neural network  The errors represent the gradients of each action in the game towards the direction that increases their expected rewards  Since the loss values of the network are rated by their cumulative rewards  we expect the mean of the distribution to be influenced by a few gradients with positive rewards after parameter updates  Indeed  regions around green points seem to have small gradients  indicating the preference of the network to keep those weight values  On the contrary  other regions often have large gradients  meaning that the back propagation algorithm updates their values in seek of better regents for maximizing the overall rule board  As part of this tutorial  you will be guided through implementing a gradient policy algorithm in Neon to train an agent to play Pong  You will learn how to use the auto diff interface to perform automatic differentiation and obtain gradients given an op-tree  And how to implement a neural network that adjusted parameters and policies with gradients to get better rewards over time directly from experiences  This model samples the expected action  moving a pile up or down from a distribution when no gradients are available and high-uncertainty is in place  We start by importing all of the needed ingredients  We have included functions implementing the forward and back propagation steps shown here  The forward step generates a policy given a visual representation of the environments toward variable x  or the back propagation function updates the layers of the network modulating the loss function values with discounted rewards  This function sample action  chooses an action moving up with the probability that is proportional to the predictive probability  This is part of the magic behind reinforcement learning  We are able to optimize objective functions under very uncertain conditions using this stochastic process  Once you run the program  you will see the algorithm in action  It should be noted that because this algorithm is placed in scenarios with extremely high uncertainty  the model will take many hours before it produces meaningful results  [MUSIC]
g28LSN0cCfw,Intel's Roadmap  Recognizing the evolution of computing and AI opportunities  Intel has prioritized artificial intelligence by creating a new AI products group or AIPG  In this lecture  we will discuss what Intel is doing to build products that empower a new generation of AI solutions and transform industries  AI remains slow  but one of the fastest growing workloads in the data center  It's comprised primarily of classical machine learning and deep learning  The share of deep learning is also growing  Intel architecture based servers are used in nearly all classical machine learning deployments  and in the vast majority of deep learning deployments as well  Intel has targeted accelerators for both training and interference to improve deep learning performance  Intel has the most dense and power efficient transistor technology on the planet  The experience of successfully driving several major computing transformations  and the complete portfolio required to unleash the full potential of AI  Intel is committed to AI  and is making major investments across technology  training  resources  and R&D to advance AI for businesses and society  We have a commitment to our partners  The industry as a whole and global society to accelerate AI development  deliver end-to-end solutions  and lead the next generation of computing transformations  Within our industry  Intel can make and deliver upon this commitment because of our comprehensive technology portfolio  an unparalleled portfolio developed through acquisition and innovation  We acquire recognized AI leader  Nervana Systems  to accelerate training time  which is a critical phase of the AI development cycle  The technology innovations from Nervana will be optimized specifically for deep networks to deliver the highest performance  The Saffron cognitive platform leverages associative and machine learning techniques for memory-based reasoning and transparent analysis of multi-source sparse dynamic data  Movidius  a leading edge computer vision powerhouse  gives us a well-rounded presence for technology RDH  Embedded computer vision is increasingly important and Intel has a complete solution  with Intel's RealSense cameras seeing in 3D as the eyes of a device  Intel CPU's as the brain  and Movidius Myriad vision processors as a visual cortex  This vision processing units or VPUs  delivers performance with an equally important low power and thermal footprint  Going forward  we expect to see intelligence built into every device with data center function distributed across a continuum from the edge to a cloud  and real communication occurring over high speed 5G networks  At this juncture  we will have both the infrastructure and the algorithms to create a more advanced AI with the ability to think independently and our deep collaboration on the emerging 5G standards will make that possible  Intel has a full stack of entwined AI products for building AI solutions at enterprise scale  It has a competitive hardware portfolio for machine and deep learning  But software is also critical to unleashing the full compute potential  To that end  Intel offers an optimized software stack in order to deliver game changing AI applications  At the lowest level  Intel optimize primitive functions that are used across a wide array of machine and deep learning frameworks and solutions  including the Math Kernel Library or MKM  the Data Analytics Accelerated Library or DAAL and the Intel Python Distribution  At the framework level  Intel is committed to optimizing the most popular analytics machine and deep learning frameworks by the end of 2017  As far as tools  Intel offers the deep learning SDK to accelerate deep learning training and deployment and they Saffron Natural Intelligence Platform for reasoning systems  Intel's end-to-end hardware portfolio  it spans the data center  gateway and edge  Some computer engines span an entire spectrum  while others are tailored specifically for one or two kinds of applications  In the comments slides  we'll explore the positioning of each product in this broad portfolio  first for the gateway and edge and then for the data center  Intel is the only technology provider with a range of solutions for AI at the edge to meet its customers unique requirements  From the most general purpose on the left to the most targeted narrow acceleration on the right  Atom cores and Xeon's can handle a variety of workloads  including basic deep learning inference and classical machine learning within a larger application  The Intel processor graphic such as Iris  Iris Pro and HD Graphics are good for a higher inference throughput  and can also provide performance for media or graphic intensive workloads that often come along with AI workloads  The Movidius Myriad family provides low power engine for computer vision workloads where deep learning and low precisioning inference is needed  Finally  the Gaussian mixture model and neural network accelerator is for low power  always in speech recognition eye block  Building on the Intel Xeon processor  a potent performance for the widest variety of air workloads including deep learning  Intel's portfolio also includes more targeted accelerators  As you go farther to the right  Intel's portfolio includes compute engines that deliver greater deep learning performance  albeit with fewer other capabilities  For more intensive deep learning environments  the Intel Xeon 5 processor  code name Knights Mill  will deliver enhanced planning training performance and Intel FPGA technology  delivers enhanced deep learning inference  When available  the Crest family will deliver unparallelled deep learning training and inference performance  Before we look at AI product position in the data center  let's first look at the workhorse in the data center  The Intel Xeon processor is an excellent choice to begin your AI journey with potent performance for all types of AI  including deep learning  thanks to Intel's optimization of the most popular open source deep learning frameworks  Built in return of investment  For more intensive deep learning environments  the Intel Xeon 5 processor code name Knights Mill  will deliver enhanced deep learning training performance  The processor has integrated Intel's Omnibus fabric increases the price performance and reduces communication latency and has delivered up to 400 gigabytes of memory with no PCI performance lag  Why FPGAs is for AI? FPGA's offer a number of unique values to this rapidly growing field  First  FPGAs offer high throughput  low latency processing of complex algorithms  such as deep networks  Unlike other technologies  the flexibility of the FPGA fabric enables direct connection of various inputs  such as cameras without needing an intermediary  This eliminates unnecessary data movement  allows even faster system latency and allowing you to process data and make decisions in real time  Let's take a closer look at the Crest family  The Crest family design includes mostly multipliers and local memory and it skips elements such as caches that are needed for graphic processing  but not for deep learning  As a result  the Crest family achieves unprecedented compute density  Intel's MKL DNN or the Math Kernel Library for Deep Neural Networks is highly optimized using industry leading techniques and low level assembly code where appropriate  The API has been developed with feedback and integration with them major framework owners and as an open source project  will attract new and emerging trends in these frameworks  Intel is using this internally for our own work in optimizing industry frameworks  as well as supporting the industry with their optimisations  At Intel  we need to support many different frameworks  and we need to allow users to run on multiple hardware targets  and use distributor training strategies to enable the large models of the future  To accomplish this  we develop the Nervana graph  The Nervana takes the computational graph from each framework and creates an intermediate representation which is executed by calling the math accelerator software libraries for each hardware  It also reduces the need for framework and model direct optimizations  However  it does not replace the low level software and math accelerator libraries  Intel has been optimizing the popular AI frameworks for Intel architecture and seen dramatic speed ups  This enables customers to select their favourite framework  We will enable more frameworks in the future with Intel Nervana graph  Each of these frameworks have a varying degree of optimizations  So take a look at the roadmap for more details  Neon is Intel's innovation framework that will continue to support the state of the art models  In order to achieve our vision for artificial intelligence  we must address each phase of the AI development cycle  Data scientists today are forced to spend unacceptable amount of time pre-processing data  iterating on models and parameters  waiting for training to converge  and experimenting with deployment models  With with each step being either too labor or compute intensive  Intel will deliver the technology breakthrough  both hardware and software  required to massively compress each part of the cycle  To address the compute and labor intensive challenges of the deep learning innovation cycle  we spent the last two and a half years building a full stack deep learning platform  These tools cover the complete deep learning lifecycle  from data loading  all the way to model deployment and inference  This stack includes the Crest ASIC for deep learning acceleration  Neon and other frameworks  All of this is wrapped in a wholesale solution for easy onboarding and scaling production  The Intel Nervana deep learning platform  offers organizations the most comprehensive way to build AI applications that can scale across the enterprise  After a model has been trained  Intel has tools to optimize these models for inference performance where there are power constraints or model size constraints at the edge  The deployment tool can target various hardware  while Movidius has tools specific for their Myriad product family  If optional customization is needed  additional libraries are available  Intel Nervana AI Academy provides training in machine and deep learning with associative frameworks  tools  and libraries  The organization sponsor meetups  webinars  and sessions  and industry events to support developers that are scientists and students education and advancement in the field of artificial intelligence  During today's lecture  we'll learn what Intel is doing to build products that will power a new generation of AI solutions and transform industries  We're truly at the dawn of an AI computing era  and Intel is in the strongest position to lead in this burgeoning space 
R36g6qit1bE,"Sets - Basics and Vocabulary  Okay  Welcome  everyone  to our first video  And this is gonna be on the basics of set theory  The point of this is really just to give us some very basic vocabulary that we're gonna agree on  We're gonna start with the idea of what is a set  We're then gonna go through a fancy word called cardinality  which is really just a fancy word for size  Then we're gonna go over two different ways to make sets from other sets called intersection and union  All this in the middle is gonna be a little bit of dry  a little bit of abstract  Fortunately  a little light at the end of the tunnel  Next video  we're gonna go over an example from medical testing  Where we're gonna see that just the simple ideas of size of a set and intersections and unions  already allow us to understand a real-world data science example  Okay  fine  Let's jump in  So  what is a set? As always in this course  I'd like to give you an example and then work over through the abstract details of what it is  So here's an example of a set  I'll write a capital letter A  an equal sign  an open bracket  and then some stuff inside the set  So here I'm gonna do one  two  minus three  seven  But let me give you another example of a set  Let's say that E is equal to open braces  the word apple  the word monkey  my co-author Daniel Egger  and close braces  Okay  So what can possibly be a general notion that meets these two ideas? Basically  a set is a collection of stuff  We should write that down  A set is a collection of things  It doesn't really matter what those things are  A little bit of vocabulary  a set is made up of elements  So in this example we have here  the set A is made up of four elements  the element one  the element two  the element minus three  the element seven  For this example on the right  the set E is made up of three elements  the element apple  the element monkey  and the element Daniel Egger  There's really no requirement of what those elements are  a set can really be made up of any things  We tend to use these braces to sort of contain the set  These are the things inside the set  A little bit of notation  When I write something like this  two and this funny little half E is in A  this symbol here stands for two is an element of A  For example  minus three is also in A  However  the number eight is not in A  We tend to write that by negating that â€“ that little symbol there  that's what it means to be in there  Okay fine  The last little notion is the idea of cardinality or size  The definition of cardinality is just size of a set A is the number of elements in it  We usually use this notation  absolute value of A  looks like the absolute value symbol which we'll talk about later  So in this particular case  where A is equal to one  two  minus three  seven  the cardinality of A is four and the cardinality of E  in this case â€“ I'll let you think about that for a little bit â€“ just three  Okay  That's really all there is to cardinality  Fine  Let's jump to the next topic  Let me write three sets for you here  A is equal to one  two  minus three  seven â€“ as you've seen before  Let's say B is equal to 2  8  -3  10  and D is equal to 5 and 10  Okay  Now you'll notice that of these three sets  they share some elements in common  some elements they don't share in common  There's two concepts called intersection and union which really allows to talk about those more rigorously  So first I'll just write down an example and we'll figure out the definition  A and then this funny symbol here B  This funny symbol is usually read intersect  This is the intersection of A and B  This is a set which is defined to be the set of elements which are in both A and in B  so the set of elements that A and B share in common  To work out what that is in this case  it's pretty easy to see that two is both in A and in B  and minus three is in both A and B and nothing else is  So A intersect B is equal to the set two  minus three  Okay  let's also work out for example the B intersect D is equal to the set of things which B and D have in common  And if we see  the only element in there is 10  Now here's a funny little trick question  let's work out A intersect D  A intersect D is defined to be the set of elements which both A and D have in common  and unfortunately there are none  so that set is empty  There's a special notation for that which is a zero with a cross through it  This is called the empty set  Hard to spell  And by convention  we always say that the cardinality of the empty set is zero â€“ there's nothing in it  Okay  In this really simple example  let me give you a more complicated way of writing it which will prove fruitful later  Another way of writing A intersect B is to give you a recipe for computing it yourself  So the definition of A intersect B  instead of listing out the elements  I can list it this way  it's a set of x  I don't know what x is  but now I'm gonna give you conditions that x satisfies  that's what this little colon here means  The set of x such that x is in A and x is in B  This notation here is very  very important and we're gonna see it over and over again  So we have defined the elements of a set not by listing them explicitly  but by giving conditions they must satisfy to get in  You can almost think about it like membership in a club  This x are the people trying to get in the club  this colon is like the bouncer at the door of the club checking your ID and saying whether you get in or not  So imagine any x comes along  for example  two comes along and says  \""Hey  I want to get into A intersect B  I hear this coll music in there \"" The bouncer asks  \""Okay  let's check  Are you in A? Yup  And let's check  are you in B? Yup  Okay  you get in \"" On the other hand  suppose one comes along  One comes along and the bouncer checks  \""Are you in A? Yup  Are you in B  Nope  And so you don't get in \"" That's the basic idea behind this definition  Okay  fine  Let's rewrite those sets and give you a different definition  So A is equal to one  two  minus three  seven  B is equal to 2  8  -3  10  and D is equal to 5  10  The next idea we're gonna define here is the idea of the union  A and this funny upside down intersection symbol  A union B â€“ this is read union  If intersection has the idea of and in here  union  you should think of as the idea of or  A union B is equal to the set of things which are in A or in B or in both  That's much more tolerant condition  So in this case  this will be one two minus three  seven  Two is already in A so I don't have to write it again  8  10  In bouncer-club notation  this is set of x such that x is in A or x is in B  Work at another notation  let's say A union D is equal to 1  2  -3  7  5  and 10  Okay  so that concludes our first video  Just to recap what we've learned  You've learned what a set is  you've learned two funny ways to write it  one just by making some braces and putting some things inside the braces â€“ thus gives you the elements of a set â€“ and another way you've learned it is by this notation here which I'd like to whimsically call the bouncer-at-a-club notation  where we give you a variable x and we say what condition it has to satisfy to get in the set  You've also learned what the cardinality of a set is and you've learned how to turn two sets into intersections  which is what they have in common  and union  which sort of says what they all contain together  That concludes our video  In the next video  we'll see how to use this terminology to understand a real-world example "
GE1YyJ4A_7w,Sets - Medical Testing Example  Okay  welcome back everyone to our second video  As promised in the last one  I'm now going to give you a real world example to ground some of the abstract notions from before  Let's give an example from medical testing  but all we're going to do is use just the vocabulary of set theory  By the end of this video  we're going to be able to understand the idea of false negatives  false positives  false negative rate  false positive rate  and things like that  All this will be very  very relevant and not just to medical testing vocabulary in the future  but also to lots of other things  like machine learning  So let's jump right in  let's invent an awful condition which we're going to call VBS  for Very Bad Syndrome  This is something that we don't want to have  and something that fortunately scientists have just developed tests for  Let's suppose that there's a set of people X  X is going to be equal to the set of people  In a clinical trial  That are taking the test that tells you whether or not  or purports to tell you whether or not  you have VBS  Let's divide X up into two sets  let's say that S is going to be equal to the set of people x in X  such that x has VBS  So why are we using S  S somehow stands for sick  these are the set of people who genuinely have VBS  Let's let H  H for healthy  stand for the set of x in X such that x does not have VBS  Okay  so now  let's note that X is equal to S union H  because you either have VBS  or you don't  It's a bit of idealized world  where we have good diagnoses  so you either have it  or you don't  And notice that S intersect H  therefore  is the empty set  If you either have it or you don't  there is no one who both has it and doesn't  Okay  in some sense the whole point of medical testing is to figure out whether you're in S  or you're in H  Let's think about what the test tells us  so let's take P equal a set of people x in X  such that x tests positive for VBS  That is  they come into the lab  they take the test  and whatever marker the test has for positive  they test positive  The doctor looks at the test and says  the test says you have VBS  it's very important to realize that is a different concept than  you have VBS  All that we're saying is you test positive for VBS  And let's let N  N for negative  be the set of x in X  Such that x tests negative for VBS  So again you take the test  doctor or the clinician looks at the test  Turns red or whatever the test needs to do  and it indicates you're test negative  Doesn't mean you're in the clear  doesn't mean you're okay  it just means you've tested negative  Now again  assuming the test is deterministic  notice  again  we have that P union N is everyone  and P intersect N is no one  Just like we had S union H is everyone  and S intersect N is no one  Okay  now  in an ideal world  we would have that S = P  That is  the sick people would be the ones who test positive  the ones who test positive would be the ones who are sick  and we would have H = N  This is not always the case  let's write four intersections which allow us to talk about those discrepancies  when that's not always the case  Let's consider S intersect P  let's consider H intersect N  Let's consider S intersect N  And let's consider H intersect P  Let's think about what these are  how big we want them to be  what they mean in the real world  and so on  So first S intersect P  what does it mean to be S intersect P? First it means that you're in  S  which means you have VBS  and it means you're in P  which means you test positive  These are what are often called the true positives  So the bad news is that you have VBS  the good news is the test told you accurately that you have VBS  so perhaps you can seek treatment  The second test  the second set here  H intersect N  that means you don't have VBS because you're in H  you're healthy  At least in regards to this disease  and  you are an N which mean you test negative  These are true negatives  In some sense these are the most fortunate  the very good news is that you don't have the disease  And the somewhat good news is  the test told you  so you don't have to worry  Okay  these next two are ones which we would prefer not exist  We would really prefer that the next two sets were empty  they almost never are  So S intercept N means that you're in S  so you have the disease  but you're also in N  which means you tested negative  So these are what are called false negatives  False negative results  it's not really nice to call the people false negatives  but sometimes we do  And those are people who have the disease  but unfortunately the test told them not  so in a way  they have false hope  They don't have enough information that might help them act on the disease  act on treatment for the disease  We'd like that set of people to not be big  Often tests aren't that good  so often the site is really large  Let's talk about H intersect P  those are people who are in H  which means they do not have the disease  but they're in P  which means they test positive  As you might guess  those people are false positives  You know why these people look a little bit more fortunate than the ones in S intersect N  but they're still quite worried  or they don't have the disease  they don't know that  They tested positive and so they might worry unnecessarily  they might even get treatment which would have some negative side effects with no payoff  because they don't actually have the disease  So what's often interesting is comparing the cardinalities of all of these various sets and that will allow us to talk about mcap that's useful later in medical testing  And even generalizes to sort-of general machine learning  which you'll see much  much later in this course and in the following one  Let's think about the cardinality of S divided by the cardinality of X  So notice  this number has to be less than or equal to 1  because everyone who is in S is in X  So what is this equal to  this is equal to the proportion Of people in the study  Who do genuinely has VBS  By the way  a little side note  When you design a study  you would like it that this proportion might actually represent the proportion of people in a much larger population of VBS  a representative sample  That might be true  that might not be true  and care is needed when you design studies to think about that relation  So for example if you take the people in the study are just people in a VBS clinic  Probably this proportion is very close to 1  but it doesn't represent the true proportion out in  say  the United State of America  So it depends what you're trying to estimate  Let's consider the cardinality of H divided by the cardinality of X  This is the proportion of people in the study without  w/o  VBS  What should we get  by the way  when we add these two quantities? Think about it a second  we better get 1  because you either have it or you don't  Okay  those are interesting  but here are two far more interesting quantities  Let's think about this cardinality of S intersect P  Remember what those were  those were the true positives  Divided by the cardinality of S  so what are those? In the numerator  on top  we have the number of people who are true positives  and in the denominator we have the number of people who are sick  This is what is called the true positive rate  We'd like that to be big  A number we would like to be small is  let's look at H intersect P  The cardinality of that  divided by the cardinality of H  So in the numerator  we have the false positives  the people who are actually healthy  but the test tells them they're positive  In the denominator  we have the people who are actually healthy  This is called the false positive rate  We would like that to be as close to 0 as possible  often it isn't  And then two other quantities which might make sense  The size of S intersect N  divided by the size of S  Let's let you think about what that is  So what is that  that's the false negative rate  And again  we would like that to be as small as possible  Finally  H intersect N divided by H is the true negative  To summarize and simplify  really  violently so  all of medical testing theory into one sentence  You would like the test to be such that the true positive rate and the true negative rate are essentially 1  and so that the false positive rate and the false negative rate are essentially 0  That never  ever happens  if so  you'll win Nobel prizes  you'll make a lot of money  So what generally happens is the true positive rates  ideally  are close to 1  And the false positive rates ideally are close to 0  Later  when you start thinking about business analytics  you might start thinking about what type of false positive rates are acceptable  How large can they be  how small can they be? And it's important to realize that this generalizes far beyond medical testing  If you have something that's true  you're either sick or you're healthy  you're either green or you're blue  And you have a test that tells you whether or not the test thinks you're green or blue  You can use vocabulary like true positive  false negative  True positive  false negative  and a lot of that goes into machine learning  supervised learning  as you'll see later 
ryKmxjqG_e0,Sets - Venn Diagrams  Welcome back everyone  We're gonna finish up our suite on set theory by giving you a neat way to visualize sets  something called the Venn diagram  What I'll do is I'll start by going over what Venn diagrams are and what they're used for  One of the neat tricks we're gonna do is we're gonna prove something called the inclusion-exclusion formula which tells you how big the union of two sets is in terms of the sets and their intersections  And then we'll revisit our medical testing example just to see how Venn diagrams work  And again  Venn diagrams are just a way to look at sets and visualize what's going on  There's no proof to it  but it's often a good visualization trick  Okay  let's get to it  So  let's first write a set in our old notation  So we have a set A is equal to curly braces 1  5  10  2  So let's remember this means that our cardinality of A is equal to what? Cardinality of A is equal to four  Okay  So here we've written the set by just listing out its elements explicitly  Another way to write it is to write a big circle and just put the elements kind of floating inside it anywhere-- 1  5  10  2  so this is A  We just think of A  what we're making explicit here is A is a bag  sort of a ball with some things in it  1  5  2  The ordering doesn't matter  there's nothing there  it's just A has four things in it â€“ there they are  Okay  And by the way  you can write this any way you want  So this might be  for example  the same as 1  2  10  5  It's just a visual notion  Okay  So what? Well  this gives us a cool way to visualize things like intersections  So for example let's write A again  A equals 1  10  5  2  Let's take the set B is equal to 5  -7  10  3  And say that C is equal to 8  11  And just a way to depict  let's draw A as 1  2  10  and 5  And now let's choose a different color â€“ let's draw same color actually  let's draw B  B has 5  -7  10 3  Notice that A intersect B is 10 and 5 â€“ they share that in common  We can show that by having the two sets overlap  So here  what are the extra things in B? -7 and 3  And notice in some sense this is A  this over here is B  but inside here is A intersect B  which we know is equal to 10 and 5  but we can visually see it  What about poor little C here  C is an 8 and 11  8 and 11 share nothing in common with anything else  So over here is C  and we're really making a visual point that C is disjoint from A  A intersect C is the empty set  B intersect C is the empty set  Okay  So that's kind of neat  And I want to use what I have over there to make one more point  So here is a formula you will often see called the inclusion-exclusion formula  And this is one of the formulas that's always true in general  We're gonna see visually that it's true here  and we're gonna check it by example  What this formula is concerned with is if you take A union B  Let's remember what A union B is  it's a set of things which are in A or B  In terms of this picture by the way  A and B is everything you see  right? That's something the point  If A is a little ball and B is a little ball  A union B is just the union of the two  The inclusion-exclusion formula tells us that the cardinality of A union B  understood in terms of A and B  is equal to the cardinality of A plus the cardinality of B minus the cardinality of A intersect B  Okay  So first let's just check if that's true in this case  So  working over here  we know the cardinal of A union B  first we can just count that  What's in A union B? 1 and 2 and 10 and 5 and -7 and 3  Therefore  this is equal to six  Cardinality of A  1 and 2 and 10 and 5  is equal to four  cardinality of B  10 and 5 and -7 and 3  is equal to four  and the cardinality of A intersect B  10 and 5  is two  So  we're basically asking is 6 = 4+4-2 and yup  turns out that's true  Let's erase this question mark  let's put back in a check  So that works  Of course it works in that example  we also see visually why it works  Right? Because if we take the cardinality of A  we count it â€“ there are all those elements  Then we take the cardinality of B and we count it  we take all those elements  What's wrong with saying that the cardinality of A union B equals the cardinality of A plus cardinality of B? Well  we've double-counted  we've given ourselves too much credit because we counted 10 and 5 twice  So we've taken that into account by subtracting  in essence  one of the copies of 10 and one of the copies of 5  So  cardinality of A union B equals cardinality of A plus the cardinality of B minus the cardinality of A intersect B  Okay  fine  Now with Venn diagrams in hand  let's revisit that medical testing example  So let's remember that X was equal to all the people  so these are all the people who took some exam  Let's remember that X could be divided up into the healthy people â€“ the people who did not have I believe we called it VBS for very bad syndrome â€“ X union S  the people who did have it  So let's draw that partition here  We're gonna draw it like this  There's a line  Over here are the healthy people  and over here are the sick people  Notice we also have that H intersect S is empty  And so somehow we use that line to divide the two  We had another partition  We have that X was divided up into the people who tested negative â€“ so they took a test and made that happy because the test told them they did not have VBS  Union P  where P was a set of people who took the test and got very nervous because it told them they had VBS  So this is another partition of the set  let's use red  And then it looks sort of like this  So on this side might be the people who tested negative  and on this side might be the people who tested positive  And notice the way I've drawn it  Venn diagrams are never a way to compute something  they're way to encode visually assumptions  Notice that H and N are not the same but share a lot of area  and P and S are not the same but share a lot of area  That sort of making the point  the following point  Let's look at this guy right here  What does that consist of? That consist of people who are in S but are also in N  so this is S intersect N which we remember are the false negatives â€“ the people who do have the disease but the test tells them they don't  That's very dangerous  that means they don't get the treatment they need  Over here  what is this? These are the set of people who are in H but also in P  so that's H intersect P  which are the false positives â€“ those are also unfortunate people  a little less unfortunate  they just get nervous for no reason  Ideally  what you would like for a perfect test is for S intersect N and H intersect P to not be there  In a good-enough world  you would like those little slivers to be really small compared to the rest  Okay  that concludes our video on Venn diagrams 
ZPQqmocI2kI,Numbers - The Real Number Line  Welcome back  everyone  In the last video  we saw the idea of a finite set  We saw two videos where we saw examples of finite sets  In this video  we'll see our first example of an infinite set  And it's going to be one that really permeates all of data science  the idea of the real number line  In this video  we're going to first tell you what is the real number line  this funny symbol blackboard R  We're going to go over simple concepts you've undoubtedly seen before but we want to put into a rigorous framework  Like positive numbers  negative numbers  nonnegative numbers  nonpositive numbers  and we'll talk about a little thing called absolute value  Should be a short day  Okay  so first let's meet the real number line  What we do is we draw a line  And I refuse to ever draw it straight  Suppose this is the real number line  It goes all the way on the extreme right to positive infinity  and the extreme left to negative infinity  And you want to think of this as representing an infinite set  That infinite set is going to be called R  which we usually write this sort of blackboard R symbol  This is equal to the real numbers  So  now the idea is that every single dot along this line represents a real number  There's a ridiculously large  infinite number of them  And we're going to think about what some of those are  First  let's mark some we know  So usually  somewhere in the middle we put 0  and then we sort of make tick marks for whole numbers  So here's 1  here's 2  here's 3  here's 4  here's 5  on forever  Here's -1  here's -2  Here's -3 on forever  If we're being careful  we try to make the length of each of these little sticks the same  Okay  So what I just wrote down are whole numbers  who are often called integers  So a subset  if you called z consists of just the integers  This is dot  dot  dot  -3  -2  -1  0  1  on forever  Okay  not every single number on the real number line is an integer  In fact  most of them aren't  Suppose we blew up this little stick between one and two  Let's just take it out here and blow it up  just so we can see it better  Here's one at one end  here's two at the other end  Let's just draw that  The only thing you really need to know here  without getting into any details Is that every single thing between one and two is also a real number  Same between two and three  same between three and four  and that there's a ridiculously large infinite numbers between one and two  how do you make them all? So here's a recipe  take one put a dot  put any string of whole numbers you want afterwards So  for example  1 1 is in there  There it is  about there  1 1  about a tenth of the way  So is 1 4  there it is  So is 1 1538  which might be about there  And in fact  anything that you do whether you continue on finally or really infinitely is a real number  That's the case for any subinterval in here  So if I take this subinterval and I'd blow it up out here  here's -3  here's -2 right about there  might be minus 2 5  So really  a real number is any integer dot any string of integers you might possibly want  It's a little bit of a nicety  which we won't get into  which is that some of these strings of integers terminate  some of them don't  You've probably heard of real numbers like pi  Pi  somehow you've probably seen is approximately 3 14128  so on so forth  one of the things that goes bump in the night  There are real numbers which are represented as streams of decimals which continue on forever  don't repeat don't have any pattern  Those are called irrational numbers  Weâ€™re not going to think about those here  Really here the take home message is just a real number is a number along this line and there are a whole bunch of them  Okay  one of the things we often do as mathematicians is we compartmentalize big sets into smaller and smaller sets and give different categories for what things are  So the first very big division of the real numbers is into positives and negatives  So here's our friend  the real number line again  Here's zero  Anything to this side of zero will be called positive reals  Positive real numbers  Anything to this side  if we have a negative real numbers  So an example of a positive real number might be 5 3 might be 0 001  For example the negative real number right here might be negative 11 7 if we include zero  So the positive reals but including zero we often write the non negative reals- and if we include 0 on the other side we go from right  non positive reals  Okay  fine  let's draw that real number line again  Let's try to get straighter each time  An important thing to realize so that in some sense numbers come in pairs  positive and negative versions of the same number  So here's 0  Suppose I take the number over here 7 1  It has a friend on the other side called negative 7 1  Just like here you have 10  here you have negative 10  Now notice 7 1 is not equal to -7 1  10 is not equal to -10  However  7 1 and -7 1 have one important thing in common  which that they have the same distance to zero  The distance from here to here is about 7 1  is exactly 7 1  And the distance from here to here is 7 1  There's a concept called absolute value  Let's define that  The absolute value- Of a real number and let's say the real number is X for the definition  which we denote like this  X with a little symbol around it  Notice that it looks exactly like the definition of cardinality of a set  which is unfortunate  Is the distance- From X to 0  so you want to start at x  you want to walk to 0 and let's figure out how many units you've walked  We'll notice over here that the absolute value of 7 1 is 7 1 and the absolute value of minus 7 1 Is 7 1  which by the way is the same thing has negative  -7 1  That's not just a huge little formula  We all know that negative times negative equals positive  But it allows us to make a general definition  So let's write that down in here  through the general rule For any real number  for any x in R  the following is true  The absolute value of an x can be one of two things  What I'm going to do here is give you what's called a definition by cases  Part of the point of teaching this is not just to give you the formula for the absolute value  But also you're going to see this definition by cases throughout data science  Death of value of x is equal to plain old x if x is non negative  But it's equal to negative x if x is negative  Let's check if that's true  and while we check this  this'll sort of show us how to parse the definition by cases  So let's compute the absolute value of 8 7  So according to this definition  8 7 is our It's either going to be 8 7 or negative 8 7  But which case happens? 8 7 is non-negative so i'm in this case up here  So this is just equal to 8 7  And that's true  Right  I draw my real number line  Here's zero  Here's 8 7  That distance is 8 7  Let's check a different one  This shouldn't surprise you  let's check a -1  So let's say let's check -10  the absolute value of -10  What should this be by the way? If here is -10  it should be this distance  Which is 10  So I go through my definition  -10  okay  10 is negative  Therefore  this formula tells me to take the negative of -10  Is equal to negative -10  which lo and behold is equal to 10  which is what I wanted  Okay  that concludes this video where we've learned what the real number line looks like  what it means to be positive  negative  non-negative and non-positive  and also about absolute values  In the next video  we're going to jump into the idea of inequalities  What it means for one number to be less than another number  less than or equal to another number  and so on  And we'll connect that back to absolute value 
s7u6Ti1MrmU,Numbers - Less-than and Greater-than  Okay  welcome back everyone  We're going to start a few video series on the idea of inequalities  And this video it's just going to give you a short one  It's going to give you the basic idea  The whole point here is to introduce you to symbols like a < b  x > y  c is less than or equal to d  z is greater than or equal to w  and this really funny one down here  which we often say  e << f  The first four of these are really well-defined mathematical concepts  the other one is a bit fuzzy  but you'll see it throughout all data science  so we really need to name it here  Let's start from the top and draw a real number line  And suppose here's 0  here's the number 2  here's the number 3 1  and let's consider this statement  2 < 3 1  This funny little pacman symbol here pointing that way is read  is less than  So here's a statement  let's think about what the statement means and see why it's true  This statement  2 < 3 1 means  2 is to the left  Of 3 1 on the real number line  r  which is in fact true  2 is the left of 3 1  Let's say 11 78 < 3 1  say it's about there  Fact  11 78 is also to the left of 3 1 on the number line  So  in general  when I write A less than B  I mean A  wherever it is on the real number line  And I want to put and pick a B to make that true  B has to be somewhere over here  So A has to be to the left of B  A common misconception is that when you say A < B  say that means literally A is smaller than B  That actually works when A and B are both positive  Somehow 2 is smaller than 3 1 by the plain English meaning  but I would not say by plain English meaning that -11 78 is smaller than 3 1  It's really just to the left of it  When we write something like 3 1 > 2  this just means 3 1 is to the right of 2 on the real number line  Which is true  In fact  A < B is true if and only if  so this is a fun little mathematical symbol for if and only if  B > A  Okay  so we've really figured out what less than means  and what greater than means  Here's this funny symbol  x << y  And you actually never see this in a proper math textbook  because it's not really a proper mathematical concept  But you'll see this all the time in data science  What this really means  this means  x is much  much less than y  So for example  1 << 1 000 000  might be a reasonable thing  I'd say reasonably not true  There's no real way for a judge to judge whether this is true or false  Whereas a statement like 2 < 3 1  you can say is that true or false  It's more much  much less  It's kind of in the eye of the beholder  but we tend to agree what it means  Somehow  1 000 000 towards one  and that's also very important  Okay  so we figured out what A < B means  let's think about less than or equal to  So this statement a  I'm going to make this funny symbol  So this read  less than or equal to  First just in terms of writing  the way you write this is you first write the less than sign and then you put one line under there  And sometimes you really should put two for less than or equal to  and every now and then you'll see that  that a is less than and then the equals sign of b  But almost always you see less than and one line under it  What does this mean? This means that a < b  or a = b  It's literally just a shorthand in terms of the real number line  If this is b here  a less than or equal to b  means either a is less than b  which means a is somewhere here to the left of b or a is plopped right on top of b  and a equals b  Okay  fine  So how do we check if a claim  like a is less than or equal to b is true  if someone gives us specific examples? Let's see  So suppose someone proposed to me that 2 is less than or equal to 3 1  Do we believe them? Well  that person's really saying that at least one of two claims is true  This means that  2 < 3 1  or 2 = 3 1  The first of these claims is true  The second claim is false  but taken together the claim is true  That's the nice thing about or you only need to satisfy one requirement  Say on the other hand someone claims that 2 is less than or equal to 2  So that means that the person says either  2 < 2  or 2 = 2  First one's false  Second one's true  Taken together  they're true  And let's take one where we lose  Suppose that someone wants to say that 2 is less than or equal to 0 8  This means that either  2 < 0 8  or 2 = 0 8  First one of these is false  the second one is false  and so we don't win  It's false overall  Okay  that concludes the video on basic inequalities  We've learned what less than means  greater than  less than or equal to  and greater than or equal to  all of which are proper mathematical concepts  We've also learned what it means to say that A << B  which is not a proper mathematical concept  but it's an intuitive one people use a lot  And that concludes our video for today 
ayP5Ey-djgw,Numbers - Algebra With Inequalities  Welcome back everyone  We're going to continue with our suite on inequalities  Last time  we just showed you the basic ideas of what less than  greater than  less than or equal to than  greater than and equal to than looked like and have a picture of them on the number line  Now  we get a little bit more technical  Nothing too technical and show you how to do some algebraic manipulation with inequalities  We'll start by reminding you how to do it with equalities  First  what you do mechanically and why you would do it  And  when you do it is to solve for X  Then  we'll show you what is probably some new stuff  Which is  how to do it with inequalities  what works and then a very very big warning that there's one thing you can do with algebra equalities namely multiplying by negative numbers  That you have to be very very very careful about with inequalities to give away the punchline at the beginning  The only real difference is when you multiply by a negative number in equality you have to flip the sign  And  I will show you what that means  Let's get into it  OK first  what is probably some very very familiar material  was taught by algebra with inequalities  Suppose we start with something really simple like four equals four  Here's something that seems so obvious  that no one really appreciates what a powerful tool it is in an algebraic person  What am I saying? What I'm saying  is that if you take an equality and add the same thing to both sides  So take four and lets add three to the left side  lets add three to the right side and lets add the equal sign here  This is still true  if four equals four then four equals three equals four equals three  In fact  this is equal to seven  and this is equal to seven  So  let's put that as a general rule  If A equals b  then A plus C equals b plus C or A  b and C can be any numbers positive  negative  integers  fractions whatever  Who cares? Well  who cares is  this allows you to do algebra  For example  suppose someone tells you there's a mystery symbol called X  And X plus three equals ten and they demand that you solve for X  Meaning  what they're really asking you is  what is X? If X plus three equals ten  The simple one like this  you can sort of stare at it and say X probably has to be seven  But  you can also do it rigorously and technically using this algebraic rule  This part less tool you have in our arsenal  How could you do it? Well  you look here and you want to isolate x  If you knew that X was equal to something  you'd be done  If that's what it means to solve for X  So  let's Isolate it by getting rid of the thing next to it  Let's subtract three from both sides  In other words  if we write X plus three  minus three  this is equal to 10 minus three  X to three minus three is equal to X  10 minus three is equal to seven  And hooray! We found out what we want to find out which we sort of knew already  Okay  So  that's nice  Let's start again with four equals four  And  let's multiply it by say two  Two times four equals two times four  This works  This is eight  And this is eight  So  I can multiply my number  I can also multiply by a negative number  So  if I start with four equals four and then multiply by negative three  Negative three times four  equals negative three times four  Over here this gives me negative 12  over here this gives me negative 12  And I'm happy  General rule  if A b and C are numbers  and C is not zero  And A equals b  then C times A equal C times b  Who cares? Well  again it allows us to do algebra  So  suppose someone comes along and says minus five X equals 15  Here  you solve for X  So  if we have to stare at this  with a little practice  You could probably figure out that X equals minus three  Another way to do it is to isolate the X  X is being multiplied by minus five  I need to somehow cancel that out  Let's multiply by negative one fifth  Right  So  i multiply by negative one fifth times minus five X that equal to negative one fifth times 15  Negative one fifth times minus five is one  This gives me that X is equal to minus 15 over five which is minus three  X equals minus three  I struck oil and I'm happy  OK fine  So  that tells us that out those two algebraic tools are in our arsenal  That you can add the same thing to both sides of the equation and you can multiply both sides of an equations still say true allow us to solve for things  OK  Let's go to inequalities  Suppose we start with four is less than seven  Suppose I add something to both sides  So  four is less than seven  Is it true that four plus two is less than seven plus two? Yup  Because  four plus two is six  seven plus two is nine  That works  Right  If I start with four is less than seven and I subtract something  So  say four minus one is less than seven minus one  That work  That says that three is less than six  And that works  So  general rule  If A is less than b then A plus C is less than b plus C  For any written numbers A  B and C we can even picture that on the number line  Right suppose here is A  here's b  That means  A is less than b is being left of it  And I add C  if C is positive that means I'm shifting A by C and I'm shifting b by C  There's no way that A plus C can jump over b plus C  Likewise  if C is negative  That's why that works  And  that's often very powerful  That allows us to do algebra inequalities just before  So when suppose someone tells us that X plus three is less than 10  And  they ask us to tell us more about what that means about X  So  if X plus three is less than 10  I can subtract three from both sides  And you all should practice with that  So  let's just see  I subtract three from the left I get X  I subtract three from the right I get 10 minus three  In other words X is less than seven  So  what does that mean? That means that  Whatever on earth X is  And it could be anything  It's not one particular thing  It has to be to the left of seven  In other words  X has to live in this blue zone here  Any number wants to be as long as that doesn't exceed seven  And that's what the information that X plus three is less than 10 looks like  By the way  in a later video we are going to talk about this being a half open array  This is going to be a symbol minus infinity to seven and X is in there  We'll tell you what that means  Don't get confused about it now I just want you to see that in advance  OK that's great  Now  here's a little warning  When you multiply inequalities by both side  it's little more dangerous  We start again with five is less than eight  And first let's multiply both sides by a positive number  So for example  three times five less than eight times five  Question mark  Yeah  That works because  three times five is 15  eight times five  oh dear arithmetic in public  eight times five  gears churning were eight times five is 40  I think that's right  So  15 is less than 40 and that works  Here's a problem  Lets take five less than eight  And let's multiply both sides by a negative number say minus one  If I do minus one times five  and I ask are you less than a question mark minus one times eight  Turns out we get an electric shock through the keyboard  If  we ever try to write this because minus one times five is minus five less than question mark minus eight  No way  Right  Here's the real number line  here's zero  here's five  here's eight  Unfortunately  here's minus five  here's minus eight  minus five is not till after minus eight  This is completely awful  In fact  what is true is that minus five is greater than minus eight  Here's the general rule  Suppose  A is less than b  If C is greater than zero  A times C is less than b times C  But  if C is less than zero  Then A times C is greater than b times C  Just as we saw here  See you have to flip the inequalities  Let's see one example of that  So we do a little algebra  Suppose  someone tells us that minus two times X is less than 10  What does it mean about X? Well  lets multiply both sides by minus one half  So  if I multiply this side by and minus one half times minus two X  multiply this side by minus one half minus one half times 10  But now  I have to flip the inequality  Right  Because  I multiplied both sides of inequality by negative number  This gives me X is greater than minus five  Which pictured on the real number line here's zero  here's minus five  That means  X can be anything in here  So  if X and anything in here  and I feed X to this equation up here  This inequality up here  I make it true  That's really just the only pitfall you have to worry about  But  it can be a serious one  OK  That concludes all with you 
hScleoVj0pw,"Numbers - Intervals and Interval Notation  So here's the real number line  We've talked before about finite sets  but in fact the real number line is an infinite set  Think of the large infinite number of things on this  Now there's a lot of subsets of this which are also ridiculously large and infinite  So let's just jump right in  this symbol  [2  3 1]  This is an infinite set  but if an infinite set has got a finite bound -- we'll talk about how to describe what this is -- this is equal to the set of all numbers X in R  which satisfy two conditions  First  X has to be greater than or equal to two  and X has to be less than or equal to 3 1  In other words  to draw this on the real number line  here's two  here's 3 1  and X is trapped in between these two boundaries but it can hit up to both  So X is any number in the world as long as X is greater than or equal to two  less or equal to 3 1  So let's name some things in here  For example  2 3 is in [2  3 1] because two is less or equal to 2 3  and 2 3 is less or equal to 3 1  Three is in there  also 3 1 is in there  but one is not in the closed interval from two to 3 1  because one is not in fact less or equal to two even though it is actually less or equal to 3 1  well it's not greater or equal to two  OK  so that's a closed interval  Let's introduce the next idea  Suppose I give you (5 8) and here I use parentheses instead of those bracket symbols  This stands for an infinite set  the set of X in R  such that X is strictly greater than five and just strictly less than eight  So  the way we note this is  here is zero  We make a little open symbol for five  little open symbol for eight  then we just take all the things in between  So the idea is you have to be between five and eight but you can't hit up against those end points  So for example  5 5 is in the open interval from 5-8  so is 5 0001 is in the open interval from 5-8 but sadly five is not in the open interval from 5-8  because five is not less than five even though it is less than eight  So you might want to think  by the way  what's the difference between the closed interval from 5-8 and the open interval from 5-8? They differ in exactly two numbers  They differ in five and eight  Five and eight are in the top one and not on the bottom  Okay so those are two extremes  here's two things in the middle  which we call \""half-open intervals\""  Let's take for example  (-7 1 15]  You might already be able to guess this  On the open side  that means we use a strict inequality  and on the other one we use less than or equal to  This is an infinite set  Set of all X in R such that -7 1 is strictly less than X is less than or equal to 15  So how am I be drawing that  here is an open -7 1  There is zero  just so we know where we are  and there the closed 15 off the scale and all that stuff in there  Okay  we'll take the other extreme  So let's say for example  [20 20 3)  This will be the set of all X in R plus the 20 is less three or equal to X and less than 20 3  Draw again the real number line  Here's my zero  let's see here's the 20 and here is an open 20 3  There's the stuff in here  But a little point to make  in some sense this seems really small  20 to 20 3 is really tiny on the real number line compared to -7 1 to 15  It's got infinitely large number of numbers in there  That's the hilarious thing about the real number line  There's a lot of fancy math which you will get into behind it  We've seen closed intervals  like [2  3 1]  We've seen open intervals  like (5 8)  And within two species of half open intervals  like (2 3] and like [20 20 3)  Sometimes if we want fancy vocabulary  the first of these half open interval is called left open  this is called right open  that doesn't really matter  OK  let's show you one more slightly more exotic form  Suppose we write  close_two_comma_infinity  This just stands for the set of all X in R  such that X is the greater than or equal to 2  full stop  You don't have to be less than infinity because every number is less than infinity  We often draw that on the real number line  here is zero  there's a two and we just take all the stuff here  kind of going on forever  This is often what we call a \""ray\""  You could also have for example  a minus_infinity_ to_7 1_open  This is the set of all X in R  that should be X is less than 7 1  and you get the idea  Okay  let's close by tying in to the algebra we recalled in this video  We're already comfortable with the idea that if someone asked you to solve for X  and X+5=10  you do some algebra and you solve that X equals five  So X=5 is the answer  a number is an answer  Suppose on the other hand  someone gives you the following problem  Tell me everything you know about X if the following is true  1_is_less_or_equal_to_X_plus_five_is_less_than_ten  So notice here  a single number is not the answer  For example  if X=4  then 4+5=9  nine is less than 10  nine is greater than or equal to one  but 3 9 would also work  In fact  it turns out that the answer is an interval  If we do a little bit of algebra  let's subtract five from all sides of this  Let's subtract five from the left side  I get -4 is less than or equal to  subtract five from the middle I just get plain old X that's why I subtracted five  that wasn't random  Subtract five from the right  I get 10 minus five or just five  In other words  the first puzzle tells me that the answer is any X in this range  So in other words  as long as X is in the half open interval from [-4 5)  that's the answer  That tells me  any X in here tells me  that this up here is true  OK  that concludes everything "
8j4Z0IrS96Q,Sigma Notation - Introduction to Summation  Okay  welcome back everyone  the point of today's video is to understand what we mean by sigma notation  And we often use for sigma this big Greek letter  sigma  which kind of looks like a big pointy S  And as always in these lectures  the point is not necessarily to bombard you with computations or to judge you on right or wrong answers  But simply to demystify and explain notation  which would otherwise be mystifying  So what we're going to do in this lecture is work through three examples  Each one of them an example of sigma notation  each one of these things here  The sum from i = 1 to 4 of i squared  the sum from i = 1 to 5 of 2i + 3 and the sum from j = 3 to 7 of j over 2  These are all just fancy ways of writing numbers  In fact  tell you the answers in advance  this first number equals 30  the second number equals 45 and this last number equals 25 halves  How on Earth did I know that? Point of this lecture is to learn why that's true  Let's dive in to do the first one  We're going to compute the sum from i = 1 to 4 of i squared  What I'm going to do is just do this myself first  and then walk you through and unpack  how I did that  Okay  so the sum from i = 1 to 4 of i squared is equal to 1 squared + 2 squared + 3 squared + 4 squared  Okay  so suppose someone paid me to do this problem  I'd consider myself done  I've worked it out  I've translated it from sigma notation to something someone who knows arithmetic could do  A person that is really a stickler for details  they're going to say well keep going  but I'm going to say  maybe you need to pay me more money  After we're done with that negotiation  we'll say  okay  from here  it's arithmetic  that's just equal to dot dot dot  The dots covering up the fact that I've done this in advance  This is equal to 30  the answer we saw before  But the real point of today's lecture is to understand this first equals sign  How on Earth did I know that this funny symbol  the sum from i = 1 to 4 of i squared is equal to 1 squared + 2 squared + 3 squared + 4 squared? Okay  so let's walk through that  The first thing to realize looking at this symbol is there's a bunch of things  First  there's a counter in here  there's a symbol in here i squared  That same person walking out to me on the street is pretty annoying by now says  I'll give you ten bucks if you tell me what i squared is  No deal  it's not really a fair question  I know what i is  but actually I had some hints  Down here at the bottom  I know how to start my range of i  I know that I should start from i = 1  and at the top  I know that I should finish when i is 4  Okay  so let's do some scratch work and work that out on the side  Here I have i = 1  here I have i = 2  here I have i = 3 and here I have i = 4  So you'll notice that starting range  starting from 1  that finishing range ending at 4  And actually there's something here that's a little unfair  nothing in the symbol tells me that I count by one  going from the bottom of my range to the top of my range  That's okay  that's sort of a cultural agreement  You start from the low i  you end at the top i  and you count by one  Okay  fine  so what do we do? To each one of these i's we do what this thing in the middle tells us to do  In this case  this thing in the middle  very bossy  tells us to square i  So if i = 1 then i squared = 1 squared  If i = 2  then i squared = 2 squared  If i = 3  then i squared = 3 squared  I think you get the hang of it  i = 4  and i squared = 4 squared  Okay  we've done the scratch work on the side  What do we do next? Well sigma  What you ought to think about this is equal to sum  Meaning  we take all of these answers here and we add them up  And that's how we get what we get up here  We've computed on the side that 1i = 1i squared = 1 squared and so on  and then we add them all up and get our answer  For those of you who are sort of business minded you can think of this as a process  Which can be broken up into parallel processes undertaken by different workers  So one worker can compute what 1i = 1  One worker can do 1i = 2  One might i = 3  One might i = 4  doesn't really matter when they do them  They do them at the same time in parallel  The end of the day  they compare their answers  The last worker adds them all up  and we get the answer we get over here  or over here  Okay  fine  let's do another example  very much like the same a little bit of a twist  So the second promised example is the sum from i = 1 to 5 of 2i + 3  Only thing that changed from last time  is we changed the top of the range and we changed the thing inside  So just like before  let's do it one more time in full detail  Then go from i = 1  i = 2  i = 3  i = 4  and i = 5  What do I do to i = 1? I do what I'm told to do to any i by the symbol  So in this case  I take it and I multiply it by 2 then I add 3  Here  I get 2 times1 +3  Here  I get 2 times 2 + 3  Here  I get 2 times 3 + 3  Here I get 2 times 4 + 3 and here I get 2 times 5 + 3  End of the day  take them all up  add them up  Just like we did before  So we do that up here  this is = 2 times 1 + 3 + 2 times 2 + 3  And you notice this is getting a little tedious  that's sort of the point  adding up long strings of numbers is tedious  Really the added value here is that the sigma notation give you a compact way of representing the work order  This is what you're going to have to do should you choose to do it  but you won't choose to do it  Then we have 2 times 5 + 3 and agin  we're really done  I consider myself done  if you really want an answer  want to pay me a little more money  maybe I've done this work in advance  Turns out  this is equal to 45  which you already knew because I told you in the beginning  But maybe you believed me and maybe you didn't  Okay fine  let's do one more example  This case  we're going to break the idea that you have to start from 1 with a counter  We're also going to break the idea that you have to use i  Let's take the sum from j = 3 to 7  j over 2  You know what? We're going to be big kids about this now and not even do our little scratch work  because I think we get the idea  Just tell this to do something to j  What do we do to j? We divided it by 2  Okay  boss  but which j do we do that to? We do it from j = 3 incrementing 1 up to j7  okay? So this is = 3 over 2 + 4 over 2 + 5 over 2 + 6 over 2 + 7 over 2  We work all that out  I really don't like arithmetic and I really don't like fractions  So here  I'm going to be upfront and say I did this myself at home  actually at 5 in the morning  This is equal to 25 divided by 2  Okay  so those were three easy examples  Now I'm going to give you the 100 gold coins gold star problem  Which I want you to compute the sum from R = 3 to 7 of R over 2  and I want you the pause the video and think about this  Okay  so there's two way we've done this  One is to do it directly just so we did before  the other is to be little bit clever and say  nah  it's a trick question  it's really 25 over 2  How come? Well the only differences between these two is one's above the j  One's got an R  otherwise it's the same range of j and the same range of R and the same thing that's being done to j and R  There's something special about j and R  In fact  j and R are examples of something we call  Dummy indices  And we mean to be super disrespectful as dummy makes it sound  J and R  you dummies are not real variables  their not real unknowns like in algebra when you solve 2x plus 1 equals a  And you have to solve for x in order to get points or not  They don't have any independent existence  there's no answer to what they are  They're just symbols for counters  They say  start at something equals 3  increment that something up to 7  do the following to something  To really drive that point home  note that the sum from smiley face equals 3 to 7 of smiley face over 2  what do you think that is? Well it's just 25 halves 2  there's nothing special about it  That said  let's not get too wild  there is generally a cultural agreement that when we use dummy indices  We tend to use symbols like i  j  K  maybe L  maybe R  sometimes M  sometimes N  In other words  we tend to use things from around the middle of the alphabet  But thatâ€™s just an agreement  no particular reason  you could use a b smiley face  maybe try to draw a little doggy  Really very  very poorly because thatâ€™s not what I do  People will look at you funny  but you'll be just as perfectly right 
buhZRdH58W0,Sigma Notation - Simplification Rules  Okay  welcome back everyone  Last video we did some elementary examples of sigma notation  Today we're going to make it a little bit more complicated  and we're going to go over some rules  For manipulating  Slash simplifying  Or making for complicated  if you like  sigma notation  I'm going to start with our first rule  itâ€™s just an example  Last time  remember  that we worked through that the sum from i=1 to 4 of i squared was 30  But now we're going to give you a different problem  which when you think about it is not going to be too different  The new problem is going to ask you the sum from i=1 to 4 of 3i squared  Now in fact  we could work this out directly  just as we did before  Make our little scratch work with our i and add it up  But let's work through this just enough so that we can see the pattern  see the point of this example  So if we work this out  this is equal to 3 times (1) squared + 3 times (2) squared + 3 times (3) squared + 3 times (4) squared  Now again  to the guy who's paying us to do this  we're done  you can make that into a number  Let's do that a little bit further  just for our own purposes  If we use some stuff from elementary mathematics  namely something you may remember as the distributive property  multiplication over addition  let's pull out the 3  This Is equal to 3 times [1 squared + 2 squared + 3 squared + 4 squared]  That's equal to 3 times our old friend  which we're now getting pretty bored of  as we do with old friends  the sum from i=1 to 4 of i squared  So notice the pattern here  I started off with an expression  the sum from i=1 to something  And internal here to the thing inside the sigma notation was an expression 3i squared that had a constant in it  namely 3  I can pull that constant outside and get that the sum from i=1 to 4 of 3i squared is the same thing as 3 times the same exact expression  but without the 3 sitting there  Turns out this is a general rule  Let's try one more example  if I did the sum  From r=4 to 25 of 18 r cubed  I really don't want to work that out  But I can tell you this is the same thing as 18 times the sum from r=4 to 25 of r cubed  whatever on earth that is  And this is always true  Whenever you have an expression with a constant inside the sigma  you can pull it outside  and just then evaluate it  Why is this true? There's something from elementary mathematics which you may or may not have heard called the distributive property  Namely  if you have A times (b + c) that's the same thing Ab + Ac  And all we're doing is we're taking this expression and we're doing it over and over again with the sigma notation  Okay  so that's lesson number one  that's one way of simplifying sigma notation expressions  Let's give you a new problem  The sum from i=1 to 4  and internal here we're going to have i squared plus 2i  So let's just work this out directly  So this is equal to 1 squared + 2 times (1) + 2 squared plus 2 times (2) + 3 squared + 2 times (3) + 4 squared + 2 times (4)  And again we could say we're done  down tools  that's it  But let's break this up for our own purposes  Notice in this sum we have terms that look like two different things  One looks like things squared  like these ones  And one looks like two times things  like these  Let's break those up and write those separately  This is the same thing as being equal to (1 squared + 2 squared + 3 squared + 4 squared) + (2 times (1) + 2 times (2) + 2 times (3) + 2 times (4))  Let's pause for a second and remember why that's so  Basically it's so because I could add numbers in any order I want  Someone told me to add them the way I have up here  but I can add them the way I have down here  I'm just shuffling them around  Why did I do that? Well let's recognize  what's this guy here? That's equal to our old friend  the sum from i=1 to 4 of i squared  Fine  what's this guy here? That's equal to a new friend  that's equal to the sum from i=1 to 4 of 2 times i  Notice we have a rule here  I started off with the summation and then there's something inside the summation which was broken up into two different things  i squared + 2i  I can fragment that  break that up into two different summations  should I want to  Why is that true? It's true because when you add up things in any order you want  Okay  let's do our last rule here  This one seems really silly and really simple  but it makes you think about it a little bit  Let me give you the sum from k=1 to 10 of 5  Seems like a trick question  Notice the thing in the middle has no dependence on k  What do we do? This is almost convention  this is equal to  each time I evaluate something I'm going to evaluate it from k=1 to 10  But I don't do anything with k  I just write down whatâ€™s in there  This is equal to 5 + 5 + 5 +  plus  plus  dot  dot  dot  dot  + 5  How many of them? 10 of them  So this is equal to 10 times 5 = 50  The general rule here  whenever youâ€™re summing up a constant  in this case it's 5  but it could be anything  You just add up that constant the number of times you're supposed to do  So for example  the sum from r=1 to 7 of 8 would be 8 + 8 + itself 7 times  This is the same thing as 8 times 7 or 56 
BKkpEjbD8xM,Sigma Notation - Mean and Variance  Welcome back  everyone  We're going to finish up our video series on summation and use of the summation symbol  and we're going to remind you what mean and variance are  And the main point of this lecture is either remind you or tell you about these concepts for the first time and to tie them back in to sigma notation  It's one of the main uses of the sigma notation  What I'm going to do here is scare you a little bit by showing you the punch on a video at the beginning  But we'll walk through this very  very slowly and unpack it  So the whole point of the video  if you understand this screen you've understood everything  Is if we have a set of numbers X which has N numbers in them  X1 to Xn  they're just real numbers  The mean of that set  the symbol mu sub X  could be expressed as In summation notation this way  So t his is the mean of x  Sort of an average  And this ridiculous set of symbols here  this is called the variance of x  The point of the video is to understand what that is  And finally by the way  that's just plain old sigma  Which is the square root of sigma squared  This is the standard deviation  And we'll walk through all that  Okay  now that we've seen that fancy sigma notation let's work a really simple small example of numbers and then generalize  So suppose we have a set Z which has 3 things in it  Clears 1  5  and 12  and if you remember the old notation the cardinality of Z is 3 because there are three elements in z  If we take the mean of Z  what that really means is we add up all of the numbers  1 + 5 + 12  and we divide by how many numbers we have  in this case three  So if we do that  never a good idea to do arithmetic in public  obviously I've done this in advance  So it's 18 over 3 which is 6  that's the mean  And there's lots of notation for it  The most correct general notation might be the Greek symbol mu for mean and we usually put a sub z down here to say it's hey it's that set z we're using  Sometimes you'll be it's mu of z  z argument to a function  Often you'll just see mu  because we know what we're talking about already  Okay  that's a simple example of numbers  Let's do a slightly harder example with symbols instead  Suppose I give you a set y  which consists of four numbers but I don't tell you what they are  y1  y2  y3  and y4  Then the mean of Y  mu sub Y  would be just one-fourth times y1 + y2 + y3 +y4 and aha  Here comes the punchline  Let's express this in sigma notation  This is one-fourth times the sum from i = 1 to 4 of y sub i  Right  and that's really the mean  And remember  i is a dummy index  I could use j  I could use a smiley face  I could use whatever I want  So let's not get too radical  let's use i  Okay  so we generalized  let's generalize a little bit further  In general  suppose we have set x consisting of n arbitrary numbers  They exist  but I'm not going to tell you what they are  x1  x2 up to xn  The mean of x  Is pretty easy to guess this now  Mu of x is equal to 1 over n times the summation from i = 1 to n of x sub i  That's the mean using sigma notation  By the way  it's worth thinking a little bit about the two different philosophical functions of i and n  i is not a real variable  right? i is just a counter  It's says hey  count from 1 up to n  That's why we call it a dummy variable  Hey you dummy  you're not doing anything except taking a counter off from 1 to n  N really is a variable  right? N tells you when to stop  I'm not telling you what n is  If n is 10  you'd stop at 10  If it's 11  you'd stop at 11  and so on  right? In the previous example  we saw n was 4  Okay  before we jump into variance  I want to take a little sideline and tell you about something called mean centering  The reason I'm telling you about this is this is a common trick used throughout a lot of data science techniques  You'll see it later in linear regression  for example  Punchline here is if you have a set of numbers you often want to change the side of numbers so that its mean is centered at 0  There's lots of reasons why  Let me just walk you through the mechanics of what it means to mean center data no pun intended  Here's our friend Z  It has three elements in it  1  5  and 12  Previously we computed that the mean of Z  6  Over here we see three elements of Z in blue  1  5  and 12  And there's the mean  6  there  Let's form a new set  let's call it Z prime  And what I'm going to do is to every element in Z  I'm going to subtract off the mean  So 1-6  5-6  12-6  No dear  three examples of arithmetic in public  This is -5  this is -1  and this is 6  So there's Z prime  Note  if I compute the mean of Z prime  I'm going to get 0  This is -5 + -1 + 6 over 3 and that works out to be 0  unsurprisingly  What we are actually doing here  through all the copy of the number line  is we are saying  hey  see that red dot there at 6  I'm going to pretend that's 0  How am I going to pretend it's 0? I'm going to yank it back over to 0  and I'm going to sort of be accountable for what I've done  so I'm going to shift everything over  So in other words if you like  here is 0  6 is going into 0  That's going to become a red dot  And then everyone comes along for the ride  So 1 has to go all the way to- 5  5 has to go to- 1  Everyone's getting shifted the same amount  12's gotta get shifted over here to 6  And that's mean centering  The punchline is whenever you mean-center data  you produce a new data set which sort of has the same relationships  but the mean is 0  There's lots of reasons why you want to do that  which we'll get into later  All right  the last topic of today's video is a statistical concept call variance  I should mention  one of the points of the mean  is when you have a large set of number  In this case  large is 30  but large could be 3 million  Statisticians and data scientists often don't like large amounts of numbers and large amounts of information  So they want to summarize the set by a small set of numbers  So summarizing a set by its mean is about the simplest thing you can do  but it gives some information  What we're going to see here is an example of where it obviously doesn't give complete information  Here is a set Z  which is 1  5  12  We're getting bored with this  it's our friend  we get bored with our friends  Mu sub Z is 6  fine  Here's another set W  5  6  7  If you calculate mu sub W  turns out that's also 6  so you can check yourself  So obviously it's not the case  unsurprisingly it's not the case  that the mean is not a unique classifier of a set  We have two sets with the same mean  Okay  let's look at these sets on the number line and see that obviously the mean's not telling the whole story  Here is 0  Here's the mean at 6  Let's draw a Z in blue  So here's 1  here's 5  here's 12  And let's say let's draw W in yellow  So yellow actually has a dot right here on top  5  A dot right there at 6  and a dot right there at 7  Okay so they have the same meaning  but if you had to say in a word what the difference it between yellow and blue you might say spread out  Okay  that's two words  right? You might say that blue is more spread out than yellow  And there's a statistical mathematical data science concept called variance which basically tells you how spread out data is  Let me just write that down for you in general and then we'll compute it for these examples  So if X is equal to x1 to xn  the variance of x is this  it's got this funny symbol called sigma squared x  It's Greek symbol sigma  we square it  and we put sub x to denote that we really care about x  This is 1 over n  And then this is going to look really intimidating but it's not  We take the sum from i = 1 to n  We look at xi  We ask how far is xi from the mean  xi- mu of x  We're going to square it  Talk about a little bit in a sec on why we want to square it  This is the variance  What this is really doing  let's look at the term inside the square  For every single xi  xi- mu sub x is really the question how far are you from the mean? The reason we square it is we don't really care if you're to the right of the mean or the left of the mean  which care how far are you  So for example  if xi was 1 and the mean was 6  that would be a pretty big number  If xi was 5 and the mean was 6  then it'd not be that particularly big of a number  And then essentially what we're doing here is we're taking the average of those numbers  right  xi-mu sub x squared  We're taking the mean of those numbers which is why we're dividing by n  That's the idea of a variance  Something else you'll often see  If I take sigma of x  which is just the square root of sigma squared  this is called the standard deviation  Of x  Okay  great  so let's calculate that  For these two examples  Before we do it  though  we should sort of home cook it and know what the answer is  right? We know that Z and W have the same mean  Z is more spread out than W  Somehow  I'm trying to sell you on the idea  trying to get you to buy that the variance is a way of quantifying the notion of being spread out  So whatever sigma z is and sigma w is  sigma z better be much bigger than sigma w  otherwise this is junk  Okay  so let's close just with our example  Let's recall z is 1512  W is 5  6  7  And the mean of Z is 6 which turns out to be the mean of W because that's how we cooked it  Let's start with the easy one  So sigma squared of w is going to be  in this case n is 3 so 1 over 3  Times the sum from i = 1 to 3 of  let's call this w1  w2  w3  And this over here is z1  z2  z3  The sum of wi minus mean of w squared is equal to one-third  So now the first one  w1 is 5- 6 squared + 6- 6 squared + 7- 6 squared  And if work that out  that turns out to be one third times -1 squared + 0 squared + 1 squared  Which turns out to be two-thirds  Great  and so therefore the standard deviation  sigma of w  is just the square root of two-thirds  All right  if we do another calculation  we do sigma squared z  this turns out to be one-third times 1- 6 squared + 5- 6 squared + 12- 6 squared  Now this is really not arithmetic I'm going to do in public  so do it dot dot dot  Last refuge of something who doesn't want to do arithmetic  and this turned out to be equal to 62 thirds Which I don't really care what that is  Punchline is I think that justifies saying much much greater than two-thirds  Which justifies our intuition that Z and W have the same mean  But Z is much more spread out than W as measured by the variance 
q6VbIk4ELAA,Cartesian Plane - Plotting Points  Welcome back  everyone  We're going to begin our next unit by talking about the Cartesian plane  Our first video is going to be called plotting points in the plane  The point here is really simple  We've already gotten you familiar with the real number line  which is a way of representing numbers on a line  Now we'll look at the Cartesian plane  which is what we're looking at here  This is often denoted R2  just like the real number was denoted R  we have this fancy black part R2  And it's just a way of representing two pieces of information  We have two axes here  the horizontal one is going to be called the x axis  this is going to be called the y axis  The correspondence between ordered pairs of numbers  as we see over here and points in this plane  So first  a very special one is called the origin  Right here at the meeting of the two lines  This is (0  0)  often denoted as big O  And now  I'll just show you how to plot these others and we'll get the basic idea  So if we plot A  the x coordinate  the first number is 2  the second number is 3  The first number's instruction tells us to walk two units to the right  So let's walk one  let's walk two  Now  from there  let's walk three units up  So three  I got up one  I got up two  I go up three  About there is the point (2  3)  that's A = (2  3)  B  look at the first coordinate  that's -1  that tells me to move one unit to the left  now five up  One  two  three  four  and five  and so there is B  C tells me to go four units to the right  One  two  three  and four  and then half a unit down  Not really to scale but that's all right  And remember  these instructions can really be any real numbers  so we could've gone 4 1  4 2  and so on  Here we want (4  -5)  and D  walk 5 units to the left  2 3 4 5  and about 5 units down  down to about here  And there's D which is equal to (-5  -5)  And that's the entire idea of how to plot points in a plane  One thing I'll sort of apologize in advance for  although it's not my fault  it's really mathematicians' fault  is you'll notice this symbol here looks terrifyingly like the symbol for an open interval  It's not  when we're in the context of plotting points on the plane  this just means the x-coordinate is -5  the y-coordinate is -5  Sometimes in other textbooks  you might see this instead  -5  -5  or A = (2 3)  that means just the same thing  Okay  fine  Certain parts of this plane are really important and we distinguish those  Here's the x-y plane again  here's x and here's y  again  here's the origin  I mentioned the x-axis before  let's formally define that  The x-axis is going to be the set of all points x-y in the Cartesian plane  x-y in R2  such that their y coordinate is zero  Which makes sense  this is the x axis  This right here  as you might expect  is the y axis  This is the set of all points (x  y) in the Cartesian plane such that x = 0  So the instruction that starts off by telling you from the origin to go right or left  tells you to go zero  right or left  and then up  So for example  one of the points on the y-axis might be (0 1)  One of the points on the x-axis might be over here  (-5 0)  And I think you get the idea  Okay  if we remove the axes  then you notice we divide the Cartesian plane into four separate regions  and these we call quadrants  So the first quadrant  Consists of  Points (x y) such that the x coordinate's positive and the y coordinate's positive  The second quadrant over here  as you might expect  As a set of points (x y) in the Cartesian plane such that  let's see  The x coordinate's going to be negative and the y coordinate's going to be positive  and I'll leave it to you to figure out the definitions of the third quadrant  And the fourth quadrant  Okay  who cares about plotting points on the plane? Well  let's give a real world example  So over here  let's draw our plane again  A way of plotting a table of data  where each object or person corresponds to two different numbers  And you want to visually represent the relationship between those two different numbers  So suppose we have three people  and we measure their height in cm  and their weight in Kg  And want to call the three people  unimaginatively  A  B  and C  Suppose A has the average height and weight for an American male  which we've looked up on the internet is 177 cm and 88 8 kg  If we plot A over here  so we have to figure out a scale  But if we go all the way over here and say that's about 177  and up a little less than that over here  and there's about A  A = (177  88 3)  Now suppose B is the average height and weight of the American woman  so 164 centimeters  currently  this is 2016 in case we're looking at this later  and the kilograms is 74 7 kilograms  So you'll notice the average American woman is both shorter and less heavy than the average American male  Therefore  this point B is going to be down this way  to the southwest of that person  so about here  and about here  and there's B  And visually we see that relationship  the fact that you have to go left and down means that the average American woman is both shorter and lighter  Now notice if we move anywhere along this line  what are we doing? If we take any other person who sits on this line  that person will be the same average height  If we move up  that person will get heavier  If we move down  that person will get lighter  So by the way  a little bit of personal information  I'm about here  You can probably figure out where to put yourself on this  Same wise as we move from the average American female  and we move say along this horizontal line  what would we be doing? Any person along that line would have the same weight as the average American female  If we move left  it should be getting shorter  If we move right  it should be getting taller  And that's the basic idea  Notice  by the way  a difference between abstract mathematics and real data  In this particular case  only the first quadrant really makes any sense  no one is negative weight and negative height 
ILyRgG9Rs0U,Cartesian Plane - Distance Formula  Welcome back everyone  Our video today is called Distance in the Plane and I'm going to talk about three things  The first thing I'm going to do is  I'm either going to tell you for the first time or remind you about the distance formula that computes the distance between two points in the Cartesian plane  We're both going to remind ourselves why the formula is what it is and we'll work a few examples  Then we'll immediately show you some stuff which is almost certainly not familiar to you  but is new data science concepts  which is the idea that as long as you could have a notion of distance between points in the plane  you can talk about nearest neighbors  That turns out to be very  very important for machine learning  We shall see in later data science courses  And then we can also talk about clustering  That's also very  very important for machine learning in later data science courses  In fact  just to use words that you'll see later  nearest neighbors is one of the main methods in supervised learning and clustering is one of the main methods in unsupervised learning  But that's really fancy  Let's start really simple  Before we can draw the Cartesian plane  let me remind you of something that you probably saw in high school  maybe in middle school  depends where you took it  which is the idea of the Pythagorean Theorem  So suppose I draw a right triangle  so that means a triangle where one of the angles is a right angle  Well  it's close to a right angle that I can do without having a straight edge  And my little [inaudible] here  so there's that  Suppose this side length here is x  this side length here is y  and this side length here is z  so this is the hypotenuse  What the Pythagorean Theorem tells us  Pythagorean Theorem  going back to Pythagoras  fun to Wikipedia him and find out his amazing life  tells us that Z squared  the square of the linked hypotenuse is equal to x^2 + y^2  And that's just one of the things that's true  you can look up proofs of it  Which is the same thing as writing that z is the square root of x^2 + y^2  That's really what makes the distance formula tick  Let's see what I mean by the distance formula  We're gonna start abstract  and I want to give you some examples  So this gives you two points in the plane  There's the point A  equal to (a  b)  and here's the point C  is equal to (c d)  and then we draw the line segment between them like that  And we want to ask the question  how far apart are C and D? What the distance formula says is that the distance  Dist  from A to C is the square root of the difference in the x-values  (c-a)^2  plus the square of the difference in the y-values  +(d-b)^2  Why on earth would that be true? Well  let's draw a right triangle  Let's take this here and let's draw a dotted line  let's draw a dotted line there  And if you paid attention during the points in the plane lecture  you can convince yourself that the coordinates of this point here  let's see  it has the same y-coordinate as A and it has the same x-coordinate as C  So the points there  that is in fact (c b)  Therefore  the length of this is c minus a  and the length of this is d minus b  So we have a right triangle with sides lengths c minus a and d minus b  Therefore  the length of this hypotenuse  once this hypotenuse is  of course  is the distance between A and C  is given by this formula down here  So  if you didn't understand that  that's OK  You can also just memorize this formula  But often  people like to know why a formula is true so they can derive it later in a pinch or really just understand it  OK  let's work some examples  I'm gonna give you a whole bunch of points of the plane and we'll compute distance between a bunch of them  So  let's start with the point A is (1 1)  and let's take the point B way up here  not really to scale  It's (5 4)  And let's start by computing the distance between A and B  Let's do that over here in our scratch paper  The distance between A and B  just by our distance formula  is gonna be the square root of the difference in the x values squared  so (5-1)^2 + (4-1)^2  Now we have to do a little bit of arithmetic  So that's the square root of 4^2 + 3^2  which is equal to the square root of 16 + 9  which is the square root of 25  which magically works out to be a whole number  it's 5  Electronic hands up if you think I rigged that to make that a nice whole number just to get us off to a gentle start  So that means that the length of this line between A and B  let's draw it in  the length of that line is five  It's five units apart that way  which is interesting  right? Because it's not true that you need to go 5 units in the x direction to get from A to B or five units in the y direction  but you do need to go five units to get from A to B  They're fairly far away  All right  Let's also draw the origin  This is the point big O  this is (0 0)  And let's compute the distance between A and the origin  It's equal to the square root  so the distance between the x-values  so (1-0)^2 + (1-0)^2  Stop for a second by the way and point out that (1-0)^2 is the same thing as (0-1)^2  That is  it doesn't matter whether you do the x-value of A minus the x-value of O or the x-value of O minus the x-value of A  which makes sense because the distance from A to zero should be the same as distance from O to A  should be symmetric  So if we work this out  this is just the square root of two  In other words  for the fans of the Pythagorean Theorem  that length there is square root of two  there's a right triangle  OK  let's do one more point  Let's look at B equals (1 3/2)  so the distance of that line  Now  here you don't really need a fancy formula  you notice the only difference between them is an x-values  It's pretty clear the distance between A and B is just 3/2-1  just a half  So I'll let you work that out with the distance formula  but you can just write down the distance between  I'm sorry that should not have been B  so let's call that D  That's the sort of thing you edit out but we're gonna keep that here just to give it a sense of reality  The distance between A and D is one half  Okay  That's cool  So  by the way  the square root of two is approximately 1 4  All right  so we have these three distances here  Here's the key concept  Let's consider the set S  which is equal to the origin  B and D  Notice I just computed the distances from A to these three points O  B and D  The distance from A to O is 1 4  approximately  distance from A to D is one half  which is equal to 0 5  and the distance from A to B is five  So here's the following statement  the meaning should be pretty clear  The nearest neighbor of A in the set S is D because it's the nearest point  The second nearest neighbor  second NN of A in S is the O  the origin  And the third nearest neighbor and the farthest-away point is B  That's something we often use in data science  You have these three points  O  D and B  and you want to say  if A had to be most like one of them  which one would it be? In this case  we see what it is  Okay  One last little use of distance formulas that we use in data science is the idea of clustering  You'll see later and later in many  many courses why this is important  Let's suppose we have configuration of points in the plane  So here is lots and lots of points here that look like this  and let's say here's another bunch of points that look like that and say another clump over here  Visually  if we look at these points  there might be many  many many of these points  visually  intuitively  we say there are three clusters  three clumps  We didn't define what a clump is or a cluster  but somehow it looks like I've got three of them  Right? Over here there's cluster one  cluster two  cluster three  So if these were sort of three different people measured by some blood measurement or something like that  we would say there's three stereotypical groups  group one  group two and group three  Distance is a good way of expressing membership in a cluster  Essentially what you might say is if A and B are in cluster one and C is in cluster two and D is in cluster three  so we'll put those points down  say that's A  that's B  there's the point C  there the point D  then the distance between A and B  whatever it is  is much  much less  remember that symbol  which we've seen before  doesn't really have a formal meaning but we know what it means in the picture  much  much less than the distance between A and C  and that's also much  much less than the distance between A and D  So having this distance formula  this distance metric  often allows you to break points up into stereotypical clusters or clumps  and somehow  whatever these are measuring  A and B are much  much more similar than A is to C and A is to D  Okay  That concludes our video 
vo7LN-zMI2s,Cartesian Plane - Point-Slope Formula for Lines  Welcome back everyone  This is going to be the first video in a two-part series called lines on the plane  This is the imaginative title  lines on the plane part one  The point of this video  which should be a little bit more technical than some of the other videos It to de-mystify formulas for equations of line  as you have undoubtedly seen before  The first one  which we're going to talk about in part one is what we call the point slope formula for a line  Y minus y nod = m times x minus x nod  Don't worry about all those bewildering symbols in part two we'll show that from that equation you can derive something else called the slope intercept form of the line  Y = mx + b  That's often more familiar to people  and it usually makes people more comfortable  The first one's a little bit more natural to derive  What we'll do for both of these is work up slowly  show you why the formulas do describe a line  and also tell you what it means to say that a formula describes a line  What does that weird idea even mean? Okay  let's start slowly and talk about the idea of the slope of the line segment  So  first abstractly  here's our x  y plane  here's x  here's y  Cartesian plane R2  Suppose here's the point capital A  which x-coordinate little a  y-coordinate little b  And suppose here's the point capital B  with x-coordinate little c  y-coordinate little d  Let's draw a line segment between them  Just a line between the two of them  Remember the length of this line segment is what we were calling for the distance between A and B  So the slope of the line segment between A and B  which we denote with AB with an arrow  by definition  is we usually use the letter M to symbolize it  But the definition because it's the difference between the y-coordinate of the second point  minus the y-coordinate of the first point  divided by the x-coordinate of the first point  taking away the x-coordinate of the first point  c-a  This is often seen as rise divided by run  and we'll understand by example why we use those terms  Okay let's compute a few actual real world numerical examples  So  Suppose I take the point 1  2  there's A is 1  2 looks about right to scale  And let's take the point 3  3  There's the point B  it's (3 3)  Let's draw the line segment between them  Let's ask that  what's the slope? So the slope of that  the slope of line segment AB  by definition  is M = 3- 2  that's the rise  difference in Y values  divided by 3- 1  the run difference in x values  That turns out to be 1 over 2  so the slope is one half  What does it mean to say that the slope of that line segment is one half? Basically that's the answer to the following question  If I were to start at a  I want to stay on the line  but I want to increase my x coordinate by a unit  how much do I need to increase or decrease my white board  It turns out here that if I start on A and I move one unit over  so there to x coordinate 2  And I want to stay on this line  what does my y coordinate have to be? Turns out it has to be 2 plus one-half  In other words the coordinates of this point 2  five-half notice the scale factor makes sense  If I go over 2 units  so I run 2 units in the x-coordinate direction  I have to rise 2 times one-half units in the y-direction  and so I get to the point 3  3  I think that makes sense to everyone  This line segment  by the way  is what we call positive slope  because when I run  I have to actually rise up  Let me show you a line segment with negative slope  We'll take the point C here  with coordinates -1 1  And let's draw a line segment to the origin  This is 0 0  the origin  There's a nice little segment  The slope  of the line segment C0 Is 0 -1 divided by 0- (-1)  careful about subtracting a negative  that's -1 over 1 is -1  So there is negative slope  That make sense  if I'm going to run one unit in the x direction I have to rise -1 or  one might say in plain English  fall one unit down the Y axis  down here  That's the idea  We've talked about the slope of little D line segments  Now  let's talk about the slope of the great big lines  in fact the equation of great big lines  So  let's take the line which goes from the point 2  1  and go through 3 2  right there  There's a line which you can convince yourself pretty quickly  The line segment has slope one  But now let's continue that line infinitely in both directions  Down like that  Okay  and let's call this line little l  That line  Okay  here's the nice thing about it being a line  so look at the line sitting between (2 1) and (3 2)  that has slope 1  If I look at the line sitting between (2 1) and any other point on the line  that has to have the same slope  So in other words  if this is some point right here on the line  x  y  The line segment between 2 1 and x y has to have slope 1  In other words  1 has to equal the difference of y -1 divided by x minus 2  The difference in the rise from 2 1 to x y divided by the run  Now lets rewrite that as y- 1 = 1 index- 2  This is actually a really profound statement  Right? Because xy was arbitrary with any point on the line  This has to be true  So in other words  the line is an exclusive club which is defined by this formula that is as a set  the line is a set of 0 xy in the Cartesian plane  Such that the following relationship in x and y values is true  Y-1 is equal to 1 times x-2  Lets check if that works  right we know that three two is on the line lets ask the question is three two on the line we know it is visually but lets check if this formula works if I plug in two for y and three for x I get that 2- 1 is equal to question mark  1 times 3 minus 2  And if you work that out this is 1 is equal to 1 times 1 and that works beautifully  So any again  if we think about sets as this little colon is like the bouncer at a club  Any point xy that wants to be on the line has to check whether this is true  So for example  the point (5 1) which is visually not on the line is also not on the line because if you plug in 5 for x into this formula and 1 for y  you get an incorrect statement  Okay  let's write that down formally what we just had  This is called a Point-Slope Formula of a line  If a line  l  has slope M and if x not y not  which usually nots to signify a specific point  Although we don't tell you what it is  is any point on the line  and l has the equation  Y- y0 = m (x- x0)  Okay  in the next video we will show you a simpler formula for the line over slope intercept and we'll also work an example or two  That concludes this video 
dkAr9ThdSUU,"Cartesian Plane  Slope-Intercept Formula for Lines  Welcome back everyone to part two in our Part 2 Series on Lines in a Plane  We're just going to jump right in and talk about the example we finished with last time  So we have this line here  L  What we know about L is that it goes through the point (2 1) and it has sloped M=1  So therefore  the point slope formula of the line is y-1= 1(x-2)  Any other point (x y) that wants to be on the line has to satisfy this equation  Okay  the point of this video is to show you a simpler formula called  \""the slope intercept formula\""  Though it's probably more widely used  but the reason I showed you the point slope formula first  is that you can derive slope intercept formula from the points of the formula fairly easily  So first  here is a really key concept  This line L has infinitely many points  (2 1) is 1  (3 2) is 1  (4 3) is another  But one point is really  really important for some reason  this one here  This is what we call  the Y intercept  The Y intercept is the unique point where this line meets the y-axis  So think about what the coordinates of this point are  The coordinates of this point  the X coordinate I can tell you right away is zero because the Y-axis consists of all points (x y) with X coordinate zero  I don't know what the Y coordinate is  What we often do in math  if we don't know something but we want to compute with it and do algebra with it  just give it a symbol  So it's called the (0 b)  b is often what we use for Y intercept  Okay  now let's be a little bit of a detective here and let's hunt for what b must be  It turns out that if I know that the point (0 b) is on the line L  and I know the equation for the line  I can find out what b is  right? Because here is (0 b) and (0 b) is on the line  So over here is a little detective work  I know that (0 b) is on the line and so  if I plug in zero for X and b for Y  I have to make truth  So b-1=1(0-2)  Now it's just algebra  arithmetic really  b-1=-2 or b=-1  That passes the smell test  right? That looks like that it could easily be drawn to scale but that could easily be minus one  at least it's not positive or anything  So in fact  let's erase this  That has coordinates (0-1)  Great! So what? Well  let me now rewrite the slope intercept form with the point for a formula line but using (0-1)  So if I write  and I know that (0-1) is on the line  and I know the slope M=1  the point slope formula tells me that y-(-1)=1(x-0) is another perfectly good equation for that line  This is y+1=1x or y=1x-1  This is the slope intercept formula for a line  Let's say that in general forms  If a line L has slope M and L hits the y-axis at the point (0 b)  then Y=Mx+b is an equation for the line  M is the slope  B is the Y intercept or the Y value of the line intercept  That's often a nice  pleasing way of describing Y because it can pretty much allow you to draw a line just by seeing it  So for example if I give you the equation Y=2x+1  let me draw that here  I can immediately draw for you that line or at least sketch it  I know the Y intercept is one  right about there  I know the slope is two which means steeper than 45-degree angle  so about like that  Cool  now suppose someone tells me that they want me to draw a line with the same Y intercept  let's draw it in blue  the same Y intercept but sloped still positive but less positive  Then that could be  say for example  this one  Suppose someone tells me that in red  they want me to draw a line with the same slope as L  the slope of two  but negative Y intercept  let's say down here  Now I'm gonna try my best to make it parallel to Y  the line  there you go  That's the point  The slope tells you in this formula Y=Mx+b  the slope tells you how to angle the line  the Y intercept tells you where to anchor it on the Y-axis  So that's somehow much more pleasing than the point slope formula  Okay  let's finish with this one example  Let's give you the following problem  Line_L has points (1 1) and (3 0) on it  Find an equation for L  Okay  fine  First step is let's draw point to C  So here is (1 1)  here is about (3 0)  and there is the line between them  All right  now let's find the equation for the line  We can do it point slope formula  we can do point intercept  whatever we want  First let's figure out the slope  So the slope of L is  of course  the slope of this line segment between (1 1) and (3 0)  So M is equal to zero minus one divided by three minus one is negative 1/2  So this line has slope -1/2  And now to the point slope formula for the line  So let's take (1 1) as our point  So y-1=-1/2(x-1)  That's one equation for this line  In a sense we're done  We would then try to find the Y intercept  we could do any number or other things  Here's a fun idea by the way  this probably occurred to a lot of you  more skeptical-minded of you  I used the point (1 1) but no one told me I had to  I could've also use the point (3 0) and drawn the point slope formula that way  So another equation for the same line is y-0=-1/2(x-3)  This seems like a contradiction  all right  cause these are two very different looking equations for the same line  Turns out they're the same  You can do a little bit of algebraic manipulation to turn this top equation into this bottom equation  I'm going to challenge you in your own time to do so and to check that's true  Okay  That concludes our video "
EO4B_ykP8f8,Functions - Mapping from Sets to Sets  Okay  welcome back  everyone  We're going to start with the first video in our module on functions  And this is going to be a little idiosyncratic  We're not going to talk about functions to start  the way most of you have probably seen it in early school  You're probably all used to seeing the idea of a function as a graph  That is if I draw this in the upper left corner here  your idea of the function is something like that  Some squiggly line  here's x  here's y and someone says that y = f(x)  We'll get there  that's actually a way to visually depict a function from the real line to the real line  But what we're going to do here  and trust me there's a good pedagogical reason for it  we're going to start a little bit more abstract  Talk about the general concept of a function from a set to a set  Just work with me  and then we'll get eventually to the idea of where these graphs come from and what they have to do with what I'm talking about here  So here's a picture I want you to have in mind  Supposed you have a set A  it's the little bubble on the left  and a set B  the little bubble on the left  The general definition is that a function f from A to B is a rule or a formula or a machine  which simply transforms every element a and A into some element f(a) and B  So  really  by machine I mean it  So here's A  imagine this little cartoon of a journey  Here's A  little a walking along  gets fed into the machine  Lord knows what happens in here  we're sort of covering it up  censoring it with these blue dots  but some crazy stuff happens  And at the end of the day  out comes an f(a)  that's an output over here  So it's very mechanistic  a is sort of an input  f(a) is an output  I think you can write that down  There's the input to the function  there's the output  And the idea is the function is the machine  it's not a graph  It's not any of the individual input output  It is the rule which transforms one to the other  Okay  great  that's enough talking  Let's get some examples  Let's take a really  really simple example of a set  Suppose our set A consists of 1  and 2  and 10  And suppose our set B consists of Apple  Daniel Egger  which we'll call DE  and Monkey  And we might define a function f from A to B  And really here you should have the absolute freedom to do whatever you want with the function  You might say that f(1) = Apple  You might say that f(2) = Apple  And you might say that f(10) = Monkey  So to give you that in pictures  here's 1  here's 2  here's 10  Here's Apple  here's Daniel Egger  here's Monkey  And f sends 1 to Apple  2 to Apple  and 10 to Monkey  That's it  that's a function  it's a machine that does this  Why it does that we don't really know  Okay  the short of this isn't just a little gross abstract concept  Let me recast one of the things we did before in functional language  Suppose for example  that x is equal to all the people in the VBS study  That's the third time we've mentioned it  It may actually be a real disease  see if we can get a patent on the treatment  All people in the VBS study  And suppose y consists of two symbols  plus for positive  minus for negative  And let's define a function called test from x to y  This function is the medical test that we take when we want to tell whether or not you have VBS  All we're going to do here is say that test for the particular person  If we write the test of a person equals plus  that means that person tested positive  If we write that test of a person equals minus  that means that person tested negative  And so it's just a way of operationalizing the idea  it's that function  Let's give another example  Suppose you're running a business  And let's say that capital Y stands for all the years  So this might be going from 2010  2011  2012  on forever  and over here on the other side we have the real number line  Define a function called Profit  From Y to R  Where profit of a particular year is equal to the profit in that year  profit / loss in that year  You can see here why the target of the function  the real line  is the real line  because you might want positive or negative  It might be that profit In 2011 was $1 007 that might be the profit in 2012 was -10 000  This might be a typical thesis on running  We're not doing that well for example  So that's the basic concept  There's really not much more to it that's a function from a set to a set  Okay  so let's get a little bit more advanced here and tie into something that you see a lot in machine learning  The dirty truth is that you don't actually have functions in life  usually  You don't know what every input to output is  Very often what you do in what's called supervised learning is figure out functions from a little bit of examples of input and output  So  for example  the profit function  From years to the real line  you actually knew what that was  You knew what every output would be for every input  If you know what profit of the year was for every year  you'd be in business  as they say  If you knew what the result of test was on every person  you wouldn't have to give them the test  Very often what you do in Supervised Learning  is you're often given some examples  So for example if you're trying to figure out you have a set A and a set B and you have a mystery function f and you're trying to figure out  You're often given some examples of a and A and outputs f(a) and B  So you might be given some people in their test results  you might be given some years of the profit in your mission  Figure out the function  to figure out a trend  a pattern  some call trend analysis  pattern analysis  Not getting into the details  it's a really really big rabbit hole  but the whole field of supervised learning is literally about that  You're given examples of input output pairs  try to figure out the function 
4kPPSWdKTBw,Functions - Graphing in the Cartesian Plane  Okay  welcome back everyone  This video is called Graphs of Functions  And what we're gonna do in this video is bring you from the more abstract land we had in the last video  where I convinced you that a function was something called F from A to B  which was a rule  which took something in an arbitrary set A  and transformed it via a crazy machine into something in an arbitrary set B  That was on our picture of a function  Here's A  comes through  Here's F of A  and it lands in here  On the other hand  you probably also have a picture of a function in mind which looks like what's over here on the big screen  Namely  this line here  with this blue curve here  What those actually are  are graphs of functions  Those are graphs of functions  which happen to be from the set  the real line  to the set  the real line  And the point of today's video is to convince you of that and to talk to you about the difference between a graph of a function and the function itself  as well to build some important vocabulary for later  Okay  let's jump right in  So suppose we have a function F  from the real line to the real line  So here is a copy of the real line  Here is a copy of the real line thought of as sets  Here's our little conveyor belt  Here's the machine that is the function  who knows what's going on in there? And here is the conveyor belt coming out  And in some sense  every single real number X  comes into the machine  moves in  some stuff happens to it  and it comes out as F of X  it lands over here  That's still the same picture  It's still valid  The problem is  as we saw before in the previous module  there is a ridiculously large infinite number of real numbers  And so unlike a function of a finite set to a finite set  we can't just specify what happens  we can't say that F of one equals apple  F of two equals Daniel Egger  because there are infinitely many inputs  So usually when we define a function for the real line to the real line  we have to give you a formula  So for example we might see a formula like F of X  equals 2X minus one  What this is  this is a formula  This is a rule that tells you how to operate the machine  Any input you give the machine  it tells you how to make the output  So for example if one comes in  we know that F of one  follow the rule and plug in 1 for x  2 times one minus one that turns out to be equal to  2(1) - 1 = 1  If we plug it in zero  F of zero equals two times zero  minus one is minus 1  If we plug in a real number which is not an integral like 5 1  we get 2(5 1) - 1  or 10 2 - 1  never get to do arithmetic in public  so I'm doing it like this  equals 9 2  And that's the point  that's the whole idea of the function  And you can have more complicated formulas  So for example we could have G of X equals the absolute value of x which we saw before  right? That's a form which was often given by cases  This is equal x if x is greater than or equal zero  And minus X if X is less than zero  In both cases  both F and G here  are functions  They take input values  X  and they spit out F of x or G of x in this case you just have a formula for how to do it  OK  so what? What's the point of a graph  Well what's the correspondence when we graph what we saw? So lets draw what you all know is the graph of the expression Y equals 2 x minus one  You saw before how to plot the equation in a line  So there's a Y intercept  minus one  Always good to review  Slope is 2  so it goes about like that  So  this symbol here  this is actually not the function  this is the graph of the function  This is the graph of the function F from R to R which formula is F of x equals 2 X minus one  Now what's the point about that distinction  But  this is a nice visual picture  This allows you to depict all input and output pairs at once  For example if x equals zero  I look at zero and then I sort of think about where zero goes until I meet the graph and that point there  that's the point (0  f(0)  which we know is equal to (0  -1)  If I say take the point  5 1  to about there  and I look till it meets the graph  there it is  That's the point  (5 1  f(5 1))  OK  so in general  if let's say G is a function from R to R  The graph of G  an important concept here  is a set of points in the plane  So lets give it a name  lets call it graph of G  this is equal to the set of all points  x comma y  âˆˆ R two  such that y is equal to G of x  And that's a really important distinction  the visual way of drawing the graph  Lets see some examples  So  for example look draw up front  Suppose  G of x is the absolute value of x  Let's draw the graph  We already know that G of x is equal to x if x is greater than or equal to zero and minus x if x is less than zero  It turns out that this graph looks like this Well  We've seen that before  Often people will write then  y equals absolute value of x here  What that is telling you is  that every single point on this graph  the y coordinate is equal to the absolute value of the x coordinate  So in other words  if this is two  the y coordinate there is two  which is the absolute value of two  This is in fact the point  two  comma the absolute value of two  On the other hand  if I take minus two  and I look at the y coordinate  that's the absolute value of minus two  So this point here  is  minus two  comma  the absolute value of of minus two  And that's the idea of a graph  two Okay  lets do one more example  Suppose H of x is equal x squared  That's one of the things that we saw at the very beginning  So lets draw our set of axis  One of things we're going to learn here is how to graph a function if you don't know what the graph looks like  The other two are sort of cheating  There's no magic bullet here  there's no tried and true answer  Really  often what you do is test out a bunch of input and output pairs  see a pattern  and try and draw a curve through it  The astute listeners among you will realize that's exactly what you do in supervised learning  you try to figure out what the function is going to look like  like querying  by asking a few inputs and seeing what the outputs are  So lets make  for example  a table  Here is H of x and lets figure out a table  So if x equals zero and H of x would be zero squared equals zero  So lets plot the point  (zero  zero) on the graph  If x equals one then H of x is one squared equals one  Lets plot the point (1 1) on the graph  If x equals two  then two squared equals four  Lets plot the point (2  4)  three  three squared equals nine  We're going way up there  So somehow it looks like a curve going up like that  Lets try some negative numbers  Negative one  we know that negative one squared equals one  And pretty soon we're going to see a pattern of symmetry like that  And that's about what the graph of y equals x squared looks like  That's really how you graph functions  You don't know what they're supposed to look like  In a later video  you're going to learn a bunch of patterns and what later functions look like  what quadratic functions like this one look like  what cubic functions look like  what exponential functions looks like  and things like that  We are just going to close the video now by telling you something important  called the vertical line test  To illustrate what that really gives you  let me give you an example  I'm going to draw three curves on the plane and only one of them is actually the graph of a function  There is one guy  There's another one  and choose another color  lets try yellow  say take a third graph like this  Okay  so those are three purported graphs  Here's an interesting fact  red could be a graph of a function  Red could be the graph of say  y equals x minus one  Blue could be the graph of a function  even if I can't think of a formula  that also could be a graph  Here's the wonderful fact  yellow cannot be the graph of a function  There is no function whose graph is yellow  If you think a little bit about it  you'll see why  It violates something called the vertical line test  Namely  if I draw a vertical line  I can find a vertical line which hits the graph at two different points  Why is that a problem? Well if this little point here is x  in essence I'm being proposed two different things for the value of the yellow function of x  There's that one and there's that one  and that's illegal  right? Remember a function is a rule which takes one of the things in a set and assigns it to an element on the other set  There can't be any ambiguity  That's not the case with any of these other graphs  right? If I draw a vertical line here  it intersects the right guy at exactly one point  and it intersect the blue guy at exactly one point  So if this is the point x  I want to know what's the red function of value of x  there it is  What's the blue function of the value of x? There it is  Any other vertical line  same exact thing  it intersects the red line at one point  so there's x  there's the red value of x and there the blue value of x  There's that ambiguity with yellow  So lets actually write down what the vertical line test says  any vertical line  intersects the graph of a function once  If it intersects it more than once  we violate things here  Okay  that concludes this video 
SgXOXg6gYHw,Functions - Increasing and Decreasing Functions  Welcome back everyone  We're gonna continue with our module on functions  And here  we're really going to focus our attention to functions from the real line to the real line  And the idea of today's lecture is the idea of an increasing or decreasing function  These are two special classes of functions  It's important to understand that most functions don't fall onto either  but we need to understand which one's fall into which class  So if we look around the left  we have three functions  We have the graph of f_of_x  which is the red graph  We have a graph of g_of_x  which is the blue graph  And we have the graph of h_of_x  which is the yellow graph  You'll notice f_of_x seems to go up as you move along the x axis and you look at where you are on the red graph  you're always climbing  Here's the graph of g has the opposite and always falling as I move along  Whereas the graph of h has a falling period and then an increasing period  We say that f is a strictly increasing function  We say that g is a strictly decreasing function  And turns out that h is neither  As we often do as mathematicians  were not satisfied just with visual intuitive definitions  we like to be annoying and write down some symbols  So over here on the right I've done that  Let's look at what these symbols mean  and let's look at how to translate those intuitive definitions we already see  Remember  the idea is that the red should be strictly increasing  the blue should be strictly decreasing  h is neither  To the definitions  So let's let f be a function from real line to real line and f can stand in for any of these  We say that f is strictly increasing if- and this condition is tricky to parse  Whenever two inputs  a and b  have a relationship that a is less than b  it must be that the output f_of_a  f_of_b have the same relationship  Let's check that f is strictly increasing  For example  if this is the point a and this is the point b  notice that a and b are less  a is less than b  So look up here  there is f_of_a on the graph  there's f_of_b  And notice  they have the same order relationship  And I didn't just get lucky and pick those two  Those could've been anywhere along the x axis  It turns out that f  put that in red  f is strictly increasing  We say that f is strictly decreasing if whenever a is less than b  the order of relationship flips the output so f_of_a is greater than f_of_b  Turns out the blue guy  that's true  but if you look at the blue guy  here is a less than b  so if they were to hit the graph  there's g_of_a  there's g_of_b  You know they're getting closer to each other  g_of_b is actually less than g_of_a  And that's true no matter where I picked a and b  You can check for yourself that  So let's write that in here  G is strictly decreasing  On the other hand  I'll just give you the answer right away  h is neither  Can you convince yourself why that's true? If I took a and b say over here  and I looked at where a hits the graph and I looked at where b hits the graph  notice that the order of relationship flips  a is less than b  but h_of_a is greater than a_of_b  On the other hand  if I took a and b where we have them over here and I looked at where they hit the graph  h_of_a is actually less than h_of_b  So there's no consistency here  Okay  so we've seen some picture examples  Let's write down some actual functions with formulas and lets figure out whether they're strictly increasing or decreasing  And a little secret here as I'm also going to show you some functions we haven't seen before  So when we consider the function f(x) = 2 to the x  Then we considered g(x) = 3 to the minus x  and one you've seen before which is h(x) = x squared  Now let's figure out which of these are strictly increasing  which are strictly decreasing  which are neither  Over here  our axis  And let's first start by drawing the graph of f(x) = 2x to the X  You may not seen this before  this is called an exponential function  And the ideas that up here  x is in the x spot  Let's figure out a table of values and see what that might look like  If x is zero  this is x and this is f(x)  if x is zero  then we have two to the zero  which is one  and here's the 0 01  X is one  we have two to the one equals two  x is two  we have two to the two which is four  x is three  two cube = eight  Shoots all way up there  X is minus 1  2 to the minus 1 is 1 over 2 to the 1 is 1/2  and so on  So the graph actually end up looking like this sort of  and on  and on  So there's f(x) = 2 to the x  That's the graph f(x) = 2 the X  Turns out that's strictly increasing  as you could probably tell  Okay  in blue  let's figure out the graph of g(x) = 3 to the minus x  So let's make a own table of values  here's x  here's g(x)  there's 0  so g(0) would be 3 to the 0 is 1  So it's here  1 will be 3 to the minus 1  1/3 and we get the pattern  it's going to go quite steeper  and up like that and down like that  So  g(x) is strictly decreasing  Okay  Let's draw the graph of h(x) = x squared in yellow  So let's see that should go through the point 1  1 and 1 minus 1  looks about like that  Now h is neither strictly increasing nor strictly decreasing  Sometimes is going up and sometimes is coming down  Here's a statement which you should intuitively understand what this means  but h is strictly increasing on the interval from zero to infinity  In other words  if I restrict myself only the things in this interval here  and I only use those in inputs  h satisfies the definition of strictly increasing  I plug in two points in that- it goes up  and h is strictly decreasing on the interval from minus infinity to zero and you convince your self  So that's interesting  OK Let's understand what this might have to do with say  real world examples  So we've seen some examples of functions that I've just made up  here are some examples that might make more sense in the real world  a world we might have increasing functions and decreasing functions  On the left  let's imagine plotting over here years since birth of a typical child  and over here  lets plot height  We won't really commit to scale of units  will just get a sense of the general shape  So then of course nothing to the left of the y axis matters  because they aren't negative years since birth  This will almost entirely be an increasing function  Right  you'll start at year one  we'll measure you about here and you'll go up  you go up really violently and then just start to level off  Right around here that's probably about 17  you'll stay sort of stable for a long  long time  maybe a little bit of growth  and you might get down a little bit sadly if you stoop  So that's a function which is increasing for a long bit  then it's sort of flat and then goes down  As always the mathematical notions will be much more precise and what happens in real life  wouldn't be a smooth curve it would depend on the measurements of the doctor's office  Over here  suppose we have years since purchasing a car  and over here we have value of a car  Without even drawing it  you probably predict that's going to be a decreasing function  Right  here's year one  it's one of my cars  it's going to start about a thousand dollars  Perhaps that's revaluing me  and then it's going to decrease  Soon as you drive it off the lot goes down a little bit and it's going to plateau somewhere near the bottom  OK  we're going to close with just a simple visual test to tell whether or not our function is increasing or decreasing  Notice the red graph is a graph of the strictly increasing function  The blue graph is a graph of a function which is neither strictly increasing nor strictly decreasing  Really simple way to do it is called the horizontal line test  You may notice  if I draw any on a line here in green  it hits the right graph exactly once  no matter where I draw the line  Which makes sense  right  Because that's the value that's hit that one time  It could never  ever return to that value  because if it did- the graph would have had to bend back down  So in other words there's the value I hit  and I don't get to hit that value again because the graph is gone  See you later  it's all on the train  On the other hand you'll notice blue  there are many horizontal lines which hit the graph twice  This horizontal line here hit here  Now blue goes down  again we've left the line forever but it starts to come back up again and hits the graph  And so that's really the horizontal line test  The function is strictly increasing or strictly decreasing if whenever you graph it every single horizontal line has to intersect exactly once 
y3CtaMFMxjc,"Functions - Composition and Inverse  Welcome back everyone  We're going to continue with our module on functions  And here  we're gonna talk about two important things to do with functions  But first  we're gonna talk about what it means to compose two functions  Then I'll just give you a basic idea of what that means with several examples  In one of those examples we'll walk you into a warning  the warning to say what we write at the front is that you can't necessarily compose functions in any order you want  and we'll show you why that's true  Then we'll talk about a very specific type of composing functions which will be able to tell you that one function is an inverse of another  In a nutshell  two functions are inverse  if when you compose them  they undo what they did before  To give you a basic idea  we'll show you a neat geometric picture and then a warning â€“ the warning is that not every function has an inverse  and we'll see a geometric reason why  OK  fine  let's get to it  So let me give you the basic picture of what it means to compose functions  Suppose we have f(x) = x^2 and g(x) = x+5  Now I'm not gonna graph these for a second  Let's remember that functions are actually machines that map sets to sets  So here's a copy of the real line  Here's a copy of the real line  and here's the function f that goes from here to here  So remember this machine takes any point x and maps it to x^2  Here's another copy of the real line  here is an arrow  and here's g  So this machine takes anything here and maps it to that thing plus five  Okay  I'm now going to draw  connect the triangle here with a dotted line  And I'm going to do this funny symbol g circle f  We read this  g composed with f  g following f  sometimes you say  What does that mean? By definition  g composed with f  of an input thing x is equal to  first  you take x and feed it to f  you let the machine go whir  chunk  it spits out an output f(x)  Now you feed that output to g as an input  so g(f(x))  So  it looks mysterious  It's not that hard at all  Let's figure it out here  Suppose I have a typical input x  and let's compute what g of f(x) actually is in this example  So that's supposed to be g of f(x)  Okay  fine  Great  buddy  What's f(x)? f(x) is x^2  so this is g of x^2  Now here's the hard part  x^2 is now the input to the machine g  The machine takes anything and adds five to it  Don't be deceived by the fact that you have an x here â€“ that just stands for any input  The input in this case is x^2  so what do we do to x^2? We add five to it  So at the end of the day  we get x^2 + 5  For example  g of f of two will be g(f(2))  g(f(2)) is 2^2  so that's g of two squared  which is g(4)  And then I take four and I add five to it  and so I get nine  That's what it means to compose functions  Here's the warning  Suppose let's do it in the reverse order  f of g of x  By definition  that would be f(g(x))  which in this case would be f(x+5)  Okay  now I take x+5 and I stick it in here  and there  So f says take whatever input you have and  buddy  square it  So  f(x+5) is (x+5)  quantity squared  which unfortunately is not the same as x^2 + 5 as we all know  And so the punchline is that you can't necessarily compose things in whatever order you want  Often you can  but you don't get the same answer  Okay  that's what it means to compose functions  It's pretty simple  There's a special type of function that when you compose them  you undo what you did  Let me just give you an example and then tell you the general term  So suppose we start with f(x) = 2x  Suppose someone magically hands us g(x) = 1/2 x  Let's see what happens when we compose these  So note  g of f of x is equal to g(f(x))  f(x) is 2x  so now we have g(2x)  Great  so here's 2x  Let's plug 2x in for x into the g machine and see what happens  If I do that  I get 1/2 times the input  which is 2x  and now you see the punchline â€“ one half times two is one  and this is back to x  Notice that's true for every single x  In other words  g of f of three  if I follow through all this madness  I'm gonna get three  g of f of negative pi  if I follow through all this madness  I'm gonna get negative pi  In this case  we say that  f and g are inverses of each other  That is  g undoes what f does  And we write often this  g is equal to f to the -1  Notice that doesn't mean one over f  right? That's a very  very unfortunate notation  Often when we say two to the -1  we mean one half  That's not really what we mean here  We mean it's thing that undoes  the machine that undoes the original machine  If you liked your x and some idiot came along and multiplied it by two and you weren't happy with that  you didn't put a work order in for that  g is a function which undoes what that idiot does and put it right  sort of a way of getting rid of the stupid action  That's the point  Okay  cool  So  let's understand an interesting geometric relationship between inverses on the graph  which is this picture we have right here  So what I've just drawn here is the graph of y = 2x in green  And  of course  someone told you that y = 1/2x is the inverse of that  But  suppose you didn't know that  here's an interesting picture  So suppose we have this graph here  and let's think about a function as the input-output machine  So we have f(x) = 2x  We want to know how to undo that  Suppose for example we know that f(x) = 4  and what we want to know is what is this particular value of x  right? That's sort of how you would undo it  Someone took an x and turned it into four  we didn't like that  we want x back  How do we get our x back? If this is our picture of y = 2x  here's four  We can take this horizontal line at y=4 and dash it over until we hit the graph  Remember the horizontal line test  Spoiler  that's coming up in the next slide  So here's this horizontal line  it hits y = 2x  and now let's just see what that x value is  That x value is in fact two  right? If we just drop it down here  we see two  And that's true for any of these  right? If I take any dotted line here and drop it  whatever that x value is here is whatever input we needed to make that output  So in essence what you're doing to find the inverse is you're just swapping the roles of x and y  One way to really do that is literally swap the y and x axes  swap their role  And what that means to do is to reflect the entire picture in this blue dotted line  y=x  in that 45 degree line  If you do that to the green graph  you get this red graph here which is y = 1/2 x  No point really dwelling on that  That doesn't really give you an algorithm for finding inverses  but that's really the picture  that's what's happening when you're finding inverses of functions from the real line to the real line  That picture leads us to the following very interesting warning  So first let me just write this  warning  not every function f from R to R has an inverse  In fact  in some vaguely defined sense  most don't  By the way  if you'd like a non-vague definition of most  take lots and lots of probability courses â€“ that gets to be some really interesting abstract math  which we're not gonna talk about here  But let me just give you an example  Let's take for example f(x) = x^2  Let's draw the graph  There is no inverse function here  And let's think about pulling the same trick I did before  Let's stretch this all the way up  And here's a particular value here  And let's figure out which x led to that value if I plugged in f(x)  so if someone told me  for example  that f(x) = 4  Well  I could drop the horizontal line this way and say  \""Aha! That's my x \"" Unfortunately  that same horizontal line hits the graph in another point â€“ remember the horizontal line test â€“ and drop that down here and hits it somewhere else  You actually know what these values are  right? This is 2 and -2  That's the whole problem  right? If I take the square root of four  I actually have to take plus or minus  the square root of four plus or minus  that is  I have plus or minus two  Those are two values which give when I square them  give me four  That's the whole problem  Remember that functions are machines which take one input and give you one output  So I can't define a machine which takes four and sends it to 2  -2  so there's no uniquely defined inverse  The punch line here  so the warning  say it negatively  is if the graph of f fails the horizontal line test  which we remember from the previous video  then f has no inverse  And if we remember what the horizontal line test tells us  it means that the only invertible functions are ones that are either strictly increasing or strictly decreasing "
5wRPin4oxCo,"Tangent Lines - Slope of a Graph at a Point  Okay  Welcome back everyone  This is the beginning of a three-video series on tangent lines  And honestly  the entire point of the first video is that I want you to understand everything that's written on this screen  I'm gonna go through what's on this screen and take a few more screens to really describe all the different concepts  The key idea here is  for the moment  let's imagine we just have the graph of a function  y = f(x)  and we're watching this graph here in green  Now suppose we take a particular point x = a  So right here is x = a  And we want to ask the question  how fast is f(x) changing at x = a? I want you to appreciate that's a really actually tricky question that we take for granted all the time  It's like saying right here on this point in the road  I'm moving 55 miles per hour  That doesn't mean in the next hour I will have moved 55 miles or it will take me one hour to move the next 55 miles  It says right now I'm moving 55 mph  It's a really tricky idea  Though we often use calculus to describe as instantaneous rate of change  The cool geometric picture is if you look at this red line I've drawn right here  that's what's called the tangent line to the graph of the function at x = a  That's a key geometric concept  If you think back  way back  several videos ago  the only thing we know how to take slopes of are lines  we understand how to take the slope of a line  This is gonna be a line  and its slope is actually gonna be that instantaneous rate of change  That's what we've written here  This is the tangent line to the graph of the function at the point  and its slope gives us the rate of the change there  If you take nothing else out of the video  that's a key idea  Even though the graph itself is not a line  it's a curve â€“ at each point  I can draw a line that's tangent and its slope is what we call that instantaneous rate of change  That's also called the derivative of the function at that point  and that's this little symbol here  f'(a)  Now how do you actually compute f'(a)? Understand that's really tricky because if you want to compute a slope of a line  we need two points on the line â€“ we really only have this one point  this red point here  That's this tricky formula here  Don't be scared of the limit â€“ we're gonna unpack that as we go through the screens  but those are the key ideas  To work up that though  let me start with a much  much simpler example  So suppose right here  we have the graph y = 3x  And for the moment  let's take a really fanciful example  Any business majors in here  please don't be offended  I know this is simple  I'm gonna pretend this model's the revenue versus the price of an item  So there's a particular item I'm gonna sell  I want to sell it for x dollars â€“ that's the price â€“ and then I want to see how much revenue comes in  Okay  so y is it unrealistic  you'll notice it's just the graph of y = 3x  What this is saying is that as I increase the price  the revenue goes up forevermore  So obviously  what I should do is set the price to be one million bajillion trillion dollars  Obviously  that's why it's unrealistic  But for the moment  let's suppose that right now my business has set the price of the item as a dollars â€“ it's gonna cost you a dollars for the item  And I want to ask the question  if I increase the price  will the revenue go up? Obviously  it will  but I want to know how much  What's the rate of change at this point? It's a really simple question with a line  Let's see why that's true  Let's go up here to the point on the graph  So that's the point (a  3a)  Now suppose I increase the price of my unit by $1  so here's a+1  Hit the graph  What are the coordinates of that point? That was our a+1 and 3(a+1)  Okay  Now let's figure out the slope of this line between the two  The slope of that line segment  I think about calculating that  is going to be the rise â€“ the difference in the y values  so 3(a+1) - 3a â€“ divided by the run â€“ the difference in the x values  so a+1 - a  And if you work out all that algebra â€“ that's not really the point of this video â€“ you're gonna get three  Okay  The key idea here is that there's nothing special about increasing a $1  If I increase $2  that rise-over-run calculation will be exactly the same â€“ that's what's special about a line  The slope of the line doesn't really matter which line segment you take  So in other words  if I increase $1  the revenue will go up three times $1  If I increase $2  the revenue will go up three times $2  That's really what the slope of the line is saying  If you remember back to the slope of line videos  that was really the key point of what the slope of the line means  Okay  That's great  Let's return to a much more realistic model  So if you look at the same picture  now notice it's the same picture as we had in the very first except I've added y = f(x) and I said that I'm graphing price versus revenue  So this green curve  I pointed to you that's a more realistic price versus revenue curve  What this is saying is normally when I raise the price  my revenue goes up because I get more dollars for each unit I sell  However  eventually  I keep raising the price and people get mad at me and say  \""I'm not really gonna buy your product  It's not worth that much money  buddy \"" And so the revenue starts to go now  Okay  So that's essentially what this graph here means  Now let's ask the question again  If I'm at a dollars and I want to raise my price  does my revenue go up? And if so  by how much? Does it go down? And if so  by how much? In other words  what's the instantaneous rate of change of the revenue at this price point? A very key idea here is the following  What makes this different from a line? So the answer to that question depends on my price point  So for example  suppose my price point was right here â€“ at B â€“ and let's look right up there  Without even drawing the tangent line  you can probably guess that if I increase the price a little bit from B  my revenue would go up but by not nearly as much as it would if I increase it from a  And it gets even worse over here at c  If that's my price point and I increase the price  my revenue is actually gonna go down  Right? So it's a bad idea that I put it there  Business majors among you might say it's a bad idea to have that price at c in the first place â€“ that's another matter  But the key takeaway  the key mathematical takeaway here is that the slope of the tangent line changes depending on where you are on the curve  Notice that's not the case when my green curve was a line  but it is the case here  Okay  So  the answer to the question  'By how much will the revenue increase if I increase my price a little bit from the price point a?' The answer to that is the slope of this red line  That's what we call the derivative of the function at the price value x=a  So the thing we're after is we're after something called f'(a) â€“that's the derivative of the function f at the point x=a  Great  so it has a name  but now we need to calculate it  Okay  So I imagine you're watching the video on slopes of lines  A fair question to ask you would be â€“ and if you did some of the quiz questions  you'll know we asked you this question â€“ given two points  calculate the slope of the line segment between them  That's a fair question  I think you're all pretty good at doing that by now  Here's an unfair question  calculate the slope of that red line  and all I'm gonna do is give you the coordinates of that red dot  So I'm gonna tell you what a is  I'm gonna tell you what f(a) is  go nuts and give me the slope  That's not fair  You have a right to be angry at me for asking that question  So let me ask you another question which you can do  although the answer won't give you the answer you want  What I'm gonna do is I'm gonna take another point of the line and I'm gonna draw a little line segment between them  So  let's think about what all those points are  Let's say here is the point a+h  h is sort of a notional symbol but it stands for a little bit  So in other words  right here  this is h  Let's think about what the coordinates of that point are  First of all  the coordinates of this point right here  we know what those are â€“ those are (a  f(a))  And we know what the coordinates of this point are â€“ they're (a+h  f(a+h))  Okay  fine  So  I know how to calculate the slope of that line segment  What is it? Well  what it is is the difference in the y values â€“ what we call the rise  which is f(a+h) - f(a) â€“ over the difference in the x values â€“ which is the run  which is a+h - a  but looking at this picture  that difference is just h  so let's put it there  Alright  So  let's pause here for a second  I don't know the slope of the tangent line but I would like to know it  I do know the slope of this little line segment assuming you give me values for this  but if you give me values for the function  I can give you values here  so I know how to make that a number  Here's where calculus comes in  I can write an =  I can write a limit sign  and I can say h goes to zero  Okay  Believe it or not  in the next video  we'll calculate explicitly what this means  But for now  the really conceptual point is it says actually I don't really want this point to be here  I don't want that point to exist at all  I want h to be zero because if I move h towards zero  this little line segment snaps toward pointing in the same direction as the tangent line  So as I move h towards zero  the slope of that little line segment â€“ imagine it moving along  snap  snap  snap â€“ snaps to the slope I care about  That's why I put a limit as h goes to zero here  It's not h equal zero  but h gets closer and closer to zero  Okay  That's the point of this lesson  In the next video  we'll calculate an explicit example with an actual function  and we'll show you how this formula lets you describe that slope  Thanks for listening "
7gcnji89Hkm,Tangent Lines - The Derivative Function  Welcome back everyone  this is going to be the second video in our lesson on tangent lines  You remember in the last video  we closed by unpacking this rather opaque formula  F prime of a is equal to to the limit  if h goes towards 0  of f(a) + h- f(a) over h  What we're going to do in this video is really dive into that formula in a very explicit concrete example  So the example the only example you're going to see this entire video if we're going to look at the graph of the function f(x) = x squared  which is what I've drawn here  And we're going to focus on three notional points on the graph  I'm not going give them names but that's where I label them on the x and y axes  Whoops  forgot to label my axes  Here's the x-axis  here's the y-axis  And this is the graph of y = x squared  You'll recall this expression f'(a) is equal to the limit  blah  blah  blah  is a number  and what is that number? It's the slope of the tangent line to the graph of the function y = x at the input value x = a  In other words  the slope of that little nugget of a line there  I only drew part of the line because I don't want to make the figure too cluttered  But if I extended that line in all directions  that would be the tangent line to that graph  We're going to calculate that  Before we do though  let's remind us  take ourselves and look at it and say should that be positive or negative? That should be positive  the slope of that little line segment is positive  Therefore  whatever f prime of a is  it better be positive  not negative  That's always a good thing to do before calculating is kind of reality check your answers  because everyone make errors but you may as well want to catch them  Okay  suppose instead of a we started with x = b  So we ask the question what's f prime of b? What's the slope of this little red tangent line? Not only should it be positive  but it should be greater than f prime of a  In other words  whatever we calculate  the f prime of b  whatever it turns out to be  better be bigger than f prime of a  which itself better be bigger than 0  On the other hand  suppose we calculated f prime of c  The slope of this little nugget of a tangent line  That's negative  right? because it's going down  So it better be that 0 is greater than f prime of c  whatever it is  Okay  fine  So now what we're going to do is we're literally going to go in and work out this expression explicitly  Now  word of warning  a lot of what you learn in calculus  the sort of dirty  awful stuff that we're not going to teach here  is lots and lots of tricks for evaluating derivatives you have to get around this limit  But it's actually really instructive  And a simple example to work explicitly with this definition  So we're going to show you how to get the formula for the derivative from this definition  and let's just compute  So f prime of a = limit of h goes to 0  and f(a) + h minus f(a) over h  That's always true  But in this case we have a formula for f(ax)  Okay  so this is equal to the limit as h goes to 0  divided by h  Okay  what's f(a)? If f(x) equals x squared  then f is just a machine which turns input into input squared  So f(a) is a squared  So I'm going to subtract an a squared  On the other hand  suppose they feed in the input a + h to the machine  What does the machine do with that input? It squares that entire input and outputs a + h quantity squared  Okay  so we have the limit as h goes to 0 of (a+h) squared- a squared/h  For the moment  don't be scared of this limit  Let's let it go along for the ride  This is equal to the limit if h goes to 0  Expand that out  I get a squared + 2ah + h squared- a squared over h  Great  these a squareds cancel  I can factor out an h is equal to the limit of h goes to 0 of h times 2a + h divided by h  Just algebra  nothing to be scared about  simple shuffling around  Doesn't really matter  Notice I have an h on the top and the bottom  We know what to do with that in algebra class  It doesn't really matter what the h is  I can cancel it  So it's then equal to the limit  As h goes to 0 of 2a + h  Okay  so far we've avoided talking about what limits are  Here we can't get away without even think about it  This is equal to  if h is tiny  this is 2a plus a tiny number  the limit as h goes to 0  I'll make the arrow is a little bit better  As h goes towards 0  says if h gets smaller and smaller  what is this approaching? Well we can essentially set h equal to 0 here because it's going away  because it's equal to 2a  In other words my conclusion is that no matter what a is  derivative of the function f at the point x = a is equal to 2a  That's really interesting  let's see if that makes sense  So what what that says is okay  first of all a is positive  if a is positive and so this a 2 times a is positive  that's great since  On the other hand using the same exact things here  a wasn't really special  I can also get in this particular case f prime of b is going to be 2b  All right  sort of  monkey see monkey do  plug in the a for the b  and so on  And notice  if a is less than b  then 2a is less than 2b  so we make this happy here  As you move from a to b  and you think about how that tangent line is moving  it's getting more and more positive as you're going  which makes sense  On the other hand f prime of c is 2c  okay  and look c is negative  so 2c is negative  that makes a lot of sense  Now suppose we make c more and more negative? So we move the x value along to the left  notice the slope is getting more and more negative steeper down  down  down which makes senses as well  Okay  hopefully  I've convinced you that makes sense  Let's wrap all that knowledge together  Let's integrate it into the next screen  Okay  what have I drawn here? I have again  in green  the graph of f(x) = x squared  Now instead of telling you a fixed reference point  I've drawn this blue line here  And I'm claiming so the equation of that line is y = 2x  but I've written this expression f prime (x) = 2x  So what does that mean? Normally we think about a derivative as a number  it's the slope at this value or at this value or at this value  But this guy here is derivative function  In other words if f(x)=x squared is a function that takes in as input x and returns as output x squared  f prime (x) is a derivative input output machine  It takes in as in output x and returns as output the slope of the tangent line to the graph of f(x) = x squared  at the input value x  So let's think about why that's true  If we look at f prime of x = 2x  what's true about that line? When x is positive  2x is positive  as x get more and more positive  2x gets more and more positive  And that's right  right? So in other words  if I take this value here  where that hits the blue line  that number is suppose to tell me the slope of the tangent line right there  If I take this value  where that hits the blue line  that number  it's supposed to tell me the slope of the tangent line  right there  and that's good  That number is higher than that number  and that slope is more positively steeped than that slope  Cool  on the other hand  when x is negative  the blue line gives you a negative value  As you make x more and more negative  it gives you more  and more a negative value  which is great if x is right here  The y value on that blue line there should be the slope of the tangent line to this curve  which makes sense  Look  it's pretty negative  it's pointing steeply down  Notice  by the way  if we look at this and take this literally  f prime of 0 = 2 times 0  which is 0  What that is saying is that when x = 0  the slope with the tangent line to this curve is 0  That is just a horizontal line which is true  And in fact  if we follow along this tangent line  f prime of x = 2x  it starts off being really negative  which means the graph is pointing way down  And as I move toward the origin  it's getting less and less negative  so the slope is kind of bottoming out and hollowing out and flattening out  By the time we get to the origin  it's horizontal  and now I move away from the origin toward the right  and the slope is getting more and more positive as I go  which is exactly right 
hidnf6Kj2By,Using Integer Exponents  Hello  I'm Daniel Egger  Welcome to the Data Science Math Skills Section on Exponents and Logarithms  We'll be beginning now with the basic integer exponents  And we'll start with positive integer exponents  Positive Integer Exponents are numbers used for counting  like 1  2  3  and so on  When we have a number like 9  which is equal to 3 times 3  Three is a factor of nine  and it repeats  it occurs twice  In the number 27  the factor 3 occurs 3 times  And of course in the number 81  the factor 3 occurs 4 times  We can use positive integer exponents  which are numbers placed in the upper right corner of a factor to count how many times the same factor repeats in a number  So 9 equals 3 times 3 equals 3 To the 2nd  where 2 is the exponent  27 equals 3 times 3 equals 3 to the 3rd  where 3 is the exponent  And 81 equals 3 times 3 times 3  equals 3 to the 4  where 4 is the exponent and so on  So we read this as 3 raised to the 4th power or simply 3 to the 4th for short  So let's look at another example  The number 248 832 is just equal to 12  times 12  times 12  times 12  times 12  And you can see the great convenience in being able to write this as 12 raised to the 5th power  where 5 is the exponent and this number is 12 to the 5th  Note that for historical reasons  numbers raise to the second power have special names as well as a regular name  So this is 4 to the 2nd but it's also 4 squared  And this is 4 to the 3rd  but it's also 4 cubed  Next we're going to talk about 0 as an exponent  These problems are easy  why do I say that? Because by the definition of exponents any number except 0 raised to the 0 with power equals 1  So for example  3 to the 0 = 1  2 to the 0 = 1  2pi to the 0 = 1  1 / x cubed to the 0 = 1 so long as x does not equal 0  and so on  Even for mathematicians zero to the zero a little too weird and so that is simply undefined  Next we'll look at negative integer exponents  2 raised to the exponent -1 is read 2 to the -1  And it is equal 1 over 2 to the 1 or 1/2  2 to the minus 2 is equal to 1 over 2 to the 2  or one quarter  And 2 to the minus 3 is equal to 1 over 2 to the 3  or one eighth  So you should see the pattern here  Raising a number to a negative exponent is the same as dividing it by the same integer if that integer were positive  Now let's consider dividing by a negative exponent  By the exact same logic  1 over 2 to the -1  simply = 2 to the 1 or 2  1/2 to the -2 simply = 2 to the 2nd or 2 squared or 4  1/2 to the -3 simply = 2 to the 3rd or 2 cubed or 8  We express the general rule for negative exponents using letters to stand for just about any number  And the general rule looks like this  x to the minus n equals 1 over x to the n and 1 over x to the minus n equals x to the n  One of the most common uses of exponents is for something that we call scientific notation  Scientific notation is a way to write numbers that otherwise would have a very large number of zeros in them  What we do is we take the part of the number that has significant digits  here 5 972 and we put one digit to the left of the decimal place  So we have 5 972 and then we multiply by 10 raise to the appropriate exponent  Which in this case is 24  And 10 to the 24 means that you are going to have  you're going to move the decimal place 24 spaces to the right  Get it? So let's try another one  If we have a number less than one  we are going to be moving to decimal place to the left  And we are going to have a negative exponent  So in this case  we have the mass of an electron  and this would be 9 109 times10 to the -31 because we're moving the decimal place 31 places to the left  Key thing to remember is that we only need to keep these significant non-zero digits and we always have one digit  to the left of the decimal place  One digit to the left  And that's all you need to know for scientific notation  And that concludes our first video on exponents 
8bnkKnmw3Q2,Simplification Rules for Algebra using Exponents  There are five simplification rules for keeping notation straight and doing algebra using exponents  If you memorize and practice these five rules  you will be able to understand and solve just about any algebra problems based on integer exponents  We'll mention all five rules briefly  Then go through a number of examples of simplifying exponential expressions  discussing for each which of the five rules to apply  Note that we use the word power in these rules in a special sense  to mean the value of an exponent  Our first rule is the multiplication rule  When taking the product of the same factor raised to different exponents  sum the exponents  And this looks like this  x to the n  and then the same factor again raised to a different exponent  And this is = x raised to the sum (n + m)  The second rule is the power to a power rule  The idea here is that you have a number that contains an exponent  And you're raising the entire thing to an exponent  And the way that you simplify this is that you take the product of the exponents  And that becomes your power  The product to a power rule  Is relevant when we have two different factors  And they are raised to a common exponent  And what we do is distribute the exponent  So we have each element of the product raised separately to that exponent  And let me illustrate that because that might not be completely obvious  Let's say we have (2*3) to the 3rd  Well  it should be clear that that would be equal to (2*3)(2*3)(2*3)  And then we gather up the 2s  so we have 2 to the 3 times 3 to the 3  okay? Now let's consider the fraction to a power  Situation here is that we have one number on top  and another number on the bottom  and we're raising the entire fraction to an exponent  And again  we distribute the exponents  So this becomes = x raised to the n / y to the n  When raising a ratio of two integers to a power  distribute the exponent to each number  Finally we have the division and negative powers rule  and this works as follows  If we have x to the n / x to the m  this is the same thing as x raised to the (n-m)  Now you might notice that this really combines rules we already have  Because what we're really doing here is we're saying  when we have x to the n x to the -m  that this is the same as x to the (n-m)  Okay? Now we're going to work through some examples  And you can pause the video  And see if you can identify which rule to apply to simplify and solve the equation  What is (7 to the 3rd)(7 to the 7th)? Well  we apply the multiplication rule because we have a shared factor of 7  So this is 7 to the (3+7)  or 7 to the 10th  What about (4 to the 3rd) raised to the 5th? Well  here we have a power raised to a power  So we have 4 to the 3 times 5  or 4 to the 15th  What about (8*9) to the 7th? Well  here we distribute the exponent using the product to a power rule  And we have 8 to the 7th times 9 to the 7th  This is an example of when our scientific notation might be useful  Because this is 1 00306x10 to the 13th  What about (2/7) to the 3rd? This is an opportunity to apply the fraction to a power rule  We have 2 to the 3rd / 7 to the 3rd  or 0 023323615  Now what about 10 to the 5th / 10 to the 3rd? This is = 10 to the 5th times 10 to the -3  which is = 10 to the 5-3  Which is = 10 squared  which is = 100  Now let's try some slightly harder examples  The way that I would tackle a problem like this is I would isolate each separate factor  So we have x to the 3rd / x to the 3rd  we have y to the 4th / y to the 5th  and we have z to the 5th / z squared  Using our  Negative exponents rules  this is equal to x to the 3-3  y to the 4-5 z to the 5-2  Which is simply equal to x to the 0  which is equal to 1  So we have 1 times y to the -1 z cubed  Or if you prefer  z cubed over y  Let's try one more  Here we'll start by isolating each factor  and then we'll do the -1 at the very end  So we have the product to a power rule  So we have x squared y squared on the top  And x to the -3 y squared on the bottom  So this is going to be = x to the 2--3 times y to the 2-2  so this is = x to the 5th  However  We have multiplication by -1  so this is = x to the -5  or 1 / x to the 5th  And now you should try some on your own for on the practice quiz  We want to touch briefly on one more topic  Which is how you handle an exponent that is  itself  a fraction  The answer is that you treat it as two separate operations  Where the upper number is a standard exponent and the lower number is a root  So in the example we have here  we have the 8 raised to the 2/3rds power  What that means is 8 squared  cube root of that  Or it's equally accurate to say the cubed root of 8 squared  The order does not matter  So let's see  what would that be? Well  the cubed route of 8 = 2 because 2*2*2  or 2 to the 3rd = 8  So we would have the cubed route of 8  which is 2  squared  which would be equal to 4  Would be equally accurate to say that we have 8 squared  which is = 64  And we take the cubed root of that  and that would be =  Cubed root of 4*4*4  which would be = 4  Let's consider another example  125 to the 4/3  We would take the cubed root of 125  which is = 5  And we would raise that to the 4th power  So that's 5*5*5*5  which is = 625  So as long as you treat each part of a rational  or fractioned  exponent as a separate operation  you should be able to do these problems without too much trouble  So that concludes our exponent rules  And what I suggest is that you simply practice them a little bit  And they will become second nature to you  And they will not seem difficult if you just work some problems and practice 
Hvubf8g4jhb,How Logarithms and Exponents are Related  Once you have a basic understanding of exponents  it should not be too much of a leap to understand logarithms  The reason why is that these two concepts are very closely related  A logarithm is really the answer to the question raised to what power? So for example  if you have 2 times 2 times 2 or 8  and you want to know what power is being use  well  it's 2 to the 3rd  In just the same way  the logarithm to the base 2 of 8 is 3  The logarithm to the base 2 of 16 would be 4  The logarithm to the base 2 of 32 would be 5 and so on  So  we'll be talking about logarithms the next couple videos  And I hope that after you've worked some examples  you'll see that just like exponents they follow some simple basic rules and they're really not as intimidating as they might have seemed  So  a formula for logarithms can have two different forms  It can have the exponential form where we have a number raised to the power x and this equals something  So  to use the example that I just used  you can have b=2  x= 3 and 2 to the 3=8  which is N  Or  we can express the same idea in logarithmic form  And here  we write log to the base b  We use the letter b  because we are referring to the base of a logarithm of the number N  So log to the base 2(8)=X=3  and that's the logarithmic form of the same idea  namely that 2 to the 3=8  So 2 to the 4=16  log2(16)=4  What is b? b is the base  So that's 2  What is N? N is 16  And what is the exponent X? That would be 4  Okay  so we have the exponential form and the logarithmic form  and they capture the same idea which is the relationship between three numbers  a base  an exponent and the result  So  just as we have the simple rule when raising any number to the power 0  any number to the power 0  you may remember is equal to 1  Similarly  the log to any base  this number here is the base  of 1 is equal to 0  Right  why? Because 2 to the 0=1  10 to the 0=1  20 to the 0=1  when expressed in exponential form  Just as with exponents  we have a few basic rules that we will use to solve and simplify problems that contain logarithms  Our three primary rules are the product rule  the quotient rule and the power and root rule  The product rule looks like this  Log of a product (XY)  Now when I write log without a base  what this notation means is that this is true for any base  So  this is a general formula and I don't need to specify the base as long as I keep the base the same within any given problem  So any log(XY) = log X + log Y  So  let's do a quick example  We have logb(35)  This is equal to log b(7) + log b(5)  because 7 times 5 equals 35  Very similarly  the quotient rule says that if I have the log of a fraction or a quotient  X over Y  this is equal to log X- log Y  So if I had log of 64 divided by 2  this would be the same as log of 64 minus log of 2  Power and Root Rule is the rule that we apply when we have a number raised to a power  and we want to take its logarithm  This is equal to nlogx  So there are many situations when this makes it quite easy to solve an otherwise hard problems  So for example  we might have the log of the square root of some number and this is simply going to be one-half the log of that number  This rules works when N is positive or negative  an integer or a fraction  So we know that 35=7 times five  So this will be equal to logb(7) + logb(5)  35 is also equal to 70/2  So this would also be equal to logb(70)- logb(2)  Log2(16/4)  Well  that should be obvious that  that would be equal to log2(16)- log2(4)  Log2(16) is 2 raised to what power equals 16? So  that would be 2 times 2 times 2 times 2  or 2 to the 4  Log2(4) would be 2 raised to what power equals 4? So that would be 2 times 2  or 2 to the 2  okay? So log2(16/4) is going to be equal to 4 minus 2 or 2  Okay  let's look at a few more examples  Log2(1 000)  cube root of 1 000  would simply be one-third log2(1 000) Log10(7) to the fifth would simply be 5 times log10(7)  Log of anything of x to the -1  would be- logb of x  So  those are our basic rules  And if you apply them  you can simplify just about any logarithm  So let's try combining several rules together at the same time  okay? We're going to use our product and root rule  but first we're going to use our product rule  So we know that this is equal to logb of x squared + logb of y to the -3  which of course is equal to 2logb of x- 3logb of y  okay? Now  we apply our quotient rule  So  logb of x squared divided by x to the minus one-half is going to be equal to logb of x squared minus logb of x to the minus one-half  So  we have 2logb of x  And now we have a minus  a minus which is a plus  + one-half logb of y  okay?  Here's a handy trick just in general  When we have log a + log b = log of a times b  It's also going to b equal to log a- log of 1 over b  So we simply put in a minus 1 and a minus 1  And there are problems where that is very useful  One of the sophisticated ways that we can use logarithms is to treat both sides of a logarithmic equation as if we had an exponent  So we have an equation  either a logarithmic or an exponential equation of the form x=y  and we treat both sides of that equation as if they were exponents of the same number  Let me show you what I mean  Okay  we have log2(39x/x-5)=4  So  what we're going to do is we're going to just take 2 to the log2 of all of this  = 2 to the 4  So  2 to the log2 of anything simply equals that thing  So we have 39x/x-5=16  Then we can multiply both sides of this equation by x-5  and we can simplify  So we have 39x=16x  80  Now  we subtract 16x from both sides  So  we have 23x = -80  We have x = -80/23 
mhuGT6gjE8h,The Change of Base Formula  Next  we'll consider how to change the base for a formula  Generally in data science we use base ten and base two  Occasionally  we use something called NL or natural log  which is log to the base e which we will discuss later  But in any case  regardless of base  it's possible to convert an equation from one base to another base using a simple formula  If we want to calculate log to the base 2 of 12  we can use our pocket calculator or our Excel spreadsheet  or our computer  and we're going to get 3 585  Log to the base 10 of 12 is 1 079  so this is a very important point to keep in mind which is that you do not get the same answer if you use a different base  You'll notice that the base to the log 10 is quite a bit smaller  it's roughly one-third as much as log to the base 2  Similarly  if we have log to the base 2 of 7 that gives us 2 807 log to the base 10 of 7 is 0 8451  What's going on here? Well we have 2 to the 3 585 = 12 and we have 10 to the 1 079 = 12  And similarly we have 2 to the 2 807 = 7 and 10 to the 0 8451 = 7  If our old base is x and our new base is a  we take our original formula which is log to the base x of b  We divide by the log to the old base of the new base  That mean work an example and show you just what I mean So we have log to the base 10 of 12  We want to convert this to base 2 so we are going to divide by log to the base 10 of 2  Log to the base 10 of 2  Log to the base 10 of 2 = 0 30103  So we have 1 079 divided by 0 30103 and that equals 3 585 that is log to the base 2 of 12  Similarly  if I want to convert log 2 of 7 into log 10 of 7  I'm going to divide by log to the base 2 of 10  Log to the base 2 of 10 is 3 3219  so 2 8073 divided by 3 3219 is equal to 0 8540  To convert one base to another  divide the original base by the log to the original base over the new base to get the result in the new base  We have practice problems and the way to get familiar with this is just to work some examples and it becomes more intuitive with time 
NIbh7bk9k0n,The Rate of Growth of Continuous Processes  You will sometimes hear people refer to an exponential rate of growth  and there are two different ways you can have an exponential rate of growth  You can have a discrete exponential rate of growth  and you can have a continuous exponential rate of growth  A discreet rate is very straightforward  Let's say that I have $1 00 and it's growing at a certain rate  r  and I have a certain number of time intervals  t  So the rate represents how much my money would grow in discreet intervals of time  t  So  if I had a rate of 100% per year  that would be r and t = 1  Then at the end of one year  I would have $2 00  and that would be a 100% discreet exponential rate of growth  At the end of two years  I would have $4 00  At the end of three years  I would have $8 00 and so on  But what we're especially interested in here is something called continual exponential growth and a very special constant known as Euler's constant  or e  I'm going to show you right now how we can develop an intuition for what this special number e is  Let's suppose that we had our 100% interest rate  And a clever man said  hey  if you're willing to pay me 100% interest for a year  wouldn't you be willing to pay me 50% interest for six months? And the bank would probably say  well  yeah  that seems fair  And they say  okay  well great  So I would like 50% interest for six months and then I would like interest on my interest for another six months  So  if I have a factor of 1 + 1 and I repeat it one time  I will have 2  But let's say I have a factor of (1 5) and I repeat it 2 times  then I'm going to have a result of 2 25  And the same clever man might say  well gee  if you agreed to pay me interest twice a year  wouldn't you agree to pay me 4 times a year including interest on my interest? So what we're doing here is we're saying 1 25 raised to the 4th power or 1 5 raised to the 2nd power  where this value is our rate per unit time and these are our number of time intervals  So you'll notice  this is decreasing and this is increasing  all right? And if we were paid every 3 months  we'd received $2 44 for every dollar  So an obvious question to ask is does this number keep getting bigger forever and ever? As I make this time intervals smaller  does my potential wealth just increase and increase in an unlimited way? And the answer is surprisingly perhaps is no  It does increase  but eventually it levels off  So let's just take a look at what that would look like  If I'm getting paid interest every month  my factor would be 1 08  It would repeat 12 times  so I would have 1 08 to the 12th and the result would be 2 613  If I'm getting paid interest every week  that would be a factor of 1 019 times 52  or 1 019 raised to the 52nd power  which would be  2 693  If I was paid interest everyday  this would be 1 002739 times 365  And that would be equal to 2 7146  You notice  the numbers are not increasing as rapidly and there are 8 760 hours in a year  So  if I raised  I would be receiving $2 71813  And at the minutes of which there are  let's see  let's say 525 600  I would receive 2 71828  And then of the seconds of which there are 31 536 000 in a year  then I would receive 2 71828  And although this number continues on  dot  dot  dot  to five significant digits  it has stabilized as I get to a minute or a second  This number is known as Euler's constant or e  2 71828  So  let's consider an example of a problem  Let's consider a baby elephant that grows continuously at a known rate  So let's say I have a baby elephant  It weighs 200 kg and I know that it grows at a continuously compounded rate of 5%  So it has a growth that is continuously compounded  continuous at 5% per year  I want to know what this elephant weighs in three years  It will weigh (200 kg) e to the ( 05) times t  which is 3  It would weigh 200 times e to the 0 15  Which is = 232 4 kg  In addition to this very  very valuable concept of e  we also have the concept of log to the base e  Which is actually written ln(x)  which stands for natural log  Why is it the natural log? Because it's the log that we use to calculate naturally occurring continuous rates of growth  So let's suppose that we have some rabbits  And these rabbits with unlimited food increase in mass with their babies at the rate of 200% per year  So let's say that we start with a population of male and female rabbits that weighs 10 kg  And they have unlimited food and resources  And what we want to know is if they're increasing at a continuously compounded rate of 200% per year  How many years  Until they weigh as much as the Earth  which would be 5 972 x 10 to the 24 kg  The way that we set up this problem is as follows  We have 5 972 x 10 to the 24 kg = 10 kg times e to the 2t  where 2 is equal to 200%  that's the rate  And t is equal to the number of years required to create a 5 972 x 10 to the 24 kg worth of rabbits  okay? So first  we can divide both sides by 10  So now we have 5 972 x 10 to the 23 = e to the 2t  okay? And now we're going to use a little trick which is that we're going to take log to the base e of both sides of this equation  So we have ln(5 972 x 10 to the 23rd) = 2t  meaning that t is simply equal to the natural log of this number divided by 2  And you can use your pocket calculator  or Excel  or your computer to calculate the natural log of this number and divide by 2  And what you will find is it will take 27 37 years for the rabbits to weigh as much as the Earth  And that is all you need to know about continuously compounded returns and continuous growth e and the natural log  Thank you 
cnjU2Bijm1d,Probability Definitions and Notation  Hello  I'm Daniel Egger  Welcome to the probability portion of data science math skills  I'll start by providing a definition of probability and a probability distribution  Probability is the degree of belief in the truth or falsity of a statement  So  whenever there is uncertainty  there's a value assigned to a statement that is greater than 0 and less than 1  So we can define this as the range of uncertainty  When I am certain that a statement is true then that statement is assigned probability 1 and if I'm certain the statement is false  then it's assigned probability is 0  So let's say I'm sitting in my office that has no window to the outside world  and I don't know whether it's raining or not  I'm going to assign some probability within this range and then when I learn the true state of the weather outside  then I'm going to either have certainty that this statement is true  So this is true with certainty  Or I am going to have certainty that the statement is false  Okay  The notation that we use is  we write P of x meaning the probability of the statement x or the probability of the outcome represented by the statement x  We also use the tilde to indicate the negation of a statement and whenever we have a statement and its negation  we have a simple binary probability distribution  In other words  any time we have a statement like it is raining  and the negation to that statement  it is not raining  those statements together will form a probability distribution  One of those statements must be true when we have complete information about the situation  Note also that even before we have complete information about a situation  the probabilities for those two statements must still sum to 1  So for example  if I think there's a 3 out of 4 chance  so 75% chance that it's raining outside right now  then I must think that there's a 1 in 4  or 25% chance that it is not raining outside right now  All right  this principal is known as the law of the excluded middle  and this illustrates the basic rule of probability distributions  which is that all of the outcomes that make up a distribution must have probabilities that sum to 1  In fact  what defines a probability distribution is it is a collection of statements  Two or more  where those statements are exclusive and exhaustive  Exclusive means that given complete information  no more of one of the statements can be true  So it should be obvious that we have the statements it is raining and it is not raining  only one of those statement can be true at a time  In addition the statements that make up a probability distribution must be exhaustive  And that means that at least one of the statements must be true when we have complete information  okay? So I'm sitting inside  I don't know whether it's raining or not  I assign a degree of probability that reflects uncertainty to both the statement it is raining and the statement it is not raining  but when my information is complete  exactly one of those statements must be true  Now it's often the case that we have more than two statements that form a probability distribution  These statements are exclusive and exhaustive  And many situations  we have a large number of statements and we have no real basis to choose one outcome as more probable than another  So for example  we might have a deck of 52 cards  in that case n = 52  and I'm wondering what is the probability that I might draw an ace  And let's say  what is the probability that I would draw an ace of spades from a well-shuffled deck of 52 cards? There's nothing special about the ace of spades as far as I know  We assume the deck is well-shuffled  So under the principle of indifference  the probability that I would assign would be 1/52  It follows from the principal of indifference that we can calculate many  many probabilities as follows  The probability of a certain event  an event being a collection of individual outcomes  is defined as the number of outcomes that are in the event divided by the total number of possible outcomes in the universe  So in our deck of cards example  we might say that our event is drawing a queen and there are four queens in the deck of 52 cards  So  we have four outcomes  queen of hearts  queen of diamonds  queen of spades  queen of clubs  that are within the definition of the event  And we have a total number of outcomes that's equal to total number of cards  and so our probability of drawing a queen would be 1 in 13  Let me give you another example  What is the probability that a six-sided die will come up even? The event is even  So we look for the outcomes that meet the definition of even and they are two  four and six  There are three of those outcomes and the universe of possible outcomes contains six outcomes  and so the probability that the die comes up even is three over six or one half  And this simple concept allows us to solve a very large number of probability problems 
khjbs8H7Bqf,Joint Probabilities  Next we're going to talk about joint probabilities  Joint probabilities are the probabilities that two separate events from two separate probability distributions are both true  So  we're looking at the probability that A is true and B is true and our notation allows us to abbreviate this simply placing both capital letters together with a comma between them  This is read as the joint probability of A and B or the probability that A is true and B is true  An important point here to note is that the ordering does not matter in joint probabilities  So also  I want to point out that our standard notation for probability uses capital letters to refer to the entire probability distribution  That's the entire collection of exclusive and exhaustive statements and we use lower case letters to refer to the individual statements and we can reference an individual probability the same way  okay? So  what we've written here suggests that the joint probabilities are equal for regardless of which order we write the terms  Or which order we consider the probability of distributions and the same is true of each individual probability within the distribution  So the probability of X1 and Y1 occurring together is the same as the probability of Y1 and X1 occurring together  This may perhaps seem obvious to you but  it turns out to be a very  very useful principle as well  Now  we're going to talk about the definition of the independence of two probability distributions  If two probability distributions are independent  knowing the outcome of one does not our belief in the truth of the other  Therefore  knowing the outcome of one  does not change the probability of the other  When two probabilities are independent  you can think of them as being unrelated and unconnected  I'm tossing a coin  It has a 50 percent probability of coming up heads  And I'm rolling a die and it has a 1 in 6 probability of coming up as a 3  okay? The joint probability heads and 3 is = to the probability of heads which is 1/2 times the probability of coming up three on a six-sided die is 1/6 and that = 1/12  So  for probability problems involving independent distributions  this is the formula to calculate the probability that both events happen at the same time  We have a special name for the product of the two probabilities that is known as the product distribution  okay? So  when the joint distribution equals the product distribution  the two probability distributions are by definition independent  We often use Venn diagrams  These are diagrams that show the intersection and union of sets to illustrate the intersection of two sets of events  And  get developed some intuition about what it means to say the probability that both events occur  So for example  if we have the probability of flipping a coin and it comes up heads and the probability of tossing a die and it comes up three  Okay  we can represent this area as the product of the two  okay? You can think of this outer area as the universe of all possible outcomes  okay? And this gives us the probability that both of the events that we have defined as relevant occur at the same time  Perhaps the Venn Diagram is even more useful in capturing the concept of or probability  What is the probability that our coin comes up heads or  now interestingly  this is written with a plus sign  But in probability  what we're doing is we're looking at the probability that either one event occurs or the other occurs or both occur  okay? So  we're interested in the probability that the coin comes up heads or the die comes up 3  okay? Well  if we simply added the two circles together  we'd count the central area twice  So to avoid that  we subtract the central area from the sum of the two circles  So we are subtracting the joint probability which as we know is equal to the product probability 1/12  So  the probability that either the coin comes up heads or the dye comes up 3 would be 1/2 + 1/6- 1/12 or 6/12 + 2/12- 1/12 = 7/12 
i8dG6jF3naO,Permutations and Combinations  Now we're going to talk about how probabilities can be thought of as the probability of certain events occurring in a certain order  or the probability of a certain group of events occurring  So  for example  I might have five people working for me and I have five different assignments  And I need to place those people in those five different assignments  I can place the first person in any one of the assignments  So I have five people and I have five assignments  I can place the first person in five places  Now one square's occupied  so I can place the second person in four places  three spaces  two places  and now there's only one possible place to place the last person  okay? So  you see that there are 120 possible ways that I could put 5 people in 5 different jobs  Here the order of the assignment matters  so this is known as a permutation  okay? But let's say that I had five people and I wanted to form a team of five people who are all equal  There's no ordering  there's no specific role assigned to each person  Well  now there's really only one combination possible  I just need to assign my five people to the team  When order doesn't matter  there are always fewer possibilities  because multiple orderings of the same committee are accounted as one combination  Here we have 6 orderings of 3 distinct objects  so that would 6  1  2  3  4  5  6  permutations  But  the members of the group are constant  and so there's only one combination  Let's imagine a lottery starting with a bowl that has six different balls in it  each labeled  So they're labeled 1  2  3  4  5 and 6  And we're going to draw four balls  one at a time  to identify the winning number for the lottery  There are two ways you can think about the rules to this lottery  Either the order of the numbers matters or it does not  If the order matters  then we're talking about the number of permutations  The way four positions  one through four  can be filled by drawing from six unique objects  So  the first ball could be any of 1  2  3  4  5  6  and having drawn those 6 there are 5 left  so we have any of 5 to go in the next place  And that leaves 4 to go in the next place  and that leaves 3 that could go in the last place  So  our total number of permutations would be 6 x 5 x 4 x 3 = 360  If you need to predict the order to win the lottery  then a fair bet would pay 360 to 1  or $360 on a $1 wager  The number of permutations has a general formula that can be written n factorial / (n-m) factorial  Where the exclamation mark stands for factorial and n is the number of unique objects  so n=6  And m is the number of unique attributes  or here the orderings from first to fourth  So m = 4  n- m would be equal to 2  So we would have 6 factorial / 2 factorial  which of course is equal to 6 x 5 x 4 x 3  or 360  On the other hand  if in order to win the lottery it doesn't matter what order we select the balls  we simply have to select the four correct numbers  Now we're talking about combinations  So let's imagine that I reach in here and I draw out the 4  the 2  the 3 and the 5  okay  4  2  3  and 5  Now let's suppose that I only need to guess the correct four numbers  but not their ordering  In this case  I am going to divide 360 by 24 to get 15 possible combinations of 4 balls  And now  to win a lottery based on combinations  I would only receive $15 on a $1 wager if the lottery were fair  Because now I have a 1 in 15 chance of guessing the 4 numbers correctly  Why 15? Because there are 24 different ways that 4 balls can be ordered  So we need to divide 360 by 24  And (360 / 24) = 15  Why 24? Well I've written them out here for you  Because the first number  Second number  third number and the fourth number can be first  Then we have three numbers that could be in second place  And then for each of those combinations we have two remaining  So we have 4 x 3 x 2 or 24 possibilities  The formula that gives the general answer 15 can be expressed as n factorial / (n- m) factorial times m factorial  And this formula has a special name  it's called n choose m  And here we are showing the value n choose m for n = 6 and m = 4  it's 360  Divided by 24  which is equal to 15  Now let's consider another example  I need to send a dump truck  a bulldozer  and a steamroller to a construction project  And I have eight potential drivers  each of whom is qualified to drive all three machines  If I care about exactly which driver operates which machine  then I could have any of 8 drivers drive the first vehicle  7 drive the second vehicle  and 6 drive the third vehicle  So  I have 8 x 7 x 6 = 336 unique ways that I could send out teams to drive 3 machines  So  here  the general formula is for permutations  which is n factorial divided by n minus m factorial  n = 8 and m = 3  So we have 8 factorial /(8-3) factorial  which is 5 factorial  and that would give us 8 x 7 x 6  okay? On the other hand  if it doesn't matter which driver drives which machine  I just want to know how many distinct teams of three drivers I might send to the construction site with my vehicles  In that case I use the formula for the number of combinations  which would be n factorial divided by n minus m factorial times m factorial  Or 8 choose 3  which would be 8 factorial / 5 factorial times 3 factorial  So I'm going to divide my previous answer by 6  And now I'm going to get 56 possible teams that I could send  And that is the difference between combinations and permutations  Next  we need to consider the concept of with replacement and without replacement when we're defining the probability of a certain situation  When we draw a card from a deck of cards  say in a poker hand  that card is removed from the deck and there's no longer any possibility of getting that card again until we have a new hand and a new situation  So for example  if I draw an ace on my first card  I would have a 1 in 13 chance of doing that  And my probability of drawing an ace on the second card would be different  There are now 3 aces left in the deck out of 51 cards  And so my probability of drawing an ace on the second card would be 3/51  okay? So any type of situation where we're removing things from the realm of possibility is without replacement  okay? On the other hand  if I'm trying to predict or guess a number between 000 and 999  it's certainly allowed to use each of the digits  0 through 9  more than once  You can use each one two or three times  and so there we're generating a number and the digits are being used with replacement  So between the concept of permutation and combination and the concept of with replacement and without replacement  we have almost all probability situations that are likely to arise in a basic probability course 
cnOnj3bBq7S,"Using Factorial and â€œM choose Nâ€  Let's talk a little bit about urns  Everyone who teaches probability is always talking about urns  An urn is a container and you can't see into that container  And there are usually two different colors of marbles  although sometimes there are There are more colors  And the ideas that you're drawing marbles out of the container  There are two basic ways that we can draw marbles out of this container  We can draw them with replacement or without replacement  If we draw them with replacement  then we have a two thirds probability of drawing white marble and a one third probability of drawing a blue marble  Okay  Each time we draw and the draws are independent  so our probability of drawing a white marble two times in a row  two  two  White in a row would be (2/3)(2/3)=4/9  On the other hand  if I am drawing without replacement then on my first draw I may draw a white marble  in which case I have a two thirds probability of doing  in which case my probability of drawing a white marble on the second draw is one half  Or  I could draw a blue marble on the first draw  In which case  there is no chance of drawing two white marbles in a row  okay  So my probability of drawing two white marbles in a row without replacement would be two-thirds times one-half which would be 2/3 times 1/2 would be 2/6 or 3/9  So a minute ago I was telling you that the way that you can arrange people who in different in jobs is kind of a product where you have more slots open at the beginning and fewer at the end  We have a general term for this type of arithmetic where we take a number and we multiply it by a smaller one smaller  the integer one smaller and the integer one smaller than that until we get down to 1  It's called factorial  So this Notation five with an exclamation mark is read \""Five Factorial\""  and it is equal to 5 times 4 times 3 times 2 times 1  which is 120  Okay  when we have  One factorial divided by another  What we're actually saying is that we have this factorial on top  7 6 5 4 3 2 1 and this factorial on the bottom 5 4 3 2 1 and as you see most of these numbers cancel out  because these Products are equal to 1  and so the resulting number would be 7 times 6  One more thing to know about factorials  By convention  the notation 0 factorial is equal to 1  When we are drawing  N items from a group of m items without replacement  The number of unique groups that we can form has a special name  it's called m choose n  This type of problem comes up so much in probability that it has its own name in its own notation  so if I want to know how many unique committees of five people I could form from a group of ten people  So I have  I start with m equals 10  and n equals 5  This would be described as 10 choose 5  And it can be written using this special notation like this  And what does that equal? It is equal 10 factorial  Divided by 10 minus 5 factorial or 5 factorial times 5 factorial okay? This terminology will come up again and again and again in probability In our current example  we're interested in how many teams  Meaning unique teams where all the rules are equivalent of five people  Can be formed [SOUND] from 10 people  So  we're interested in the number of unique teams  This is referred to as 10 to 5  which is equal to 10 factorial divided by 5 factorial times 5 factorial  And I'm going to show you a little trick for when you want to get an idea of your answer when you don't have your calculator handy  Which is going to be for the 10 9 8 7 6 And then 5 times 4 times 3 times 2 times 1 is going to cancel out the five  one of the five factorials  So  that's cancelled out and the other five factorial will be 5 times 4 times 3 times 2 times 1 and we can say 10/5 is 2  9/3 is 3  8/4 is 2  6/2 is 3 and 7  So we have 2x3x2x7x3  Which is equal 252  [BLANK AUDIO]"
Bhwi5N1dnv3,The Sum Rule  Conditional Probability  and the Product Rule  Next we're going to talk about marginal probabilities and the sum rule  It often happens in probability problems that we know the joint probabilities that two things will happen together  And we don't know  we want to know the individual probability that only one of those things will happen  regardless of the other event  When we want to know what is the probability of x1 and we are given the probability of x1 and y1  x1 and y2  x1 and y3  then we can refer to the probability of x1 as the marginal probability of x1  okay? You can think of it as being out on the margins of this matrix  okay? The sum rule tells us that the marginal probability  the probability of x 1  is equal to  assuming that y is a proper probability distribution meaning its statements are exclusive and exhaustive  equal to the sum of the joint probabilities  So the probability of x1 = 1 +  1% + 10% + 4% = 15%  okay? Similarly the probability of y2  the marginal probability of y2 = x1y2 + x2y2 + x3y2  Which would be 79% or  79  okay? That we can do that  that we can add together joint probabilities to get a marginal probability is due to something called the sum rule  Here are two versions of the sum rule written out  The first is for a binary probability distribution  We have probability of B  probability of not B and we have the joint probability of A and B and we have the joint probability of A and not B  and together they sum to the probability of A  Similarly  if we a whole series of probabilities  n probabilities  we can sum the n joint probabilities to get the marginal probability of A  So it's exactly the same principle  Next we're going to talk about conditional probability  Conditional probability is defined as the probability that a statement is true given that some other statement is true with certainty  Everything to the right of the line is considered true with certainty  So what this notation means is if B is true with certainty  what is the probability of A in that case? So for example  if I throw a six-sided die and it comes up odd  what is the conditional probability that it is a 3? Well that would be three odd rolls  one  three and five  so the conditional probability would be one-third  What about if I throw a three with certainty? What is the conditional probability that my throw is odd? Well  in this case  the probability is one  It's odd with certainty if it's three with certainty  So  what we're looking at here are relationships of dependents rather than independents  The general formula that we use for calculating conditional probabilities is that we take the relevant outcomes  the ones that meet our definition of A  and we divide them by the total outcomes in our universe  However  our outcome has shrunk  it is cut down because B must be true  So the example of the die that I just gave you  we are cutting down the universe from six possibilities one  two  three  four  five  six to the odd possibilities one  three  and five  And in that case  the relevant outcome of three and the odd outcomes  one  three  five  there's one of those and three of those and so my probability is one-third  My conditional probability of throwing a three if I know that the die that I threw is odd  Now we want to relate our concept of joint probability  our concept of marginal probability  and our concept of conditional probability  And we do this using a very important rule called the product rule  The product rule tells us that the conditional probability of A given that B is true with certainty is equal to the joint probability that both A and B are true divided by the marginal probability that B is true  okay? So you notice that over here we're not assuming that B is true  The only way that we assume B is true  is when it's to the right of this special magic line  this line is what makes it true  over here it is not need to equal anything other than zero  The product rule allows us to develop a new definition of independence  You may remember that our definition of independent distributions is that the joint distribution is equal to the product distribution  okay? Well how do we get from our old definition to our new definition? The answer the is that we divide both sides by probability of B  Okay  so we have the joint probability of (A  B) divided by probability of B and we have probability of A(B) divided by probability of B assuming the probability of B does not equal 0  And now we use product rule to say that this term is equal to probability of A given B  And that is equal to probability of A  Our intuition about what this means is that knowing that B is true tells us nothing about the probabilities of A  B  the outcome B has no effect on the probability A and therefore they are independent  The converse is also true  If the conditional probability of A does not e  I'm sorry  If the conditional probability of A given B does not equal to probability of A  then they are dependent  So any two distributions are either independent or dependent  there's no middle way 
xdGaO2NwdeS,Bayesâ€™ Theorem (Part 1)  Now we come to one of the most famous and powerful theorems in probability theory  Bayes' Theorem  We start with the familiar product rule  You'll notice that if we multiply both sides of this equation by the probability of B  We'll get something that's equivalent  still  due to the product rule  that looks like this  The conditional probability of A given B  Times the marginal probability of B is equal to the joint probability A B  okay  Now  we can substitute probability of B and A for probability of A and B because we know that they are completely equivalent  And then we can apply the product rule again to probability of B and A in the form that we've written it in  its equivalent  So this would be probability of B given A times probability of A = probability of (A B)  okay? And then we are left with this equivalence  which is Bayes' Theorem  That is Bayes' Theorem  One of the most powerful uses of Bayes' theorem is for something we call inverse probability problems  Inverse probability problems are those where the answer is in the form of the probability  That a certain process with a certain probability parameter is being used to generate the observed data  And I'll give some examples of this so this will make more sense in a moment  But we sometimes use the Greek letter theta to refer to the different parameter that might be causing the data  Or generating the data that we observed  And we  sometimes also  just use Ai  Indicating one of the possible processes that are generating the observed data  They're essentially  for our purposes  equivalent  Okay? So  we're going to look at an example  Which is  let's say that we have two urns  And one has 20% white marbles and one has 10% white marbles  We observed three white marbles in a row being drawn with replacement from one or other of the urns  But we don't know which urn we are observing  What is the probability that we are observing Urn 1 and what is the probability that we are observing Urn 2? These probabilities are the probabilities of the process Urn 1 or the parameter white  probability of white = 0 2  Urn 2 is the process Urn 2 or the parameter that probability of white on each individual draw is equal to 0 1  So in a more conventional forward probability problem  We're interested in the probability of a certain observed outcome given a known process  So  for example  if we know that the urn is Urn 1  that's a known process  Then the probability that we would observe three white marbles in a row would be 0 2 times 0 2 times 0 2 or 8 one-thousandths  That would be a conventional probability problem  In this problem we are starting with the known outcome  and we're interested in how probable is it that we observed Urn 1  Or Urn 2  okay  an unknown process  Okay  written in the terminology of conditional probability  We're interested in what is the probability of the process parameter? This is urn 1  or urn 2  This is probability of white = 0 2 or probability of white = 0   And they're equivalent given the observed data of three white marbles in a row  Bayes's theorem tells us that this is equal to  if this is probability of Ai given B  Ai given B  Then that is equal to the probability of B given Ai times probability of Ai divided by probability of B  Which here we have broken up using the sum rule to be the series of joint probabilities  The probability of the observed data with parameter 1  Plus probability of observed data with parameter 2  Each times the original probability of that parameter or process  Okay  so part of Bayes' Theorem is the particular equation  what is the probability of the observation given the parameter? Right  So this is the observation and this is A1  Okay  This is A2  This portion of the solution to Bayes's theorem is known as the likelihood  Solving for it is solving a simple forward probability problem  So the probability of observing three whites in a row  if we know we're observing r in 1 is 8 in 1000  Similarly if the likelihood of each individual white marble being drawn with replacement is 10%  Then the probability of three whites in a row is 1 in 1 000  And these terms are known as likelihoods  the likelihood of the data given the parameter  Okay  so what is our solution? Well we're going to start with the principle of indifference tells us that we have no basis for choosing between the two urns  So P(A1)= 5  P(A2)=  5  In other words before we observe any data  we are neutralized to which urn we are observing  they have equal probability  Because this is before any data are observed we call this the prior probability of the parameters  or of the process  okay? So  we're going to look at P(B given A1)P(A1)/P(B)  Which is going to be equal to P(B A1) + P(B  A2) by the sum rule  Which by the product rule is going to be the equal to P(B given A1)P(A1) + P(B given A2)P(A2)  right? So all of this is equal to probability of B  okay? So you see  this is Bayes' Theorem for A1  And we can also solve for A2  But because they are only two possibilities  they are going to sum to 1  So if we solve for one we've also solved for the other  So P(B given A) as we just said a minute ago is 8/1 000  And that's times 1/2 divided by 8 times 1000 times 1/2  Plus the probability of observing three white marbles if we have a 10% chance of drawing a white marble  Which is going to be 1/1000 times 1/2  the prior probability of Urn 2  The 1/2s cancel out  And we're left with 8/9  Which is the probability that we observed Urn 1  And obviously the probability that we observed Urn 2 would be 1 minus 8/9 or 1/9  And that is the application of Bayes' Theorem using inverse probability 
6biqHnG8wjM,Bayesâ€™ Theorem (Part 2)  One of the really powerful things about Bayes' Theorem is that it allows for updating our probabilities based on new data  So let's suppose that we have a fourth marble  And it is also white  How would we update our probabilities? Well  we would set up the same type of base theorem formula that we did before  But these values  P(A1)  P(A2)  they become our New Prior probabilities  They become our New Priors for short  so we have the same base theorem formula  but instead of using one-half and one-half by the principal of indifference  we now can use eight-ninths and one-ninths  So let me show you what I mean  The probability that we have urn 1 given 3 whites and a 4th would be equal to Probability that we would observe a white marble from urn one  which would be the probability that we would observe a white from urn 1  times the prior probability that that is the urn that we're observing  divided by the total probability of the data  Which would be the probability that we are observing white  given that's urn 1 times eight-ninths plus the probability of observing white given that it's urn 2 times one-ninth  So this on the top is going to be equal to 0 2 times eight-ninths  And on the bottom we have (0 2)(8/9) + (0 1)(1/9)  But if we convert them to percentages  the probability of urn 1 is 94 12% now and probability of urn 2 is 5 88%  So this probability is down  it was previously 11 1%  and this probability is up  it was previously 88 9%  The different parts of the Bayes' equation have special names  here  I've written the theta to indicate the different parameters that were possible  In this problem urn 1 had a parameter of 0 2 probability white and urn 2 had a parameter of probability 0 1 of white  But you can use A if that seems more familiar and comfortable  The result is the posterior probability  Posterior means after  so this is after new data observed  So our best estimate of the probability of which parameter is active  The 0 2 parameter or the 0 1 parameter after new data  The prior probability is what we started with either before any data were observed or before new data were observed  Before any data we would have 0 5  Before new data  we would have eight-ninths and one-ninths  The standard forward probability part of the problem  the probability of the data given each parameter sub i is known as the likelihood   And the probability of the data on the bottom is the marginal probability  Which as we know is equal to that term on the top for the first and that term on the top for the second and so on for  however  many terms there are 
Niwn9mkTb3x,The Binomial Theorem and Bayes Theorem  We have only one more topic to complete our basic probability modules  This is something very special called the binomial theorem  It's binomial because it's used when there are two possible outcomes  A success or a non-success  So  for example  if I'm flipping a coin  and I consider heads to be a success  the number of heads that I get would be the number of successes  the number of coin flips would be the number of trials  So  I'm interested in trials and I'm interested in successes and I'm talking about a binary outcome  There are only two outcomes  like heads or tails  Okay? The individual probability of success with a fair coin could be 0 5 but using the binomial theorem  I'm not limited to fair coins or situations where the probability of success is 0 5  The probability can be any value greater than zero and less than one  So let's say I want to know what is the probability of getting a certain number of heads in a string of coin tosses  The binomial theorem will tell me the answer  And this is the binomial theorem  It's n choose s times the probability raised to the number of successes times 1 minus the probability  Note that this is also the probability of failure raised to the (n â€“ s)  and this would be the number of failures  So we have the number of ways in which you can have s successes in n trials  multiplied by the probabilities of an individual success  raised to the number of successes  multiplied by the probability of failure raised to the number of failures  Okay  so  let's try an example  Let's say that I have 72 heads out of 100 coin tosses of a fair coin  Okay  what is the probability that  that outcome will occur? In other words  with 100 coin tosses I will get exactly 72 successes or 72 heads  Well  my n is equal to 100  my s or number of successes is equal to 72  and my probability is equal to 0 5  And my formula will be 100 choose 72 times 0 5 to the 72 times 1 minus 0 5 or also 0 5 to the 100 minus 72 or 28  Okay and we'll just pause here for a minute  if you'd like to try this on your calculator or Excel  you can do so and I'll give you the answer in a moment  We have 100 Choose 72 times 0 5 to the 72 times 0 5 to the 28 and this is equal to 3 94 times 10 to the minus 6  So  we have an extremely small probability of getting exactly 72 heads in a series of 100 heads  But now let's ask a little bit more interesting problem or question  Let's say  here's our question  Either we have a fair coin  It's probability is 0 5 of heads  Or we have a bent coin  It has a probability of 0 55 of heads  Okay  We've observed the 72 heads out of 100 and we want to know what is the probability that we've observed that the fair coin and what is the probability that we've observed vertically  And this how we would set this problem up using Bayes theorem  We'd say  probability of observing the fair coin given 72 heads of 100 is equal to probability of observing 72 heads of 100 given the fair coin Times the probability that  that coin is fair and because we have no basis for knowing whether it's fair or not  we're going to start with a prior probability of one half by the principle of indifference and then we're going to divide this by the total probability of the data which is since we are limited to only the two possible coins is going to be equal to the probability of 72 heads out of 100 given the fair coin times one half plus the probability of 72 heads of 100 given the bent coin times one half  Okay  so as I mentioned before  the probability on the top is 3 94 times 10 to the minus 6  And a quick application of the binomial theorem will tell us that the probability of 72 successes in 100 trials\ngiven the bent coin is \n0 0001972  So we take this  divided by this plus this and what we're find out is that the probability that the we are looking at the fair coin is less than 2% and the probability that we are looking at the bent coin is greater than 98%  So Bayes theorem has allowed us to determine with near certainty which process with its known parameter is responsible for the data that we have observed  And this is the power of Bayes theorem combined with the binomial theorem 
hjiG6Vjw5vG,Getting a handle on vectors  [SOUND] So the first thing we need to do in this course on Linear Algebra is to get a handle on vectors  which will turn out to be really useful for us in solving those linear algebra problems  That is the ones that are linear in their coefficients such as most fitting parameters  We're going to first step back and look in some detail at the sort of things we're trying to do with data  And why those vectors you first learned about in high school were even relevant? And this will hopefully make all the work with vectors a lot more intuitive  Let's go back to that simpler problem from the last video  the histogram distribution of heights of people in the population  There aren't many people above about 2 meters  say  And there aren't really very many people below 1 5 meters  Say we wanted to try fitting that distribution with an equation describing the variation of height in the population  And let's say that equation has just two parameters  One describing the center of the distribution here  and we'll call that mu  And one describing how wide it is  which we'll call sigma  So we could fit it with some curve that had two parameters  mu and sigma  I would use an equation like this  I'd call it f(x)  some function of x where x is the height  = 1 over sigma root 2 pi times the exponential of - (x - mu) squared divided by 2 sigma squared  So this equation only has two parameters  sigma here and mu  and it looks like this  And it has an area here of 1  because there's only 100% of people in the population as a whole  Now don't worry too much about the form of the equation  This is what happens  this is called the normal or Gaussian distribution  And it's got a center of mu and a width of sigma  And it's normalized  so that it's area is 1  Now  how do we go about fitting this distribution? That is  finding mu and sigma  well  the best possible mu and sigma that fits the data as well as is possible  Imagine that we had guessed that the width was wider than it really is  but keeping the area of 1  So if we guessed that it was a fatter and probably a bit shorter distribution  something like that  say  So this one has something like that  This one has a wider sigma  but it's got the same mu  It'd be too high at the edges here  and too low in the middle  So then we could add up the differences between all of our measurements  and all of our estimates  We've got all of these places where we underestimate here  and all of these places where we overestimate here  And we could add up those differences or  in fact  the squares of them to get a measure of the goodness or badness of the fit  And we'll look in detail at how we do that once we've done all the vectors work  and once we've done actually all the calculus work  then we could plot how that goodness varied as we change the fitting parameters  sigma and mu  and we get a plot like this  So if we had a correct value  our best possible value for mu here  and our best possible value for the width  sigma here  We could then plot  for a given value of mu and sigma  what the difference was  So if we were at the right value  we'd get a value of goodness where the sum of the squares of the differences was nought  And if mu was too far over  if we had mis-estimated mu and we got the distribution shifted over  so the width was right  but we had some wrong value of mu there  that we get some value of all the sums of the squares of the differences of goodness being some value here that was higher  And it might be the same if we went over the other side and we had some value there  And if we were too wide  we'd get something there or too thin  we'd get something that was too thin like that  something like that  say  So we'd get some other value of goodness  We could imagine plotting out all of the values of where we have the same value of goodness or badness for different values of mu and sigma  And we could then do that for some other value of badness  and we might get a contour that looked like this  and another contour that looked like this  and so on and so forth  Now  say we don't want to compute the value of this goodness parameter for every possible mu and sigma  We just want to do it a few times  and then find our way to the best possible fit of all  Say we started off here with some guess that was too big a mu and too small a width  We thought people were taller than they really are  and that they were tighter packed in their heights than they really are  But what we could do is we could say well  if I do a little move in mu and sigma  then does it get better or worse? And if it gets better  well  we'll keep moving that direction  So we could imagine making a vector of a change in mu and a change in sigma  And we could have our original mu and sigma there  And we could have a new value  mu prime  sigma prime  and ask if that gives us a better answer? If it's better there or if mu prime sigma prime took us over here? If we were better or worse there  something like that  Now actually  if we could find what the steepest way down the hill was  then we could go down this set of contours  this sort of landscape here towards the minimum point  towards the point where get the best possible fit  And what we're doing here  these are vectors  these are little moves around space  They're not moves around a physical space  they're moves around a parameter space  but it's the same thing  So if we understand vectors and we understand how to get down hills  that sort of curviness of this value of goodness  that's calculus  Then once we got calculus and vectors  we'll be able to solve this sort of problem  So we can see that vectors don't have to be just geometric objects in the physical order of space  They can describe directions along any sorts of axes  So we can think of vectors as just being lists  If we thought of the space of all possible cars  for example  So here's a car  There's its back  there's its window  there's the front  something like that  There's a car  there's the window  We could write down in a vector all of the things about the car  We could write down its cost in euros  We could write down its emissions performance in grams of CO2 per 100 kilometers  We could write down its Nox performance  how much it polluted our city and killed people due to air pollution  We could write down its Euro NCAP star rating  how good it was in a crash  We could write down its top speed  And write those all down in a list that was a vector  That'd be more of a computer science view of vectors  whereas the spatial view is more familiar from physics  In my field  metallurgy  I could think of any alloy as being described by a vector that describes all of the possible components  all the compositions of that alloy  Einstein  when he conceived relativity  conceived of time as just being another dimension  So space-time is a four dimensional space  three dimension of metres  and one of time in seconds  And he wrote those down as a vector of space-time of x  y  z  and time which he called space-time  When we put it like that  it's not so crazy to think of the space of all the fitting parameters of a function  and then of vectors as being things that take us around that space  And what we're trying to do then is find the location in that space  where the badness is minimized  the goodness is maximized  and the function fits the data best  If the badness surface here was like a contour map of a landscape  we're trying to find the bottom of the hill  the lowest possible point in the landscape  So to do this well  we'll want to understand how to work with vectors and then how to do calculus on those vectors in order to find gradients in these contour maps and minima and all those sorts of things  Then we'll be able to go and do optimizations  enabling us to go and work with data and do machine learning and data science  So  in this video  we've revisited the problem of fitting a function to some data  in this case  the distribution of heights in the population  What we've seen is the function we fit  whatever it is  has some parameters  And we can plot how the quality of the fit  the goodness of the fit  varies as we vary those parameters  Moves around these fitting parameter space are then just vectors in that space of fitting parameters  And  therefore  we want to look at and revisit vector maths in order to be able to build on that  and then do calculus  and then do machine learning  [SOUND]
Jnsu2jd9sBs,Operations with vectors  So  we have these things called vectors like this guy here  What we want to do first is get an idea of what makes a vector  a vector  What we'll do in this video is explore the operations we can do with vectors  the sort of things that we can do with them that define what they are  and the sort of spaces they can apply to  So  a vector  we can think of as an object that moves us about space like this guy here  This could be a physical space  or a space of data  At school  you probably thought of a vector as something that moved you around a physical space  but in computer and data science  we generalise that idea to think of a vector as maybe just a list of attributes of an object  So  we might think of a house  say  so here's a house  and this might have a number of attributes  we could say it was 120 square metres in floor area  it might have two bedrooms  say  it might have one bathroom  that would be sort of sensible  and it might be worth 150 000 Euros  say  And I could write that down as the vector  120 square metres  2 bedrooms  1 bathroom  and 150 000 Euros  While in physics  we think of this as being a thing that moves us about space  in data science  we think of this vector as being a thing that describes the object of a house  So  we've generalized the idea of moving about space to include the description of the attributes of an object  Now  a vector is just something that are based on two rules  Firstly  addition  and secondly  multiplication by a scalar number  We'll do this first  think of a vector as just a geometric object starting at the origin  so something like this  So we get a vector  r  there  Vector addition is then when we just take another vector  so let's take another vector like this guy here  let's call him  s  and where we put s on the end of r  So then  that's s  and therefore  if we put s on the end of r  we get a sum that's r then going along s  We call that guy r plus s  Now  we could do this the other way round  We could do s and then r  and that would be s plus r  So  you go along s and along that way  and that would be s plus r there  s plus r  And we see that they actually give us the same thing  the same answer  So  r plus s is equal to s plus r  so it doesn't matter which way round we do the addition  So  the other thing we want to be able to do is scalar multiplication  that is to scale vectors by a number  So a number a  say  make it twice as long or half as long  something like that  So we say that  say 3r was doing r three times  that would be 3r there  where a was three  or we could do a half r  which should be something like that  The only tricky bit is what we mean by minus number  and by minus r  we mean going back the other way by a whole r  So  we take r  we go back the other way the same distance  that would be minus r  So  in this framework  minus means going back the other way  At this point  it's convenient to define a coordinate system  So  let's define space by two vectors  First  call the first one that takes us from left to right  and is of unit length  length one  Let's call that a vector  i  We'll have another vector here that goes up-down  a vector j  it's also of unit length  of length one  And then we'd say  just use our vector addition rules  if we wanted a vector r here  something like this  that was 3  2  by which we mean we go 3i's  1i  2i  2i  and then 2j's  So  we go 3i's plus 2j's and that gives us a vector r here just from our vector sum  And what we mean in the 3  2 is do 3i's added together  or scalar multiple of 3i's  and then do a scalar multiple of 2j's as a vector sum  And that is what we mean by a coordinate system of defining r as being 3  2  So then if I have another vector  s  let's say s is equal to -1i's and 2j's  that is  it takes us back 1i and up 2j's  that's s  Then  r plus s would be that  we just put s on the end of r  and then r plus s is going to be therefore that total vector  That's going to be r plus s  And we can just add up the components  right? So  r is 3i's and s takes us back 1  So  it's three plus minus one  this gives us 2i's  and in the j's  r takes us up two and s takes us up another two  So  that's a total of 4j's  So we can just add up the components when we're doing vector addition  So  we can see that because we're doing this component by component  then vector addition must be what's called associative  Formally  what this means is that if we have three vectors  r  s and another one  t  it doesn't matter whether we add r plus s and then add t  or whether we add r to s plus t  it doesn't matter where we put the bracket  We can do this addition and then that one  or we can do this addition and then that one  So  a consequence of it not mattering what order we had  so s plus r is equals to r plus s  we can also see that therefore it doesn't matter what order we do the additions and if we've got three and that's called associativity  That's formally that definition  And vector addition  we can see  when we're adding it up like this  will be associative  So  I've just got rid of the s's and so on  So  we can talk about another issue  which is in a coordinate system  what do we mean by multiplication by a scalar? So  if you want to take a multiplication by a scalar  let's say  2  then we define this to mean that 2r would be equal to 2 times the components of r  So 2 times 3 for i's  and 2 times 2 for the j's  so we've got 2 there multiply by 2  and that will give us 6  4  So  2r will be doing r  and then doing another r  that would be 2r  which should be at the vector 6  4  going along 3i's  4  5  6i's  and up 4j's  Now  you need to think about another question  which is minus r  So r is this  minus r is then that  which will be -3  -2  So then  we see sort of obviously  kind of  that r  plus minus r is equal to three plus  minus three on the i's  and two plus  minus two on the j's  which is equal to 0  0  So  if we do r and then add minus r  we end up back at the origin  duh  And therefore  we've defined what we mean by vector subtraction here  Vector subtraction it's just addition of minus one times whatever I'm doing  putting after the minus sign  So  if we think of another vector  s  we had s was -1  2 before  right? -1i plus 2j's  So then  r minus s would be this  So  that's minus s there is equal to go along one on the i's  and minus two on the j's  So  r minus s  add up the components  let's switch to an addition  So  r minus s is this vector here  that's r minus s  If we add up the components of that  it's 3i's plus 1  three plus one on the i's  and two plus minus two on the j's  so that gives us the vector 4 0  So  if we do r is go along three  and minus s is go along one  we've got a total of four  And if r is go up two  and minus s is go down two  we've ended up going up-down zero in total  So  then we've not only done addition by components  we've done now what we mean by vector subtraction as well  as being addition of a negative one multiple of the thing that we're doing in the minus part  And that's vector subtraction and addition by components  So  let's come back to the house example for a moment  So we said  we had a house  that's my house  that was 120 square metres  two bedrooms  one bathroom  and 150 000 Euros  So  if I put the unit in  that's square metres  that's its number of beds  that's its number baths  and that's its thousands of Euros that it's worth  So  two houses now is equal to  the vector addition of those things is equal to 2  and the way we're defining vector addition times 120  2  1  150  which will be equal to 240  4  2  300  So  we'd say that in the scheme  the way we're defining it  then two houses would be 240 square metres  that would makes sense  four bedrooms  two bathrooms  and worth 300 000 Euros  if I bought two houses identically next to each other  And that would be a scalar multiple or an addition of one house to another  One house plus one house  so we could keep on doing that with three houses  or differently shaped houses  or whatever it was  or negative houses  The way we've defined vectors  that will still apply to these objects of houses  So  that's vectors  We've defined two fundamental operations that vectors satisfy  that is addition  so like r plus s here  a multiplication by a scalar  so like 2r here and minus s here  And we've explored the properties that those imply like associativity of addition and subtraction  what subtraction really means of vectors r plus minus s  being r minus s  And we've noticed that it can be useful to define a coordinate system in which to do our addition and scaling  so like r 3 2 here  using these fundamental basis vectors  These things that define the space  i and j  which we call the basis vectors or the things that define the coordinate system  We've also seen that although  perhaps  it's easiest to think of vector operations geometrically  we don't have to do it in a real space  We could do it with vectors that a data science lists of different types of things like the attributes of a house  So  that's vectors  that's all the fundamental operations 
Nhu2a8Hb3hK,Modulus & inner product  So we've looked at the two main vector operations of addition and scaling by a number  Those are all the things we really need to be able to do to define what we mean by a vector  the mathematical properties that a vector has  Now  we can move on to define two things  the length of a vector  also called its size  and the dot product of a vector  also called it's inner scalar or projection product  The dot product is this huge and amazing concept in linear algebra  with huge numbers of implications  I will only be able to touch on a few parts here  but enjoy  It's one of the most beautiful parts of linear algebra  So when we define a vector  initially  say this guy r here  we did it without reference to any coordinate system  In fact  the geometric object  this thing r  just has two properties  its length and its direction that it's pointing that way  So irrespective of the coordinate system we decided to use  we want to know how to calculate these two properties of length and direction  If the coordinate system was constructed out of two unit vectors that are orthogonal to each other  like i here and j here in 2D  then we can say that r is equal to a times i  plus b times j  When I say unit about i and j  I mean that of length one  which people will often denote by putting a little hat over them like this  Then from Pythagoras  we can say that the length of r is given by the hypotenuse  So what I mean by that is  if we draw a little triangle here  then we've got this length here is ai  So if we write the length being  with these two little vertical lines  it's just of length a  because i is of length one  This side here is bj  and that's of length b  So this side here is from Pythagoras  is just a squared plus b squared  all square rooted  and that's the size of r  So we can write down r  quite often people will do this  write r down like this  just ignoring the i and j and writing it as a column vector  So r is equal to a-b  The size of r  we write down as being the square root of a squared plus b squared  Now  we've done this for two spatial directions defined by unit vectors i and j that are at right angles to each other  But this definition of the size of a vector is more general than that  It doesn't matter if the different components of the vector or dimensions in space like here  or even things have different fiscal units like length  and time  and price  We still define the size of a vector through the sums of the squares of its components  The next thing we're going to do is to find the dot product  One way among several  multiplying if you'd like two vectors together  If we have two vectors  r and s here  r here has components r_i  r_j  so r in the i direction  r in the j direction  and s has components s_i and s_j  then we define r dotted with s to be given by multiplying the i components together  So that's r_i times s_i  and adding the j components together  so that's r_j times s_j  The dot product is just a number  a scalar number  about three  given by multiplying the components of the vector together in turn  and adding those up  So in this case  that would be three and two  for the rij  and minus one  and two for s  So if we do that  then we get a sum  the r s is equal to minus three plus four  which gives us one  So r s in this case  it's just one  Now  we need to prove some properties of the dot product  First  it's commutative  What commutative means is that r s is equal to s r  It doesn't matter which way around we do it  It doesn't matter because when we put these numbers in here  if we interchange those  the rs and Ss  we get the same thing when we multiply minus one by three  it's the same as three times minus one  So  it doesn't matter which way round we do the dot product  s r is equal to r s  which means it's commutative  Second property we want to prove the dot product is distributive over addition  By which I mean that if I've got a third vector here now t  that r dotted with s plus t is equal to r dotted with s  plus r dotted with t  I can multiply it out in that way  It's probably feels mundane or obvious  but let's prove it in the general case  So let's say I've got some n-dimensional vector r  components r_1  r_2  all the way up to r_n  and s  is the same as components s_1  s_2  all the way up to s_n  and t has components t_1  t_2  all the way up to t_n  Then let's multiply it out  So if we take the left-hand side  r dotted with s plus t  that's going to be equal to r_1 times s_1 plus t_1  We take the components  Then r_2  component r_2  times components s_2 plus t_2  Then all the dimensions in between  and then finally  r-n times s_n plus t_n  Then what we can do  is we can then sort that out  So we've got multiply that out  So we've got r_1  s_1  plus r_1  t_1  plus r_2  s_2  plus r_2  t_2  plus all the ones in between r_n and s_n  plus r_n  t_n  Then we can collect it together  So we've got the r_1  s_1 times r_2  s_2  all the way to r_n  s-n  That's of course  just equal to r dotted with s  If we collect the r_t terms together  we've got r_1  t with t_1  r_2  t_2  all the ones in between r_n  t_n  That's just r dotted with t  So we've demonstrated that this is in fact true  that you can pull out plus signs and dots in this way  which is called being distributed over addition  The third thing we're going to look at is what's called associativity  So that is  if we take a vector  a dot product  and we've got r dotted with some multiple of s  where a is just a number  it's just a scalar number  So we're multiplying s by a scalar  What we're going to say is that  that is equal to a times r dotted with s  That means that it's associative over scalar multiplication  We can prove that quite easily  just in the 2D case  So if we say we've got r_1 times a s_1 plus r_2 times a s_2  that's the left-hand side  just for a two-dimensional vector  Then we can pull the a out  So we can take the a out of both of these  happens then we've got r_1  s_1  plus r_2  s_2  That's just r s  a times r s  So this is in fact true  So we've got our three properties that the dot product is commutative  We can interchange it  Is distributed over addition  which is this expression  and its associative over scalar multiplication  We can just pull out scalar numbers out  As an aside  sometimes you'll see people in physics and engineering write vectors in bold  numbers or scalars in normal font or they'll underline their vectors to easily distinguish them from things that have scalars  Whereas in math and computer science  people don't tend to do that  It's just the notation difference between different communities  and it's not anything fundamental to worry about  The last thing we need to do before we can move on is  draw out a link between the dot product and the length or modulus of a vector  If I take a vector and dot it with itself  so r dotted with r  what I get is just the sums of the squares of its components  So I get r_1 times r_1  plus r_2 times r_2  and all the others if there were all the others  So I get r-1 squared plus r_2 squared  Now that's quite interesting because that means if I take the dot product of a vector with itself  I get the square of its size of its modulus  So that equals r_1 plus r_2 squared  square rooted  all squared  So that's mod r squared  So if we want to get the size of a vector  we can do that just by dotting the vector with itself and taking the square root  That's really neat and really hopefully  quite satisfying 
Lkw9H2bsk8H,Cosine & dot product  Let's take the cosine rule from algebra  which you'll remember probably  vaguely from school  I might have said  if we had a triangle with sides a  b  and c  then what the cosine rule said was that c squared was equal to a squared plus b squared minus 2ab times the cos of the angle between a and b  cos that angle theta there  Now  we can translate that into a vector notation  We call this vector r here  and we call this vector s here  Then this vector will be minus s plus r  so that vector will be r minus s  minus s plus r  So we can say that c squared was the modulus of r minus s squared  and that would be equal to the modulus  the size of r squared plus the size of s squared minus 2 mod r mod s cos theta  Now  here's the cool bit  We can multiply this out using our dot-product  because we know that the size of r minus s squared is equal to r minus s dotted with itself  Now  that's just that  and we can multiply that out and then we'll compare it to this right hand side here  So r minus s dotted with r minus s  Well  that's going to be  if we need to figure out how to multiply that out  that's going to be equal to r dotted with r  and then take the next one  minus s dotted with r  minus s dotted with r again  if you take minus s and that r  Minus s dotted with r again  and then minus s dotted with minus s  So that is  we've got the modulus of r squared here  and we dot r with itself  minus twice s dotted with r  and then minus s dotted with minus s  Well  that's going to be the size of minus s squared which is just the size of s square  Then we can compare that to the right-hand side  When we do that comparison  compare that to the right-hand side  the minus r squareds are going to cancel  the r squareds even  the s squareds are going to cancel  So we get a result which is that minus twice s dotted with r  is equal to minus twice modulus of r  modulus of s  cos theta  That is  and then we can lose the minus sign  minus signs will cancel out just multiply through by minus 1  Then the 2s we can cancel out again  So we can say that the dot-product r s  just to put it in a more familiar form is equal to mod r mod s cos theta  So what we found here is that the dot product really does something quite profound  it takes the size of the two vectors  If these were both unit length vectors those will be 1 and multiplies by cos of the angle between them  It tells us something about the extent to which the two vectors go in the same direction  because if theta was 0 then cos theta would be 1  and r s would just be the size of the two vectors multiplied together  If the two vectors on the other hand we're at 90 degrees to each other  if they were  r was like this and s was like this and the angle between them  theta  was equal to 90 degrees  cos theta  cos 90 is 0  and then r s is going to be  we can immediately see  r s is going to be some size of r  some size of s  times 0  So if the two vectors are pointing at 90 degrees to each other  if they what's called orthogonal to each other  then the dot product it's going to give me 0  If they're both pointed in the same direction  so s was like that and the angle between them is nought  Cos of nought is equal to 1  and then r s is equal to the mod r times mod s  just the multiplication of the two sizes together  Fun one  last fun one here  is that r and s are in opposite directions  So let's say s was now going this way  and the angle between them was a 180 degrees cos of 180  180 degrees is equal to minus 1  So then  r s will be equal to minus the size of r times the size of s  So what the dot product here really does with this cos  it tells us when we get the minus sign out that they're going in opposite directions  So there's some property here in the dot product  we've derived by looking at the cosine rule  that we've derived here  when the dot product's 0 they are 90 degrees to each other  they're orthogonal  when they go in the same way we get a positive answer  when they're going more or less in opposite directions we get a negative answer for the dot product 
bu9Hh1hVG8b,Projection  Now  there's one last thing to talk about in the segment which is called projection  Projection  And for that  we'll need to draw a triangle  So if I've got a vector R and another vector S  Now  if I take a little right-handed triangle  drop a little right-handed triangle down here where this angle's 90 degrees  then I can do the following  If I can say that if this angle here is theta  but cos theta is equal to  from sohcahtoa  is equal to the adjacent length here over the hypotenuse  that is  and this hypotenuse is the size of S  Now  if I compare that to the definition of the dot product  I can say that R dotted with  we'll have fun with colors  dotted with S is equal to mode R size of R  times the size of S  times cos theta  But the size of S times cos theta if i put S up here  just need to put my theta in there  cos S  cos theta is just the adjacent side  so that's just the adjacent site here in the triangle  So  the adjacent side here is just kind of the shadow  if I if I had a light coming down from here  it's the shadow of S on R  That length there  it's kind of a shadow cast  If I had a light at 90 degrees to R shining down on S  and that's called the projection  So what that dot product gives us  is it gives us the projection here of S on to R times the size of R  And one thing to notice here is that if S was perpendicular to R  if S was pointing this way  it would have no shadow  That is if cos theta was 90 degrees that shadow would be no  the cos theta would be no here and I get no projection  So  the other thing the dot product gives us  is it gives us the size of our times some idea about the projection of S on to R  The shadow of S onto R  So  if I divide the dot product R S by the length of R  just bring the R down here  I get mode S cos theta  I get that adjacent side  I get a number which is called  cause R S is a number and the size of R is a number  and that's called the scalar projection  And that's why the dot product is also called the projection product  because it takes the projection of one vector onto another  We just have to divide by the length of R  and if R happened to be a unit vector or one of the vectors we used to find the space of length one  then that would be of length one and our dot S would just be the scalar projection of S onto that all that vector defining the axes or whatever it was  Now  if I want to remember to encode something about R  which way R was going into the dot product or into the project book product could define something called the vector projection  And that's defined to be R S over mode R dotted with itself  So R R mode R squared  that's R S over R R if you like because mode R squared is equal to R R  And we multiply that by the vector R itself  So that is that's dot products just a number  these sizes are just a number  and R itself is a vector  So what we've done here is we've taken the scalar projection R S over R  this guy that's how much S goes along R  and we've multiplied it by R divided by its length  So we've multiplied it by a vector going the direction of R but it's been normalized to have a length one  So that vector projection is a number  times a unit vector that goes the direction of R  So if R say was was some number of lengths  the vector that will be R divided by its size  say if that was a unit length vector I've just drawn there  and the vector projection would be that number S R  that adjacent side  times a vector going in the unit length of R  So that's  if you like the scalar projection also encoded with something about the direction of R  just a unit vector going in the direction of R  So we've defined a scalar projection here  and we've defined a vector projection there  So good job  This was really the cool video for this week  we've done some real work here  We found the size of a vector and we defined the dot projection product  We've then found out some mathematical operations we can do with the dot product  This distributes over vector addition and associative with scalar multiplication and that its commutitive  We then found that it finds the angle between two vectors  the extent to which they go in the same direction  or then it finds the projection of one vector onto another  That's kind of how one vector will collapse onto another  which is what we'll explore in the next two videos  So  good work now's a good time to pause  and try some examples  but put all this together and give it all a workout on a bit of a try before we move on 
bhy2G8axhY8,Changing basis  Now  so far  we haven't really talked about the coordinate system of our vector space  the coordinates in which all our vectors exist  But it turns out that in doing this thing of projecting  of taking the dot-product  we're projecting our vector onto one  Which we might use as part of a new definition of the coordinate system  So in this video  we'll look at what we mean by a coordinate system  and then we'll do a few case of changing from one coordinate system to another  So remember that a vector like this guy r here is just an object that takes us from the origin into some point in space  Which could be some physical space or could be some space of data  like bedrooms  and thousands of years for the house  or something like that  What we haven't talked about so far really  is the coordinate system that we use to describe space  So we could use a coordinate system defined by these two vectors here  I'm going to give them names  We call them i and j before  I'm going to give names e1 and e2 and found to be of unit length  So I'm going to give them a little hat meaning they have unit length  I'm going to define them to be the vectors one  zero and zero one  If I have more dimensions in my space  I could have e3 hat  e4 hat  e5 hat  e million hat  whatever  Here the instruction then is that r is going to be equal to doing a vector sum of two e1s or three e1s  and then some number of e2s  So we'll call it during 3 e1s hats plus 4 e2 hats  and some will write it down as a little list  three four  So always  the three four  here's the instruction to do 3 e1 hats plus 4 e2 hats  But if you think about it  my choice of e1 hat and e2 hat is arbitrary  It depends entirely on the way I set up the coordinate system  There's no reason I couldn't have set up some quota system  at some angle to that  Or even use vectors to find the axis that weren't even at 90 degrees to each other  were of different lengths  I could still have described r as being some sum of some vectors I used to define the space  So I could have another set of vectors b  I'll call b1 here in the vector two one  and I could have another vector here b2 as the vector minus two four  and I've defined it in terms of the coordinates e  I can then describe r in terms of  your using those vectors b1 and b2  is just the numbers in r would be different  So we call the vectors we use to define the space  these guys e or these guys b  we call them basis vectors  So the numbers I've used to define r  only have any meaning when I know about the basis vectors  So r refer to these basis vectors e is three four  but r referred to the basis vectors b also exists  We start out with the numbers earlier  So this should be amazing  the vector r has some existence in a deep mathematical sets  completely independently of the coordinate system we use to describe the numbers in the list  describing r  All the vector that takes us from there from the origin to that  still exist  independently of the numbers we used in r  This is neat  Right  So the fundamentally idea  Now  if the new basis vectors  these guys b  are at 90 degrees to each other  then it turns out that projection products has a nice application  We can use the projection or dot-product to find out the numbers for r in the new basis b  so long as we know what the bs are in terms of e  So here I've described b1 as being two one  as being e1 plus e2  twice e1 plus e2  I've described b2 as being minus two e1s plus four e2s  If I know b in terms of e  I'm going to be able to do  use the projection product to find r described in terms of the bs  But this is a big if  the b1 and b2 have to be at 90 degrees to each other  If they're not we end up being in big trouble and need matrices to do what's called a transformation of axis from the e to the b based on basis vectors  We'll look at matrices later  but this will help us out a lot for now  Using dot-products in this special case where the new basis vectors are orthogonal to each other  is computation a lot faster and easier  it's just less generic  But if you can arrange the new axis to be orthogonal  you should  because it makes the calculations much faster and easier to do  So you can see that if I project r down onto b1  so I look down from here  and project down at 90 degrees  I get a length here for scalar product  and that's scalar projection is the shadow of r to b1  A number of the scalar projection describes how much of this factor I need  The vector projection is going to actually give me a vector in the direction of b1  of length equal to that projection  Now  if I take the vector projection of r onto b2 going this way  I'm going to get a vector in the direction of b2 of length equal to that projection  If I do a vector sum of that vector projection  plus this guy's vector projection  I'll just get r  So if I can do those two vector projections  and add that their vector sum  I'll then have our b being the numbers in those two vector projections  So I found how to get from r in the e set of basis vectors  to the b set of basis vectors  Now  how do I check that these two new basis vectors are at 90 degrees to each other? I just take the dot-product  So we said before the dot-product cos Theta was equal to the dot of two vectors together  so b1 and b2  divided by their lengths  So if b1 b2 is zero  then cos Theta zero  if they're at 90 degrees to each other  they are orthogonal  So I don't even need calculate length  So I'll just calculate the dot-product  So b1 b2 here  I take two times minus two  and I add it to one times four  which is minus four plus four which is zero  So these two vectors are at 90 degrees to each other  So it's going to be safe to do the projection  So having thought through it  let's now do it numerically  So if I want to know what r described in the basis e  and I'll use pink to write  If I take r in the basis e  and I'm going to dot him with b1  The vector projection divides by the length of b1 squared  So r in e dotted with b1 is going to be three times two  plus four times one  divided by the length of b1 squared  So that's the sum of the squares of the components of b  that's two squared plus one squared  So that gives me  six plus four is 10  divided by five which is two  So this projection here is of length two times b1  So that projection there  that vector is going to be two times b1  So that is  in terms of the original set of vectors b  original vectors e  re b1 over b1 squared  times b1 is two times the vector two one  is the vector four two  I can do  then now this projection onto e2  So I can do re dotted with b2 I divided by b  The length of b2 squared  and re b2 is three times minus two  plus four times four  divided by the length of b2 squared  which is minus two squared plus four squared  So that's  three times minus two is minus six  plus four times four is 16  So that's a minus six plus 16 is 10  divided by this length here is  four squared is 16  two squared is four so 20  So that's equal to a half  So this vector projection here is that guy times b2  So that's re b2 over the modulus of b2 squared  that's my half  times the vector b2  So that's a half  times the vector b2  which is minus two four  Now  if I add those two together  four two  this bit  that vector projection  plus this vector projection  So this guy is going to be a half b2  plus half minus two four is minus one two  If I add those together  I've got three four  which is just my original vector r three four in the basis e  So in the basis of b1 and b2  rb is going to be two a half  Very nice  Two a half  So actually in the basis b it's going to be two a half there  So rb is 2 times b1 plus a half times b2  Very nice  So I've converted from the e set of basis vectors to the b set of basis vectors  which is very neat  Just using a couple of dot-products  So this is really handy  this is really cool  We've seen that a vector describing our data  isn't tied to the axes that we originally used to describe it at all  We can re-describe it using some other axis  some other basis vectors  So the basis vectors were use to describe the space of data  and choosing them carefully to help us solve a problem will be a very important thing in any algebra  and in general  What we've seen is we can move the numbers in the vector  we use to describe a data item from one basis to another  We can do that change just by taking the dots or projection product  in the case where the new basis vectors are orthogonal to each other 
tUh2gV8g2Ue,Basis  vector space  and linear independence  So we've seen how we don't just have to have basis vectors that are our normal one hour and 01  the so-called natural basis  We can have different basis vectors that we define how we move about space  In this video  we're going to define what we mean by a basis  by a vector space  and by the term linear independence  which is going to let us understand how many dimensions our vector space possesses  So first  let's define what we mean by a basis  A basis is a set of n vectors that are not linear combinations of each other  which means they are linearly independent of each other and their span  the space they describe  The space is then n dimensional  This first criteria I can write down by saying that they are linearly independent  If I can't write any of them down by taking some combination of the others  So for example  let's say I've got a vector b_1 here  by taking multiples of b_1  I can get anywhere along the 1D space of a line  If I take a second vector b_2  that isn't just a multiple of b_1  then I can get anywhere in the plane of this board by taking combinations of b_1 and b_2  Some number of b_1's plus some number of b_2's  and this is a 2D space  Now  let's take a third vector b_3  Now  for b_3 to be a valid third basis vector  it has to be impossible for me to find some numbers a_1 and a_2 such that I can satisfy this sub  So it has to be impossible for me to find b_3 at some combination of b_1's and b_2's where a_1 and a_2 are just numbers  That has to be impossible  and if it is impossible  b_3 is a third basis vector and b_3 is linearly independent  If it is possible for me to find an a_1 and a_2 that satisfies that sum  b_3 is then linearly dependent on b_1 and b_2 and it lies in the same plane as b_1 and b_2  and if it's impossible for me to find an a_1 and a_2  b_3 must have some component out of the board so I can then use b_3 to give me a three-dimensional space  So that lets us define what we mean by the number of linearly independent basis vectors in our space  If I had a four basis factor b_4 that wasn't a linear combination of b_1  b_2  and b_3  I'd have a four-dimensional space and so on up to as many dimensions as I like  Now  notice what my basis vectors b don't have to be  They don't have to be unit vectors by which I mean vectors of length one and they don't have to be orthogonal  that is at 90 degrees to each other  But everything is going to be much easier if they are  So if at all possible  you want to use orthonormal basis factor sets  90 degrees of unit length  Now let's think about what happens when we map from one basis to another  The number line of the axis of the original grid then projects down onto the new grid and potentially has different values on that grid but the projection keeps the grid being evenly spaced  Therefore  any mapping we do from one set of basis vectors  from one coordinate system to another  keeps the vector space being a regularly spaced grid  Well  our original vector rules of vector addition and multiplication by a scalar still work  It doesn't [inaudible] fold space which is what the linear bit in linear algebra means  Things might be stretched or rotated or inverted  but everything remains evenly spaced and linear combinations still work  Now  where the new basis vectors aren't orthogonal  then to do the change from one basis to another  we won't just be able to use the dot product  we'll need to use matrices instead which will meet in the next module  So that's the formal definition of what we mean by a basis and by linear independence 
Bu8haH8gahk,Applications of changing basis  For example  say I have a bunch of 2D data points like this  Say they all  more or less  lie on a straight line  We can we can see that  these guys all more or less lie on a line that's going to be something like that  I could imagine rediscovering that data by mapping them onto that line and then saying how far they're along that line  So I can map this guy down onto the line and I could say the origin maps down there and I could then say this data point is that far along the line  So he's that fall on the line and he's this far away from the line  So I've got two dimensions here  How far I am along the line and how far I am from the line  These guys they're all slightly different distances from the line  As a little bit of an argument in stats as to whether we do the distance that way vertically or that way as a projection for the distance from the line  But it's a theoretical argument  but notice that this distance from the line is effectively a measure of how noisy this data cloud is  If they are all tight on the line they'd all be very small distances away and if they were all quite spread they'd be quite big distances away  So this distance from the line this is effect of the noise and that's information that isn't very useful to us  So we might want to collapse it  Except that that noise dimension tells me how good this line fit is  If the best fit line was all skewed  was all wrong I'd get a much bigger numbers for the noisiness  and if the best fit line was as good as possible I get the minimum possible number for the noisiness  So that noise dimension contains information that's going to tell me how good my fit is  So when I'm doing data science it tells me how good my fit to my data is  The way I've defined these two directions along the line and away from the line  they are orthogonal to each other  So I can use the dot-product to do the projection to map the data from the x-y space unto the space of the line  along the line and away from the line  which is what we deal into in the last little segment  Now if we're thinking about a neural network in machine learning that recognizes faces say  maybe I'd want to make some transformation of all the pixels interface into a new basis that describes the nose shape  the skin hue  the distance between the eyes those sorts of things and discard the actual pixel data  So the goal of the learning process of the neural network is going to be to somehow derive a set of basis vectors that extract the most information-rich features of the faces  So in this video we've talked about the dimensionality of a vector space in terms of the number of independent basis vectors that it has and we found a test for independence that the set of vectors are independent if one of them are linear combination of the others  We've talked more importantly about what that means in terms of mapping from one space to another and how that is going to be useful in data science and machine learning  
B7skL7gyg3H,Matrices  vectors  and solving simultaneous equation problems  [MUSIC] So back at the start of the course  we encountered the apples and bananas problems  How to find the price of things when we only have the total bill  And we've looked at vectors so far  And now we're going to look at matrices  And these are objects that rotate and stretch vectors  But they're also objects that let us solve these sorts of problems  So let's go back to that apples and bananas problem  Say I walk into a shop and I buy two apples  and three bananas  and that that costs 8 euros  So I'm saying 2 apples  3 bananas  cost 8 euros  Now say I go to the shop on another day and I buy 10 apples and 1 banana  And that that costs me  or the shopkeeper charges me 13 euros  And I want to discover what the price for 1 apple and 1 banana is  so I can decide which offers better value or even just predict my bill  Now you might say this is silly  What shop doesn't have sticker prices after all? But actually in business with complicated products and service agreements and higher purchase  this sort of thing  price discovery  happens all the time  Think about what happens when you buy a car for instance  Now these are just simultaneous equations  but I can write them down in another way  The way I would write this down with matrices would be as follows  So I'd write it down as this matrix  what I'm now calling a matrix  an object with numbers in 2 3 10 1  a b  = 8 13  And these things I call matrices  This is a 2 by 2 matrix  This is a 2 row by 1 column matrix  And this is another 2 row by 1 column matrix  And the instruction here is to multiply this out in the following way  So I would multiply the elements in the rows by the elements in the column  So I'd multiply 2 by a  plus 3 times b  So that row times that column  and I'd say that equaled the top row on the right-hand side  And I do the same for the next row  that row times that column is 10a plus 1b  is equal to the row on the bottom on the right-hand side  And that looks like my two simultaneous equations  But this is really handy  because these things  notice  look like vectors  So this matrix operates on this vector to give this other vector  and my question is  what vector transforms to give me this guy on the right? Now let's look at what happens if I multiply this matrix here by the unit basis vector  the x axis vector  Well when I do that multiplication  I'm going to get 2 times 1 plus 3 times 0  And I'm going to get 10 times 1 plus 1 times 0  So I get the vector 2 10  So what this does is it takes the little unit vector which we called e1 hat  and it transforms it to another place  Which is 2 10  which is going to be up here somewhere  So that's e1 hat  changed  and that's equal to 2 10  Now if I do that with the other basis vector  If I do 2 3 10 1 multiplied by 0 1  Then I'm going to get 2 times 0 times plus 3 times 1  10 times 0 plus 1 times 1  I'm going to get 3 1  So the other basis vector  e2 hat  gets transformed over to 3 1  which is going to be over here somewhere  So that's e2 changed  I'm using the prime here to indicate changed  3 1  So what this matrix does  is it moves the basis vectors in some way  it transforms them  it changes the space  So what this matrix 2 3 10 1 does is it's a function that operates on input vectors and gives us other output vectors  And a set of simultaneous equations here is asking  in effect  what vector I need in order to get a transformed product at the position 8 13  In order to get an output of 8 13  Now we can see what we mean now by the term linear algebra  Linear algebra is linear  because it just takes input values  our a and b  and multiplies them by constants  So everything is linear  And it's algebra  that is it's a notation describing mathematical objects and a system of manipulating those notations  So linear algebra is a mathematical system for manipulating vectors in the spaces described by vectors  So this is interesting  There seems to be some kind of deep connection between simultaneous equations  these things called matrices  and the vectors we were talking about last week  And it turns that the key to solving simultaneous equation problems is appreciating how vectors are transformed by matrices  which is the heart of linear algebra  >> [MUSIC]
sjwu8bUHa8B,How matrices transform space  [MUSIC] Previously we introduced the idea of a matrix and related it to the problem of solving simultaneous equations  And we showed that the columns of a matrix just read us what it does to the unit vector along each axis  Now we'll look at different types of matrices and what they do to space  And what happens if we apply one matrix transformation and then another  which is termed composition  Now because we could make any vector out of a vector sum of the scaled versions of e1 hat and e2 hat  Then what that means is the result of the transformation is just going to be some sum of the transform vectors  which I'm calling e1 hat and e2 hat  This is a bit hard to see but what it means is that the grid lines of our space stay parallel and evenly spaced  They might be stretched or sheared  but the origin stays where it is and there isn't any curviness to the space  it doesn't get warped  And that's a consequence of our scalar addition and multiplication rules for vectors  So that is if I write down the matrix as capital A and the vector its transforming is r  whatever it was  a b in our apples and bananas problem  And that gives me some altered version  We said it was 8 13 before  but I'm going to call it r transformed or r prime  Then we can look at what happens when I do algebra with it  So if I multiply r by some number  just a number  let's call it n  And if I apply A to (nr)  what I'm saying is that I will get nr prime  And hopefully you can see if I put an n in there  when I multiply it all out I'm going to get an n in there  Similarly  if I multiply A by the vector (r+s)  then I will get Ar + As  So if I get that multiplication  get that  do the whole thing again with another vector  and get another vector s  and add those two  that will be true  So what I'm saying here is that if I can then think of these as being the original basis vectors  e1 hat and e2 hat  I'll then get some addition of e2 and e1 primed  So if I say that's ne1 hat + me2 hat  I'll get nAe1 hat + mAe2 hat  which is ne1 prime + me2 primed  Which is just the vector sum of some multiple of those  So the space gets transformed  e1 and e2 get moved  And then I can just add up vectors with them  So that's very nice  that means that all of our vector sum rules work  Now maybe that's a bit confusing so let's try it with an example  So I've got my matrix A here from my apples bananas problem  So I have 2 3 10 1  or if we like  the vector 2 10 and the vector 3 1  Now let's try an example like a vector 3 2  Now if I multiply that out just straightforwardly  as we probably did at school  I've got 2 times 3 plus 3 times 2  So that's 6 plus 6  that's 12  And I've got 10 times 3  which is 30  plus 1 times 2  which is 2  So that's 32  But I could think of that as being 2 3 10 1  times (3 times 1 0 + 2 times 0 1)  That is  3 of our e1 hat and 2 of e2 hat in a vector sum  Now I can take the 3 out so that's 3 times (2 3 10 1 times 1 0)  + 2 times (2 3 10 1 times 0 1)  And this  we know that this is what happens from e1 hat to get to e1 prime here  so that's 3 times 2 10  And that's 2 times what happens to e2 hat  and that goes to 3 1  So that gives us 6 plus 6 is 12 and 30 plus 2 is 32  So it really is true  these rules really do work  We can think of a matrix multiplication as just being the multiplication of the vector sum of the transformed basis vectors  So pause now for a moment and try that maybe with an example of your own  And verify that that really does work because that's really quite deep  This matrix just tells us where the basis vectors go  That's the transformation it does  It's not a complicated  multiplying out thing  We don't need to worry about the mechanics of doing the sum  We can just think of it in terms of what it does to vectors in the space  [MUSIC]
n9uhU3hBhua,"Types of matrix transformation  Now  let's look at some special types of matrices that do simple things and then we'll think about how to combine them to do complicated things  First  let's think about a matrix that doesn't change anything  A matrix is just composed of the basis vectors of the space  so 1  0  0  1  So that  if I multiply by some vector x y  that's not going to change x y  If I multiply it out  I'm getting at one times x  plus zero times y  zero times x plus one times y  x y  So it doesn't change the vector x y  The- is just composed of the basis vectors here  and it doesn't change them  And that's called therefore  the \""Identity Matrix\""  It's the matrix that does nothing and leaves everything preserved  And it's called \""I\""  Now  what if I have different numbers along the leading diagonal? Something like 3  0  0  2 for instance  Well  that's going to scale the x axis here by a factor of three  it's going to go to 3  0  when I multiply it out  and the y axis is going to scale by a multiple of two  It's going to go from 0  1 to 0  2  So I've scaled space this way by a factor of three  and that way  by a factor of two  So my little unit square has gone to being a rectangle  From being a square originally  it's scaled up this way  three times  and this way  two times  And of course  if the scale factor here was a fraction  if it was a third or something  then other squished space that way  Instead  I'd have gone that way and made it thinner and taller  something like that  So fraction then  squishes space  The next thing to think about is  what happens if I've got a matrix where I scale by say  minus one here of one of the axis  Well  what that is going to do  to the original axis  is it's going to flip them around  So if I've got 1  0 here  and 0  1 being the other axis of course  then  that's going to scale the first one over here to being -1 0  and of the 0 2 is going to scale the other axis up to a 0 2 here  So  my original little cube  goes from here to here  So  it's changed in area by a factor of two  one times two but it's also flipped over  The x axis has gone over there  Now  what does that mean? Well  if I had previously an axis system where I went using my right hand from my right hand there  that's my first one  that's my second axis around  and I went anti-clockwise  now  I'm going the other way  Now  I've got a left-handed coordinate system  and I go clockwise  So I need to get my left hand out to describe them now  So I've changed the sense of the coordinate system and flipping it over  Now  the other matrix we need to think about  get another pen  is -1  0  0  -1 and that inverts everything  It inverts both axis  it takes 1 0 to -1 0  and it takes 0 1 down here to 0 -1  So it flips everything in both coordinates  and that's called an \""Inversion\""  Another matrix I can think of is 0 1 1 0  That's kind of fun  What that does is it takes i hat 1 0 here it takes it to 0 1 so it takes it to there  0 1 that guy goes there  and it takes the other axis  which was there  It was also 0 1  and it takes it to 1 0  So  what it does is it flips them around  It's like I put a mirror in at 45 degrees  I could have another mirror  which would be 0 -1 -1 0  And that would take 1 0 here and it would make it 0 -1  take it down there  0 -1  and it would take this axis  0 1 and it would make it to -1 0  So make it over there  So that's like having a mirror plane in there  Just for completeness  I can think of another two mirrors  and they would be -1 0 0 1 and that flips the x axis  That's like a vertical mirror  that's that guy  And I can have another one  which would be 1 0 0 -1  And that flips the horizontal axis  That flips this guy down but leaves this guy unchanged  So those are all my mirrors  Another thing I want to think about are shears  say I wanted to keep e1 hat where it was  at 1 0 but move e2 hat over  So e2 hat is 0 1  So I want to move e2 hat over to here say  So I want to get e2 to primed to be equal to 1 1 say  Now  in which case I just write down my matrix  right? So I now can say that e1 becomes itself  1 0 and e2 becomes 1 1  That would be the transformation matrix for that shear  It would be shearing the unit square over from being a little square to being a little parallelogram here  something like that  Of course  I could shear the x axis as well  I could do some combination of shears  but that is basically how a shear would look  Now  the last sort of shape change I can do after stretches  inversion  mirrors  and shears is a rotation  If I take e1 hat again  and e2 hat  If I rotate them round  well  e1 hat is going to go round here  So  e1 primed is going to be equal to 0 1  And e2 hat is going to go round to here  and e2 primed is going to become -1 0  So  that 90 degree rotation there is going to have the transformation matrix 0 1 -1 0  And in general  I can write down the rotation by an angle here  let's say an angle here of theta  I can write that down as being cos theta  sine theta  sine theta  cos theta  And I need to put a minus sign in here where positive theta are actually that way  So we did a rotation by minus 90  so sine of minus 90 is minus one  And that's a general expression for a rotation in 2D  If I wanted to do it in 3D  I need to think about the axis I was doing it along or around  so far as rotating about z  I would preserve all of the z's for a 3D rotation something like that  This isn't really a course about matrix rotations and matrix geometry  and so on  that would be something like crystallography of this course about data science  So we don't need to think too much about rotations  but it is interesting  If we need to do things like transform faces if we wanted to do facial recognition  we'll want to do these sorts of stretches of mirrors and shears and rotations to faces  To get them all facing like that  rather than facing like this  or sort of some funny angle that we had from our camera that was looking at somebody  So we do need to do this in data science on occasion  and that's rotations  So what we've described in this video is we've described all the possible sort of changes we can do with a matrix  So now  we need to think about how we do a rotation and then a stretch  And that's the next little part "
Hus2b7ygYg8,Composition or combination of matrix transformations  Now  what's the point of telling about all these different geometric transformations? Well  if you want to do any shape alteration  save all the pixels in an image of a face or something like that  then you can always make that shape change out of some combination of rotations  shear structures and inverses  That is if I first apply one translation A1 to a vector r  then that makes some first change  Then if I apply another translation A2 to the results of that  then I can perform a composition of the two translations  What I've done is I've performed first A1 and then A2 to r  Now  maybe this isn't so obvious  so let's slow down into a concrete example  Let's start out with our basis vectors  So we've got e1 as being one  zero  and e2 as being zero  one  Now  let's take our first transformation A1 as being a 90 degree anticlockwise rotation  So what happens if we rotate this by 90 degrees anticlockwise? So e1 comes down to some transformed e1  let's call it e1 prime of zero  minus one  So put zero  minus one in there and our e2 rotates down here  to be some transform version of e2  which will then be e2 primed  which will be one  zero  So then we've got an overall A1 which describes a 90 degree rotation  Now  let's take another matrix which also transforms our original basis vectors  original e1 and e2  What we'll say is that is a let's say a mirror  So that moves a vertical mirror moves e1 to e1 prime  is going to be minus one  zero  It's going to leave e2  where it was if I just reflect vertically  So my transformation A2 now is going to be minus one  zero  and it leaves the other one the same zero  one  Now  let's ask ourselves what happens if I do A2 to A1  So now I'm going to reflect over the result of doing A1  So e1 prime is actually when I reflect vertically  going to stay in the same place  That's going to give me e1 let's say double-prime for doing it twice  E2 prime it's going to reflect over here  and it's going to become e2 double prime is going to be minus one  zero  So I'm going to have an overall result of doing A2 to A1  which is going to be  well I get e1 prime I first write down zero  minus one and e2 double prime I'm going to write down minus one  zero  Now  I can work out what that is  actually in a matrix way without having to draw it all out by saying  A2 to A1 is doing A2 which is minus one  zero  zero  one  to A1 basis vectors  A1 was zero  minus one  one  zero which is just the two transform basis vectors the single primes  When I do that  I just have to do this matrix to that basis vector and then this matrix to that basis vector  What that looks like  if I do that is I do this matrix to that basis vector so I'll do that row times that column  That gives me minus one times zero plus zero times minus one that gives me zero  That one to that one  So the second row first column zero times zero plus one times minus one gives me minus one  Then I do that A2 translation to the second column  Now  to the second basis vector with A1  So minus one times one  plus zero times zero  of minus one times one second row zero times one plus one times zero gives me zero  So I do the row times the column for all the possible row and column combinations  So that is in fact the same thing  So we can have discovered really how to do matrix composition  or matrix multiplication doing one matrix to another transformation matrix  Notice that geometrically  we can show that A2 then A1  isn't the same as doing the translations in the other order first A1 and A2  So just watch that for a minute  So this one we've done A2  and then what if we then do A1  A1 is a 90 degree rotation so if we rotate e1 prime  then we find that e1 double prime  would be equal to our original e2 that's A1  Our e2 primed  which was just staying where it was  when we rotate that down  then that'll come down to here  That'll come down to one  zero  so our e2 double prime will be one  zero  Let's look how matrix wise  So if we do A1 to A2  A1 was zero  one  minus one  zero  If we do that to A2  which was minus one  zero  zero  one that we're going to do that matrix multiplication  What that's going to give us is zero times minus one plus one times zero  zero  then this one times this one  minus one times minus one plus zero times zero is one  This row second column zero  zero  one  one and second row second column minus one  naught  naught  one gives me naught  So that is the first column is zero  one  and the second column is one  zero  So that isn't the same as doing the operations in the other sequence  You see these minus signs are flipped  In fact what happened is  this composition rotating and then flipping over  is the equivalent of reflecting the whole lot  the original basis vectors in this mirror  and flipping and then rotating round it effect is I've just flipped  my original axis e1 and e2 which is putting a mirror in here  So doing the two operations in opposites in different sequences  doesn't give you the same operations  What we've shown is that matrix multiplication isn't commutative  A2  A1 isn't the same as A1  A2  So we have to be very careful with matrix multiplication  We can do them in any order meaning they're associative  That is if we do A3 to A2  to A1  we can do A2  A1 and then do A3  or we can do A3  A2 to A1  So we could do that  and then that  Those are the same  they're associative  But we can't interchange the order  We can't swap them around  It's not commutative  So this is interesting  There seems to be some deep connection between simultaneous equations  these things called matrices  and the vectors we're talking about in the last module  It turns out the key to solving simultaneous equation problems  is appreciating how vectors are transformed by matrices  Which is the heart of linear algebra 
Huwb9hBus6V,Solving the apples and bananas problem  Gaussian elimination  So  now we're finally going to find a way to solve the apples and bananas problem  And along the way  we're going to find out about a thing called the inverse of a matrix and a method for finding it  So  let's go back to the apples and bananas problem  We said that I walked into a shop and I bought two apples and three bananas  and that that cost 8 dollars or euros or whatever my currency is  And I went to the shop on another day and I buy ten apples and one banana  and I got charged 13 euros for that  So I wrote that down as a matrix times the vector  And I could call that matrix  A  and I call this vector  r  and I could call that output vector  s  So  A operates on r and gives me s  What I want to do is I want to find out what Rr is  what this vector a b is  in order to give me the right numbers for the output vector  Now  let's just think about this matrix  A  for a moment  Let's see if I can think of another matrix  I'm going to call this matrix A to the minus one  that when I multiply it by A  gives me the identity matrix  I call this matrix here the inverse of A because what it does to A is it exactly reverses whatever A does and gives me just the identity  So  if I take this equation  Ar equals s  if I multiply it on the left by A to the minus one on both sides  well  what I find is  is that this is just going to be the identity which is the matrix that does nothing  So  when I multiply it by I  just get r  so then I've got r is equal to A to the minus one times s  the inverse matrix of a times s  So if I could find the inverse of A  the matrix where multiplying by A gives me the inverse  if I could find the inverse of A  I can solve my problem and I can find what my a and b are  and I've solved my apples and bananas problem  Now  hold that thought about inverses for just a moment  So  we said the inverse of A times A was the identity matrix  We'll just park that up there for a moment  Now  I actually don't have to go all the way to the inverse to solve the apples and bananas problem  I can do it just by substitutions  So let's look at a slightly more complicated problem and see what I mean  So  take a matrix that looks like like this  1 1 3  1 2 4  1 1 2  And I'm going to say that I have apples and bananas and carrots  say  Okay they're not a fruit but they're another sort of fruit like object that we eat  So  I'm saying that I bought one apple  one banana  and three carrots  and that cost me  let's say 15 euros  Another day I went and I bought 1  2 and 4  and that cost me 21 euros  I don't know that I bought 1  1  and 2  and that cost me 13 euros  Now  the thing to notice about a system of equations like this is that I can  if I take this row off of another row  I haven't changed anything  right? So  I know that an apple  a banana  and three carrots cost 15  So if I take an apple  a banana  and three carrots  and 15 off of the next row  I haven't changed anything really about that row  and the same for the third row  So  I can make this problem simpler by going and doing that  that sort of a process of elimination  So if I call this row one  row two  and row three  if I take row one off of row two  then I would have  take row one off of row two  I'll have 0 1 1  If I take row one off of row three  I'll have 0 0 minus 1  I'd still have a  b  and c  If I've taken on the right-hand side  I've taken row one  I've taken row one off of row two  so I've taken 15 off of 21  so I'll have six left  I'm taking row one off of row three  so I'll have 13 minus 15 is minus two  Ah  interesting  So now  I can see that actually minus C is equal to minus two  so I can multiply that row three by minus one  Let's change pens  And I've got a solution now for c  that c is equal to two  so that's interesting  Now  I've got what's called a triangular matrix  that is  everything below the body diagonal is zero  And that's a very interesting sort of matrix  I've reduced it to what's called Echelon form  All the numbers below the leading diagonal is zero  Now  I can do what's called back substitution  I can take my answer for c here and I can put my answer for c back into the first two rows  So  if I take one c equals two  I know that one times c here is two here  so I could take that off there and I could do the same  I can take three times that of the first row  So  if I go take three times it off the first row  I'm going to get 1 1 0  a  b  c  and I've taken three times the first one off of here  so I've taken three twos off of here  so I'm going to get nine  And if I take that third row off of the second row  I'm going to get 0 1 0  And I've got two off of six is four  and I haven't changed the last row at all  So now  I know that c here is equal to two  I know that if I multiply that row by that column  I've got just b is equal to four  And I've got a plus b is equal to nine  but I can tell you my answer for b back off this first row  So  I've got then 1 0 0  0 1 0  0 0 1  a  b  c  and I've taken one b  I've taken four off of the first row  so that's 5 4 2  So  my solution here for my apples  bananas and carrot problem is the apples cost five euros  bananas cost four euros  and carrots cost two euros  And that's solved the problem for me  Very nice  So I didn't really have to compute the inverse at all  But I've only found out the answer for this specific output vector  So we said that this was A times r is equal to s  I've only found it for this specific s  If I did the inverse in the general case  I could do it for any s  I could find r for any s  So  what I've done here is I've done what's first  elimination  I've taken multiples of rows and I've taken them off of each other to get to having a triangular form where the bottom corner here is zero  So I've just got ones on the leading diagonal  And then  I've done back substitution  Back substitution  I've gone back up of putting the numbers for c back into the first two rows  and then for b back into the first row  and so on  to get my solution to my problem  And that's very nice  This is actually one of the most computationally efficient ways to solve this problem  and it's going to work every time  and it's very simple to understand how we can do it in relatively few operations  So it's really  really nice  And one thing to notice here is that  in doing so  what I've done is I've transformed A into the identity matrix  This is just the identity matrix  There's ones only on leading diagonals and zeros everywhere else  So  this is going to be a key to a way to find the inverse which is what we'll look at next 
Jubs7Gsy7bs,Going from Gaussian elimination to finding the inverse matrix  Now  let's think about how I can apply this idea of elimination to find the inverse matrix  which solves the more general problem no matter what vectors I write down on the right hand side  Say I have a 3 by 3 matrix A and its inverse B  which I multiply together to get the identity matrix I  So before we had the matrix for A was 1 1 3  1 2 4  and 1 1 2 and I'm going to introducing some notation  I'm going to call B composed of elements b11  b12  b13  where the first digit represents the row  So then I'll have b21  The second digit represents the column  So this would then be b22 and this one will be b row 2 column 3  I have b31  b32  b33  So that is  that's the row and that's the column equals I  What we're saying here is that B  is actually the inverse of A  So that is if I multiplied A by its inverse  I'll get the identity and actually the inverse is special because I can do it either way around  I can apply it A on the right or on the left and it'll still work because I times the inverse of A  is just the inverse of A  so it doesn't really matter which way round I do it  But I'm just doing it here  so I've got some b's to play with  I'm not going to get confused with my a's or my b's  But for this example  I'm saying B is the inverse matrix of A  Now  notice that this first column of B here  is just a vector  It's a vector that describes what the B matrix  the inverse of A  does to space  Actually it's the transformation that that vector does to the x-axis if you like  The identity matrix is just 1 0 0 0 1 0 0 0 1  So I could write this down  I could write down this A  that big square guy  times b11  b21  b31 and I would get just the first column of the identity matrix 1 0 0  Now  I could solve that by my elimination method and back substitution just in the way I did before  except I'd be juggling different numbers here  Then I could do it again for the second column of B  the second vector of B  for the second vector of the identity matrix and I could do it again for the third one  I'd be doing that in some series if you like  but here's the fun bit  I could do it all at once  So I'm going to do this process of elimination and back substitution all at once  I'm going to do it for all the columns on the right hand side simultaneously  So if I take the first row here and I take it off the second row as before  I'm going to get rid of this one  And if I take the first row off of the third row  I'm going to get rid of both this one and this one  So  if I put the unaltered ones down here 1 1 3  if I take that off of the second row  I'm going to get 0 1 1  And if I take that off the third row  I'm going to get  0 0 minus 1  So on the right hand side  I'm going to have the unaltered one is 1 0 0  and I've taken that off of the second row  So I've taken one off that  that's minus 1 1 0 and I've taken that off of the third row as well so I've got minus 1 0 1  And so now  I can multiply that third row through by minus 1 and that's in the form I then want  Where I've got  ones on the leading diagonal and zeroes below it  When I multiply that through by minus 1  I get a plus there  and a minus there  Now  I can substitute that third row back into the second and first rows  so I can take one of it off of the second row and I can take three of it for the first row  So now the unaltered one is the bottom one  1 0 minus 1  If I take one of those off of the second row  then I've got 0 1 0 there  I take that off of the second row over here  So if you take 1 off of minus 1 you get minus 2  Takes 0 of 1 and I've got one  take minus 1 off of zero  I'm effectively adding 1  Then I want to do that again to the first row  I want to take three of them off to make this guy here zero  So I've got then 1 1 0 there and I want to take three of these off the first row  so I take 3 off of 1 gives me minus 2  take 0 off there and I've got to take 3 of minus 1 off the 0 so that gives me plus 3  So we're nearly there I've just got this irritating one here  I've got to take this row off of this row and I'll be home  So if I take  my third rows unaltered  my second row is going to be unaltered  1 0 minus 1  minus 2 1 1  I'll take that row off of that row  then that altered one gives me 1 0 0  take the 1 off there  take this row off of this row  minus 2 off of minus 2  I've got a take minus 2 off of minus 2  so that gives me 0  got to take 1 off of 0  that gives me minus one  And I've got to take 1 off of 3 which gives me 2  So that's my answer  So now  I've got the identity matrix here for A  In effect I've transformed A to its identity matrix  I've got my B matrix which I haven't really changed  my identity matrix over here  I've changed  So now  I've got an identity times a B matrix is equal to this guy  So the identity times something is just itself  So this is in fact  my answer for the inverse of A  or B  So I've found a way here to find the inverse of a matrix just by doing my row elimination and then my back substitution  which is really cool  So what we've done is  we found an inverse matrix A to the minus one here  And if we multiply A times A to the minus one we'll get the identity and prove to yourself if you like  just pause for a moment and have a go at doing that for yourself and verify that that times that does in fact give you the identity matrix  And in school you probably did this a different way but computationally  this way is much easier  particularly when you come to higher dimensions  a hundred rows and columns or something like that  There are computationally faster methods of doing what's called a decomposition process  And in practice what you do in any program that you write  is you simply call the solver of your problem or the function  something like inv(A) or whatever it is and it will pick the best method by inspecting the matrix you give it and return the answer  But the point here is to show how these problems are actually solved in a computer and also we'll observe some of the properties of these methods in different circumstances that will effect the sorts of things we want to do  when solving these sorts of problems  So what we've done here is  figured out how to solve by sets of linear equations in the general case  by a procedure we can implement in a computer really easily  And we've made that general by finding a general method to find the inverse of a matrix  in the general case for whatever is on the right hand side of our system of equations and hopefully that's really satisfying and really nice 
biKeJmEkbA1,Determinants and inverses  In the final video in this module  we're going to look at property of a matrix called the determinant  More happens when the matrix doesn't have linearly independent basis vectors  picking up on the basis vectors we looked at in the last module  Let's go back and look at a simple matrix like a o  o  d  What this matrix does  is it scales space  So if we have our original basis vectors e1 hat an d e2 hat  it scales them out by a factor of a in this direction to a o  and by a factor of d in this direction to o d  and we call those e1 prime and e2 prime  Now  see what I've done to the space  It was originally this size one-by-one  What I've done is I've scaled the space every vector in the space by a factor of a this way and by a factor of d this way and therefore  I have scaled the space by a factor of ad  All areas in the space everything's got bigger by a factor of ad  I call that the determinant of the transformation matrix  Now  if I instead have a matrix which is a o b  d instead the same with a b  what that does is e1 hat stays as it was  it scaled by a factor of a  but e2 hat goes somewhere different  so e1 hat goes to a o  but e2 goes to bd somewhere like this bd  Now  see what's happened to the space  I've gone from being something like this  to being something like this I've made a parallelogram  but the area of the parallelogram is still ad  It's base times its perpendicular height  So the determinant is still ad  of this transformation matrix  Now  if I have a general matrix a  c  b  d and it sounds the original unit square into a parallelogram like this  Where you saw this factor to here this factor to here  Then to find the area of this parallelogram I'm going to have to do a bit of maths  So I'm going do that and then we'll have a look at it in a moment  So I've done the maths and it's here  you can do geometry and puzzle out for yourself and pulls it if you'd like to verify if I'm correct  but the area of the parallelogram here is actually ad minus bc  I'm going to denote the operation of finding that area with vertical lines like this and we call that finding the determinant of a  which you probably saw in school  In school  when you looked at matrices you probably saw that you could find the inverse of a matrix like a  b  c  d  by flipping the terms on the leading diagonal  and taking the minus of the off-diagonal terms for two by two  So let's do that and see what we get  So when we multiply that out  we get ad minus bc  interesting  Then on this element here is a minus b plus ba  So I've got a minus b plus ba so that's zero  When we do this term we get cd minus cd  When we do this term here  we get cb times minus b times minus bc plus ad  So that's ad minus bc  interesting  So that's the determinant  So if I multiply by 1 over the determinant ad minus bc  then these terms will become one and I'll get the identity matrix  So if this was matrix a  and I think if this and this together as being a_-1  then I've got this is in fact when I pre-multiply by one over the determinant  is in fact the inverse of a  So we proved here that inverse you learned in school  is in fact correct  But the interesting thing is that this determinant there  this now that it scales space  If we then take this matrix when we do the flipping around we haven't changed it's scaling of space we need to undo that scaling and bring it back down to a scale of one  So the determinant here is what we need to divide the inverse matrix by in order for it to probably be an inverse  Now we could spend another video looking at the extension of the idea of well actually the form to find out  how to find the determinants in the general case computationally  But this is both tricky to show and it's pointless  Knowing how to do the operations isn't a useful skill anymore because we just type det A into our computer  and polyphenol MATLAB will do it for us  From a learning perspective  it doesn't add much to val echelon  Val echelon does actually we slowly went through it  I'm not going to teach you how to find determinants in the general case  If you want to know  lookup QR decomposition then follow that through and that's how computation you go and find it out  over the linear algebra test book is the other place to look  That's how you do it in the general case  Now let's think about this matrix A here  It transforms e1 and e2 hat to two points on the same line  It transforms e1 hat to 1 1  and it transforms e2 hat from there over to 2 2  They're both points on the same line  They are just a multiple of each other they're not linearly independent  What this matrix in fact does  is it transforms every point in space on a line  Notice that its tone of that matrix is going to be 0  if I take a  d minus b  c determinant of a is nought because the any area was gone onto a lot and therefore that area is nought  So having computed by the geometrically or computationally  you get a determinant of nought  So if I had a three by three matrix with a similar situation describing a 3D space  and if I had the same position where one of the new basis vectors was just a linear multiple of the other two  it wasn't linearly independent  then now would mean the new space was either a plane or if there was only one independent basis vector  a line like we have here  In either case the volume enclose would be zero  so the determinant would be zero  Now  let's turn back to our val echelon form and apply that idea  Let's take this set of simultaneous equations say  You can see that the third row is just the sum of the first two  So row three is equal to row one plus row two  You can see that column three is just given by two of column one plus column two  If you want to pause for a moment to verify that it really is true  So this transformation matrix doesn't describe three independent basis vectors  One of them is linearly dependent on the other two  So this doesn't describe any 3D space it collapses it into a 2D space  So let's see what happens when I try to reduce this val echelon form  If I take off the first row from the second  okay so far so good  I've got that my a b c stays the same  and I take the first one off the second one  So I've got 12 take 12 of 17  I get five  If I then take the first and second ones off the third one  I've now got zeros everywhere here  If I do that on here I take 12 and 17 of 29 I get zero here  So now I've got zero C equals zero  which is sort of true but not useful  Then an infinite number of solutions for C in effect  any value of C would work  So now I can't solve my system of equations anymore  I don't have enough information  So if we think about this from solving simultaneous equations point of view  my mistake was when I went into the shop to buy apples  bananas  and carrots  the third time I went in I just ordered a copy of my first two orders  So I didn't get any new information and I don't have enough data therefore to find out the solution for how much I individual apples and bananas and carrots costs  So what we've shown is that where the basis vectors describing the matrix are linearly independent  then the determinant is zero  and that means I can't solve the system of simultaneous equations anymore  Which means I can't invert the matrix because I can't take one over the determinant either  That means I'm stuck this matrix has no inverse  So there are situations where I might want to do a transformation that collapses the number of dimensions in the space but that will come at a cost  Another way of looking at this is that the inverse matrix lets me undo my transformation  it lets me get from the new vectors to the original vectors  If I've done two dimension by turning a 2D space into a line  I can't undo that anymore  I don't have enough information because I've lost some of it during the transformation  I've lost that extra dimension  So in general  it's worth checking before you propose a new basis vector set and then use a matrix to transform your data vectors  that this is a transformation you can undo  by checking that the new basis vectors are linearly independent  So what we've done in this last video in this module is look at the determinant  how much we grow space  We've also looked at the special case where the determinant is zero  which means that the basis vectors are linearly independent  which means the inverse doesn't exist  In this first module on matrices what we've done so far is define what a matrix is as something that transformed space  We've looked at different arch terms of matrices like rotations and inverse [inaudible] and shears and how to combine them by doing successive transformations  We've looked at how to solve systems of linear equations  by elimination and how to find inverses  Then finally  we've looked at determinants and linear independence 
LObukme4YCa,Matrices changing basis  We have said before that the columns of a transformation matrix  are the axes of the new basis vectors of the mapping in my coordinate system  We're now going to spend a little while looking at how to transform a vector from one set of basis vectors to another  Let's now have two new basis vectors that describe the world of Panda Bear here  and Panda's world is orange  So Panda has got first a basis vector there  and then another basis vector there say  And let's say in my world  Panda Bear's basis vectors are at three  one and at one  one  And my basis vectors here are e2 hat equals nought  one  and and e1 hat is equal to one  nought  So those are my basis vectors and the orange ones are Panda's basis vectors  Now  and so Panda's basis vectors  the first one for Panda  is his one  zero  and the second one is zero  one in Panda's world  So Bear's basis vectors are three  one and one  one in my blue frame  That is  I can write down Bear's transformation matrix as three  one  one  one  Now  let's take some vector I want to transform  Let's say that vector is in Bear's world  is the vector a half of three one in Bear's world  So it's three over two  one over two  So the instruction there is do three over two have of three one and then do one over two a half of one one in my frame if you like  So in my world  that's going to give me the answer of three times three over two  plus one times one over two is nine  ten halves  which is five  and one times three over two  plus one times a half  so that's a total of two  So that's the vector five  two in my world  five  two  Those two are the same thing  So this is Bear's vector  and this is my vector  So this transformation matrix here are Bear's basis in my coordinates  in my coordinate system  So that transforms Bear's vectors into my world  which is a bit of a problem  Usually  I'd want to translate my world into Bear's world  So we need to figure out how to go the way  So my next question is  how do I perform that reverse process? Well  it's going to involve the inverse  So if I call Bear's transformation matrix B  I'm going to want B inverse  B to the minus one  And the inverse of this matrix well  it's actually pretty easy  We can write down the inverse of that matrix pretty easily  It's going to be a half of one  three  flip the elements of the leading diagonal and put a minus on the off diagonal terms  And we can see the determinant of that's three minus one over two  So we divide by the determinant  that's a half  So that's going to be B to the minus one  And that's my basis vectors in Bear's coordinates  So that's my basis in Bear's world  So my one  zero is going to be a half of one minus one in Bear's system  and my zero  one is going to be a half of minus one  three in Bear's system  And we can verify that this is true if we take this guy  a half one minus one  and compose it with Bear's vectors  we've got one plus minus one of those is going give me three plus one is three minus one is two  one minus one is zero  so that's two  zero halve it  gives you one zero  So that really does work  If I take a half one minus one of Bear's vectors  I'll get my unit vector  Okay  So that really does do the reverse thing  So then if I take my vector  which was five  two  and then I do that sum  I should get the world in Bear's basis  So I've got five times a half minus a half using that guy  plus two times minus one  three and that will give me a half of three  two when I multiply all out  And if I do the same thing here I got five times one  minus one times two gives me three over two  gives me three over two  it all works out if you do it that way  or if you do it that way  you'll still get that answer  So that's Bear's vector again  which is the vector we started out with  So that's how you do the reverse process  If you want to take Bear's vector into my world  you need Bear's basis in my coordinate frame  and if you want to do the reverse process  you want my basis in Bear's co-ordinate frame  That's probably quite counter-intuitive  So let's try another example where this time Bear's world is going to be an orthonormal basis vector set  So here's our basis vectors one  zero  zero  one in my world in blue  and Bear's world is in orange  Bear's world has one  one times  and I've made a unit length so it's one over root two  a minus one  one again  unit lengths of one over root two  so there are those two  And those you could do a dot product to verify that those two are at 90 degrees to each other  and they're Bear's vectors one  zero and zero  one  So then I can write down Bear's transformation matrix that transforms a vector of Bear's  Now  if I've got the vector in Bear's world  this two  one  then I can write that down  and I will therefore get the vector in my world  So when I multiply that out  what I get is I'll get one over root two  times two minus one  which is one  and then one times two  plus one times one  gives me three  So in my world of vector is as I've written down  one over root two times one  three  So if I want to do the reverse transformation  I need B to the minus one  B to the minus ones is actually quite easy because this is an orthonormal basis  The determinant of this matrix is one  so it all becomes quite easy  So I just get one over root two  keep the leading terms the same  flip the sign of the off diagonal terms because it's a two by two matrix  that's really simple  And if you go and put that in  if you say  if I take one over root two times one minus one  so I take one of those plus one of those  multiply by root two  I do in fact  get one  zero and the same for zero  one it all works  So then if I take the vector in my world  which is one  three  I multiply it out  then what I get is the vector in Bear's world  So that's one plus three  which is four  one  minus one plus three is two  and I've got one over root two times one over root two  so that's a half  So in Bear's world  this vector is two one which is what we actually said so it really works  Now  this was all prep really for the fun part  which is  we said before in the vectors module that we could do this just by using projections  if the new basis vectors were orthogonal  which these are  So let's see how this works with projections  So let's try it with projections  What we said before was that if I take my version of the vector  and dot it with Bear's axis  so the first of Bear's axis is that in my world  then I will get the answer of the vector in Bear's world  So that gives me one over root two  times one over root two  which is a half of one plus three  which is four  So that gives me two  And that's going to be the first component of Bear's vector because it's the first of Bear's axis  And I can do it again with the other of Bear's axes  So that's one over root two  one  three  that's the vector in my world with the other of Bear's axis  which is one over root two  times minus one  one  And when I do that dot product  what I'll get of one over root two is we'll multiply to give me a half again  and I've got one times minus one  plus three times one  is a total of two  which is one  and that's Bear's vector notice two  one  So I've used projections here to translate my vector to Bear's vector just using the dot product  Now remember  with the vector product  what I'd have to do is I'd have to remember to normalize when I did the multiplication by Bear's vectors  I'd have to normalize by their lengths  But in this case  their lengths are all one  So it's actually really easy  So we don't have to do the complicated matrix maths  we can just use the dot product if Bear's vectors are orthogonal  Now  there is one last thing  If you try this with the example we did before with Bear's vectors of three  one and one  one  So before we had those being Bear's vectors  If you try the dot product with those because they're not orthogonal to each other  it won't work  Give it a go for yourself and verify that that it really won't work  that they need to be orthogonal for this to work  If you have them not being orthogonal  you can still do it with the matrix transformation  you just can't do it with a dot product 
vcxVYxusPI9,Doing a transformation in a changed basis  Now  let's look at how we do the transformation of a vector that's already in a change basis  Last time  we looked at bears basis  Bears basis had a first axis of three  one and a second axis of one  one  Let's say I have a vector x  y defined in bears basis  As I want to transform it by doing something like a rotation of 45 degrees  But the problem is  I don't know how to write a 45 degree rotation in bears funny coordinate system  I only know how to write down a 45 degree rotation in my normal 1  0  0  1 system  So in my system which is 1  0  0  1 a 45 degree rotation rotates 1  0 up like that  So if it's still a unit vector at 1  0 over root 2  1  over root 2  that is a normalized 1  1  It takes 0  1 round to minus 1 over root 2  1 over root 2  That is that 45 degrees there  that 45 degrees there  So I can write it down the rotation in my notation  Let's call it R being a 45 degree rotation  as being 1 over root 2 times 1  1 minus 1  1  That's what a 45 degree rotation is in my world  So what I need to do is first transform the vector x  y into my basis  I do that by multiplying it by B  Then I can apply my nice sensible rotation R to that vector that's now in my basis  So what I get here when I do B  I've got the vector in my coordinate frame  Now the problem is bear doesn't care about my world  he wants to get the rotation in his basis  So then I have to transform the resulting vector  back into bears basis  I do that by applying B to the minus 1  B to the minus 1  I get by flipping the terms on the leading diagonal  taking minus the off-diagonal terms  and dividing by the determinant here which is 2 so I multiply it by a half  So that then gives me  the vector back in bears frame  So overall what I've done  is I've done B to the minus 1 times R  times B  What that's giving me  is it's given me the rotation  in bears coordinate system  Which is really neat  So now what we have to do is do the sums  When we do that R  B gives us this  and B to the -1 RB gives us this  I've written them down there  so pause and ponder if you want to verify those on your own  So this is what a 45 degree rotation looks like in bears coordinate system  Notice that it's completely different to the one in my standard basis  It isn't very easy necessarily or obvious to intuit just out of your head  you have to do the calculation  So if you want to do some translation but in some funny basis  this equation B to the minus 1 RB  is going to be very useful  To step back here  the point is that if we want to transform to normal form normal coordinate systems  then the translation matrices also change  We have to be mindful of that  This is the algebra you see all the time  We've got the transformation matrix R  wrapped around by B  B is the minus 1  that does the translation from my world to the world of the new basis system  So thanks a lot bear  you really helped us out to understand all this stuff  So what we've done in these two videos is we've looked at how the numbers in a vector change  where we change the basis  We thought about the coordinate systems and how to do transformations in non-orthogonal coordinate systems  It's been quite hard work  but this really sets us up for example in principal component analysis  to operate in different basis systems 
9UVgklPAiw2,Orthogonal matrices  Now  it's going to be really useful if we can make a transformation matrix whose column vectors make up a new basis  all of whoms't vectors are perpendicular or what's called Orthogonal to each other  In this video  we're going to look at this and why this is  But first  I want you to find a new operation on the matrix called the transpose  This is where we interchange all the elements of the rows and columns of the matrix  so I'll denote the transpose as T and I'll say that the ijth element of the transpose of A  is equal to the elements on the opposite diagonal A_ji  So if I had a matrix like 1 2 3 4  and I wanted to know its transpose  then I will interchange the elements that are off the diagonal  So the one and the four stay where they are  Because if is  I find i and j are the same  1 1 1 1 they would stay the same  the same for 2 2  But the element 1 2 interchange to the element 2 1 so they go like that  So 1 2 3 4 becomes  when I transpose it 1 3 2 4  Now  let's imagine that I have a square matrix of dimension n by n  it defines a transformation with a new basis and the columns of that transformation matrix A  Some vectors a_1  a_2 that are the basis vectors in the new space as we've said before and I'm going to make this a special matrix while I impose the condition that these vectors are orthogonal to each other and they have unit length  That is  a_i dotted with a_j is equal to zero if i isn't equal to j  that is they're orthogonal  and it's equal to one if i is equal to j  that is they are of unit length  Now  let's think about what happens if I multiply this matrix on the left by a transpose  So A transpose is going to be given by just flipping the rows and columns of all of the elements across the leading diagonal of A  So that is  I'm going to have a_1 is going to become a row  a_2 is going to become the next row all the way down to a_n because I flip all of the elements across the leading diagonal  So while I multiply A_T by A  let's see what happens  it's going to be quite magical  So if I get A_T times A  then I'm going to get  well  the first element here is going to be the first row times the first column  and that's a_1 dotted with a_1  and that's one  The second element here  the first row and the second column is a_1 dotted with a_2 and that's zero  and a_1 dot a_3 all the way up to a_n is going to give me a whole series of zeros where this guy remember is a_n  Then when I do the second row on the first column  I'm going to get another zero  I want to do the second one and the second column I'm going to get a one  Second row  third column zero all the way across  I want to do the third one  I'm going to get the same again  I'm going to get zero zero  It only says it's a_3 times a_3  and that's going to be that element all the way across and what I'm gradually building up here  is I'm building up the identity matrix  So what I found is that A_t times A gives me the identity matrix  and what that means is  is that A_T is a valid inverse of A  Which is really kind of neat  A set of unit length basis vectors that are all perpendicular to each other are called an orthonormal basis set  and the matrix composed of them is called an orthogonal matrix  One thing also to know about an orthogonal matrix is that because all the basis vectors  any of unit length  it must scale space by a factor of one  So the determinant of an orthogonal matrix must be either plus or minus one  The minus is what arises in the new basis  if the new basis vector set flits space around  They stay inverted  they make it left-handed  Notice that if A_T is the inverse then I should be able to post multiply A by A_T and get the identity  So I can do this on the way round  Which also means by the same logic that the rows of the orthogonal matrix are orthonormal  as well as the columns which is neat  and we saw in the last video that actually the inverse is the matrix that does the reverse transformation  So the transpose matrix of an orthonormal basis vector set is itself another orthogonal basis vector set which is really neat  Now  remember that in the last module on vectors we said that transforming a vector to a new coordinate system was just taking the projection or dot product of that vector  so say that's the vector  with each of the basis vectors so basis factor this one  basis factor that one  and so on so long as those basis vectors were orthogonal to each other  If you want to pause and think about that for a moment in the light of all we've learned about matrices  just look and think about this for a moment  Now in data science what we're really saying here is that wherever possible  we want to use an orthonormal basis vector set when we transform our data  That is  we want our transformation matrix to be an orthogonal matrix  That means the inverse is easy to compute  It means the transmission is reversible because it doesn't collapse space  It means that the projection is just the dot product  Lots of things are nice and pleasant  and easy  If I arrange the basis vectors in right order  then the determinant is one  and that's an easy way to check and if they aren't just exchange a pair of them and actually then they will be determinant one rather than the minus one  So what we've done in this video is look at the transpose and that's led us to find out about the most convenient basis factor set of all which is the orthonormal basis factor set which together make the orthogonal matrix whose inverse is its transpose  So that's really cool 
LKopqruvB7k,The Gramâ€“Schmidt process  We've said several times now that life is much easier if we can construct an orthonormal basis vector set  but we haven't talked about how to do it  So in this video we'll do that starting from the assumption that we already have some linearly independent vectors that span the space we're interested in  So let's say I have some vectors V  and I've got a whole group of them v1  v2  all the way up to vn  There's enough of them that they span the space  So let's sort of sketch them out  So I've got to say a v1 here  we have a v2 over here  another v3 down there somewhere and they're linearly independent  let's assume that  If you want to check linear independence you can write down their columns in a matrix and check the determinant isn't zero  If they were linearly dependent  that would give you a zero determinant  But they aren't orthogonal to each other or of unit length  My life would probably be easier if I could construct some orthonormal basis somehow  And there's a process for doing that which is called the Gram-Schmidt process  which is what we're going to look at now  Let's take arbitrarily  the first vector in my set  call him v1  so we take v1  In this first step she  lets call him she  she gets to survive unscathed  So we're just going to normalize her and we're going to say that my eventual first basis vector e  is going to be equal to v1  just normalized to be of unit length  Just divided by that  its length  So  e is just going to be some normalized version of v1  And I can now think of v2 as being composed of two things  One is a component  let's do this in orange  a component that's in the direction of e1 like that  plus a component that's perpendicular to e1  But the component that's in the direction of e1  I can find by taking the vector projection v2 onto e1  So I can say v2 is equal to the vector projection of v2 onto e1 dotted together  And if I want to get that actually as a vector  I'll have to take e1  which is of unit length so I'd have to divide by the length of e1 but the length of e1 is one  So  forget it  And if I take that off of v2  then I'll have this guy  and let's call him u2  So I can then say that u2  so plus u2  so I can then rearrange this and say that u2 is then equal to v2 minus this projection v2 e1 times e1  And if I normalize u2  if I take u2 divided by its length  then I'll have a unit vector which is going to be normal to v1  So if I take a normalized version of that  let's say it's that  that will be e2  And that will be at 90 degrees to e1  So it'll actually be there  e2  once I've moved it over  And that will be another unit length vector normal to e1  So that's the first part of taking an orthonormal basis  Now my third vector v3 isn't a linear combination of v1 and v2  so v3 isn't in the plane defined by v1 and v2  So it's not in the plane of e1 and e2 either  So I can project v3 down  let's say something like that  onto the plane of e2 and e1  and that projection will be some vector in the plane composed of e2s and e1s  So I can then write down that v3 minus v3 dotted with e1  e1's  That's going to be the component v3 that's made up of e1's  minus v3 dotted with e2  e2's  That's the component of v3 that's made up of e2's  And then all that's going to be left is going to be this perpendicular guy there  so that's going to be a perpendicular vector which we'll call u3  which is perpendicular to the plane  This is some funny 3D space so the diagram gets quite messy  And then if I normalize u3  divide by the length of u3  then I'll have a unit vector which is normal the plane  normal to the other two  So now I've got an orthonormal basis for e1  e2  e3  and I can keep on going through all the vn's until I've got enough orthonormal basis vectors to complete the set and span  the space that I originally had  But I've gone from a bunch of awkward  non-orthogonal  non-unit vectors to a bunch of nice orthogonal unit vectors  an orthonormal basis set  So that's how I construct an orthonormal basis set  I make my life easy so that my transformation vectors are nice  my transformation matrices are nice  sorry  And so that I can do the transposes  the inverse  and all those lovely things  So I can use dot product projections for the transformations  all those nice things that are going to make my life very very much nicer whenever I'm doing any transformations  or rotations  or whatever it is I want to do with my vectors  So that's going to be really nice  This is a really nice process  And what we'll do next is we'll apply this  we'll try this for an example and see how it rolls and then apply that to doing a transformation 
uvdcGF6S5kj,Example  Reflecting in a plane  [MUSIC] Okay  so let's put all this together  Let's use our transformations knowledge and our basis knowledge in order to do something quite tricky  And see if we can't actually make our life quite simple  What I want to do here is know what a vector looks like when I reflect it in some funny plane  For example  the way this board works  when I write on the whiteboard here  if you're looking at it  all the writing would appear mirrored  But what we do to make that work is we reflect everything in post production  left-right  and then everything comes out okay  The example we're going to do here asks what the reflection of Bear  say  something in a mirror would look like to me if the mirror was off at some funny angle  Now my first challenge is going to be that I don't know the plane of the mirror very well  But I do know two vectors in the mirror  (1  1  1) and (2  0  1)  And I've got a third vector  which is out of the plane of the mirror  which is at (3  1  -1)  that's my third vector  So I've got vectors v1  v2  and v3  and these two guys are in the plane of the mirror  We could draw it something like v1 and v2  and they're in some plane like this  and v3 is out of the plane  So I have got v3 there  v1  and v2  So first let us do the Gram-Schmidt process and find some orthonormal vectors describing this plane and its normal v3  So my first vector e1 is going to be just the normalised version of v1  v1 here is of length root 3  1 squared plus 1 squared plus 1 squared all square rooted  So it's going to be over root 3 times (1  1  1)  that's a normalised version of v1  So then we can carry on  and I can find u2 = v2- some number of e1's  So then the sum number's going to be a projection of v2 onto e1  times e1  So that's going to be (2  0  1) -(2  0 1) dotted with e1  which is 1 over root 3  (1 1 1) times 1 over root 3 (1 1 1)  because that's e1  So that's (2  0  1) minus  the root 3s are going to come outside  so I can just have them being a third  (2  0  1) dotted with (1  1  1) is 2 plus 0 plus 1 is 3  so that actually goes and has a party and becomes 1  And yeah  okay  I confess  I fixed the example  So it's (2  0  1)-(1 1 1)  which is going to give me (1  -1  1-1 is 0)  So (1 -1  0)  that's u2  Now if I want to normalize u2  I can say e2 is equal to the normalised version of u2  Which is just 1 over root 2 times (1 -1  0)  So then I just need to find u3  and I can do the same again with u3  I can say that that's equal to v3- (the projection of v3 onto e1)  -(the projection of v3 onto e2)  So that's going to be (3  1  -1)- (3 1 -1) dotted with 1 over root 3 (1  1  1)]  and that's a number  And it's going in the direction of the unit vector v1- v3 dotted with e2  So that's (3  1  -1) dotted with 1 over root 2 (1  -1  0)  times e2  which is 1 over root 2 (1  -1  0)  So it's quite a complicated sum  But I've got an answer here which I can do  (3  1  -1) minus  the 1 over root 3s come out again  3 plus 1 minus 1 is 3  so that goes  Then I've got (1  1  1) there  so that becomes 1  Minus  the halves are going to come out  the 1 over root 2s  I've got 3 minus 1 minus 0  so that's 2  so they cancel and become one again  As I said  I fixed the example to make my life easy  So then I've got (3  1  -1) -(1  1  1)-(1  -1  0)  So therefore  I get an answer for u2 being (3  -1  -1)  so that's 1  1 minus 1 is 0  minus -1 is plus 1  -1 minus 1 is -2  And so I can then normalize that and get e3  So e3 is just the normalised version of that  which is going to be 1 over root 6 of (1  1  -2)  Now let's just check  so (1  1  -2) is normal to (1  -1  0)  it is normal to (1 1 1)  Those two are normal to each other  so they are a normal basis set  Just need to make sure that's 1 over root 6  they are all of unit length  So I can write down my new transformation matrix  which I'm going to call e  It's the transformation matrix described by the basis vectors e1  e2  e3  So I've got e1  e2  e3 all written down as column vectors  And that's going to be my transformation matrix  that first contains the plane  notice  and then contains the normal to the plane  So I've redrawn everything just to get it all more compact so we can carry on  We've got our original two vectors v1 and v2  and we've defined e1 to be the normalised version of v1  And we've defined e2 to be the perpendicular part of v2 to e1  normalised to be of unit length  So these all are in a plane  and then e3 is normal to that plane  It's the bit of v3 that we can't make by projecting on to v1 and v2  then of unit length  Now say I've got a vector r over here  r  Now what I want to do is reflect r down through this plane  So I'm going to drop r down through this plane  There he is when he intersects the plane  And then out the other side to get a vector r prime  And let's say that r has some number like  I don't know  (2  3  5)  (2  3  5)  Now this is going to be really awkward  this plane's off at some funny angle composed of these vectors  And even these basis vectors adds up some funny angle  and then how do I drop it down and do the perpendicular? There's going to be a lot of trigonometry  But the neat thing here is that I can think of r as being composed of a vector that's in the plane  so some vector that's composed of e1s and e2s  Sorry  so here's e1 here  And some vector that's normal  so some vector that's some number of e3's  And when I reflect it through the bit that's in the plane is going to be the same  But this bit that's some number of e3's  this bit here  I'm just going to make into minus this bit here  So if I wrote that down as a transformation matrix  the transformation matrix in my basis E  is going to be to keep the e1 bit the same  keep the e2 bit the same  So that's the e1 bit  that's the e2 bit  and then reflect the e3 bit from being up to being down  (0  0  -1)  so that's a reflection matrix in e3  so that's a reflection in the plane  So just by thinking about it quite carefully  I can think about what the reflection is  And that T_E is in the basis of the plane  not in my basis  but in the basis of the plane  So that is easy to define  And therefore  if I can get the vector r defined in the plane's basis vector set  in the E basis  I can then do the reflection  And then I can put it back into my basis vector set  And then I have the complete transformation  So a way of thinking about that is that if I've got my vector r  and I'm trying to get it through some transformation matrix to r prime  But that's going to be hard  that's going to be tricky  but I can transform it  into the basis of the plane  So I can make an r in the basis of the plane  and I'm going to do that using E to the -1  E to the -1 is the thing that got me into  I've got a vector of mine  into Bear's basis  remember  Then I can do my transformation  I can do my reflection in the basis of the plane  I can do my reflection transformation and then I will get r  That's been transformed  primed  in the basis of the plane  Then I can read that back into my basis by doing E  because E is the thing that takes Bear's vector and puts it back into my basis  So I can avoid the hard thing by going round  doing these three operations  So r  E to the minus 1  E inverse  T in the E basis  E  If I do those three things  I've done the complete transformation  and I get r prime  So this problem reduces to doing that matrix multiplication  So we've got that  we've got that  so we can just do the math now  and then we'll be done  So I've just put the logic up there so that I can have the space down here for later  And I've put the transformation we're going to do there  A couple of things to note  one is because E  we've carefully constructed by our Gramâ€“Schmidt process to be orthonormal  we know that E transpose is the inverse  So calculating the inverse here isn't going to be a pain in the neck  The other thing is  compared to the situation with Bear where we're changing bases  here we're changing from our vector r to Bear's  or actually the plane's coordinate system  Then we're doing the transformation of the reflection in the plane  and then we're coming back to our basis  So the E and and the E to the -1 are flipped compared to the last video  because we're doing the logic the other way around  It's quite neat  right? The actual multiplication of doing this isn't awfully edifying  it doesn't build you up very much  it's just doing some arithmetic  So I'm just going to write it down here  and if you want to verify it you can pause the video  and then we'll come back and we'll comment on it  So this is TE times the transpose of E  then I take that and multiply it by E itself  And I get E TE E transpose  which is this guy  simplifies to this guy  so that's T  All comes out quite nicely  so that's very  very nice  So then we can apply that to r  So we can say that T times r is equal to T times our vector (2  3  5)  and that's going to give us r prime  And that gives us an answer of one-third of (11  14  5)  So r prime here is equal to one third of (11  14  5)  So that's a process that would've been very  very difficult to do with trigonometry  But actually  with transformations  once we get into the plane of the mirror  and the normal to the mirror  then it all becomes very easy to do that reflection operation  And it's quite magical  it's really amazing  So that's really nice  right? It's really cool  So what we've done here is we've done an example where we've put everything we've learned about matrices and vectors together to describe how to do something fun like reflect a point in space in a mirror  This might be useful  for instance  if you want to transform images of faces for the purpose of doing facial recognition  Transform my face from being like to being like that  And then we could use our neural networks  our machine learning to do that facial recognition part  In summary  this week we've gone out into the world with matrices and learned about constructing orthogonal bases  changing bases  and we've related that back to vectors and projections  So it's been a lot of fun  And it sets us up for the next topic  which Sam is going to lead  on eigenvalues and eigenvectors  [MUSIC]
HSDFbuVY3Wl,What are eigenvalues and eigenvectors? The word  'eigen' is perhaps most usefully translated from the German as meaning characteristic  So when we talk about an eigenproblem  we're talking about finding the characteristic properties of something  But characteristic of what? This module  like the previous weeks  will try and explain this concept of eigen-ness primarily through a geometric interpretation  which allows us to discuss images rather than immediately getting tangled up in the maths  When I first learned this topic  not only did we start by grinding through formulae  we also spent very little time discussing what all the maths was doing  This topic is often considered by students to be quite tricky  But it's my belief that once you know how to sketch these problems  the rest is just algebra  So  as you've seen from previous weeks  it's possible to express the concept of linear transformations using matrices  These operations can include scalings  rotations  and shears  Often  when applying these transformations  we are thinking about what they might do to a specific vector  However  it can also be useful to think about what it might look like when they are applied to every vector in this space  This can be most easily visualized by drawing a square centered at the origin  and then seeing how your shape is distorted when you apply the transformation  So if we apply a scaling of two in the vertical direction  the square would now become a rectangle  Whereas  if we applied a horizontal shear to this space  it might look something like this  Now  here's the key concept  we are using our little square to help us visualize what is happening to many vectors  But notice that some vectors end up lying on the same line that they started on whereas  others do not  To highlight this  I'm going to draw three specific vectors onto our initial square  Now  let's consider our vertical scaling again  and think about what will happen to these three vectors  As you can see  the horizontal green vector is unchanged pointing in the same direction and having the same length  The vertical pink vector is also still pointing in the same direction as before but its length is doubled  Lastly  the diagonal orange vector used to be exactly 45 degrees to the axis  it's angle has now increased as has its length  I hope you can see that actually besides the horizontal and vertical vectors  any other vectors' direction would have been changed by this vertical scaling  So in some sense  the horizontal and vertical vectors are special  they are characteristic of this particular transform  which is why we refer to them as eigenvectors  Furthermore  because the horizontal vectors' length was unchanged  we say that it has a corresponding eigenvalue of one whereas  the vertical eigenvector doubled in length  so we say it has an eigenvalue of two  So  from a conceptual perspective  that's about it  for 2D eigen-problems  we simply take a transform and we look for the vectors who are still laying on the same span as before  and then we measure how much their length has changed  This is basically what eigenvectors and their corresponding eigenvalues are  Let's look at two more classic examples to make sure that we can generalize what we've learned  Here's our marked up square again  And now let's look at pure shear  where pure means that we aren't performing any scaling or rotation in addition  so the area is unchanged  As I hope you've spotted  it's only the green horizontal line that is still laying along its original span  and all the other vectors will be shifted  Finally  let's look at rotation  Clearly  this thing has got no eigenvectors at all  as all of the vectors have been rotated off their original span  In this lecture  we've already covered almost all of what you need to know about eigenvectors and eigenvalues  Although we've only been working in two dimensions so far  the concept is exactly the same in three or more dimensions  In the rest of the module  we'll be having a look at some special cases  as well as discussing how to describe what we've observed in more mathematical terms 
3MFhuyekL9O,Special eigen-cases  As we saw previously  eigenvectors are those which lie along the same span both before and after applying a linear transform to a space  And then  eigenvalues are simply the amount that each of those vectors has been stretched in the process  In this video  we're going to look at three special cases to make sure the intuition we've built so far is robust  and then we're going to try and extend this concept into three dimensions  The first example we're going to consider is that of a uniform scaling  which is where we scale by the same amount in each direction  As you will hopefully have spotted  not only are all three of the vectors that I've highlighted eigenvectors  but in fact  for a uniform scaling  any vector would be an eigenvector  In this second example  we're going to look at rotation  In the previous video  we applied a small rotation  and we found that it had no eigenvectors  However  there is one case of non-zero pure rotation which does have at least some eigenvectors  and that is 180 degrees  As you can see  the three eigenvectors are still laying on the same spans as before  but just pointing in the opposite direction  This means that once again  all vectors for this transform are eigenvectors  and they all have eigenvalues of minus one  which means that although the eigenvectors haven't changed length  they are all now pointing in the opposite direction  This third case  we're going to look at a combination of a horizontal shear and a vertical scaling  and it's slightly less obvious than some of the previous examples  Just like the pure shear case we saw previously  the green horizontal vector is an eigenvector and its eigenvalue is still one  However  despite the fact that neither of the two vectors shown are eigen  this transformation does have two eigenvectors  Here  I've now added the second eigenvector on to the image  and it shows us that although the concept is fairly straightforward  eigenvectors aren't always easy to spot  Let's now apply the inverse transform and watch our parallelogram go back to its original square  But this time  with our eigenvector visible  Hopefully  you're at least convinced that it is indeed an eigenvector as it stays on its own span  This problem is even tougher in three or more dimensions  and many of the uses of eigen theory in machine learning frame the system as being composed of hundreds of dimensions or more  So  clearly  we're going to need a more robust mathematical description of this concept to allow us to proceed  Before we do  let's just take a look at one quick example in 3D  Clearly  scaling and shear are all going to operate in much the same in 3D as they do in 2D  However  rotation does take on a neat new meaning  As you can see from the image  although both the pink and green vectors have changed direction  the orange vector has not moved  This means that the orange vector is an eigenvector  but it also tells us  as a physical interpretation  that if we find the eigenvector of a 3D rotation  it means we've also found the axis of rotation  In this video  we've covered a range of special cases  which I hope have prompted the questions in your mind about how we're going to go about writing a formal definition of an eigen-problem  And this is exactly what we're going to be discussing next time  See you then 
KPqL1e8G8le,Calculating eigenvectors  [MUSIC] Hopefully  you will all now have a reasonable feeling for what an eigen-problem looks like geometrically  So in this video  we're going to formalise this concept into an algebraic expression  which will allow us to calculate eigenvalues and eigenvectors whenever they exist  Once you've understood this method  we'll be in a good position to see why you should be glad that computers can do this for you  If we consider a transformation A  what we have seen is that if it has eigenvectors at all  then these are simply the vectors which stay on the same span following a transformation  They can change length and even point in an opposite direction entirely  But if they remain in the same span  they are eigenvectors  If we call our eigenvector x  then we can say the following expression  Ax = lambda x  Where  on the left hand side  we're applying the transformation matrix A to a vector x  And on the right-hand side  we are simply stretching a factor x by some scalar factor lambda  So lambda is just some number  We're trying to find values of x that make the two sides equal  Another way of saying this is that for our eigenvectors  having A apply to them just scales their length or does nothing at all  which is the same as scaling the length by a factor of 1  So in this equation  A is an n dimensional transform  meaning it must be an n by n square matrix  The eigenvector x must therefore be an n-dimensional vector  To help us find the solutions to this expression  we can rewrite it by putting all the terms on one side and then factorizing  So (A - lambda I) x = 0  If you're wondering where the I term came from  it's just an n by n identity matrix  which means it's a matrix the same size as A but with ones along the leading diagonal and zeros everywhere else  We didn't need this in the first expression we wrote  as multiplying vectors by scalars is defined  However  subtracting scalars from matrices is not defined  so the I just tidies up the maths  without changing the meaning  Now that we have this expression we can see that for the left-hand side to equal 0  either the contents of the brackets must be 0 or the vector x is 0  So we're actually not interested in the case where the vector x is 0  That's when it has no length or direction and is what we call a trivial solution  Instead  we must find when the term in the brackets is 0  Referring back to the material in the previous parts of the course  we can test if a matrix operation will result in a 0 output by calculating its determinant  So  det (A - lambda I) = 0  Calculating the determinants manually is a lot of work for high dimensional matrices  So let's just try applying this to an arbitrary two by two transformation  Let's say  A = (a  b  c  d)  Substituting this into our eigen-finding expression gives the following  Det (a b c d)- lambda 0 0 lambda  Evaluating this determinant  we get what is referred to as the characteristic polynomial  which looks like this  So lambda squared -(a + d) lambda + ad - bc = 0  Our eigenvalues are simply the solutions of this equation  and we can then plug these eigenvalues back into the original expression to calculate our eigenvectors  Rather than continuing with our generalized form  this is a good moment to apply this to a simple transformation  for which we already know the eigensolution  Let's take the case of a vertical scaling by a factor of two  which is represented by the transformation matrix A = 1  0  0  2  We can then apply the method that we just described and take the determinant of A minus lambda I and then set it to zero and solve  So det (1- lambda  0  0  2- lambda) = 1- lambda  2- lambda  which is of course equal to 0  This means that our equation must have solutions at lambda equals 1 and lambda equals 2  Thinking back to our original eigen-finding formula  (A - lambda I) x = 0  We can now sub these two solutions back in  So thinking about the case where lambda = 1  we can say (1- 1  0  0  2- 1) times this x vector  x1 and x2  must equal to (0  0  0  1) x1  x2  therefore we've got 0 and x2 must equal 0  Now  thinking about the case where lamda equals 2  at lamda = 2  you get 1- 2  and 2- 2  And then you get of course minus 1  0  0  0  Which equals to minus x1  0  which equals zero  So what do these two expressions tell us? Well  in the case where our eigenvalue lambda equals one  we've got an eigenvector where the x2 term must be zero  But we don't really know anything about the x1 term  Well  this is because  of course any vector that points along the horizontal axis could be an eigenvector of this system  So we write that by saying @ lambda = 1  x  our eigenvector  can equal anything along the horizontal axis  as long as it's 0 in the vertical direction  So we put in an arbitrary parameter t  Similarly for the lambda = 2 case  We can say that our eigenvector must equal 0 t  Because as long as it doesn't move at all in the horizontal direction  any vector that's purely vertical would therefore also be an eigenvector of this system  as they all would lie along the same span  So now we have two eigenvalues  and their two corresponding eigenvectors  Let's now try the case of a rotation by 90-degrees anti-clockwise  to ensure that we get the result that we expect which  if you remember  is no eigenvectors at all  The transformation matrix corresponding to a 90-degree rotation is as follows  A = (0  -1)  (1  0)  So applying the formula once again we get the det (0- lambda- 1)  (1  0- lambda)  which if you calculate this through  comes out to lambda squared + 1 = 0  Which doesn't have any real numbered solutions at all  Hence  no real eigenvectors  We can still calculate some complex eigenvectors using imaginary numbers  but this is beyond what we need for this particular course  Despite all the fun that we've just been having  the truth is that you will almost certainly never have to perform this calculation by hand  Furthermore  we saw that our approach required finding the roots of a polynomial of order n  i e   the dimension of your matrix  Which means that the problem will very quickly stop being possible by analytical methods alone  So when a computer finds the eigensolutions of a 100 dimensional problem it's forced to employ iterative numerical methods  However  I can assure you that developing a strong conceptual understanding of eigen problems will be much more useful than being really good at calculating them by hand  In this video  we translated our geometrical understanding of eigenvectors into a robust mathematical expression  and validated it on a few test cases  But I hope that I've also convinced you that working through lots of eigen-problems  as is often done in engineering undergraduate degrees  is not a good investment of your time if you already understand the underlying concepts  This is what computers are for  Next video  we'll be referring back to the concept of basis change to see what magic happens when you use eigenvectors as your basis  See you then  [MUSIC]
AB6eB_G8K1p,Changing to the eigenbasis  [MUSIC] So now that we know what eigenvectors are and how to calculate them  we can combine this idea with a concept of changing basis which was covered earlier in the course  What emerges from this synthesis is a particularly powerful tool for performing efficient matrix operations called diagonalisation  Sometimes  we need to apply the same matrix multiplication many times  For example  imagine a transformation matrix T represents the change in location of a particle after a single time step  So we can write that our initial position  described by vector v0  multiplied by the transformation T gives us our new location  v1  To work out where our particle will be after two time steps  we can find v2 by simply multiplying v1 by T  which is of course the same thing as multiplying v0 by T two times  So v2 equals T squared times v0  Now imagine that we expect the same linear transformation to occur every time step for n time steps  Well we can write vn is T to the power of n  times v0  You've already seen how much work it takes to apply a single 3D matrix multiplication  So if we were to imagine that T tells us what happens in one second  but we'd like to know where our particle is in two weeks from now  then n is going to be around 1 2 million  i e   we'd need to multiply T by itself more than a million times  which may take quite a while  If all the terms in the matrix are zero except for those along the leading diagonal  we refer to it as a diagonal matrix  And when raising matrices to powers  diagonal matrices make things a lot easier  In fact  have a go just now to see what I mean  All you need to do is put each of the terms on the diagonal to the power of n and you've got the answer  So in this case  T to the n is a to the n  b to the n  and c to the n  It's simple enough  but what if T is not a diagonal matrix? Well  as you may have guessed  the answer comes from eigen-analysis  Essentially  what we're going to do is simply change to a basis where our transformation T becomes diagonal  which is what we call an eigen-basis  We can then easily apply our power of n to the diagonalized form  and finally transform the resulting matrix back again  giving us T to the power of n  but avoiding much of the work  As we saw in the section on changing basis  each column of our transform matrix simply represents the new location of the transformed unit vectors  So  to build our eigen-basis conversion matrix  we just plug in each of our eigenvectors as columns  C equals eigenvector 1  eigenvector 2  and eigenvector 3 in this case as we are using a three-dimensional example  However  don't forget that some of these maybe complex  so not easy to spot using the purely geometrical approach but they are appear in the maths just like the others  Applying this transform  we find ourselves in a world where multiplying by T is effectively just a pure scaling  which is another way of saying that it can now be represented by a diagonal matrix  Crucially  this diagonal matrix  D  contains the corresponding eigenvalues of the matrix T  So D equals lambda 1  lambda 2  and lambda 3  with 0's elsewhere  We're so close now to unleashing the power of eigen  The final link that we need to see is the following  Bringing together everything we've just said  it should now be clear that applying the transformation T is just the same as converting to our eigenbasis  applying the diagonalized matrix  and then converting back again  So T = CDC inverse  which suggests that T squared can be written as CDC inverse  multiplied again by CDC inverse  So hopefully you've spotted that in the middle of our expression on the right-hand side  you've got C multiplied by C inverse  But multiplying a matrix and then by its inverse is just the same as doing nothing at all  So we can simply remove this operation  Equals CDDC inverse  And then we can finish this expression by saying  well this must be CD squared C inverse  We can of course then generalize this to any power of T we'd like  So finally we can say that T to the power of n is going to equal CD to the power of n multiplied by C inverse  We now have a method which lets us apply a transformation matrix as many times as we'd like without paying a large computational cost  This result brings together many of the ideas that we've encountered so far in this course and in the next video we'll work through a short example just to ensure that this approach lines up with our expectations when applied to a simple case  See you then  [MUSIC]
MNLk7eBG8ku,Eigenbasis example  Now that we've walked through the theory of eigenbasis and diagonalisation  let's have a go at a simple 2D example  where we can see the answer graphically  so we'll know whether or not our method has worked as expected  Consider the transformation matrix  T equals 1 1 0 2  Hopefully  you'd feel fairly comfortable at this point in drawing the transformed square and vectors that we used in the previous examples  As the first column is just 1 0  this means that our i hat vector will be unchanged  However  the second column tells us that j hat  the second vector will be moving to the point 1 2  Let's also consider the orange diagonal vector to point 1 1  Multiplying through  gives us 1 1 0 2 multiplied by 1 1  So thinking about rows times columns  we're going to get 1 plus 1 and 0 plus 2  which gives us 2 2  It's interesting to consider that this particular transform could be decomposed into a vertical scaling by a factor of 2  and then a horizontal shear by a half step  Because we've chosen such a simple transformation  hopefully  you've already spotted the eigenvectors and can state their eigenvalues  These are at lambda equals 1  our eigenvector is 1 0  And at lambda equals 2  our eigenvector equals 1 1  Now  let's consider what happens to the vector  minus 1 1 when we apply T  So 1 1 0 2 apply to minus 1 1  This time is going to equal rows times columns  minus 1 plus 1  and 0 plus 2 which equals 0 2  And if we apply T again  we're going to get the following  1 1 0 2 applied to 0 2  which is going to equal rows times columns again  0 plus 2 and 0 plus 4  so this thing finally  is going to equal 2 4  Now instead  if we were to start by finding T squared  so T squared is going to equal this matrix multiplied by itself  So applying rows times columns  we're going to get one times one plus one times zero  that's one  Rows times columns here  we're going to get three  Rows times columns here  we're going to get zero  Rows times columns here  we're going to get a four  Now  we can apply this to our vector and see if we get the same result  So  1 3 0 4 multiplied by minus 1 1  is going to equal  rows times columns  so we're going to get minus 1 plus 3 and 0 plus 4  which of course  equals 2 4  We can now try this whole process again  by using our eigenbasis approach  We've already built our conversion matrix  C from our eigenvectors  So  C is going to equal 1 0 1 1  But we are now going to have to find its inverse  However  because we picked such a simple 2 by 2 example  it's possible just to write this inverse down directly by considering that C would just be a horizontal shear  one step to the right  So  C inverse must just be the same shift back to the left again  So  C inverse is going to equal 1 minus 1 0 1  It's worth noting that despite how easy this was  I would still always feed this to the computer instead of risking making silly mistakes  We can now construct our problem  So T squared is going to equal C D squared C inverse  which of course in our case  is going to equal 1 1 0 1 multiplied by our diagonal matrix  which is going to be 1 and 2  and that's all squared  multiplied by C inverse which is 1 minus 1 0 1  Working this through  we can see that  let's keep this first matrix  1 1 0 1  and work out this bit  So we'll say  this is going to be 1 and 4 on the diagonal  1 minus 1 0 1  And let's work out these two matrices here  So you've got 1 1 0 1 multiplied by  so we're doing rows times columns in each case  So for example  1 0 times 1 0  you get a 1 here  And do the second row and the first column  we get a zero there  First row and second column we're going to get a minus 1 there  And the second row and the second column  we're going to go get four here  Okay  And I'm working it through one more step  We're going to see that we get more  more grinding  We get first row  first column  one  Second row  first column  zero  First row  second column is three  And second row  second column is four  And applying this to the vector  minus 1 1  we're going to get something like this  so 1 3 0 4 applied to minus 1 1  is going to be rows times columns  so minus 1 plus 3  and 0 plus 4  which equals 2 4  Which pleasingly enough  is the same result as we found before  Now  there is a sense in which for much of mathematics  once you're sure that you've really understood a concept  then because of computers  you may never have to do this again by hand  However  it's still good to work through a couple of examples on your own just to be absolutely sure that you get it  There are of course many aspects of eigen-theory that we haven't covered in this short video series including un-diagonisable matrices and complex eigenvectors  However  if you are comfortable with the core topics that we've discussed  then you're already in great shape  In the next video  we're going to be looking at real world application of eigen theory to finish off this linear algebra course  This is a particularly famous application which requires abstraction away from the sort of geometric interpretations  that we've been using so far  Which means that you'll be taking the plunge and just trusting the maths  See you then 
AbklM0O-le1,Introduction to PageRank  [MUSIC] The final topic of this module on Eigenproblems  as well as the final topic of this course as a whole  will focus on an algorithm called PageRank  This algorithm was famously published by and named after Google founder Larry Page and colleagues in 1998  And was used by Google to help them decide which order to display their websites when they returned from search  The central assumption underpinning page rank is that the importance of a website is related to its links to and from other websites  and somehow Eigen theory comes up  This bubble diagram represents a model mini Internet  where each bubble is a webpage and each arrow from A  B  C  and D represents a link on that webpage which takes you to one of the others  We're trying to build an expression that tells us  based on this network structure  which of these webpages is most relevant to the person who made the search  As such  we're going to use the concept of Procrastinating Pat who is an imaginary person who goes on the Internet and just randomly click links to avoid doing their work  By mapping all the possible links  we can build a model to estimate the amount of time we would expect Pat to spend on each webpage  We can describe the links present on page A as a vector  where each row is either a one or a zero based on whether there is a link to the corresponding page  And then normalise the vector by the total number of the links  such that they can be used to describe a probability for that page  For example  the vector of links from page A will be 0 1 1 1  Because vector A has links to sites B  to C  and to D  but it doesn't have a link to itself  Also  because there are three links in this page in total  we would normalize by a factor of a third  So the total click probability sums to one  So we can write  L_A = (0  a third  a third  a third)  Following the same logic  the link vectors in the next two sites are shown here  And finally  for page D  we can write L_D is going to equal  so D is connected to B and C  but not A  two sides in total  (0  a half  a half  0)  We can now build our link matrix L by using each of our linked vectors as a column  which you can see will form a square matrix  What we're trying to represent here with our matrix L is the probability of ending up on each of the pages  For example  the only way to get to A is by being at B  So you then need to know the probability of being at B  which you could've got to from either A or D  As you can see  this problem is self-referential  as the ranks on all the pages depend on all the others  Although we built our matrix from columns of outward links  we can see that the rows describe inward links normalized with respect to their page of origin  We can now write an expression which summarises the approach  We're going to use the vector r to store the rank of all webpages  To calculate the rank of page A  you need to know three things about all other pages on the Internet  What's your rank? Do you link to page A? And how many outgoing links do you have in total? The following expression combines these three pieces of information for webpage A only  So r_A is going to equal the sum from j = 1 to n  where n is all the webpages of the link matrix relevant to webpage A and location j  multiplied by the rank at location j  So this is going to scroll through each of our webpages  Which means that the rank of A is the sum of the ranks of all the pages which link to it  weighted by their specific link probability taken from matrix L  Now we want to be able to write this expression for all pages and solve them simultaneously  Thinking back to our linear algebra  we can rewrite the above expression applied to all webpages as a simple matrix multiplication  So r = Lr  Clearly  we start off not knowing r  So we simply assume that all the ranks are equally and normalise them by the total number of webpages in our analysis  which in this case is 4  So r equals a quarter  a quarter  a quarter  a quarter  Then  each time you multiply r by our matrix L  this gives us an updated value for r  So we can say that ri+1 is going to be L times ri  Applying this expression repeatedly means that we are solving this problem iteratively  Each time we do this  we update the values in r until  eventually  r stops changing  So now r really does equal Lr  Thinking back to the previous videos in this module  this implies that r is now an eigenvector of matrix L  with an eigenvalue of 1  At this point  you might well be thinking  if we want to multiply r by L many times  perhaps this will be best tackled by applying the diagonalization method that we saw in the last video  But don't forget  this would require us to already know all of the Eigen vectors  which is what we're trying to find in the first place  So now that we have an equation  and hopefully some idea of where it came from  we can ask our computer to iteratively apply it until it converges to find our rank vector  You can see that although it takes about ten iterations for the numbers to settle down  the order is already established after the first iteration  However  this is just an artifact of our system being so tiny  So now we have our result  which says that as Procrastinating Pat randomly clicks around our network  we'd expect them to spend about 40% of their time on page D  But only about 12% of their time on page A  with 24% on each of pages B and C  We now have our ranking  with D at the top and A at the bottom  and B and C equal in the middle  As it turns out  although there are many approaches for efficiently calculating eigenvectors that have been developed over the years  repeatedly multiplying a randomly selected initial guest vector by a matrix  which is called the power method  is still very effective for the page rank problem for two reasons  Firstly  although the power method will clearly only give you one eigenvector  when we know that there will be n for an n webpage system  it turns out that because of the way we've structured our link matrix  the vector it gives you will always be the one that you're looking for  with an eigenvalue of 1  Secondly  although this is not true for the full webpage mini Internet  when looking at the real Internet you can imagine that almost every entry in the link matrix will be zero  i e   most pages don't connect to most other pages  This is referred to as a sparse matrix  And algorithms exist such that multiplications can be performed very efficiently  One key aspect of the page rank algorithm that we haven't discussed so far is called the damping factor  d  This adds an additional term to our iterative formula  So r i + 1 is now going to equal d  times Lri + 1- d over n  d is something between 0 and 1  And you can think of it as 1 minus the probability with which procrastinating Pat suddenly  randomly types in a web address  rather than clicking on a link on his current page  The effect that this has on the actual calculation is about finding a compromise between speed and stability of the iterative convergence process  There are over one billion websites on the Internet today  compared with just a few million when the page rank algorithm was first published in 1998  And so the methods for search and ranking have had to evolve to maximize efficiency  although the core concept has remained unchanged for many years  This brings us to the end of our introduction to the page rank algorithm  There are  of course  many details which we didn't cover in this video  But I hope this has allowed you to come away with some insight and understanding into how the page rank works  and hopefully the confidence to apply this to some larger networks yourself  [MUSIC]