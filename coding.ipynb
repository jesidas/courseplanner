{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beautiful-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers.python.layers import fully_connected\n",
    "import numpy as np\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "from data_processing.siamese_data_train_test import WORD # load the data and process it\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "functioning-material",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "max_iter = 3500 # maximum number of iterations for training\n",
    "learning_rate = 0.0001 #0.001\n",
    "batch_train = 128 # batch size for training\n",
    "#batch_train = 120\n",
    "batch_test = 256 # batch size for testing\n",
    "#batch_test = 240\n",
    "display = 100 # display the training loss and accuracy every `display` step\n",
    "n_test = 500 # test the network every `n_test` step\n",
    "summaries_dir = './siamese_summary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dying-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture of the siamese network\n",
    "n_inputs = int(100) #100 # dimension of each of the input vectors\n",
    "#n_steps = 1 # sequence length\n",
    "n_steps = 1\n",
    "n_hidden = 512 # 64#128 #256 #128 # number of neurons of the bi-directional LSTM\n",
    "n_classes = 2 # two possible classes, either `same` of `different`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hairy-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.placeholder(tf.float32, shape=[None, n_steps, n_inputs]) # placeholder for the first network (concept 1)\n",
    "x2 = tf.placeholder(tf.float32, shape=[None, n_steps, n_inputs]) # placeholder for the second network (concept 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "transparent-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder for the label. `1` for `same` and `0` for `different`.\n",
    "y = tf.placeholder(tf.int64, shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "corporate-nylon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder for dropout (we could use different dropout for different part of the architecture)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "train_phase = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pleased-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_input(x_):\n",
    "    \"\"\"\n",
    "    Reshape the inputs to match the shape requirements of the function\n",
    "    `tf.nn.bidirectional_rnn`\n",
    "    \n",
    "    Args:\n",
    "        x_: a tensor of shape `(batch_size, n_steps, n_inputs)`\n",
    "        \n",
    "    Returns:\n",
    "        A `list` of length `n_steps` with its elements being tensors of shape `(batch_size, n_inputs)`\n",
    "    \"\"\"\n",
    "    x_ = tf.transpose(x_, [1, 0, 2]) # shape: (n_steps, batch_size, n_inputs)\n",
    "    x_ = tf.split(x_, n_steps, 0) #tensor flow > 0.12\n",
    "    #x_ = tf.split(0, n_steps, x_) # a list of `n_steps` tensors of shape (1, batch_size, n_steps)\n",
    "    return [tf.squeeze(z, [0]) for z in x_] # remove size 1 dimension --> (batch_size, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cosmetic-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fc(inputs, outdim, train_phase, scope_in):\n",
    "    fc =  fully_connected(inputs, outdim, activation_fn=None, scope=scope_in + '/fc')\n",
    "    fc_bnorm = tf.layers.batch_normalization(fc, momentum=0.1, epsilon=1e-5,\n",
    "                         training=train_phase, name=scope_in + '/bnorm')\n",
    "    fc_relu = tf.nn.relu(fc_bnorm, name=scope_in + '/relu')\n",
    "    fc_out = tf.layers.dropout(fc_relu, rate= 0.1, seed=0, training=train_phase, name=scope_in + '/dropout')\n",
    "    return fc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "answering-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_model(feats, train_phase, scope_name,\n",
    "                    fc_dim = n_inputs, embed_dim = n_hidden):\n",
    "    \"\"\"\n",
    "        Build two-branch embedding networks.\n",
    "        fc_dim: the output dimension of the first fc layer.\n",
    "        embed_dim: the output dimension of the second fc layer, i.e.\n",
    "                   embedding space dimension.\n",
    "    \"\"\"\n",
    "    # each branch.\n",
    "    fc1 = add_fc(feats, fc_dim, train_phase, scope_name)\n",
    "    fc2 = fully_connected(fc1, embed_dim, activation_fn=None,\n",
    "                               scope = scope_name + '_2')\n",
    "    embed = tf.nn.l2_normalize(fc2, 1, epsilon=1e-10)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "later-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_, x2_ = reshape_input(x1), reshape_input(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "advised-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('siamese_network') as scope:\n",
    "#     with tf.name_scope('Embed_1'):\n",
    "        embed_1 = embedding_model(x1_, train_phase, 'Embed')\n",
    "#     with tf.name_scope('Embed_2'):\n",
    "        reuse=True\n",
    "        scope.reuse_variables() # tied weights (reuse the weights)\n",
    "        embed_2 = embedding_model(x2_, train_phase, 'Embed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "given-measurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and biases for the layer that connects the outputs from the two networks\n",
    "weights = tf.get_variable('weigths_out', shape=[n_hidden, n_classes],\n",
    "                initializer=tf.random_normal_initializer(stddev=1.0/float(n_hidden)))\n",
    "biases = tf.get_variable('biases_out', shape=[n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "intelligent-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "#last_state1 = tf.squeeze(embed_1)\n",
    "#last_state2 = tf.squeeze(embed_2)\n",
    "last_states_diff = tf.squeeze(tf.abs(embed_1 - embed_2), [0])\n",
    "logits = tf.matmul(last_states_diff, weights) + biases\n",
    "\n",
    "prediction = tf.nn.log_softmax(logits=logits)\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "specific-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(tf.reduce_mean(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "solid-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct_pred = tf.equal(tf.argmax(logits, 1), y) \n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), y) \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "temporal-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some tensor board stuff\n",
    "with tf.name_scope('total'):\n",
    "    cross_entropy = tf.reduce_mean(loss)\n",
    "tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(summaries_dir + '/train')\n",
    "test_writer = tf.summary.FileWriter(summaries_dir + '/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "quality-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_from_logits(y):\n",
    "    n = len(y)\n",
    "    count = 0\n",
    "    for i in range(n):\n",
    "        if y[i][2] == np.argmax(y[i][3]):\n",
    "            count = count + 1\n",
    "    return float(count)/float(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "expressed-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall_from_logits(y):\n",
    "    n = len(y)\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        if y[i][2] == np.argmax(y[i][3]):\n",
    "            count = count + y[i][2] \n",
    "        if y[i][2] == 1:\n",
    "            total = total + 1\n",
    "    return float(count)/float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "forty-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 5 \n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "dataset_name = \"NPTEL MOOC Dataset\" #'University Course Dataset'\n",
    "a= [''] #range(100, 900, 100)  #['']*50 #range(100, 1000, 100) #(100, 900, 100)\n",
    "p50 = np.zeros(len(a))\n",
    "p100 = np.zeros(len(a))\n",
    "prec = 0.0 #np.zeros(len(a))\n",
    "recall = 0.0 #np.zeros(len(a))\n",
    "Fmeasure = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "subjective-foster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size in bow  190\n",
      "209  elements removed from train set\n",
      "121  elements removed from test set\n",
      "total number of training examples =  1711\n",
      "total number of positive sample in train data =  26\n",
      "total number of positive sample in test data =  25\n",
      "../datasets/NPTEL MOOC Dataset/k100_a0.01/final.beta\n",
      "Network training begins.\n",
      "step 100, training loss: 0.21989, training accuracy: 0.945\n",
      "step 200, training loss: 0.12196, training accuracy: 0.977\n",
      "step 300, training loss: 0.11578, training accuracy: 0.977\n",
      "step 400, training loss: 0.15533, training accuracy: 0.961\n",
      "step 500, training loss: 0.08222, training accuracy: 0.984\n",
      "testing step 500, accuracy 0.988, AUC 0.596\n",
      "step 600, training loss: 0.07396, training accuracy: 0.984\n",
      "step 700, training loss: 0.09524, training accuracy: 0.977\n",
      "step 800, training loss: 0.04687, training accuracy: 0.992\n",
      "step 900, training loss: 0.09573, training accuracy: 0.977\n",
      "step 1000, training loss: 0.01958, training accuracy: 1.000\n",
      "testing step 1000, accuracy 0.965, AUC 0.616\n",
      "step 1100, training loss: 0.03458, training accuracy: 0.992\n",
      "step 1200, training loss: 0.10555, training accuracy: 0.977\n",
      "step 1300, training loss: 0.08625, training accuracy: 0.977\n",
      "step 1400, training loss: 0.03547, training accuracy: 0.992\n",
      "step 1500, training loss: 0.15051, training accuracy: 0.961\n",
      "testing step 1500, accuracy 0.988, AUC 0.590\n",
      "step 1600, training loss: 0.05534, training accuracy: 0.984\n",
      "step 1700, training loss: 0.08480, training accuracy: 0.977\n",
      "step 1800, training loss: 0.01166, training accuracy: 1.000\n",
      "step 1900, training loss: 0.11689, training accuracy: 0.969\n",
      "step 2000, training loss: 0.01144, training accuracy: 1.000\n",
      "testing step 2000, accuracy 0.977, AUC 0.826\n",
      "step 2100, training loss: 0.04361, training accuracy: 0.992\n",
      "step 2200, training loss: 0.18777, training accuracy: 0.945\n",
      "step 2300, training loss: 0.07156, training accuracy: 0.984\n",
      "step 2400, training loss: 0.09074, training accuracy: 0.977\n",
      "step 2500, training loss: 0.03514, training accuracy: 0.992\n",
      "testing step 2500, accuracy 0.984, AUC 0.702\n",
      "step 2600, training loss: 0.06164, training accuracy: 0.984\n",
      "step 2700, training loss: 0.10858, training accuracy: 0.969\n",
      "step 2800, training loss: 0.05801, training accuracy: 0.984\n",
      "step 2900, training loss: 0.06794, training accuracy: 0.984\n",
      "step 3000, training loss: 0.10906, training accuracy: 0.969\n",
      "testing step 3000, accuracy 0.977, AUC 0.719\n",
      "step 3100, training loss: 0.05836, training accuracy: 0.984\n",
      "step 3200, training loss: 0.08191, training accuracy: 0.977\n",
      "step 3300, training loss: 0.03772, training accuracy: 0.992\n",
      "step 3400, training loss: 0.05605, training accuracy: 0.984\n",
      "step 3500, training loss: 0.08332, training accuracy: 0.977\n",
      "testing step 3500, accuracy 0.996, AUC 0.396\n",
      "********************************\n",
      "Training finished.\n",
      "********************************\n",
      "Testing the network.\n",
      "Network accuracy 0.978\n",
      "********************************\n",
      "Number of Test samples :  1151\n",
      "Len test data 1151\n",
      "[(0, -1.9620225, 'database', 'calculus'), (0, -1.9620225, 'determinant', 'calculus'), (0, -1.9620225, 'dictionary', 'calculus'), (0, -1.9620225, 'probability', 'calculus'), (1, -1.9620225, 'statistics', 'calculus')]\n",
      "P@50 =  0.04\n",
      "P@100 =  0.02\n",
      "[(0, array([ 0.84470826, -0.96582395], dtype=float32), 'database', 'calculus'), (0, array([ 0.84470826, -0.96582395], dtype=float32), 'determinant', 'calculus'), (0, array([ 0.84470826, -0.96582395], dtype=float32), 'dictionary', 'calculus'), (0, array([ 0.84470826, -0.96582395], dtype=float32), 'probability', 'calculus'), (1, array([ 0.84470826, -0.96582395], dtype=float32), 'statistics', 'calculus')]\n",
      "vocabulary size in bow  190\n",
      "209  elements removed from train set\n",
      "121  elements removed from test set\n",
      "total number of training examples =  1750\n",
      "total number of positive sample in train data =  29\n",
      "total number of positive sample in test data =  22\n",
      "../datasets/NPTEL MOOC Dataset/k100_a0.01/final.beta\n",
      "Network training begins.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shira/opt/anaconda3/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:70: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "/Users/shira/opt/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100, training loss: 1.08152, training accuracy: 0.141\n",
      "step 200, training loss: 0.65882, training accuracy: 0.680\n",
      "step 300, training loss: 0.44194, training accuracy: 0.805\n",
      "step 400, training loss: 0.29817, training accuracy: 0.891\n",
      "step 500, training loss: 0.31116, training accuracy: 0.859\n",
      "testing step 500, accuracy 0.250, AUC 0.597\n",
      "step 600, training loss: 0.21356, training accuracy: 0.930\n",
      "step 700, training loss: 0.15087, training accuracy: 0.945\n",
      "step 800, training loss: 0.17163, training accuracy: 0.906\n",
      "step 900, training loss: 0.16178, training accuracy: 0.914\n",
      "step 1000, training loss: 0.11457, training accuracy: 0.953\n",
      "testing step 1000, accuracy 0.312, AUC 0.297\n",
      "step 1100, training loss: 0.12204, training accuracy: 0.953\n",
      "step 1200, training loss: 0.14001, training accuracy: 0.930\n",
      "step 1300, training loss: 0.07948, training accuracy: 0.969\n",
      "step 1400, training loss: 0.13219, training accuracy: 0.914\n",
      "step 1500, training loss: 0.09932, training accuracy: 0.945\n",
      "testing step 1500, accuracy 0.324, AUC 0.283\n",
      "step 1600, training loss: 0.10041, training accuracy: 0.961\n",
      "step 1700, training loss: 0.14309, training accuracy: 0.914\n",
      "step 1800, training loss: 0.10966, training accuracy: 0.922\n",
      "step 1900, training loss: 0.09297, training accuracy: 0.969\n",
      "step 2000, training loss: 0.14256, training accuracy: 0.930\n",
      "testing step 2000, accuracy 0.320, AUC 0.404\n",
      "step 2100, training loss: 0.05558, training accuracy: 0.977\n",
      "step 2200, training loss: 0.12535, training accuracy: 0.953\n",
      "step 2300, training loss: 0.12377, training accuracy: 0.914\n",
      "step 2400, training loss: 0.08986, training accuracy: 0.953\n",
      "step 2500, training loss: 0.08463, training accuracy: 0.977\n",
      "testing step 2500, accuracy 0.371, AUC 0.606\n",
      "step 2600, training loss: 0.07606, training accuracy: 0.953\n",
      "step 2700, training loss: 0.13988, training accuracy: 0.922\n",
      "step 2800, training loss: 0.09122, training accuracy: 0.977\n",
      "step 2900, training loss: 0.05856, training accuracy: 0.977\n",
      "step 3000, training loss: 0.08761, training accuracy: 0.992\n",
      "testing step 3000, accuracy 0.348, AUC 0.649\n",
      "step 3100, training loss: 0.06333, training accuracy: 0.984\n",
      "step 3200, training loss: 0.05588, training accuracy: 0.992\n",
      "step 3300, training loss: 0.08591, training accuracy: 0.984\n",
      "step 3400, training loss: 0.13248, training accuracy: 0.930\n",
      "step 3500, training loss: 0.06743, training accuracy: 0.953\n",
      "testing step 3500, accuracy 0.328, AUC 0.395\n",
      "********************************\n",
      "Training finished.\n",
      "********************************\n",
      "Testing the network.\n",
      "Network accuracy 0.020\n",
      "********************************\n",
      "Number of Test samples :  1112\n",
      "Len test data 1112\n",
      "[(0, -0.23253365, 'cosine', 'calculus'), (0, -0.23253365, 'dictionary', 'calculus'), (0, -0.23253365, 'integer', 'calculus'), (0, -0.23253365, 'optimization', 'calculus'), (1, -0.23253365, 'statistics', 'calculus')]\n",
      "P@50 =  0.04\n",
      "P@100 =  0.03\n",
      "[(0, array([-0.38800362,  0.95219815], dtype=float32), 'cosine', 'calculus'), (0, array([-0.38800362,  0.95219815], dtype=float32), 'dictionary', 'calculus'), (0, array([-0.38800362,  0.95219815], dtype=float32), 'integer', 'calculus'), (0, array([-0.38800362,  0.95219815], dtype=float32), 'optimization', 'calculus'), (1, array([-0.38800362,  0.95219815], dtype=float32), 'statistics', 'calculus')]\n",
      "vocabulary size in bow  190\n",
      "198  elements removed from train set\n",
      "132  elements removed from test set\n",
      "total number of training examples =  1680\n",
      "total number of positive sample in train data =  32\n",
      "total number of positive sample in test data =  19\n",
      "../datasets/NPTEL MOOC Dataset/k100_a0.01/final.beta\n",
      "Network training begins.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shira/opt/anaconda3/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:70: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100, training loss: 0.20162, training accuracy: 0.984\n",
      "step 200, training loss: 0.17705, training accuracy: 0.969\n",
      "step 300, training loss: 0.11527, training accuracy: 0.977\n",
      "step 400, training loss: 0.09478, training accuracy: 0.984\n",
      "step 500, training loss: 0.04918, training accuracy: 1.000\n",
      "testing step 500, accuracy 0.980, AUC 0.500\n",
      "step 600, training loss: 0.07849, training accuracy: 0.984\n",
      "step 700, training loss: 0.10417, training accuracy: 0.977\n",
      "step 800, training loss: 0.04263, training accuracy: 0.992\n",
      "step 900, training loss: 0.04551, training accuracy: 0.992\n",
      "step 1000, training loss: 0.11971, training accuracy: 0.969\n",
      "testing step 1000, accuracy 0.969, AUC 0.589\n",
      "step 1100, training loss: 0.08309, training accuracy: 0.984\n",
      "step 1200, training loss: 0.12050, training accuracy: 0.969\n",
      "step 1300, training loss: 0.04032, training accuracy: 0.992\n",
      "step 1400, training loss: 0.04469, training accuracy: 0.992\n",
      "step 1500, training loss: 0.12089, training accuracy: 0.969\n",
      "testing step 1500, accuracy 0.977, AUC 0.588\n",
      "step 1600, training loss: 0.09529, training accuracy: 0.977\n",
      "step 1700, training loss: 0.14264, training accuracy: 0.961\n",
      "step 1800, training loss: 0.07307, training accuracy: 0.984\n",
      "step 1900, training loss: 0.04324, training accuracy: 0.992\n",
      "step 2000, training loss: 0.04080, training accuracy: 0.992\n",
      "testing step 2000, accuracy 0.961, AUC 0.594\n",
      "step 2100, training loss: 0.05360, training accuracy: 0.992\n",
      "step 2200, training loss: 0.17854, training accuracy: 0.945\n",
      "step 2300, training loss: 0.07670, training accuracy: 0.977\n",
      "step 2400, training loss: 0.12260, training accuracy: 0.969\n",
      "step 2500, training loss: 0.13681, training accuracy: 0.953\n",
      "testing step 2500, accuracy 0.988, AUC 0.599\n",
      "step 2600, training loss: 0.06973, training accuracy: 0.977\n",
      "step 2700, training loss: 0.06963, training accuracy: 0.984\n",
      "step 2800, training loss: 0.04793, training accuracy: 0.992\n",
      "step 2900, training loss: 0.10826, training accuracy: 0.969\n",
      "step 3000, training loss: 0.03361, training accuracy: 0.992\n",
      "testing step 3000, accuracy 0.969, AUC 0.572\n",
      "step 3100, training loss: 0.01659, training accuracy: 1.000\n",
      "step 3200, training loss: 0.09653, training accuracy: 0.977\n",
      "step 3300, training loss: 0.03806, training accuracy: 0.992\n",
      "step 3400, training loss: 0.06817, training accuracy: 0.984\n",
      "step 3500, training loss: 0.05925, training accuracy: 0.984\n",
      "testing step 3500, accuracy 0.984, AUC 0.740\n",
      "********************************\n",
      "Training finished.\n",
      "********************************\n",
      "Testing the network.\n",
      "Network accuracy 0.984\n",
      "********************************\n",
      "Number of Test samples :  1182\n",
      "Len test data 1182\n",
      "[(0, -1.38678, 'database', 'calculus'), (0, -1.38678, 'determinant', 'calculus'), (0, -1.38678, 'dictionary', 'calculus'), (0, -1.38678, 'exponent', 'calculus'), (0, -1.38678, 'probability', 'calculus')]\n",
      "P@50 =  0.02\n",
      "P@100 =  0.01\n",
      "[(0, array([-0.06490164, -1.1641614 ], dtype=float32), 'database', 'calculus'), (0, array([-0.06490164, -1.1641614 ], dtype=float32), 'determinant', 'calculus'), (0, array([-0.06490164, -1.1641614 ], dtype=float32), 'dictionary', 'calculus'), (0, array([-0.06490164, -1.1641614 ], dtype=float32), 'exponent', 'calculus'), (0, array([-0.06490164, -1.1641614 ], dtype=float32), 'probability', 'calculus')]\n",
      "vocabulary size in bow  190\n",
      "194  elements removed from train set\n",
      "136  elements removed from test set\n",
      "total number of training examples =  1713\n",
      "total number of positive sample in train data =  34\n",
      "total number of positive sample in test data =  17\n",
      "../datasets/NPTEL MOOC Dataset/k100_a0.01/final.beta\n",
      "Network training begins.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shira/opt/anaconda3/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:70: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "/Users/shira/opt/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100, training loss: 0.65509, training accuracy: 0.586\n",
      "step 200, training loss: 0.46036, training accuracy: 0.766\n",
      "step 300, training loss: 0.30269, training accuracy: 0.891\n",
      "step 400, training loss: 0.21648, training accuracy: 0.938\n",
      "step 500, training loss: 0.19239, training accuracy: 0.953\n",
      "testing step 500, accuracy 0.754, AUC 0.625\n",
      "step 600, training loss: 0.14484, training accuracy: 0.977\n",
      "step 700, training loss: 0.10899, training accuracy: 0.961\n",
      "step 800, training loss: 0.19208, training accuracy: 0.961\n",
      "step 900, training loss: 0.09855, training accuracy: 0.984\n",
      "step 1000, training loss: 0.08467, training accuracy: 0.961\n",
      "testing step 1000, accuracy 0.812, AUC 0.596\n",
      "step 1100, training loss: 0.07286, training accuracy: 0.992\n",
      "step 1200, training loss: 0.09472, training accuracy: 0.984\n",
      "step 1300, training loss: 0.08716, training accuracy: 0.984\n",
      "step 1400, training loss: 0.10541, training accuracy: 0.984\n",
      "step 1500, training loss: 0.08693, training accuracy: 0.984\n",
      "testing step 1500, accuracy 0.855, AUC 0.877\n",
      "step 1600, training loss: 0.14671, training accuracy: 0.969\n",
      "step 1700, training loss: 0.09833, training accuracy: 0.984\n",
      "step 1800, training loss: 0.10493, training accuracy: 0.969\n",
      "step 1900, training loss: 0.12998, training accuracy: 0.961\n",
      "step 2000, training loss: 0.15369, training accuracy: 0.953\n",
      "testing step 2000, accuracy 0.824, AUC 0.598\n",
      "step 2100, training loss: 0.05003, training accuracy: 0.992\n",
      "step 2200, training loss: 0.05049, training accuracy: 0.992\n",
      "step 2300, training loss: 0.05852, training accuracy: 0.992\n",
      "step 2400, training loss: 0.03018, training accuracy: 1.000\n",
      "step 2500, training loss: 0.07415, training accuracy: 0.984\n",
      "testing step 2500, accuracy 0.781, AUC 0.577\n",
      "step 2600, training loss: 0.04005, training accuracy: 1.000\n",
      "step 2700, training loss: 0.12776, training accuracy: 0.961\n",
      "step 2800, training loss: 0.10748, training accuracy: 0.969\n",
      "step 2900, training loss: 0.03074, training accuracy: 0.992\n",
      "step 3000, training loss: 0.07800, training accuracy: 0.984\n",
      "testing step 3000, accuracy 0.828, AUC 0.786\n",
      "step 3100, training loss: 0.09882, training accuracy: 0.977\n",
      "step 3200, training loss: 0.13405, training accuracy: 0.953\n",
      "step 3300, training loss: 0.04115, training accuracy: 0.992\n",
      "step 3400, training loss: 0.04386, training accuracy: 0.992\n",
      "step 3500, training loss: 0.11230, training accuracy: 0.969\n",
      "testing step 3500, accuracy 0.820, AUC 0.563\n",
      "********************************\n",
      "Training finished.\n",
      "********************************\n",
      "Testing the network.\n",
      "Network accuracy 0.547\n",
      "********************************\n",
      "Number of Test samples :  1149\n",
      "Len test data 1149\n",
      "[(0, -0.4520298, 'cosine', 'calculus'), (0, -0.4520298, 'determinant', 'calculus'), (0, -0.4520298, 'dictionary', 'calculus'), (1, -0.4520298, 'statistics', 'calculus'), (0, -0.4520298, 'dictionary', 'cosine')]\n",
      "P@50 =  0.06\n",
      "P@100 =  0.04\n",
      "[(0, array([-0.15022172,  0.4092712 ], dtype=float32), 'cosine', 'calculus'), (0, array([-0.15022172,  0.4092712 ], dtype=float32), 'determinant', 'calculus'), (0, array([-0.15022172,  0.4092712 ], dtype=float32), 'dictionary', 'calculus'), (1, array([-0.15022172,  0.4092712 ], dtype=float32), 'statistics', 'calculus'), (0, array([-0.15022172,  0.4092712 ], dtype=float32), 'dictionary', 'cosine')]\n",
      "vocabulary size in bow  190\n",
      "206  elements removed from train set\n",
      "124  elements removed from test set\n",
      "total number of training examples =  1731\n",
      "total number of positive sample in train data =  33\n",
      "total number of positive sample in test data =  18\n",
      "../datasets/NPTEL MOOC Dataset/k100_a0.01/final.beta\n",
      "Network training begins.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shira/opt/anaconda3/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:70: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100, training loss: 0.44668, training accuracy: 0.977\n",
      "step 200, training loss: 0.26910, training accuracy: 0.984\n",
      "step 300, training loss: 0.20683, training accuracy: 0.977\n",
      "step 400, training loss: 0.14655, training accuracy: 0.984\n",
      "step 500, training loss: 0.15084, training accuracy: 0.977\n",
      "testing step 500, accuracy 0.961, AUC 0.236\n",
      "step 600, training loss: 0.11084, training accuracy: 0.984\n",
      "step 700, training loss: 0.11803, training accuracy: 0.984\n",
      "step 800, training loss: 0.07972, training accuracy: 0.992\n",
      "step 900, training loss: 0.08564, training accuracy: 0.984\n",
      "step 1000, training loss: 0.09064, training accuracy: 0.984\n",
      "testing step 1000, accuracy 0.992, AUC 0.342\n",
      "step 1100, training loss: 0.06272, training accuracy: 0.992\n",
      "step 1200, training loss: 0.05170, training accuracy: 1.000\n",
      "step 1300, training loss: 0.10993, training accuracy: 0.969\n",
      "step 1400, training loss: 0.08759, training accuracy: 0.984\n",
      "step 1500, training loss: 0.13295, training accuracy: 0.977\n",
      "testing step 1500, accuracy 0.980, AUC 0.657\n",
      "step 1600, training loss: 0.07866, training accuracy: 0.984\n",
      "step 1700, training loss: 0.07321, training accuracy: 0.992\n",
      "step 1800, training loss: 0.09147, training accuracy: 0.977\n",
      "step 1900, training loss: 0.09967, training accuracy: 0.977\n",
      "step 2000, training loss: 0.10131, training accuracy: 0.977\n",
      "testing step 2000, accuracy 0.988, AUC 0.563\n",
      "step 2100, training loss: 0.10880, training accuracy: 0.977\n",
      "step 2200, training loss: 0.06306, training accuracy: 0.984\n",
      "step 2300, training loss: 0.07067, training accuracy: 0.984\n",
      "step 2400, training loss: 0.07642, training accuracy: 0.984\n",
      "step 2500, training loss: 0.04222, training accuracy: 0.992\n",
      "testing step 2500, accuracy 0.980, AUC 0.709\n",
      "step 2600, training loss: 0.05450, training accuracy: 0.992\n",
      "step 2700, training loss: 0.08454, training accuracy: 0.977\n",
      "step 2800, training loss: 0.12736, training accuracy: 0.969\n",
      "step 2900, training loss: 0.05743, training accuracy: 0.992\n",
      "step 3000, training loss: 0.07404, training accuracy: 0.984\n",
      "testing step 3000, accuracy 0.984, AUC 0.567\n",
      "step 3100, training loss: 0.05968, training accuracy: 0.992\n",
      "step 3200, training loss: 0.13383, training accuracy: 0.961\n",
      "step 3300, training loss: 0.02135, training accuracy: 1.000\n",
      "step 3400, training loss: 0.08510, training accuracy: 0.977\n",
      "step 3500, training loss: 0.04017, training accuracy: 0.992\n",
      "testing step 3500, accuracy 0.973, AUC 0.498\n",
      "********************************\n",
      "Training finished.\n",
      "********************************\n",
      "Testing the network.\n",
      "Network accuracy 0.984\n",
      "********************************\n",
      "Number of Test samples :  1131\n",
      "Len test data 1131\n",
      "[(0, -0.7306376, 'dictionary', 'calculus'), (0, -0.7306376, 'exponent', 'calculus'), (1, -0.7306376, 'statistics', 'calculus'), (0, -0.7306376, 'integer', 'cosine'), (0, -0.7306376, 'calculus', 'database')]\n",
      "P@50 =  0.04\n",
      "P@100 =  0.04\n",
      "[(0, array([0.7196739, 0.6460479], dtype=float32), 'dictionary', 'calculus'), (0, array([0.7196739, 0.6460479], dtype=float32), 'exponent', 'calculus'), (1, array([0.7196739, 0.6460479], dtype=float32), 'statistics', 'calculus'), (0, array([0.7196739, 0.6460479], dtype=float32), 'integer', 'cosine'), (0, array([0.7196739, 0.6460479], dtype=float32), 'calculus', 'database')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shira/opt/anaconda3/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:70: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
      "/Users/shira/opt/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for n in range(n_runs):\n",
    "    prec_50 = []\n",
    "    prec_100 = []\n",
    "    pre = []\n",
    "    rec = []\n",
    "    fm = []\n",
    "    # load data\n",
    "    # np.random.seed(0)\n",
    "    # for n_preq in ['_preq100','_preq200','_preq300','_preq400','_preq500','_preq600','_preq700', '_preq800']:\n",
    "    #     data = WORD(dataset='eaai', pn=n_preq) # load the data\n",
    "    for n_preq in a:\n",
    "        # data = WORD(dataset=dataset_name, pn=n_preq, datapath='../ACL2017-dataset/Sample_edges_previousedge_link/k100_a0.01_preq_')\n",
    "        data = WORD(n_inputs, n, dataset=dataset_name, pn=n_preq, datapath='../datasets/NPTEL MOOC Dataset/k100_a0.01')\n",
    "        # examples_n = 10 # display some images\n",
    "        # indexes = np.random.choice(range(len(data.y)), examples_n, replace=False)\n",
    "        # for i in range(examples_n):\n",
    "        #     u, v = data.x[indexes[i]]\n",
    "        #     if data.y[indexes[i]] == 1:\n",
    "        #         print data.vocab[u], '-->', data.vocab[v]\n",
    "        #     else:\n",
    "        #         print data.vocab[u], '-x->', data.vocab[v]\n",
    "        print (\"total number of training examples = \", data.n_train)\n",
    "        print (\"total number of positive sample in train data = \", sum(data.y_train))\n",
    "        print (\"total number of positive sample in test data = \", sum(data.y_test))\n",
    "        print (data.data_path)\n",
    "\n",
    "        #Train the netowrk\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init) # initialize all variables\n",
    "            print('Network training begins.')\n",
    "            for i in range(1, max_iter + 1):\n",
    "                # We retrieve a batch of data from the training set\n",
    "                batch_x1, batch_x2, batch_y, batch_cs, batch_ct = data.get_next_batch(batch_train, phase='train')\n",
    "                # We feed the data to the network for training\n",
    "                feed_dict = {x1: batch_x1, x2: batch_x2, y: batch_y, keep_prob: .9, train_phase:True}\n",
    "                _, loss_, accuracy_, summary = sess.run([optimizer, loss, accuracy, merged], feed_dict=feed_dict)\n",
    "                \n",
    "                if i % display == 0:\n",
    "                    print('step %i, training loss: %.5f, training accuracy: %.3f' % (i, np.mean(loss_), accuracy_))\n",
    "                train_writer.add_summary(summary, i)\n",
    "                # Testing the network\n",
    "                if i % n_test == 0:\n",
    "                    # Retrieving data from the test set\n",
    "                    batch_x1, batch_x2, batch_y, batch_cs, batch_ct = data.get_next_batch(batch_test, phase='test')\n",
    "                    feed_dict = {x1: batch_x1, x2: batch_x2, y: batch_y, keep_prob: 1.0, train_phase:False}\n",
    "                    accuracy_test, pred_test, logit_test, summary = sess.run([accuracy, prediction, logits, merged], feed_dict=feed_dict)\n",
    "                    pred_prob = [p[1] for p in pred_test]\n",
    "                    if any(np.isnan(pred_prob)):\n",
    "                        print(\"nan pred prob for 1\", logit_test)\n",
    "                    fpr, tpr, thresholds = roc_curve(batch_y, pred_prob)\n",
    "                    roc_auc = auc(fpr, tpr)\n",
    "                    print('testing step %i, accuracy %.3f, AUC %.3f' % (i, accuracy_test, roc_auc))\n",
    "                    test_writer.add_summary(summary, i)\n",
    "            print('********************************')\n",
    "            print('Training finished.')   \n",
    "            # testing the trained network on a large sample\n",
    "            batch_x1, batch_x2, batch_y, batch_cs, batch_ct = data.get_next_batch(data.n_test, phase='test', one_shot=True)\n",
    "            feed_dict = {x1: batch_x1, x2: batch_x2, y: batch_y, keep_prob:1.0, train_phase:False}\n",
    "            accuracy_test, pred_test, logits_test, loss_test = sess.run([accuracy, prediction, logits, loss], feed_dict=feed_dict)\n",
    "            print('********************************')\n",
    "            print('Testing the network.')\n",
    "            print('Network accuracy %.3f' % (accuracy_test))\n",
    "            print('********************************')\n",
    "            print('Number of Test samples : ', len(batch_y))\n",
    "            pred_prob = [p[1] for p in pred_test]\n",
    "\n",
    "            fpr, tpr, thresholds = roc_curve(batch_y, pred_prob)\n",
    "            tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "            tprs[-1][0] = 0.0\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            aucs.append(roc_auc)\n",
    "\n",
    "            test_data = sorted(zip(batch_x1, batch_x2, batch_y, pred_prob, batch_cs, batch_ct), key=lambda x:x[3], reverse=True)\n",
    "            print('Len test data', len(test_data))\n",
    "            print([(x[2], x[3], x[4], x[5]) for x in test_data[:5]])\n",
    "            pat50 = sum([x[2] for x in test_data[:50]])/float(50)\n",
    "            pat100 = sum([x[2] for x in test_data[:100]])/float(100)\n",
    "            print(\"P@50 = \", pat50)\n",
    "            print(\"P@100 = \", pat100)\n",
    "            # for x in test_data[:100]:\n",
    "            #     print x[2], x[3]\n",
    "            test_data = sorted(zip(batch_x1, batch_x2, batch_y, logits_test, batch_cs, batch_ct), key=lambda x:x[3][1], reverse=True)\n",
    "            print([(x[2], x[3], x[4], x[5]) for x in test_data[:5]])\n",
    "            #pat50 = calculate_precision_from_logits(test_data[:50])\n",
    "            #pat100 = calculate_precision_from_logits(test_data[:100])\n",
    "            #print \"P@50 = \", pat50\n",
    "            #print \"P@100 = \", pat100\n",
    "            #pr = calculate_precision_from_logits(test_data)\n",
    "            #re = calculate_recall_from_logits(test_data)\n",
    "            #P = metrics.precision_score(batch_y, np.argmax(logits_test, 1))\n",
    "            #R = metrics.recall_score(batch_y, np.argmax(logits_test, 1))\n",
    "            #F1 = metrics.f1_score(batch_y, np.argmax(logits_test, 1))\n",
    "            P = metrics.precision_score(batch_y, np.argmax(pred_test,1))\n",
    "            R = metrics.recall_score(batch_y, np.argmax(pred_test,1))\n",
    "            F1 = metrics.f1_score(batch_y, np.argmax(pred_test,1))\n",
    "            pre.append(P)\n",
    "            rec.append(R)\n",
    "            fm.append(F1)\n",
    "\n",
    "            tf.summary.scalar(\"P_50\", pat50)\n",
    "            tf.summary.scalar(\"P_100\", pat100)\n",
    "            prec_50.append(pat50)\n",
    "            prec_100.append(pat100)\n",
    "\n",
    "    p50 = p50 + np.array(prec_50)\n",
    "    p100 = p100 + np.array(prec_100)\n",
    "    max_ind = np.argmax(np.array(fm))\n",
    "    prec = prec + np.array(pre[max_ind])\n",
    "    recall = recall + np.array(rec[max_ind])\n",
    "    Fmeasure = Fmeasure + np.array(fm[max_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "viral-surge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shira/opt/anaconda3/envs/python36/lib/python3.6/site-packages/matplotlib/figure.py:445: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  % get_backend())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfig = plt.figure()\\nplt.plot(a, p50, \\'-b\\', label=\\'Pairwise LDA+Siamese\\')\\nplt.title(\"Precision@50\")\\nfig.savefig(\"Precision_\"+dataset_name+\"small@50.png\")\\nfig = plt.figure()\\nplt.plot(a, p100, \\'-b\\', label=\\'Pairwise LDA+Siamese\\')\\nplt.title(\"Precision@100\")\\nfig.savefig(\"Precision_\"+dataset_name+\"_small@100.png\")\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fmeasure = Fmeasure/n_runs\n",
    "p50 = p50/n_runs\n",
    "p100 = p100/n_runs\n",
    "prec = prec/n_runs\n",
    "recall = recall/n_runs\n",
    "file = open(\"fc_relu_auc_precision_\"+dataset_name+\".txt\", 'a')\n",
    "file.write(\"../datasets/NPTEL MOOC Dataset/k100_a0.01\" + '  max_iter =' + str(max_iter) + ' learning rate =' + str(learning_rate) + ' n_hidden = '+ str(n_hidden) +'\\n') #'  decay_step ='+str(decay_step)+ '  decay_rate =' + str(decay_rate) +'\\n')\n",
    "file.write('Precision@50 : '+ str(p50)+'\\n')\n",
    "file.write('Precision@100 : '+ str(p100)+'\\n')\n",
    "file.write('Precision : '+ str(prec)+'\\n')\n",
    "file.write('Recall : '+ str(recall)+'\\n')\n",
    "file.write('F-score : '+ str(Fmeasure)+'\\n')\n",
    "file.write('AUC over iteration ; ' + str(aucs) + '\\n')\n",
    "fig = plt.figure()\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "\n",
    "file.write('mean false positive rate :'+ str(mean_fpr) + '\\n')\n",
    "file.write('mean true positive rate :'+ str(mean_tpr) + '\\n')\n",
    "file.write('mean AUC : '+ str(mean_auc) + '\\n')\n",
    "file.close()\n",
    "\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC on ' + dataset_name)\n",
    "plt.legend(loc=\"lower right\")\n",
    "fig.savefig('fc_relu_ROC_on_' + dataset_name + '.png')\n",
    "plt.show()\n",
    "'''\n",
    "fig = plt.figure()\n",
    "plt.plot(a, p50, '-b', label='Pairwise LDA+Siamese')\n",
    "plt.title(\"Precision@50\")\n",
    "fig.savefig(\"Precision_\"+dataset_name+\"small@50.png\")\n",
    "fig = plt.figure()\n",
    "plt.plot(a, p100, '-b', label='Pairwise LDA+Siamese')\n",
    "plt.title(\"Precision@100\")\n",
    "fig.savefig(\"Precision_\"+dataset_name+\"_small@100.png\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
